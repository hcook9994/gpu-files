{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ea1574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c86576f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14a0de31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_imdb = pd.read_csv(\"imdb_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5880a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_imdb=df_imdb.set_index(\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93668f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aa3ceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_imdb['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b3818ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df_imdb.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abf8b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69c8821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbers=list(df['sentiment'].unique())\n",
    "# list_zeros = [0]*len(numbers)\n",
    "# count_dictionary = dict(zip(numbers, list_zeros))\n",
    "\n",
    "# values_array_train=[]\n",
    "# values_array_test=[]\n",
    "# values_array_unlabelled=[]\n",
    "# for index, row in df.iterrows():\n",
    "#     if count_dictionary[row['sentiment']]<20:\n",
    "#         count_dictionary[row['sentiment']]=count_dictionary[row['sentiment']]+1\n",
    "#         values_array_train.append((row['text'],row['sentiment']))\n",
    "#     elif count_dictionary[row['sentiment']]<60:\n",
    "#         count_dictionary[row['sentiment']]=count_dictionary[row['sentiment']]+1\n",
    "#         values_array_test.append((row['text'],row['sentiment']))\n",
    "#     elif count_dictionary[row['sentiment']]<600:\n",
    "#         count_dictionary[row['sentiment']]=count_dictionary[row['sentiment']]+1\n",
    "#         values_array_unlabelled.append((row['text'],'UNK'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6c0ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_l=pd.read_csv(\"assigned/train_l.csv\", index_col=\"Unnamed: 0\")\n",
    "df_test_l=pd.read_csv(\"assigned/test_l.csv\", index_col=\"Unnamed: 0\")\n",
    "df_u=pd.read_csv(\"assigned/u.csv\", index_col=\"Unnamed: 0\")\n",
    "df_train_u=pd.read_csv(\"assigned/train_u.csv\", index_col=\"Unnamed: 0\")\n",
    "df_test_u=pd.read_csv(\"assigned/test_u.csv\", index_col=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ac3b2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_array_train=list(df_train_l.to_records(index=False))\n",
    "values_array_test=list(df_test_l.to_records(index=False))\n",
    "values_array_unlabelled=df_u.to_records(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "804df80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "print(len(values_array_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15a0c04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "print(len(values_array_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f64668b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2120\n"
     ]
    }
   ],
   "source": [
    "print(len(values_array_unlabelled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faaf4b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('For a mature man, to admit that he shed a tear over this film is a mature response, to a mature film.  If one need admit more then perhaps one could say that, \"Life\" can never be the same, after viewing such advent for it has moved us to the next level.  ', 0)\n"
     ]
    }
   ],
   "source": [
    "print(values_array_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c31b5578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Certainly any others I have seen pale in comparison. The series gives balanced coverage to all theatres of operation. No one country is given undue credit for the Allied victory. Laurence Olivier brings great weight and dignity to his role as narrator.', 0)\n"
     ]
    }
   ],
   "source": [
    "print(values_array_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce423411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bizarre horror movie filled with famous faces but stolen by Cristina Raines (later of TV\\'s \"Flamingo Road\") as a pretty but somewhat unstable model with a gummy smile who is slated to pay for her attempted suicides by guarding the Gateway to Hell! The scenes with Raines modeling are very well captured, the mood music is perfect, Deborah Raffin is charming as Cristina\\'s pal, but when Raines moves into a creepy Brooklyn Heights brownstone (inhabited by a blind priest on the top floor), things really start cooking. The neighbors, including a fantastically wicked Burgess Meredith and kinky couple Sylvia Miles & Beverly D\\'Angelo, are a diabolical lot, and Eli Wallach is great fun as a wily police detective. The movie is nearly a cross-pollination of \"Rosemary\\'s Baby\" and \"The Exorcist\"--but what a combination! Based on the best-seller by Jeffrey Konvitz, \"The Sentinel\" is entertainingly spooky, full of shocks brought off well by director Michael Winner, who mounts a thoughtfully downbeat ending with skill. ***1/2 from ****', 'UNK')\n"
     ]
    }
   ],
   "source": [
    "print(values_array_unlabelled[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37916145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==4.3.2\n",
    "import torch\n",
    "import io\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "from transformers import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "#!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "#!pip install sentencepiece\n",
    "\n",
    "##Set random values\n",
    "#seed_val = 42\n",
    "seed_val = 27\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d415d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88a04fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "#  Transformer parameters\n",
    "#--------------------------------\n",
    "max_seq_length = 64\n",
    "batch_size = 64\n",
    "\n",
    "#--------------------------------\n",
    "#  GAN-BERT specific parameters\n",
    "#--------------------------------\n",
    "# number of hidden layers in the generator, \n",
    "# each of the size of the output space\n",
    "#num_hidden_layers_g = 1; \n",
    "# number of hidden layers in the discriminator, \n",
    "# each of the size of the input space\n",
    "num_hidden_layers_d = 1; \n",
    "# size of the generator's input noisy vectors\n",
    "noise_size = 100\n",
    "# dropout to be applied to discriminator's input vectors\n",
    "out_dropout_rate = 0.2\n",
    "\n",
    "# Replicate labeled data to balance poorly represented datasets, \n",
    "# e.g., less than 1% of labeled material\n",
    "apply_balance = True\n",
    "\n",
    "#--------------------------------\n",
    "#  Optimization parameters\n",
    "#--------------------------------\n",
    "learning_rate_discriminator = 5e-5\n",
    "#learning_rate_generator = 5e-5\n",
    "epsilon = 1e-8\n",
    "num_train_epochs = 400\n",
    "multi_gpu = True\n",
    "# Scheduler\n",
    "apply_scheduler = False\n",
    "warmup_proportion = 0.1\n",
    "# Print\n",
    "print_each_n_step = 10\n",
    "\n",
    "#--------------------------------\n",
    "#  Adopted Tranformer model\n",
    "#--------------------------------\n",
    "# Since this version is compatible with Huggingface transformers, you can uncomment\n",
    "# (or add) transformer models compatible with GAN\n",
    "\n",
    "#model_name = \"bert-base-cased\"\n",
    "#model_name = \"bert-base-uncased\"\n",
    "#model_name = \"roberta-base\"\n",
    "#model_name = \"albert-base-v2\"\n",
    "#model_name = \"xlm-roberta-base\"\n",
    "#model_name = \"amazon/bort\"\n",
    "#model_name=\"google/electra-large-discriminator\"\n",
    "#model_name=\"google/electra-small-discriminator\"\n",
    "#model_name=\"microsoft/deberta-v2-xxlarge\"\n",
    "#model_name=\"microsoft/deberta-v3-base\"\n",
    "model_name = \"google/electra-base-discriminator\"\n",
    "\n",
    "#--------------------------------\n",
    "#  Retrieve the TREC QC Dataset\n",
    "#--------------------------------\n",
    "#! git clone https://github.com/crux82/ganbert\n",
    "\n",
    "#  NOTE: in this setting 50 classes are involved\n",
    "# labeled_file = \"./ganbert/data/labeled.tsv\"\n",
    "# unlabeled_file = \"./ganbert/data/unlabeled.tsv\"\n",
    "# test_filename = \"./ganbert/data/test.tsv\"\n",
    "\n",
    "label_list = ['UNK',0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c908c079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/electra-base-discriminator/resolve/main/config.json from cache at /Users/harrison.cook/.cache/huggingface/transformers/7d1569a4df2372d67341bda716bce4e3edf3e3ffadb97251bc4b6b35d459f624.57c13443a51769ce892714c93bb3ee3952bad66d7d9662d9de382b808377c3f8\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/electra-base-discriminator/resolve/main/pytorch_model.bin from cache at /Users/harrison.cook/.cache/huggingface/transformers/aed576b8aec823c870feda40d60bd803ac8e40056ecb7d7f43dd0b2bfd82e373.db390a2059e53ead2bb00e1a2f8cd50b0a47e1969d180cd70339ec3f6f29dce1\n",
      "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at google/electra-base-discriminator.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/google/electra-base-discriminator/resolve/main/config.json from cache at /Users/harrison.cook/.cache/huggingface/transformers/7d1569a4df2372d67341bda716bce4e3edf3e3ffadb97251bc4b6b35d459f624.57c13443a51769ce892714c93bb3ee3952bad66d7d9662d9de382b808377c3f8\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/google/electra-base-discriminator/resolve/main/vocab.txt from cache at /Users/harrison.cook/.cache/huggingface/transformers/fe616facc71d8e3afc69de3edac76bf1e4a0a741e80d9a99a2cc6a9a8f5f74b5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/google/electra-base-discriminator/resolve/main/tokenizer.json from cache at /Users/harrison.cook/.cache/huggingface/transformers/81840ac426bf0d690bfb69a4ec7d706e8853d8ab309e7decb6b72ab939d6682e.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/google/electra-base-discriminator/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/google/electra-base-discriminator/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/google/electra-base-discriminator/resolve/main/tokenizer_config.json from cache at /Users/harrison.cook/.cache/huggingface/transformers/6f8b3f5095b6f44f5c75cee3c56b971b3208b08132ba2f9fb775a4a7b7140942.4f2213f5603276adf12967b32e4444c0f187f34ca4f8b22a65f03e13514589e9\n",
      "loading configuration file https://huggingface.co/google/electra-base-discriminator/resolve/main/config.json from cache at /Users/harrison.cook/.cache/huggingface/transformers/7d1569a4df2372d67341bda716bce4e3edf3e3ffadb97251bc4b6b35d459f624.57c13443a51769ce892714c93bb3ee3952bad66d7d9662d9de382b808377c3f8\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6383a296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the examples\n",
    "labeled_examples = values_array_train\n",
    "#unlabeled_examples = values_array_unlabelled\n",
    "test_examples = values_array_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10d4fa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_loader(input_examples, label_masks, label_map, do_shuffle = False, balance_label_examples = False):\n",
    "  '''\n",
    "  Generate a Dataloader given the input examples, eventually masked if they are \n",
    "  to be considered NOT labeled.\n",
    "  '''\n",
    "  examples = []\n",
    "\n",
    "  # Count the percentage of labeled examples  \n",
    "  num_labeled_examples = 0\n",
    "  for label_mask in label_masks:\n",
    "    if label_mask: \n",
    "      num_labeled_examples += 1\n",
    "  label_mask_rate = num_labeled_examples/len(input_examples)\n",
    "\n",
    "  # if required it applies the balance\n",
    "  for index, ex in enumerate(input_examples): \n",
    "    if label_mask_rate == 1 or not balance_label_examples:\n",
    "      examples.append((ex, label_masks[index]))\n",
    "    else:\n",
    "      # IT SIMULATE A LABELED EXAMPLE\n",
    "      if label_masks[index]:\n",
    "        balance = int(1/label_mask_rate)\n",
    "        balance = int(math.log(balance,2))\n",
    "        if balance < 1:\n",
    "          balance = 1\n",
    "        for b in range(0, int(balance)):\n",
    "          examples.append((ex, label_masks[index]))\n",
    "      else:\n",
    "        examples.append((ex, label_masks[index]))\n",
    "  \n",
    "  #-----------------------------------------------\n",
    "  # Generate input examples to the Transformer\n",
    "  #-----------------------------------------------\n",
    "  input_ids = []\n",
    "  input_mask_array = []\n",
    "  label_mask_array = []\n",
    "  label_id_array = []\n",
    "\n",
    "  # Tokenization \n",
    "  for (text, label_mask) in examples:\n",
    "    encoded_sent = tokenizer.encode(text[0], add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
    "    input_ids.append(encoded_sent)\n",
    "    label_id_array.append(label_map[text[1]])\n",
    "    label_mask_array.append(label_mask)\n",
    "  \n",
    "  # Attention to token (to ignore padded input wordpieces)\n",
    "  for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]                          \n",
    "    input_mask_array.append(att_mask)\n",
    "  # Convertion to Tensor\n",
    "  input_ids = torch.tensor(input_ids) \n",
    "  input_mask_array = torch.tensor(input_mask_array)\n",
    "  label_id_array = torch.tensor(label_id_array, dtype=torch.long)\n",
    "  label_mask_array = torch.tensor(label_mask_array)\n",
    "\n",
    "  # Building the TensorDataset\n",
    "  dataset = TensorDataset(input_ids, input_mask_array, label_id_array, label_mask_array)\n",
    "\n",
    "  if do_shuffle:\n",
    "    sampler = RandomSampler\n",
    "  else:\n",
    "    sampler = SequentialSampler\n",
    "\n",
    "  # Building the DataLoader\n",
    "  return DataLoader(\n",
    "              dataset,  # The training samples.\n",
    "              sampler = sampler(dataset), \n",
    "              batch_size = batch_size) # Trains with this batch size.\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56feab4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/11/86f3z1k93pq0q7qb52tszh980000gn/T/ipykernel_65671/1314345009.py:54: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  label_mask_array = torch.tensor(label_mask_array)\n"
     ]
    }
   ],
   "source": [
    "label_map = {}\n",
    "for (i, label) in enumerate(label_list):\n",
    "  label_map[label] = i\n",
    "#------------------------------\n",
    "#   Load the train dataset\n",
    "#------------------------------\n",
    "train_examples = labeled_examples\n",
    "#The labeled (train) dataset is assigned with a mask set to True\n",
    "train_label_masks = np.ones(len(labeled_examples), dtype=bool)\n",
    "#If unlabel examples are available\n",
    "# if unlabeled_examples:\n",
    "#   train_examples = train_examples + unlabeled_examples\n",
    "#   #The unlabeled (train) dataset is assigned with a mask set to False\n",
    "#   tmp_masks = np.zeros(len(unlabeled_examples), dtype=bool)\n",
    "#   train_label_masks = np.concatenate([train_label_masks,tmp_masks])\n",
    "\n",
    "train_dataloader = generate_data_loader(train_examples, train_label_masks, label_map, do_shuffle = True, balance_label_examples = apply_balance)\n",
    "\n",
    "#------------------------------\n",
    "#   Load the test dataset\n",
    "#------------------------------\n",
    "#The labeled (test) dataset is assigned with a mask set to True\n",
    "test_label_masks = np.ones(len(test_examples), dtype=bool)\n",
    "\n",
    "test_dataloader = generate_data_loader(test_examples, test_label_masks, label_map, do_shuffle = False, balance_label_examples = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cce6ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------\n",
    "#   The Generator as in \n",
    "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
    "#   https://github.com/crux82/ganbert\n",
    "#------------------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_size=100, output_size=512, hidden_sizes=[512], dropout_rate=0.1):\n",
    "        super(Generator, self).__init__()\n",
    "        layers = []\n",
    "        hidden_sizes = [noise_size] + hidden_sizes\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
    "\n",
    "        layers.append(nn.Linear(hidden_sizes[-1],output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, noise):\n",
    "        output_rep = self.layers(noise)\n",
    "        return output_rep\n",
    "\n",
    "#------------------------------\n",
    "#   The Discriminator\n",
    "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
    "#   https://github.com/crux82/ganbert\n",
    "#------------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
    "        layers = []\n",
    "        hidden_sizes = [input_size] + hidden_sizes\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
    "\n",
    "        self.layers = nn.Sequential(*layers) #per il flatten\n",
    "        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1) # +1 for the probability of this sample being fake/real.\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_rep):\n",
    "        input_rep = self.input_dropout(input_rep)\n",
    "        last_rep = self.layers(input_rep)\n",
    "        logits = self.logit(last_rep)\n",
    "        probs = self.softmax(logits)\n",
    "        return last_rep, logits, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f27045c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/electra-base-discriminator/resolve/main/config.json from cache at /Users/harrison.cook/.cache/huggingface/transformers/7d1569a4df2372d67341bda716bce4e3edf3e3ffadb97251bc4b6b35d459f624.57c13443a51769ce892714c93bb3ee3952bad66d7d9662d9de382b808377c3f8\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The config file is required to get the dimension of the vector produced by \n",
    "# the underlying transformer\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "hidden_size = int(config.hidden_size)\n",
    "# Define the number and width of hidden layers\n",
    "#hidden_levels_g = [hidden_size for i in range(0, num_hidden_layers_g)]\n",
    "hidden_levels_d = [hidden_size for i in range(0, num_hidden_layers_d)]\n",
    "\n",
    "#-------------------------------------------------\n",
    "#   Instantiate the Generator and Discriminator\n",
    "#-------------------------------------------------\n",
    "#generator = Generator(noise_size=noise_size, output_size=hidden_size, hidden_sizes=hidden_levels_g, dropout_rate=out_dropout_rate)\n",
    "discriminator = Discriminator(input_size=hidden_size, hidden_sizes=hidden_levels_d, num_labels=len(label_list), dropout_rate=out_dropout_rate)\n",
    "\n",
    "# Put everything in the GPU if available\n",
    "if torch.cuda.is_available():    \n",
    "  #generator.cuda()\n",
    "  discriminator.cuda()\n",
    "  transformer.cuda()\n",
    "  if multi_gpu:\n",
    "    transformer = torch.nn.DataParallel(transformer)\n",
    "\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3dcbc408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 1.371\n",
      "  Training epcoh took: 0:00:18\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.537\n",
      "  Test Loss: 0.983\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 2 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 1.227\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.650\n",
      "  Test Loss: 0.910\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 3 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 1.094\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.688\n",
      "  Test Loss: 0.845\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 4 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.974\n",
      "  Training epcoh took: 0:00:16\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.738\n",
      "  Test Loss: 0.791\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 5 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.908\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.787\n",
      "  Test Loss: 0.748\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 6 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.804\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.825\n",
      "  Test Loss: 0.710\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 7 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.731\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.863\n",
      "  Test Loss: 0.660\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 8 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.650\n",
      "  Training epcoh took: 0:00:16\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.875\n",
      "  Test Loss: 0.616\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 9 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.579\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.900\n",
      "  Test Loss: 0.573\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 10 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.524\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.900\n",
      "  Test Loss: 0.520\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 11 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.452\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.938\n",
      "  Test Loss: 0.440\n",
      "  Test took: 0:00:12\n",
      "\n",
      "======== Epoch 12 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.387\n",
      "  Training epcoh took: 0:00:16\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.938\n",
      "  Test Loss: 0.375\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 13 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.318\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.963\n",
      "  Test Loss: 0.326\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 14 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.268\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.963\n",
      "  Test Loss: 0.290\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 15 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.225\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.963\n",
      "  Test Loss: 0.269\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 16 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.190\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.963\n",
      "  Test Loss: 0.255\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 17 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.158\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.950\n",
      "  Test Loss: 0.240\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 18 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.132\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.938\n",
      "  Test Loss: 0.233\n",
      "  Test took: 0:00:10\n",
      "\n",
      "======== Epoch 19 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.115\n",
      "  Training epcoh took: 0:00:16\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.938\n",
      "  Test Loss: 0.220\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 20 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.101\n",
      "  Training epcoh took: 0:00:16\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.950\n",
      "  Test Loss: 0.203\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 21 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.079\n",
      "  Training epcoh took: 0:00:16\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.963\n",
      "  Test Loss: 0.184\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 22 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.072\n",
      "  Training epcoh took: 0:00:16\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.950\n",
      "  Test Loss: 0.163\n",
      "  Test took: 0:00:10\n",
      "\n",
      "======== Epoch 23 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.066\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.950\n",
      "  Test Loss: 0.142\n",
      "  Test took: 0:00:10\n",
      "\n",
      "======== Epoch 24 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.057\n",
      "  Training epcoh took: 0:00:16\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.950\n",
      "  Test Loss: 0.127\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 25 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.049\n",
      "  Training epcoh took: 0:00:16\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.950\n",
      "  Test Loss: 0.116\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 26 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.041\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.950\n",
      "  Test Loss: 0.110\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 27 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.039\n",
      "  Training epcoh took: 0:00:18\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.950\n",
      "  Test Loss: 0.106\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 28 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.038\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.950\n",
      "  Test Loss: 0.103\n",
      "  Test took: 0:00:12\n",
      "\n",
      "======== Epoch 29 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.032\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.963\n",
      "  Test Loss: 0.104\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 30 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.028\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.963\n",
      "  Test Loss: 0.111\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 31 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.027\n",
      "  Training epcoh took: 0:00:18\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.963\n",
      "  Test Loss: 0.125\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 32 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.024\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.963\n",
      "  Test Loss: 0.144\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 33 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.024\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.963\n",
      "  Test Loss: 0.165\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 34 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.021\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.963\n",
      "  Test Loss: 0.185\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 35 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.019\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.963\n",
      "  Test Loss: 0.201\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 36 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.017\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.950\n",
      "  Test Loss: 0.218\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 37 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.019\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.938\n",
      "  Test Loss: 0.233\n",
      "  Test took: 0:00:12\n",
      "\n",
      "======== Epoch 38 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.015\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.938\n",
      "  Test Loss: 0.245\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 39 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.016\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.938\n",
      "  Test Loss: 0.252\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 40 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.013\n",
      "  Training epcoh took: 0:00:18\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.938\n",
      "  Test Loss: 0.254\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 41 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.014\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.938\n",
      "  Test Loss: 0.249\n",
      "  Test took: 0:00:12\n",
      "\n",
      "======== Epoch 42 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.012\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.938\n",
      "  Test Loss: 0.242\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 43 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.012\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.938\n",
      "  Test Loss: 0.233\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 44 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.012\n",
      "  Training epcoh took: 0:00:18\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.963\n",
      "  Test Loss: 0.222\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 45 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.010\n",
      "  Training epcoh took: 0:00:18\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.963\n",
      "  Test Loss: 0.212\n",
      "  Test took: 0:00:13\n",
      "\n",
      "======== Epoch 46 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.011\n",
      "  Training epcoh took: 0:00:18\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.963\n",
      "  Test Loss: 0.203\n",
      "  Test took: 0:00:12\n",
      "\n",
      "======== Epoch 47 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.011\n",
      "  Training epcoh took: 0:00:17\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.963\n",
      "  Test Loss: 0.195\n",
      "  Test took: 0:00:11\n",
      "\n",
      "======== Epoch 48 / 400 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss discriminator: 0.010\n",
      "  Training epcoh took: 0:00:18\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# Tell pytorch not to bother with constructing the compute graph during\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# the forward pass, since this is only needed for backprop (training).\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():        \n\u001b[0;32m--> 214\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_input_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m#hidden_states = model_outputs[-1]\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m model_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:,\u001b[38;5;241m0\u001b[39m,:]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/electra/modeling_electra.py:916\u001b[0m, in \u001b[0;36mElectraModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings_project\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    914\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_project(hidden_states)\n\u001b[0;32m--> 916\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/electra/modeling_electra.py:584\u001b[0m, in \u001b[0;36mElectraEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    575\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    576\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    577\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    581\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    582\u001b[0m     )\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/electra/modeling_electra.py:511\u001b[0m, in \u001b[0;36mElectraLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    508\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    509\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 511\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    516\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/modeling_utils.py:2928\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   2925\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m   2926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m-> 2928\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/electra/modeling_electra.py:524\u001b[0m, in \u001b[0;36mElectraLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    523\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 524\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/electra/modeling_electra.py:436\u001b[0m, in \u001b[0;36mElectraOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 436\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    438\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_stats = []\n",
    "\n",
    "accuracy_array=[]\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "#models parameters\n",
    "transformer_vars = [i for i in transformer.parameters()]\n",
    "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
    "#g_vars = [v for v in generator.parameters()]\n",
    "\n",
    "#optimizer\n",
    "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
    "#gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator) \n",
    "\n",
    "#scheduler\n",
    "if apply_scheduler:\n",
    "  num_train_examples = len(train_examples)\n",
    "  num_train_steps = int(num_train_examples / batch_size * num_train_epochs)\n",
    "  num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
    "\n",
    "  scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n",
    "                                           num_warmup_steps = num_warmup_steps)\n",
    "#   scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n",
    "#                                            num_warmup_steps = num_warmup_steps)\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, num_train_epochs):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    # Perform one full pass over the training set.\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, num_train_epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    tr_g_loss = 0\n",
    "    tr_d_loss = 0\n",
    "\n",
    "    # Put the model into training mode.\n",
    "    transformer.train() \n",
    "    #generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every print_each_n_step batches.\n",
    "        if step % print_each_n_step == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        b_label_mask = batch[3].to(device)\n",
    "\n",
    "        real_batch_size = b_input_ids.shape[0]\n",
    "     \n",
    "        # Encode real data in the Transformer\n",
    "        model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
    "        hidden_states = model_outputs.last_hidden_state[:,0,:]\n",
    "        #hidden_states = model_outputs[-1]\n",
    "        #print(hidden_states[0].size())\n",
    "        \n",
    "        # Generate fake data that should have the same distribution of the ones\n",
    "        # encoded by the transformer. \n",
    "        # First noisy input are used in input to the Generator\n",
    "        #noise = torch.zeros(real_batch_size, noise_size, device=device).uniform_(0, 1)\n",
    "        # Gnerate Fake data\n",
    "        #gen_rep = generator(noise)\n",
    "        #print(\"Length of generator output {}\".format(len(gen_rep)))\n",
    "        #print(\"Length of single generator output {}\".format(len(gen_rep[0])))\n",
    "\n",
    "        # Generate the output of the Discriminator for real and fake data.\n",
    "        # First, we put together the output of the tranformer and the generator\n",
    "        #disciminator_input = torch.cat([hidden_states, gen_rep], dim=0)\n",
    "        # Then, we select the output of the disciminator\n",
    "        features, logits, probs = discriminator(hidden_states)\n",
    "\n",
    "        # Finally, we separate the discriminator's output for the real and fake\n",
    "        # data\n",
    "        features_list = torch.split(features, real_batch_size)\n",
    "        D_real_features = features_list[0]\n",
    "        #D_fake_features = features_list[1]\n",
    "      \n",
    "        logits_list = torch.split(logits, real_batch_size)\n",
    "        D_real_logits = logits_list[0]\n",
    "        #D_fake_logits = logits_list[1]\n",
    "        \n",
    "        probs_list = torch.split(probs, real_batch_size)\n",
    "        D_real_probs = probs_list[0]\n",
    "        #D_fake_probs = probs_list[1]\n",
    "\n",
    "        #---------------------------------\n",
    "        #  LOSS evaluation\n",
    "        #---------------------------------\n",
    "        # Generator's LOSS estimation\n",
    "#         g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + epsilon))\n",
    "#         g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n",
    "#         g_loss = g_loss_d + g_feat_reg\n",
    "  \n",
    "        # Disciminator's LOSS estimation\n",
    "        logits = D_real_logits[:,0:-1]\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        # The discriminator provides an output for labeled and unlabeled real data\n",
    "        # so the loss evaluated for unlabeled data is ignored (masked)\n",
    "        label2one_hot = torch.nn.functional.one_hot(b_labels, len(label_list))\n",
    "        per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n",
    "        per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n",
    "        labeled_example_count = per_example_loss.type(torch.float32).numel()\n",
    "\n",
    "        # It may be the case that a batch does not contain labeled examples, \n",
    "        # so the \"supervised loss\" in this case is not evaluated\n",
    "        if labeled_example_count == 0:\n",
    "          D_L_Supervised = 0\n",
    "        else:\n",
    "          D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n",
    "                 \n",
    "        D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n",
    "        #D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n",
    "        d_loss = D_L_Supervised + D_L_unsupervised1U #+ D_L_unsupervised2U\n",
    "\n",
    "        #---------------------------------\n",
    "        #  OPTIMIZATION\n",
    "        #---------------------------------\n",
    "        # Avoid gradient accumulation\n",
    "        #gen_optimizer.zero_grad()\n",
    "        dis_optimizer.zero_grad()\n",
    "\n",
    "        # Calculate weigth updates\n",
    "        # retain_graph=True is required since the underlying graph will be deleted after backward\n",
    "        #g_loss.backward(retain_graph=True)\n",
    "        d_loss.backward() \n",
    "        \n",
    "        # Apply modifications\n",
    "        #gen_optimizer.step()\n",
    "        dis_optimizer.step()\n",
    "\n",
    "        # A detail log of the individual losses\n",
    "        #print(\"{0:.4f}\\t{1:.4f}\\t{2:.4f}\\t{3:.4f}\\t{4:.4f}\".\n",
    "        #      format(D_L_Supervised, D_L_unsupervised1U, D_L_unsupervised2U,\n",
    "        #             g_loss_d, g_feat_reg))\n",
    "\n",
    "        # Save the losses to print them later\n",
    "        #tr_g_loss += g_loss.item()\n",
    "        tr_d_loss += d_loss.item()\n",
    "\n",
    "        # Update the learning rate with the scheduler\n",
    "        if apply_scheduler:\n",
    "          scheduler_d.step()\n",
    "          #scheduler_g.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    #avg_train_loss_g = tr_g_loss / len(train_dataloader)\n",
    "    avg_train_loss_d = tr_d_loss / len(train_dataloader)             \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    #print(\"  Average training loss generetor: {0:.3f}\".format(avg_train_loss_g))\n",
    "    print(\"  Average training loss discriminator: {0:.3f}\".format(avg_train_loss_d))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #     TEST ON THE EVALUATION DATASET\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our test set.\n",
    "    print(\"\")\n",
    "    print(\"Running Test...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    transformer.eval() #maybe redundant\n",
    "    discriminator.eval()\n",
    "    #generator.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_test_accuracy = 0\n",
    "   \n",
    "    total_test_loss = 0\n",
    "    nb_test_steps = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels_ids = []\n",
    "\n",
    "    #loss\n",
    "    nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in test_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
    "            #hidden_states = model_outputs[-1]\n",
    "            hidden_states = model_outputs.last_hidden_state[:,0,:]\n",
    "            _, logits, probs = discriminator(hidden_states)\n",
    "            ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n",
    "            filtered_logits = logits[:,0:-1]\n",
    "            # Accumulate the test loss.\n",
    "            total_test_loss += nll_loss(filtered_logits, b_labels)\n",
    "            \n",
    "        # Accumulate the predictions and the input labels\n",
    "        _, preds = torch.max(filtered_logits, 1)\n",
    "        all_preds += preds.detach().cpu()\n",
    "        all_labels_ids += b_labels.detach().cpu()\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    all_preds = torch.stack(all_preds).numpy()\n",
    "    all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
    "    test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
    "    print(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "    avg_test_loss = avg_test_loss.item()\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    test_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Test Loss: {0:.3f}\".format(avg_test_loss))\n",
    "    print(\"  Test took: {:}\".format(test_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            #'Training Loss generator': avg_train_loss_g,\n",
    "            'Training Loss discriminator': avg_train_loss_d,\n",
    "            'Valid. Loss': avg_test_loss,\n",
    "            'Valid. Accur.': test_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Test Time': test_time\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    accuracy_array.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fa12c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15d5062e0>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accuracy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68d2ed52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9625"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(accuracy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e043c94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
