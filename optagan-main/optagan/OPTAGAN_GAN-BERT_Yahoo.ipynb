{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55271308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import datetime\n",
    "now_time = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18e4c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17b69f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['UNK',1,2,3,4,5,6,7,8,9,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac22c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_l=pd.read_csv(\"../../yahoo/assigned/train_l.csv\", index_col=\"Unnamed: 0\")\n",
    "df_test_l=pd.read_csv(\"../../yahoo/assigned/test_l.csv\", index_col=\"Unnamed: 0\")\n",
    "df_u=pd.read_csv(\"../../yahoo/assigned/u.csv\", index_col=\"Unnamed: 0\")\n",
    "df_train_u=pd.read_csv(\"../../yahoo/assigned/train_u.csv\", index_col=\"Unnamed: 0\")#.head(10000)\n",
    "df_test_u=pd.read_csv(\"../../yahoo/assigned/test_u.csv\", index_col=\"Unnamed: 0\")#.head(5000)\n",
    "df_all = pd.concat([df_train_l, df_test_l, df_u, df_train_u, df_test_u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e52d4083",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_l =  list(df_train_l.to_records(index=False))\n",
    "test_l = list(df_test_l.to_records(index=False))\n",
    "u_list = list(df_u.to_records(index=False))\n",
    "test_u = list(df_test_u.to_records(index=False))\n",
    "train_u = list(df_train_u.to_records(index=False))\n",
    "data_all = list(df_all[\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f5408f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "#  Transformer parameters\n",
    "#--------------------------------\n",
    "max_seq_length = 24\n",
    "batch_size = 92\n",
    "\n",
    "#--------------------------------\n",
    "#  GAN-BERT specific parameters\n",
    "#--------------------------------\n",
    "# number of hidden layers in the generator, \n",
    "# each of the size of the output space\n",
    "#num_hidden_layers_g = 1; \n",
    "# number of hidden layers in the discriminator, \n",
    "# each of the size of the input space\n",
    "num_hidden_layers_d = 1; \n",
    "# size of the generator's input noisy vectors\n",
    "noise_size = 100\n",
    "# dropout to be applied to discriminator's input vectors\n",
    "out_dropout_rate = 0.2\n",
    "\n",
    "# Replicate labeled data to balance poorly represented datasets, \n",
    "# e.g., less than 1% of labeled material\n",
    "apply_balance = True\n",
    "\n",
    "#--------------------------------\n",
    "#  Optimization parameters\n",
    "#--------------------------------\n",
    "learning_rate_discriminator = 5e-6 #5e-6?\n",
    "#learning_rate_generator = 5e-5\n",
    "epsilon = 1e-8\n",
    "num_train_epochs = 50\n",
    "multi_gpu = True\n",
    "# Scheduler\n",
    "apply_scheduler = False\n",
    "warmup_proportion = 0.1\n",
    "# Print\n",
    "print_each_n_step = 10\n",
    "\n",
    "#--------------------------------\n",
    "#  Adopted Tranformer model\n",
    "#--------------------------------\n",
    "# Since this version is compatible with Huggingface transformers, you can uncomment\n",
    "# (or add) transformer models compatible with GAN\n",
    "\n",
    "#model_name = \"bert-base-cased\"\n",
    "#model_name = \"bert-base-uncased\"\n",
    "#model_name = \"roberta-base\"\n",
    "#model_name = \"albert-base-v2\"\n",
    "#model_name = \"xlm-roberta-base\"\n",
    "#model_name = \"amazon/bort\"\n",
    "#model_name=\"google/electra-large-discriminator\"\n",
    "#model_name=\"google/electra-small-discriminator\"\n",
    "#model_name=\"microsoft/deberta-v2-xxlarge\"\n",
    "#model_name=\"microsoft/deberta-v3-base\"\n",
    "model_name = \"google/electra-base-discriminator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4758bf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "transformer = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7beb0062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_loader(input_examples, label_masks, label_map, do_shuffle = False, balance_label_examples = False):\n",
    "  '''\n",
    "  Generate a Dataloader given the input examples, eventually masked if they are \n",
    "  to be considered NOT labeled.\n",
    "  '''\n",
    "  examples = []\n",
    "\n",
    "  # Count the percentage of labeled examples  \n",
    "  num_labeled_examples = 0\n",
    "  for label_mask in label_masks:\n",
    "    if label_mask: \n",
    "      num_labeled_examples += 1\n",
    "  label_mask_rate = num_labeled_examples/len(input_examples)\n",
    "\n",
    "  # if required it applies the balance\n",
    "  for index, ex in enumerate(input_examples): \n",
    "    if label_mask_rate == 1 or not balance_label_examples:\n",
    "      examples.append((ex, label_masks[index]))\n",
    "    else:\n",
    "      # IT SIMULATE A LABELED EXAMPLE\n",
    "      if label_masks[index]:\n",
    "        balance = int(1/label_mask_rate)\n",
    "        balance = int(math.log(balance,2))\n",
    "        if balance < 1:\n",
    "          balance = 1\n",
    "        for b in range(0, int(balance)):\n",
    "          examples.append((ex, label_masks[index]))\n",
    "      else:\n",
    "        examples.append((ex, label_masks[index]))\n",
    "  \n",
    "  #-----------------------------------------------\n",
    "  # Generate input examples to the Transformer\n",
    "  #-----------------------------------------------\n",
    "  input_ids = []\n",
    "  input_mask_array = []\n",
    "  label_mask_array = []\n",
    "  label_id_array = []\n",
    "\n",
    "  # Tokenization \n",
    "  for (text, label_mask) in examples:\n",
    "    encoded_sent = tokenizer.encode(text[0], add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
    "    input_ids.append(encoded_sent)\n",
    "    label_id_array.append(label_map[text[1]])\n",
    "    label_mask_array.append(label_mask)\n",
    "  \n",
    "  # Attention to token (to ignore padded input wordpieces)\n",
    "  for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]                          \n",
    "    input_mask_array.append(att_mask)\n",
    "  # Convertion to Tensor\n",
    "  input_ids = torch.tensor(input_ids) \n",
    "  input_mask_array = torch.tensor(input_mask_array)\n",
    "  label_id_array = torch.tensor(label_id_array, dtype=torch.long)\n",
    "  label_mask_array = torch.tensor(label_mask_array)\n",
    "\n",
    "  # Building the TensorDataset\n",
    "  dataset = TensorDataset(input_ids, input_mask_array, label_id_array, label_mask_array)\n",
    "\n",
    "  if do_shuffle:\n",
    "    sampler = RandomSampler\n",
    "  else:\n",
    "    sampler = SequentialSampler\n",
    "\n",
    "  # Building the DataLoader\n",
    "  return DataLoader(\n",
    "              dataset,  # The training samples.\n",
    "              sampler = sampler(dataset), \n",
    "              batch_size = batch_size) # Trains with this batch size.\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76f98a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_fake(input_examples):\n",
    "  '''\n",
    "  Generate a Dataloader given the input examples, eventually masked if they are \n",
    "  to be considered NOT labeled.\n",
    "  '''\n",
    "  \n",
    "  #-----------------------------------------------\n",
    "  # Generate input examples to the Transformer\n",
    "  #-----------------------------------------------\n",
    "  input_ids = []\n",
    "  input_mask_array = []\n",
    "\n",
    "  # Tokenization \n",
    "  for text in input_examples:\n",
    "    encoded_sent = tokenizer.encode(text, add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
    "    input_ids.append(encoded_sent)\n",
    "  \n",
    "  # Attention to token (to ignore padded input wordpieces)\n",
    "  for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]                          \n",
    "    input_mask_array.append(att_mask)\n",
    "  # Convertion to Tensor\n",
    "  input_ids = torch.tensor(input_ids) \n",
    "  input_mask_array = torch.tensor(input_mask_array)\n",
    "\n",
    "  # Building the DataLoader\n",
    "  return input_ids, input_mask_array # Trains with this batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfd53b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the examples\n",
    "labeled_examples = train_l\n",
    "unlabeled_examples = u_list\n",
    "test_examples = test_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c169846",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {}\n",
    "for (i, label) in enumerate(label_list):\n",
    "  label_map[label] = i\n",
    "#------------------------------\n",
    "#   Load the train dataset\n",
    "#------------------------------\n",
    "train_examples = labeled_examples\n",
    "#The labeled (train) dataset is assigned with a mask set to True\n",
    "train_label_masks = np.ones(len(labeled_examples), dtype=bool)\n",
    "#If unlabel examples are available\n",
    "if unlabeled_examples:\n",
    "  train_examples = train_examples + unlabeled_examples\n",
    "  #The unlabeled (train) dataset is assigned with a mask set to False\n",
    "  tmp_masks = np.zeros(len(unlabeled_examples), dtype=bool)\n",
    "  train_label_masks = np.concatenate([train_label_masks,tmp_masks])\n",
    "\n",
    "train_dataloader = generate_data_loader(train_examples, train_label_masks, label_map, do_shuffle = True, balance_label_examples = apply_balance)\n",
    "\n",
    "#------------------------------\n",
    "#   Load the test dataset\n",
    "#------------------------------\n",
    "#The labeled (test) dataset is assigned with a mask set to True\n",
    "test_label_masks = np.ones(len(test_examples), dtype=bool)\n",
    "\n",
    "test_dataloader = generate_data_loader(test_examples, test_label_masks, label_map, do_shuffle = False, balance_label_examples = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bcab6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------\n",
    "#   The Discriminator\n",
    "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
    "#   https://github.com/crux82/ganbert\n",
    "#------------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
    "        layers = []\n",
    "        hidden_sizes = [input_size] + hidden_sizes\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
    "\n",
    "        self.layers = nn.Sequential(*layers) #per il flatten\n",
    "        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1) # +1 for the probability of this sample being fake/real.\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_rep):\n",
    "        input_rep = self.input_dropout(input_rep)\n",
    "        last_rep = self.layers(input_rep)\n",
    "        logits = self.logit(last_rep)\n",
    "        probs = self.softmax(logits)\n",
    "        return last_rep, logits, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d197ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The config file is required to get the dimension of the vector produced by \n",
    "# the underlying transformer\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "hidden_size = int(config.hidden_size)\n",
    "# Define the number and width of hidden layers\n",
    "#hidden_levels_g = [hidden_size for i in range(0, num_hidden_layers_g)]\n",
    "hidden_levels_d = [hidden_size for i in range(0, num_hidden_layers_d)]\n",
    "\n",
    "#-------------------------------------------------\n",
    "#   Instantiate the Generator and Discriminator\n",
    "#-------------------------------------------------\n",
    "#generator = Generator(noise_size=noise_size, output_size=hidden_size, hidden_sizes=hidden_levels_g, dropout_rate=out_dropout_rate)\n",
    "discriminator = Discriminator(input_size=hidden_size, hidden_sizes=hidden_levels_d, num_labels=len(label_list), dropout_rate=out_dropout_rate)\n",
    "\n",
    "# Put everything in the GPU if available\n",
    "if torch.cuda.is_available():    \n",
    "  #generator.cuda()\n",
    "  discriminator.cuda()\n",
    "  transformer.cuda()\n",
    "  if multi_gpu:\n",
    "    transformer = torch.nn.DataParallel(transformer)\n",
    "\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "800d0186",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stats = []\n",
    "\n",
    "accuracy_array=[]\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "#models parameters\n",
    "transformer_vars = [i for i in transformer.parameters()]\n",
    "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
    "#g_vars = [v for v in generator.parameters()]\n",
    "\n",
    "#optimizer\n",
    "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
    "#gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator) \n",
    "\n",
    "#scheduler\n",
    "if apply_scheduler:\n",
    "  num_train_examples = len(train_examples)\n",
    "  num_train_steps = int(num_train_examples / batch_size * num_train_epochs)\n",
    "  num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
    "\n",
    "  scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n",
    "                                           num_warmup_steps = num_warmup_steps)\n",
    "  scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n",
    "                                           num_warmup_steps = num_warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7db4d007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/harry/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#OPTAGAN\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import argparse\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from modules.gan import Generator, Critic\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from func import GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig, BertConfig\n",
    "from func import GPT2LMHeadModel, GPT2Tokenizer, GPT2ForLatentConnector, GPT2ForLatentConnectorValueHead\n",
    "from func import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer\n",
    "from func import XLNetLMHeadModel, XLNetTokenizer\n",
    "from func import TransfoXLLMHeadModel, TransfoXLTokenizer\n",
    "from func import BertForLatentConnector, BertTokenizer\n",
    "\n",
    "from collections import defaultdict\n",
    "from utils import (TextDataset_Split, TextDataset_2Tokenizers, BucketingDataLoader)\n",
    "import pdb\n",
    "from modules.utils import (calc_blue_parallel_func, pad_seq, rollout, rollout_test)\n",
    "#from transformers.modeling_utils import top_k_top_p_filtering\n",
    "\n",
    "\n",
    "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) for conf in (GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig)), ())\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'gpt2': (GPT2Config, GPT2ForLatentConnectorValueHead, GPT2Tokenizer),\n",
    "    'bert': (BertConfig, BertForLatentConnector, BertTokenizer)\n",
    "}\n",
    "\n",
    "num_txt = 1\n",
    "\n",
    "def load_and_cache_examples(args, tokenizer):\n",
    "    if isinstance(tokenizer, list):\n",
    "        dataset = TextDataset_2Tokenizers(tokenizer, args, args.train_data_file, block_size=args.block_size)\n",
    "    else:\n",
    "        dataset = TextDataset_Split(tokenizer, args, args.train_data_file, block_size=args.block_size)\n",
    "    return dataset\n",
    "\n",
    "def build_dataload_and_cache_examples(args, tokenizer, num_txt):\n",
    "    if isinstance(tokenizer, list):\n",
    "        args.batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "        if num_txt<=20:\n",
    "            concatenation=\"{}_{}{}\".format(args.train_data_file, num_txt, \".txt\")\n",
    "            file_path=concatenation\n",
    "            print(\"Train file used is number {}\".format(num_txt))\n",
    "            print(concatenation)\n",
    "            num_txt=num_txt+1\n",
    "        else:\n",
    "            num_txt=1\n",
    "            concatenation=\"{}_{}{}\".format(args.train_data_file, num_txt, \".txt\")\n",
    "            file_path=concatenation\n",
    "            print(\"Train file used is number {}\".format(num_txt))\n",
    "        dataloader = BucketingDataLoader(file_path, args.batch_size, args.max_seq_length, tokenizer, args, bucket=100, shuffle=True)\n",
    "    else:\n",
    "        pass \n",
    "    return dataloader, num_txt\n",
    "\n",
    "def compute_grad_penalty(critic, real_data, fake_data):\n",
    "    B = real_data.size(0)\n",
    "    alpha = torch.FloatTensor(np.random.random((B, 1)))\n",
    "    if args.cuda:\n",
    "        alpha = alpha.cuda()\n",
    "    sample = alpha*real_data + (1-alpha)*fake_data\n",
    "    sample.requires_grad_(True)\n",
    "    score = critic(sample)\n",
    "\n",
    "    outputs = torch.FloatTensor(B, 1).fill_(1.0) #args.latent_size\n",
    "    outputs.requires_grad_(False)\n",
    "    if args.cuda:\n",
    "        outputs = outputs.cuda()\n",
    "    grads = autograd.grad(\n",
    "        outputs=score,\n",
    "        inputs=sample,\n",
    "        grad_outputs=outputs,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True)[0]\n",
    "    grad_penalty = ((grads.norm(2, dim=1) - 1.) ** 2).mean()\n",
    "    return grad_penalty\n",
    "\n",
    "def train(epoch):\n",
    "    model_encoder.eval()\n",
    "    model_decoder.eval()\n",
    "    generator.train()\n",
    "    critic.train()\n",
    "    c_train_loss = 0.\n",
    "    g_train_loss = 0.\n",
    "    g_batches = 0\n",
    "    c_batches = 0\n",
    "    c_loss_0 = 1\n",
    "    g_loss_0 = 1\n",
    "    for i, x in enumerate(train_loader):\n",
    "        x = x[0]\n",
    "        if args.cuda:\n",
    "            x = x.cuda()\n",
    "        # Generate noise\n",
    "        B = args.per_gpu_train_batch_size\n",
    "        noise = torch.from_numpy(np.random.normal(0, 1, (B,\n",
    "                                 args.latent_size))).float()\n",
    "        if args.cuda:\n",
    "            noise = noise.cuda()\n",
    "        # Get original text latent embeddings\n",
    "        with torch.no_grad(): \n",
    "            pooled_hidden_fea = model_encoder(x, attention_mask=(x > 0).float())[1]\n",
    "            mean, logvar = model_encoder.linear(pooled_hidden_fea).chunk(2, -1)\n",
    "            z_real = mean.squeeze(1) \n",
    "\n",
    "        # Evaluate and get losses\n",
    "        z_fake = generator(noise)\n",
    "        real_score = critic(z_real)\n",
    "        fake_score = critic(z_fake)\n",
    "        grad_penalty = compute_grad_penalty(critic, z_real.data, z_fake.data)\n",
    "        c_loss = -torch.mean(real_score) + torch.mean(fake_score) + \\\n",
    "                 args.gp_lambda*grad_penalty\n",
    "\n",
    "        fake_score = critic(generator(noise))\n",
    "        g_loss = -torch.mean(fake_score)\n",
    "        \n",
    "        r_g = abs(((g_loss.item() - g_loss_0) / (g_loss_0 + 0.001))) \n",
    "        r_c = abs(((c_loss.item() - c_loss_0) / (c_loss_0 + 0.001))) \n",
    "        \n",
    "        # Update critic or generator\n",
    "        if ((2 + epoch) / epoch) * r_c > r_g:\n",
    "            c_optimizer.zero_grad()\n",
    "            c_batches += 1\n",
    "            c_train_loss += c_loss.item()\n",
    "            c_loss.backward()\n",
    "            c_optimizer.step()\n",
    "        else:\n",
    "            g_optimizer.zero_grad()\n",
    "            g_batches += 1\n",
    "            g_train_loss += g_loss.item()\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "        c_loss_0 = c_loss.item()\n",
    "        g_loss_0 = g_loss.item()\n",
    "\n",
    "        if args.interval > 0 and i % args.interval == 0:\n",
    "            logger.info('Epoch: {} | Batch: {}/{} ({:.0f}%) | G Loss: {:.6f} | C Loss: {:.6f}'.format(\n",
    "                epoch, args.batch_size*i, len(train_loader.dataset),\n",
    "                100.*(args.batch_size*i)/len(train_loader.dataset),\n",
    "                g_loss.item(), c_loss.item()\n",
    "            ))\n",
    "            test_noise = torch.Tensor(np.random.normal(0, 1, (1, args.latent_size))).to(args.device)\n",
    "            test_new_z = generator(test_noise).data\n",
    "            # create new sent\n",
    "            test_z = rollout_test(model_decoder, test_new_z, tokenizer_decoder, args.max_seq_length, 1, 0, 1)\n",
    "            logger.info(\"Text: {}\".format(test_z))\n",
    "\n",
    "    c_train_loss /= c_batches + 1\n",
    "    g_train_loss /= g_batches + 1\n",
    "    logger.info('* (Train) Epoch: {} | G Loss: {:.4f} | C Loss: {:.4f} | Updates G: {} | Updates C: {}'.format(\n",
    "        epoch, g_train_loss, c_train_loss, g_batches, c_batches\n",
    "    ))\n",
    "    return (g_train_loss, c_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eef4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 00:50:29 - INFO - func.configuration_utils -   loading configuration file output_dir_768_0_unsure/checkpoint-encoder-508523/config.json\n",
      "06/28/2022 00:50:29 - INFO - func.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "06/28/2022 00:50:29 - INFO - func.modeling_utils -   loading weights file output_dir_768_0_unsure/checkpoint-encoder-508523/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size_rl=32, block_dim=100, block_size=100, checkpoint_dir='output_dir_768_0_unsure', cuda=True, dataset='EMNLP', decoder_config_name='', decoder_model_name_or_path='gpt2', decoder_model_type='gpt2', decoder_tokenizer_name='', do_lower_case=False, encoder_config_name='', encoder_model_name_or_path='bert-base-cased', encoder_model_type='bert', encoder_tokenizer_name='', epochs=200, epochs_rl=100, finetune_decoder=True, generator_dir=None, gloabl_step_eval=508523, gp_lambda=10, interval=50, latent_size=768, length=20, lr=0.0001, lr_rl=1e-06, max_seq_length=24, n_layers=10, output_dir='output_dir_768_0_unsure', padding_text='', per_gpu_train_batch_size=12, prompt='', seed=0, train_data_file='../../yahoo/subdivided_large/train', use_philly=False, valid_data_file='../../yahoo/unlabelled_short/test.txt')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 00:50:33 - INFO - func.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/harry/.cache/torch/pytorch_transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "06/28/2022 00:50:33 - INFO - func.configuration_utils -   loading configuration file output_dir_768_0_unsure/checkpoint-decoder-508523/config.json\n",
      "06/28/2022 00:50:33 - INFO - func.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"latent_size\": 768,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"vocab_size\": 50260\n",
      "}\n",
      "\n",
      "06/28/2022 00:50:33 - INFO - func.modeling_utils -   loading weights file output_dir_768_0_unsure/checkpoint-decoder-508523/pytorch_model.bin\n",
      "06/28/2022 00:50:40 - INFO - func.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/harry/.cache/torch/pytorch_transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "06/28/2022 00:50:40 - INFO - func.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/harry/.cache/torch/pytorch_transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "06/28/2022 00:50:40 - INFO - func.tokenization_utils -   Adding <PAD> to the vocabulary\n",
      "06/28/2022 00:50:40 - INFO - func.tokenization_utils -   Assigning <PAD> to the pad_token key of the tokenizer\n",
      "06/28/2022 00:50:40 - INFO - func.tokenization_utils -   Adding <BOS> to the vocabulary\n",
      "06/28/2022 00:50:40 - INFO - func.tokenization_utils -   Assigning <BOS> to the bos_token key of the tokenizer\n",
      "06/28/2022 00:50:40 - INFO - func.tokenization_utils -   Adding <EOS> to the vocabulary\n",
      "06/28/2022 00:50:40 - INFO - func.tokenization_utils -   Assigning <EOS> to the eos_token key of the tokenizer\n",
      "06/28/2022 00:50:40 - INFO - __main__ -   We have added 3 tokens to GPT2\n",
      "06/28/2022 00:50:40 - INFO - __main__ -   G Parameters:255468\n",
      "06/28/2022 00:50:40 - INFO - __main__ -   C Parameters:178001\n",
      "06/28/2022 00:50:40 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_1.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file used is number 1\n",
      "../../yahoo/subdivided_large/train_1.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 1 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    120.    Elapsed: 0:00:13.\n",
      "  Batch    20  of    120.    Elapsed: 0:00:26.\n",
      "  Batch    30  of    120.    Elapsed: 0:00:42.\n",
      "  Batch    40  of    120.    Elapsed: 0:00:57.\n",
      "  Batch    50  of    120.    Elapsed: 0:01:08.\n",
      "  Batch    60  of    120.    Elapsed: 0:01:22.\n",
      "  Batch    70  of    120.    Elapsed: 0:01:33.\n",
      "  Batch    80  of    120.    Elapsed: 0:01:47.\n",
      "  Batch    90  of    120.    Elapsed: 0:02:00.\n",
      "  Batch   100  of    120.    Elapsed: 0:02:12.\n",
      "  Batch   110  of    120.    Elapsed: 0:02:24.\n",
      "\n",
      "  Average training loss generetor: 0.347\n",
      "  Average training loss discriminator: 3.965\n",
      "  Training epcoh took: 0:02:34\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 00:53:14 - INFO - __main__ -   Epoch: 1 | Batch: 0/10000 (0%) | G Loss: 0.273712 | C Loss: 1.674639\n",
      "06/28/2022 00:53:14 - INFO - __main__ -   Text: ['']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.133\n",
      "  Test Loss: 2.357\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 00:53:15 - INFO - __main__ -   Epoch: 1 | Batch: 600/10000 (6%) | G Loss: 154.376831 | C Loss: -98.768921\n",
      "06/28/2022 00:53:15 - INFO - __main__ -   Text: ['']\n",
      "06/28/2022 00:53:16 - INFO - __main__ -   Epoch: 1 | Batch: 1200/10000 (12%) | G Loss: 74.731468 | C Loss: -51.481277\n",
      "06/28/2022 00:53:16 - INFO - __main__ -   Text: ['Falk a. Appl. Shea.making. Two.. M. known to in on 30.']\n",
      "06/28/2022 00:53:17 - INFO - __main__ -   Epoch: 1 | Batch: 1800/10000 (18%) | G Loss: 73.271088 | C Loss: -55.373634\n",
      "06/28/2022 00:53:18 - INFO - __main__ -   Text: [\",-. to B... to. in.'.. and. Jewish.'s.\"]\n",
      "06/28/2022 00:53:19 - INFO - __main__ -   Epoch: 1 | Batch: 2400/10000 (24%) | G Loss: 52.415874 | C Loss: -37.907539\n",
      "06/28/2022 00:53:19 - INFO - __main__ -   Text: ['the. the. Kumarified the Balls 201448_ decline Eastern..']\n",
      "06/28/2022 00:53:20 - INFO - __main__ -   Epoch: 1 | Batch: 3000/10000 (30%) | G Loss: 47.951366 | C Loss: -37.604858\n",
      "06/28/2022 00:53:20 - INFO - __main__ -   Text: ['']\n",
      "06/28/2022 00:53:21 - INFO - __main__ -   Epoch: 1 | Batch: 3600/10000 (36%) | G Loss: 42.640793 | C Loss: -33.766441\n",
      "06/28/2022 00:53:21 - INFO - __main__ -   Text: ['Road Main. in ) UK Road\".. could.. the. Rutherford. below.v -. Imaging.']\n",
      "06/28/2022 00:53:22 - INFO - __main__ -   Epoch: 1 | Batch: 4200/10000 (42%) | G Loss: 37.413986 | C Loss: -29.714893\n",
      "06/28/2022 00:53:22 - INFO - __main__ -   Text: ['to Re... rema.. Hilton...-.-. Keeping... there.']\n",
      "06/28/2022 00:53:23 - INFO - __main__ -   Epoch: 1 | Batch: 4800/10000 (48%) | G Loss: 32.398857 | C Loss: -27.035454\n",
      "06/28/2022 00:53:23 - INFO - __main__ -   Text: ['Sp <PAD> Artist']\n",
      "06/28/2022 00:53:24 - INFO - __main__ -   Epoch: 1 | Batch: 5400/10000 (54%) | G Loss: 26.102453 | C Loss: -21.565832\n",
      "06/28/2022 00:53:24 - INFO - __main__ -   Text: ['Music 2020 reserve to Williams launch models. DC. The.../ et. majority. history. 20']\n",
      "06/28/2022 00:53:25 - INFO - __main__ -   Epoch: 1 | Batch: 6000/10000 (60%) | G Loss: 19.919430 | C Loss: -17.618599\n",
      "06/28/2022 00:53:25 - INFO - __main__ -   Text: ['and for due party . kond.']\n",
      "06/28/2022 00:53:26 - INFO - __main__ -   Epoch: 1 | Batch: 6600/10000 (66%) | G Loss: 18.933182 | C Loss: -17.045158\n",
      "06/28/2022 00:53:27 - INFO - __main__ -   Text: ['The..... 1989.....']\n",
      "06/28/2022 00:53:28 - INFO - __main__ -   Epoch: 1 | Batch: 7200/10000 (72%) | G Loss: 18.960075 | C Loss: -17.282585\n",
      "06/28/2022 00:53:28 - INFO - __main__ -   Text: ['lost there.']\n",
      "06/28/2022 00:53:29 - INFO - __main__ -   Epoch: 1 | Batch: 7800/10000 (78%) | G Loss: 16.104706 | C Loss: -14.986725\n",
      "06/28/2022 00:53:29 - INFO - __main__ -   Text: ['sponsorsiao.']\n",
      "06/28/2022 00:53:30 - INFO - __main__ -   Epoch: 1 | Batch: 8400/10000 (84%) | G Loss: 15.166998 | C Loss: -14.131714\n",
      "06/28/2022 00:53:30 - INFO - __main__ -   Text: ['Arableated withable. for. Unique.. in. 2002.24. then..']\n",
      "06/28/2022 00:53:31 - INFO - __main__ -   Epoch: 1 | Batch: 9000/10000 (90%) | G Loss: 13.410632 | C Loss: -12.906170\n",
      "06/28/2022 00:53:31 - INFO - __main__ -   Text: ['Fast Copyright Tail Charles Law On Expect Was Stand Step Acc Smry Picture.']\n",
      "06/28/2022 00:53:32 - INFO - __main__ -   Epoch: 1 | Batch: 9600/10000 (96%) | G Loss: 11.997835 | C Loss: -11.444549\n",
      "06/28/2022 00:53:32 - INFO - __main__ -   Text: ['bidder agreesing means.']\n",
      "06/28/2022 00:53:33 - INFO - __main__ -   * (Train) Epoch: 1 | G Loss: 38.9621 | C Loss: -28.8827 | Updates G: 81 | Updates C: 752\n",
      "06/28/2022 00:53:42 - INFO - __main__ -   Bleu-2:0.137 | B-Bleu-2:0.186\n",
      "06/28/2022 00:53:42 - INFO - __main__ -   * Saving. Best Score:0.323 | Bleu-2:0.137 | B-Bleu-2:0.186\n",
      "06/28/2022 00:53:42 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_2.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3226229746114385\n",
      "Train file used is number 2\n",
      "../../yahoo/subdivided_large/train_2.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 2 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    120.    Elapsed: 0:00:18.\n",
      "  Batch    20  of    120.    Elapsed: 0:00:36.\n",
      "  Batch    30  of    120.    Elapsed: 0:00:53.\n",
      "  Batch    40  of    120.    Elapsed: 0:01:11.\n",
      "  Batch    50  of    120.    Elapsed: 0:01:29.\n",
      "  Batch    60  of    120.    Elapsed: 0:01:47.\n",
      "  Batch    70  of    120.    Elapsed: 0:02:05.\n",
      "  Batch    80  of    120.    Elapsed: 0:02:23.\n",
      "  Batch    90  of    120.    Elapsed: 0:02:41.\n",
      "  Batch   100  of    120.    Elapsed: 0:02:58.\n",
      "  Batch   110  of    120.    Elapsed: 0:03:16.\n",
      "\n",
      "  Average training loss generetor: 0.543\n",
      "  Average training loss discriminator: 3.463\n",
      "  Training epcoh took: 0:03:34\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 00:57:16 - INFO - __main__ -   Epoch: 2 | Batch: 0/10001 (0%) | G Loss: 9.731853 | C Loss: -9.938903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.133\n",
      "  Test Loss: 2.338\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 00:57:16 - INFO - __main__ -   Text: [\"Circle Sn√∂ Laws is to guarantee a strike from any one. or both. the. in.'road.\"]\n",
      "06/28/2022 00:57:17 - INFO - __main__ -   Epoch: 2 | Batch: 600/10001 (6%) | G Loss: 8.173342 | C Loss: -8.331749\n",
      "06/28/2022 00:57:17 - INFO - __main__ -   Text: ['People displayed capital).']\n",
      "06/28/2022 00:57:18 - INFO - __main__ -   Epoch: 2 | Batch: 1200/10001 (12%) | G Loss: 7.877058 | C Loss: -8.514096\n",
      "06/28/2022 00:57:18 - INFO - __main__ -   Text: ['arm to h into his We Sell Merden.']\n",
      "06/28/2022 00:57:19 - INFO - __main__ -   Epoch: 2 | Batch: 1800/10001 (18%) | G Loss: 6.322692 | C Loss: -7.098281\n",
      "06/28/2022 00:57:19 - INFO - __main__ -   Text: ['rum that we defeat Correct as a Criminal.']\n",
      "06/28/2022 00:57:20 - INFO - __main__ -   Epoch: 2 | Batch: 2400/10001 (24%) | G Loss: 5.164805 | C Loss: -5.984857\n",
      "06/28/2022 00:57:20 - INFO - __main__ -   Text: ['He he training with the amateur work he he they live.']\n",
      "06/28/2022 00:57:21 - INFO - __main__ -   Epoch: 2 | Batch: 3000/10001 (30%) | G Loss: 3.608841 | C Loss: -4.403196\n",
      "06/28/2022 00:57:21 - INFO - __main__ -   Text: ['says that they have.']\n",
      "06/28/2022 00:57:22 - INFO - __main__ -   Epoch: 2 | Batch: 3600/10001 (36%) | G Loss: 3.763355 | C Loss: -3.790380\n",
      "06/28/2022 00:57:22 - INFO - __main__ -   Text: ['Q-virtuifies ... runners.\"']\n",
      "06/28/2022 00:57:23 - INFO - __main__ -   Epoch: 2 | Batch: 4200/10001 (42%) | G Loss: 3.108230 | C Loss: -3.565559\n",
      "06/28/2022 00:57:23 - INFO - __main__ -   Text: ['Philosophy is cure for light.']\n",
      "06/28/2022 00:57:24 - INFO - __main__ -   Epoch: 2 | Batch: 4800/10001 (48%) | G Loss: 3.027351 | C Loss: -2.824418\n",
      "06/28/2022 00:57:24 - INFO - __main__ -   Text: ['Te In A therefore a must be he.']\n",
      "06/28/2022 00:57:25 - INFO - __main__ -   Epoch: 2 | Batch: 5400/10001 (54%) | G Loss: 3.059800 | C Loss: -2.840739\n",
      "06/28/2022 00:57:25 - INFO - __main__ -   Text: ['I_educated.']\n",
      "06/28/2022 00:57:26 - INFO - __main__ -   Epoch: 2 | Batch: 6000/10001 (60%) | G Loss: 3.490872 | C Loss: -2.750640\n",
      "06/28/2022 00:57:26 - INFO - __main__ -   Text: ['What Little Road Run is Different.\"']\n",
      "06/28/2022 00:57:27 - INFO - __main__ -   Epoch: 2 | Batch: 6600/10001 (66%) | G Loss: 3.388671 | C Loss: -2.866935\n",
      "06/28/2022 00:57:27 - INFO - __main__ -   Text: ['Also Chef It! Honest! -------------------------------- Hely type!']\n",
      "06/28/2022 00:57:28 - INFO - __main__ -   Epoch: 2 | Batch: 7200/10001 (72%) | G Loss: 3.895878 | C Loss: -3.246556\n",
      "06/28/2022 00:57:28 - INFO - __main__ -   Text: ['It is said that all dogs can learn to speak speech.']\n",
      "06/28/2022 00:57:29 - INFO - __main__ -   Epoch: 2 | Batch: 7800/10001 (78%) | G Loss: 4.391759 | C Loss: -3.465238\n",
      "06/28/2022 00:57:29 - INFO - __main__ -   Text: ['No ID!']\n",
      "06/28/2022 00:57:30 - INFO - __main__ -   Epoch: 2 | Batch: 8400/10001 (84%) | G Loss: 4.321948 | C Loss: -3.582259\n",
      "06/28/2022 00:57:30 - INFO - __main__ -   Text: ['Modules!']\n",
      "06/28/2022 00:57:31 - INFO - __main__ -   Epoch: 2 | Batch: 9000/10001 (90%) | G Loss: 4.397008 | C Loss: -3.723976\n",
      "06/28/2022 00:57:31 - INFO - __main__ -   Text: ['pot \"bookisp\".']\n",
      "06/28/2022 00:57:32 - INFO - __main__ -   Epoch: 2 | Batch: 9600/10001 (96%) | G Loss: 4.824839 | C Loss: -4.105042\n",
      "06/28/2022 00:57:32 - INFO - __main__ -   Text: ['Tor is a biased thread.']\n",
      "06/28/2022 00:57:33 - INFO - __main__ -   * (Train) Epoch: 2 | G Loss: 4.5146 | C Loss: -4.6483 | Updates G: 159 | Updates C: 674\n",
      "06/28/2022 00:57:41 - INFO - __main__ -   Bleu-2:0.182 | B-Bleu-2:0.229\n",
      "06/28/2022 00:57:41 - INFO - __main__ -   * Saving. Best Score:0.410 | Bleu-2:0.182 | B-Bleu-2:0.229\n",
      "06/28/2022 00:57:41 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_3.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41040155224083985\n",
      "Train file used is number 3\n",
      "../../yahoo/subdivided_large/train_3.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 3 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    120.    Elapsed: 0:00:12.\n",
      "  Batch    20  of    120.    Elapsed: 0:00:26.\n",
      "  Batch    30  of    120.    Elapsed: 0:00:41.\n",
      "  Batch    40  of    120.    Elapsed: 0:00:56.\n",
      "  Batch    50  of    120.    Elapsed: 0:01:11.\n",
      "  Batch    60  of    120.    Elapsed: 0:01:25.\n",
      "  Batch    70  of    120.    Elapsed: 0:01:39.\n",
      "  Batch    80  of    120.    Elapsed: 0:01:54.\n",
      "  Batch    90  of    120.    Elapsed: 0:02:08.\n",
      "  Batch   100  of    120.    Elapsed: 0:02:23.\n",
      "  Batch   110  of    120.    Elapsed: 0:02:35.\n",
      "\n",
      "  Average training loss generetor: 0.674\n",
      "  Average training loss discriminator: 3.083\n",
      "  Training epcoh took: 0:02:49\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 01:00:30 - INFO - __main__ -   Epoch: 3 | Batch: 0/10001 (0%) | G Loss: 4.620041 | C Loss: -4.130612\n",
      "06/28/2022 01:00:30 - INFO - __main__ -   Text: ['makes \"Jesus-\" .']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.185\n",
      "  Test Loss: 2.297\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 01:00:31 - INFO - __main__ -   Epoch: 3 | Batch: 600/10001 (6%) | G Loss: 4.340141 | C Loss: -3.689429\n",
      "06/28/2022 01:00:31 - INFO - __main__ -   Text: ['From ethernet and MTT.']\n",
      "06/28/2022 01:00:32 - INFO - __main__ -   Epoch: 3 | Batch: 1200/10001 (12%) | G Loss: 4.158864 | C Loss: -3.812348\n",
      "06/28/2022 01:00:32 - INFO - __main__ -   Text: ['This kind of pronunciation may say: The magnificent :']\n",
      "06/28/2022 01:00:33 - INFO - __main__ -   Epoch: 3 | Batch: 1800/10001 (18%) | G Loss: 4.775768 | C Loss: -3.935961\n",
      "06/28/2022 01:00:33 - INFO - __main__ -   Text: ['is one of the most very profitable computer games out there.']\n",
      "06/28/2022 01:00:34 - INFO - __main__ -   Epoch: 3 | Batch: 2400/10001 (24%) | G Loss: 4.668319 | C Loss: -3.826586\n",
      "06/28/2022 01:00:34 - INFO - __main__ -   Text: ['Student Text is the current most famous book.']\n",
      "06/28/2022 01:00:35 - INFO - __main__ -   Epoch: 3 | Batch: 3000/10001 (30%) | G Loss: 4.508978 | C Loss: -4.176436\n",
      "06/28/2022 01:00:35 - INFO - __main__ -   Text: ['Sebastrowalluk may want to rewrite the Swedish Alphabet.']\n",
      "06/28/2022 01:00:36 - INFO - __main__ -   Epoch: 3 | Batch: 3600/10001 (36%) | G Loss: 4.381294 | C Loss: -3.811137\n",
      "06/28/2022 01:00:36 - INFO - __main__ -   Text: ['Snake: I\\'ve given it an evil vult.\"']\n",
      "06/28/2022 01:00:37 - INFO - __main__ -   Epoch: 3 | Batch: 4200/10001 (42%) | G Loss: 5.649576 | C Loss: -4.705462\n",
      "06/28/2022 01:00:37 - INFO - __main__ -   Text: ['Wight will often ask himself Grey to get a job.']\n",
      "06/28/2022 01:00:38 - INFO - __main__ -   Epoch: 3 | Batch: 4800/10001 (48%) | G Loss: 5.947210 | C Loss: -5.005738\n",
      "06/28/2022 01:00:38 - INFO - __main__ -   Text: ['legal policy advice not\".']\n",
      "06/28/2022 01:00:39 - INFO - __main__ -   Epoch: 3 | Batch: 5400/10001 (54%) | G Loss: 5.143305 | C Loss: -4.469017\n",
      "06/28/2022 01:00:39 - INFO - __main__ -   Text: ['Picatico tries to turn the pain focus back onto letters or chords.']\n",
      "06/28/2022 01:00:40 - INFO - __main__ -   Epoch: 3 | Batch: 6000/10001 (60%) | G Loss: 4.942832 | C Loss: -4.336596\n",
      "06/28/2022 01:00:40 - INFO - __main__ -   Text: [').']\n",
      "06/28/2022 01:00:41 - INFO - __main__ -   Epoch: 3 | Batch: 6600/10001 (66%) | G Loss: 5.125113 | C Loss: -4.365424\n",
      "06/28/2022 01:00:42 - INFO - __main__ -   Text: ['??? Look Why I Just Looks Like You Looks Like It # How To Try.']\n",
      "06/28/2022 01:00:43 - INFO - __main__ -   Epoch: 3 | Batch: 7200/10001 (72%) | G Loss: 4.932872 | C Loss: -4.447841\n",
      "06/28/2022 01:00:43 - INFO - __main__ -   Text: ['Sometimes will be trouble in school days.']\n",
      "06/28/2022 01:00:44 - INFO - __main__ -   Epoch: 3 | Batch: 7800/10001 (78%) | G Loss: 4.859580 | C Loss: -4.324154\n",
      "06/28/2022 01:00:44 - INFO - __main__ -   Text: ['New challenges people are afraid ... Confidence is religion.']\n",
      "06/28/2022 01:00:45 - INFO - __main__ -   Epoch: 3 | Batch: 8400/10001 (84%) | G Loss: 4.702653 | C Loss: -4.407788\n",
      "06/28/2022 01:00:45 - INFO - __main__ -   Text: [\"Palmer is sometimes referred to as 'Drop in VR'.\"]\n",
      "06/28/2022 01:00:46 - INFO - __main__ -   Epoch: 3 | Batch: 9000/10001 (90%) | G Loss: 4.906074 | C Loss: -4.147823\n",
      "06/28/2022 01:00:46 - INFO - __main__ -   Text: ['Telescope might have a self-guided or a self-destructive mind.[6]']\n",
      "06/28/2022 01:00:47 - INFO - __main__ -   Epoch: 3 | Batch: 9600/10001 (96%) | G Loss: 4.276958 | C Loss: -4.192852\n",
      "06/28/2022 01:00:47 - INFO - __main__ -   Text: ['']\n",
      "06/28/2022 01:00:48 - INFO - __main__ -   * (Train) Epoch: 3 | G Loss: 4.8694 | C Loss: -4.1961 | Updates G: 153 | Updates C: 680\n",
      "06/28/2022 01:00:54 - INFO - __main__ -   Bleu-2:0.196 | B-Bleu-2:0.242\n",
      "06/28/2022 01:00:54 - INFO - __main__ -   * Saving. Best Score:0.439 | Bleu-2:0.196 | B-Bleu-2:0.242\n",
      "06/28/2022 01:00:54 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_4.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4385859032969835\n",
      "Train file used is number 4\n",
      "../../yahoo/subdivided_large/train_4.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 4 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    120.    Elapsed: 0:00:14.\n",
      "  Batch    20  of    120.    Elapsed: 0:00:28.\n",
      "  Batch    30  of    120.    Elapsed: 0:00:42.\n",
      "  Batch    40  of    120.    Elapsed: 0:00:56.\n",
      "  Batch    50  of    120.    Elapsed: 0:01:09.\n",
      "  Batch    60  of    120.    Elapsed: 0:01:23.\n",
      "  Batch    70  of    120.    Elapsed: 0:01:38.\n",
      "  Batch    80  of    120.    Elapsed: 0:01:53.\n",
      "  Batch    90  of    120.    Elapsed: 0:02:07.\n",
      "  Batch   100  of    120.    Elapsed: 0:02:20.\n",
      "  Batch   110  of    120.    Elapsed: 0:02:34.\n",
      "\n",
      "  Average training loss generetor: 0.689\n",
      "  Average training loss discriminator: 2.868\n",
      "  Training epcoh took: 0:02:47\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 01:03:42 - INFO - __main__ -   Epoch: 4 | Batch: 0/10001 (0%) | G Loss: 4.529090 | C Loss: -3.749632\n",
      "06/28/2022 01:03:42 - INFO - __main__ -   Text: ['examples\".']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.210\n",
      "  Test Loss: 2.257\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 01:03:43 - INFO - __main__ -   Epoch: 4 | Batch: 600/10001 (6%) | G Loss: 4.449077 | C Loss: -4.016423\n",
      "06/28/2022 01:03:43 - INFO - __main__ -   Text: ['button pressing ectopus lottery exchopus right buy<|endoftext|> She is not fish .']\n",
      "06/28/2022 01:03:44 - INFO - __main__ -   Epoch: 4 | Batch: 1200/10001 (12%) | G Loss: 4.584173 | C Loss: -3.994284\n",
      "06/28/2022 01:03:44 - INFO - __main__ -   Text: ['barelyYou (doesnnit) we words.\"']\n",
      "06/28/2022 01:03:45 - INFO - __main__ -   Epoch: 4 | Batch: 1800/10001 (18%) | G Loss: 4.874452 | C Loss: -4.203463\n",
      "06/28/2022 01:03:45 - INFO - __main__ -   Text: ['Reddit claims that fish is quiet and lean.']\n",
      "06/28/2022 01:03:46 - INFO - __main__ -   Epoch: 4 | Batch: 2400/10001 (24%) | G Loss: 4.629632 | C Loss: -4.190031\n",
      "06/28/2022 01:03:46 - INFO - __main__ -   Text: [\"Univbot li It's true if it can walk.\"]\n",
      "06/28/2022 01:03:47 - INFO - __main__ -   Epoch: 4 | Batch: 3000/10001 (30%) | G Loss: 4.829841 | C Loss: -4.524186\n",
      "06/28/2022 01:03:47 - INFO - __main__ -   Text: ['The final word is \"toe.\"']\n",
      "06/28/2022 01:03:48 - INFO - __main__ -   Epoch: 4 | Batch: 3600/10001 (36%) | G Loss: 4.660816 | C Loss: -4.079024\n",
      "06/28/2022 01:03:48 - INFO - __main__ -   Text: ['Software and printers <BOS> v.']\n",
      "06/28/2022 01:03:49 - INFO - __main__ -   Epoch: 4 | Batch: 4200/10001 (42%) | G Loss: 4.605460 | C Loss: -4.118834\n",
      "06/28/2022 01:03:49 - INFO - __main__ -   Text: ['Research is on the card.\"']\n",
      "06/28/2022 01:03:50 - INFO - __main__ -   Epoch: 4 | Batch: 4800/10001 (48%) | G Loss: 4.694397 | C Loss: -4.219988\n",
      "06/28/2022 01:03:50 - INFO - __main__ -   Text: ['BAR toplists are the exhaustion.\"']\n",
      "06/28/2022 01:03:51 - INFO - __main__ -   Epoch: 4 | Batch: 5400/10001 (54%) | G Loss: 5.001351 | C Loss: -3.719298\n",
      "06/28/2022 01:03:51 - INFO - __main__ -   Text: [\"Upgrade''.\"]\n",
      "06/28/2022 01:03:52 - INFO - __main__ -   Epoch: 4 | Batch: 6000/10001 (60%) | G Loss: 4.767104 | C Loss: -4.070787\n",
      "06/28/2022 01:03:53 - INFO - __main__ -   Text: ['They say that Apollo knows something about Harp but not Cryoe.']\n",
      "06/28/2022 01:03:53 - INFO - __main__ -   Epoch: 4 | Batch: 6600/10001 (66%) | G Loss: 4.188366 | C Loss: -3.527977\n",
      "06/28/2022 01:03:54 - INFO - __main__ -   Text: ['\"Ha!\"']\n",
      "06/28/2022 01:03:54 - INFO - __main__ -   Epoch: 4 | Batch: 7200/10001 (72%) | G Loss: 4.888761 | C Loss: -4.000211\n",
      "06/28/2022 01:03:55 - INFO - __main__ -   Text: ['It is the \"basical\" of \"Beatlove.\"']\n",
      "06/28/2022 01:03:56 - INFO - __main__ -   Epoch: 4 | Batch: 7800/10001 (78%) | G Loss: 4.604502 | C Loss: -4.179277\n",
      "06/28/2022 01:03:56 - INFO - __main__ -   Text: ['Petra wants to be somewhere somewhere ...']\n",
      "06/28/2022 01:03:57 - INFO - __main__ -   Epoch: 4 | Batch: 8400/10001 (84%) | G Loss: 3.777257 | C Loss: -3.417652\n",
      "06/28/2022 01:03:57 - INFO - __main__ -   Text: ['Rough dress is a part of Hinduism\".']\n",
      "06/28/2022 01:03:58 - INFO - __main__ -   Epoch: 4 | Batch: 9000/10001 (90%) | G Loss: 4.787703 | C Loss: -3.977086\n",
      "06/28/2022 01:03:58 - INFO - __main__ -   Text: ['\"Hi is for rejection\".']\n",
      "06/28/2022 01:03:59 - INFO - __main__ -   Epoch: 4 | Batch: 9600/10001 (96%) | G Loss: 4.276575 | C Loss: -3.735145\n",
      "06/28/2022 01:03:59 - INFO - __main__ -   Text: ['is not the right thing.']\n",
      "06/28/2022 01:03:59 - INFO - __main__ -   * (Train) Epoch: 4 | G Loss: 4.6444 | C Loss: -3.9958 | Updates G: 226 | Updates C: 607\n",
      "06/28/2022 01:04:07 - INFO - __main__ -   Bleu-2:0.199 | B-Bleu-2:0.252\n",
      "06/28/2022 01:04:07 - INFO - __main__ -   * Saving. Best Score:0.451 | Bleu-2:0.199 | B-Bleu-2:0.252\n",
      "06/28/2022 01:04:07 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4511395839191315\n",
      "Train file used is number 5\n",
      "../../yahoo/subdivided_large/train_5.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 5 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    120.    Elapsed: 0:00:14.\n",
      "  Batch    20  of    120.    Elapsed: 0:00:28.\n",
      "  Batch    30  of    120.    Elapsed: 0:00:42.\n",
      "  Batch    40  of    120.    Elapsed: 0:00:55.\n",
      "  Batch    50  of    120.    Elapsed: 0:01:09.\n",
      "  Batch    60  of    120.    Elapsed: 0:01:23.\n",
      "  Batch    70  of    120.    Elapsed: 0:01:36.\n",
      "  Batch    80  of    120.    Elapsed: 0:01:50.\n",
      "  Batch    90  of    120.    Elapsed: 0:02:04.\n",
      "  Batch   100  of    120.    Elapsed: 0:02:17.\n",
      "  Batch   110  of    120.    Elapsed: 0:02:32.\n",
      "\n",
      "  Average training loss generetor: 0.695\n",
      "  Average training loss discriminator: 2.626\n",
      "  Training epcoh took: 0:02:45\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 01:06:53 - INFO - __main__ -   Epoch: 5 | Batch: 0/10001 (0%) | G Loss: 4.357077 | C Loss: -3.894767\n",
      "06/28/2022 01:06:53 - INFO - __main__ -   Text: ['aisles.\"']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.247\n",
      "  Test Loss: 2.214\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 01:06:54 - INFO - __main__ -   Epoch: 5 | Batch: 600/10001 (6%) | G Loss: 4.318542 | C Loss: -3.710934\n",
      "06/28/2022 01:06:54 - INFO - __main__ -   Text: ['\"Crawfish appears strange.\"']\n",
      "06/28/2022 01:06:55 - INFO - __main__ -   Epoch: 5 | Batch: 1200/10001 (12%) | G Loss: 4.520477 | C Loss: -3.616540\n",
      "06/28/2022 01:06:55 - INFO - __main__ -   Text: ['These are called an iso.']\n",
      "06/28/2022 01:06:56 - INFO - __main__ -   Epoch: 5 | Batch: 1800/10001 (18%) | G Loss: 4.798465 | C Loss: -4.036019\n",
      "06/28/2022 01:06:56 - INFO - __main__ -   Text: ['For a true Britishian, Mountaineres is strongestly.']\n",
      "06/28/2022 01:06:57 - INFO - __main__ -   Epoch: 5 | Batch: 2400/10001 (24%) | G Loss: 4.324715 | C Loss: -3.735215\n",
      "06/28/2022 01:06:57 - INFO - __main__ -   Text: ['\"That\\'s her speed\" may not seem like much to someone who owns Infos.']\n",
      "06/28/2022 01:06:58 - INFO - __main__ -   Epoch: 5 | Batch: 3000/10001 (30%) | G Loss: 4.629351 | C Loss: -3.914187\n",
      "06/28/2022 01:06:58 - INFO - __main__ -   Text: [\"A comment to is to make sure you don't shoot yourself in the head.\"]\n",
      "06/28/2022 01:06:59 - INFO - __main__ -   Epoch: 5 | Batch: 3600/10001 (36%) | G Loss: 4.278577 | C Loss: -3.637477\n",
      "06/28/2022 01:06:59 - INFO - __main__ -   Text: ['It is thus taught that life is more important than taste.']\n",
      "06/28/2022 01:07:00 - INFO - __main__ -   Epoch: 5 | Batch: 4200/10001 (42%) | G Loss: 4.318213 | C Loss: -3.703342\n",
      "06/28/2022 01:07:00 - INFO - __main__ -   Text: ['\"You\\'ve got it!\"']\n",
      "06/28/2022 01:07:01 - INFO - __main__ -   Epoch: 5 | Batch: 4800/10001 (48%) | G Loss: 4.147045 | C Loss: -3.659516\n",
      "06/28/2022 01:07:01 - INFO - __main__ -   Text: ['\"I am in love with somebody else yet\".']\n",
      "06/28/2022 01:07:02 - INFO - __main__ -   Epoch: 5 | Batch: 5400/10001 (54%) | G Loss: 4.469892 | C Loss: -3.724478\n",
      "06/28/2022 01:07:02 - INFO - __main__ -   Text: ['Drumhead is.']\n",
      "06/28/2022 01:07:03 - INFO - __main__ -   Epoch: 5 | Batch: 6000/10001 (60%) | G Loss: 3.930053 | C Loss: -3.467740\n",
      "06/28/2022 01:07:03 - INFO - __main__ -   Text: ['\"\" must reflect adversely on \"famine disorders\".']\n",
      "06/28/2022 01:07:04 - INFO - __main__ -   Epoch: 5 | Batch: 6600/10001 (66%) | G Loss: 4.747540 | C Loss: -3.791927\n",
      "06/28/2022 01:07:04 - INFO - __main__ -   Text: [\"The ‚Äúse-ash!<|endoftext|>Activism is a woman's path.\"]\n",
      "06/28/2022 01:07:05 - INFO - __main__ -   Epoch: 5 | Batch: 7200/10001 (72%) | G Loss: 4.584352 | C Loss: -3.757452\n",
      "06/28/2022 01:07:05 - INFO - __main__ -   Text: ['Lives is relentlessly joyful, cheerful, fat.']\n",
      "06/28/2022 01:07:06 - INFO - __main__ -   Epoch: 5 | Batch: 7800/10001 (78%) | G Loss: 4.302036 | C Loss: -3.759484\n",
      "06/28/2022 01:07:06 - INFO - __main__ -   Text: ['Later you refer to \"tiny Shiites\" as compared.']\n",
      "06/28/2022 01:07:07 - INFO - __main__ -   Epoch: 5 | Batch: 8400/10001 (84%) | G Loss: 3.947968 | C Loss: -3.523171\n",
      "06/28/2022 01:07:07 - INFO - __main__ -   Text: ['They work closely with tinkerpeggers.']\n",
      "06/28/2022 01:07:08 - INFO - __main__ -   Epoch: 5 | Batch: 9000/10001 (90%) | G Loss: 4.375543 | C Loss: -3.629753\n",
      "06/28/2022 01:07:08 - INFO - __main__ -   Text: [\"(or maybe it's historical sense?)\"]\n",
      "06/28/2022 01:07:09 - INFO - __main__ -   Epoch: 5 | Batch: 9600/10001 (96%) | G Loss: 3.964509 | C Loss: -3.329350\n",
      "06/28/2022 01:07:09 - INFO - __main__ -   Text: ['He shoves his meat to it.']\n",
      "06/28/2022 01:07:10 - INFO - __main__ -   * (Train) Epoch: 5 | G Loss: 4.2632 | C Loss: -3.6359 | Updates G: 225 | Updates C: 608\n",
      "06/28/2022 01:07:18 - INFO - __main__ -   Bleu-2:0.213 | B-Bleu-2:0.250\n",
      "06/28/2022 01:07:18 - INFO - __main__ -   * Saving. Best Score:0.463 | Bleu-2:0.213 | B-Bleu-2:0.250\n",
      "06/28/2022 01:07:18 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_6.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4629196201065815\n",
      "Train file used is number 6\n",
      "../../yahoo/subdivided_large/train_6.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 6 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    120.    Elapsed: 0:00:13.\n",
      "  Batch    20  of    120.    Elapsed: 0:00:27.\n",
      "  Batch    30  of    120.    Elapsed: 0:00:41.\n",
      "  Batch    40  of    120.    Elapsed: 0:00:56.\n",
      "  Batch    50  of    120.    Elapsed: 0:01:09.\n",
      "  Batch    60  of    120.    Elapsed: 0:01:24.\n",
      "  Batch    70  of    120.    Elapsed: 0:01:38.\n",
      "  Batch    80  of    120.    Elapsed: 0:01:52.\n",
      "  Batch    90  of    120.    Elapsed: 0:02:07.\n",
      "  Batch   100  of    120.    Elapsed: 0:02:21.\n",
      "  Batch   110  of    120.    Elapsed: 0:02:36.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 2.369\n",
      "  Training epcoh took: 0:02:49\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 01:10:07 - INFO - __main__ -   Epoch: 6 | Batch: 0/10001 (0%) | G Loss: 4.222330 | C Loss: -3.565387\n",
      "06/28/2022 01:10:07 - INFO - __main__ -   Text: ['breeds .']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.265\n",
      "  Test Loss: 2.162\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 01:10:08 - INFO - __main__ -   Epoch: 6 | Batch: 600/10001 (6%) | G Loss: 4.613758 | C Loss: -3.678281\n",
      "06/28/2022 01:10:08 - INFO - __main__ -   Text: ['Exiting this stage on bird is called free flow.']\n",
      "06/28/2022 01:10:09 - INFO - __main__ -   Epoch: 6 | Batch: 1200/10001 (12%) | G Loss: 3.818054 | C Loss: -3.221541\n",
      "06/28/2022 01:10:09 - INFO - __main__ -   Text: ['\"Here goes another one,\" from a parody title.']\n",
      "06/28/2022 01:10:10 - INFO - __main__ -   Epoch: 6 | Batch: 1800/10001 (18%) | G Loss: 3.995018 | C Loss: -3.366353\n",
      "06/28/2022 01:10:10 - INFO - __main__ -   Text: ['They are mostly dolls of evil!']\n",
      "06/28/2022 01:10:11 - INFO - __main__ -   Epoch: 6 | Batch: 2400/10001 (24%) | G Loss: 4.531995 | C Loss: -3.518365\n",
      "06/28/2022 01:10:11 - INFO - __main__ -   Text: ['Just what you think, if you send me legs?!']\n",
      "06/28/2022 01:10:12 - INFO - __main__ -   Epoch: 6 | Batch: 3000/10001 (30%) | G Loss: 3.899805 | C Loss: -3.294541\n",
      "06/28/2022 01:10:12 - INFO - __main__ -   Text: ['Italians interpret poetry as the second act of the mind.']\n",
      "06/28/2022 01:10:13 - INFO - __main__ -   Epoch: 6 | Batch: 3600/10001 (36%) | G Loss: 4.188849 | C Loss: -3.435604\n",
      "06/28/2022 01:10:13 - INFO - __main__ -   Text: ['petits on both workplace and job.']\n",
      "06/28/2022 01:10:14 - INFO - __main__ -   Epoch: 6 | Batch: 4200/10001 (42%) | G Loss: 3.851128 | C Loss: -3.258229\n",
      "06/28/2022 01:10:15 - INFO - __main__ -   Text: ['Richard and his product \"et allemesh\".']\n",
      "06/28/2022 01:10:15 - INFO - __main__ -   Epoch: 6 | Batch: 4800/10001 (48%) | G Loss: 3.755739 | C Loss: -3.202137\n",
      "06/28/2022 01:10:16 - INFO - __main__ -   Text: ['Teach this, that lies here!']\n",
      "06/28/2022 01:10:17 - INFO - __main__ -   Epoch: 6 | Batch: 5400/10001 (54%) | G Loss: 3.886414 | C Loss: -3.292252\n",
      "06/28/2022 01:10:17 - INFO - __main__ -   Text: ['Coward finds responses currently undisguised\".']\n",
      "06/28/2022 01:10:18 - INFO - __main__ -   Epoch: 6 | Batch: 6000/10001 (60%) | G Loss: 3.744102 | C Loss: -3.252420\n",
      "06/28/2022 01:10:18 - INFO - __main__ -   Text: ['husks try to learn both nonsense and Math.']\n",
      "06/28/2022 01:10:19 - INFO - __main__ -   Epoch: 6 | Batch: 6600/10001 (66%) | G Loss: 3.698241 | C Loss: -3.137900\n",
      "06/28/2022 01:10:19 - INFO - __main__ -   Text: ['\"I can just tell time\".']\n",
      "06/28/2022 01:10:20 - INFO - __main__ -   Epoch: 6 | Batch: 7200/10001 (72%) | G Loss: 3.782338 | C Loss: -3.000328\n",
      "06/28/2022 01:10:20 - INFO - __main__ -   Text: [\"He uses 'rhum borrows' as well as avoiding teens.\"]\n",
      "06/28/2022 01:10:21 - INFO - __main__ -   Epoch: 6 | Batch: 7800/10001 (78%) | G Loss: 3.789450 | C Loss: -3.187848\n",
      "06/28/2022 01:10:21 - INFO - __main__ -   Text: [\"Being also tells us what mammals we're attracted to.\"]\n",
      "06/28/2022 01:10:22 - INFO - __main__ -   Epoch: 6 | Batch: 8400/10001 (84%) | G Loss: 3.454080 | C Loss: -3.063711\n",
      "06/28/2022 01:10:22 - INFO - __main__ -   Text: ['The Currie comma knows humour.']\n",
      "06/28/2022 01:10:23 - INFO - __main__ -   Epoch: 6 | Batch: 9000/10001 (90%) | G Loss: 3.591798 | C Loss: -2.869757\n",
      "06/28/2022 01:10:23 - INFO - __main__ -   Text: ['Kagoromo gives a very advanced perspective on world transformation.']\n",
      "06/28/2022 01:10:24 - INFO - __main__ -   Epoch: 6 | Batch: 9600/10001 (96%) | G Loss: 3.624982 | C Loss: -3.070153\n",
      "06/28/2022 01:10:24 - INFO - __main__ -   Text: ['But it always is like Ghosts calling when possible.']\n",
      "06/28/2022 01:10:25 - INFO - __main__ -   * (Train) Epoch: 6 | G Loss: 3.8785 | C Loss: -3.2738 | Updates G: 241 | Updates C: 592\n",
      "06/28/2022 01:10:32 - INFO - __main__ -   Bleu-2:0.204 | B-Bleu-2:0.252\n",
      "06/28/2022 01:10:32 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_7.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45643042981607895\n",
      "Train file used is number 7\n",
      "../../yahoo/subdivided_large/train_7.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 7 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    120.    Elapsed: 0:00:15.\n",
      "  Batch    20  of    120.    Elapsed: 0:00:30.\n",
      "  Batch    30  of    120.    Elapsed: 0:00:45.\n",
      "  Batch    40  of    120.    Elapsed: 0:01:00.\n",
      "  Batch    50  of    120.    Elapsed: 0:01:15.\n",
      "  Batch    60  of    120.    Elapsed: 0:01:31.\n",
      "  Batch    70  of    120.    Elapsed: 0:01:45.\n",
      "  Batch    80  of    120.    Elapsed: 0:02:01.\n",
      "  Batch    90  of    120.    Elapsed: 0:02:16.\n",
      "  Batch   100  of    120.    Elapsed: 0:02:30.\n",
      "  Batch   110  of    120.    Elapsed: 0:02:45.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 2.079\n",
      "  Training epcoh took: 0:02:59\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 01:13:31 - INFO - __main__ -   Epoch: 7 | Batch: 0/10001 (0%) | G Loss: 3.807411 | C Loss: -3.225398\n",
      "06/28/2022 01:13:31 - INFO - __main__ -   Text: ['They use the concept of digital money.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.270\n",
      "  Test Loss: 2.132\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 01:13:32 - INFO - __main__ -   Epoch: 7 | Batch: 600/10001 (6%) | G Loss: 3.468308 | C Loss: -2.857921\n",
      "06/28/2022 01:13:32 - INFO - __main__ -   Text: ['To set a new record, Picctick needs to be on k-1.']\n",
      "06/28/2022 01:13:33 - INFO - __main__ -   Epoch: 7 | Batch: 1200/10001 (12%) | G Loss: 3.210629 | C Loss: -3.062437\n",
      "06/28/2022 01:13:33 - INFO - __main__ -   Text: ['In the US, you cannot have a spindle for your network.']\n",
      "06/28/2022 01:13:34 - INFO - __main__ -   Epoch: 7 | Batch: 1800/10001 (18%) | G Loss: 3.782075 | C Loss: -3.122443\n",
      "06/28/2022 01:13:34 - INFO - __main__ -   Text: ['Which drill is hiring now or will stay the same!']\n",
      "06/28/2022 01:13:35 - INFO - __main__ -   Epoch: 7 | Batch: 2400/10001 (24%) | G Loss: 3.209568 | C Loss: -2.808022\n",
      "06/28/2022 01:13:35 - INFO - __main__ -   Text: ['\"What they\\' written isn\\'t really start HemProMed!\"']\n",
      "06/28/2022 01:13:36 - INFO - __main__ -   Epoch: 7 | Batch: 3000/10001 (30%) | G Loss: 3.444105 | C Loss: -2.956007\n",
      "06/28/2022 01:13:36 - INFO - __main__ -   Text: ['Besides I am talking about writing torts...\"']\n",
      "06/28/2022 01:13:37 - INFO - __main__ -   Epoch: 7 | Batch: 3600/10001 (36%) | G Loss: 3.644709 | C Loss: -2.832908\n",
      "06/28/2022 01:13:37 - INFO - __main__ -   Text: ['\"beyond explanation\" is generally unhelpful.']\n",
      "06/28/2022 01:13:38 - INFO - __main__ -   Epoch: 7 | Batch: 4200/10001 (42%) | G Loss: 4.006342 | C Loss: -3.137744\n",
      "06/28/2022 01:13:39 - INFO - __main__ -   Text: ['Bliss is really probably who most people think is a race.']\n",
      "06/28/2022 01:13:39 - INFO - __main__ -   Epoch: 7 | Batch: 4800/10001 (48%) | G Loss: 2.936103 | C Loss: -2.724631\n",
      "06/28/2022 01:13:40 - INFO - __main__ -   Text: ['MICROBLEBELLAR is a concern for any musical player.']\n",
      "06/28/2022 01:13:41 - INFO - __main__ -   Epoch: 7 | Batch: 5400/10001 (54%) | G Loss: 3.616723 | C Loss: -2.985553\n",
      "06/28/2022 01:13:41 - INFO - __main__ -   Text: ['Books should be placed in open increments.']\n",
      "06/28/2022 01:13:42 - INFO - __main__ -   Epoch: 7 | Batch: 6000/10001 (60%) | G Loss: 3.377526 | C Loss: -2.916096\n",
      "06/28/2022 01:13:42 - INFO - __main__ -   Text: ['Discounters Island may refer to hasankles and other styles of writing.']\n",
      "06/28/2022 01:13:43 - INFO - __main__ -   Epoch: 7 | Batch: 6600/10001 (66%) | G Loss: 3.460222 | C Loss: -2.832763\n",
      "06/28/2022 01:13:43 - INFO - __main__ -   Text: ['You can also follow this pattern for many years to come.']\n",
      "06/28/2022 01:13:44 - INFO - __main__ -   Epoch: 7 | Batch: 7200/10001 (72%) | G Loss: 3.652548 | C Loss: -2.901588\n",
      "06/28/2022 01:13:44 - INFO - __main__ -   Text: ['As guardians, the missed child delivers otherworldly messages.']\n",
      "06/28/2022 01:13:45 - INFO - __main__ -   Epoch: 7 | Batch: 7800/10001 (78%) | G Loss: 3.019964 | C Loss: -2.756950\n",
      "06/28/2022 01:13:45 - INFO - __main__ -   Text: [\"Observe how happy children's voices always are!\"]\n",
      "06/28/2022 01:13:46 - INFO - __main__ -   Epoch: 7 | Batch: 8400/10001 (84%) | G Loss: 3.603495 | C Loss: -2.953227\n",
      "06/28/2022 01:13:46 - INFO - __main__ -   Text: ['Persive message consistency is not something that is routine.']\n",
      "06/28/2022 01:13:47 - INFO - __main__ -   Epoch: 7 | Batch: 9000/10001 (90%) | G Loss: 3.097963 | C Loss: -2.691054\n",
      "06/28/2022 01:13:47 - INFO - __main__ -   Text: ['The Creative > Jewish characteristics are the mystery.\"']\n",
      "06/28/2022 01:13:48 - INFO - __main__ -   Epoch: 7 | Batch: 9600/10001 (96%) | G Loss: 3.317541 | C Loss: -2.832671\n",
      "06/28/2022 01:13:48 - INFO - __main__ -   Text: ['It is intended to read or analyse fluently.']\n",
      "06/28/2022 01:13:49 - INFO - __main__ -   * (Train) Epoch: 7 | G Loss: 3.4765 | C Loss: -2.9127 | Updates G: 244 | Updates C: 589\n",
      "06/28/2022 01:13:58 - INFO - __main__ -   Bleu-2:0.194 | B-Bleu-2:0.245\n",
      "06/28/2022 01:13:58 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_8.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43944995800196185\n",
      "Train file used is number 8\n",
      "../../yahoo/subdivided_large/train_8.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 8 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    120.    Elapsed: 0:00:14.\n",
      "  Batch    20  of    120.    Elapsed: 0:00:27.\n",
      "  Batch    30  of    120.    Elapsed: 0:00:42.\n",
      "  Batch    40  of    120.    Elapsed: 0:00:56.\n",
      "  Batch    50  of    120.    Elapsed: 0:01:10.\n",
      "  Batch    60  of    120.    Elapsed: 0:01:25.\n",
      "  Batch    70  of    120.    Elapsed: 0:01:40.\n",
      "  Batch    80  of    120.    Elapsed: 0:01:54.\n",
      "  Batch    90  of    120.    Elapsed: 0:02:08.\n",
      "  Batch   100  of    120.    Elapsed: 0:02:22.\n",
      "  Batch   110  of    120.    Elapsed: 0:02:35.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 1.818\n",
      "  Training epcoh took: 0:02:50\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 01:16:48 - INFO - __main__ -   Epoch: 8 | Batch: 0/10001 (0%) | G Loss: 3.442065 | C Loss: -2.651783\n",
      "06/28/2022 01:16:48 - INFO - __main__ -   Text: ['WTF?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.287\n",
      "  Test Loss: 2.076\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 01:16:49 - INFO - __main__ -   Epoch: 8 | Batch: 600/10001 (6%) | G Loss: 3.608249 | C Loss: -2.899277\n",
      "06/28/2022 01:16:49 - INFO - __main__ -   Text: ['Kestrel is stronger on the random listeners.']\n",
      "06/28/2022 01:16:50 - INFO - __main__ -   Epoch: 8 | Batch: 1200/10001 (12%) | G Loss: 3.427771 | C Loss: -2.840657\n",
      "06/28/2022 01:16:50 - INFO - __main__ -   Text: ['Mimir makes one horrible plan for a time twenty-five years.']\n",
      "06/28/2022 01:16:51 - INFO - __main__ -   Epoch: 8 | Batch: 1800/10001 (18%) | G Loss: 3.187002 | C Loss: -2.744500\n",
      "06/28/2022 01:16:51 - INFO - __main__ -   Text: ['Food says, \"Type one!']\n",
      "06/28/2022 01:16:52 - INFO - __main__ -   Epoch: 8 | Batch: 2400/10001 (24%) | G Loss: 3.243271 | C Loss: -2.676908\n",
      "06/28/2022 01:16:52 - INFO - __main__ -   Text: ['These refer to comparisons between Washington University and Cambridge University.']\n",
      "06/28/2022 01:16:53 - INFO - __main__ -   Epoch: 8 | Batch: 3000/10001 (30%) | G Loss: 3.417011 | C Loss: -2.894614\n",
      "06/28/2022 01:16:53 - INFO - __main__ -   Text: ['B.h.']\n",
      "06/28/2022 01:16:54 - INFO - __main__ -   Epoch: 8 | Batch: 3600/10001 (36%) | G Loss: 3.048924 | C Loss: -2.745260\n",
      "06/28/2022 01:16:54 - INFO - __main__ -   Text: ['These can either be humorous or account for his involvement in the Horse Run.']\n",
      "06/28/2022 01:16:55 - INFO - __main__ -   Epoch: 8 | Batch: 4200/10001 (42%) | G Loss: 3.187756 | C Loss: -2.783297\n",
      "06/28/2022 01:16:55 - INFO - __main__ -   Text: [\"Those who don't realise the classic reading style is called dangerous.\"]\n",
      "06/28/2022 01:16:56 - INFO - __main__ -   Epoch: 8 | Batch: 4800/10001 (48%) | G Loss: 3.049879 | C Loss: -2.595651\n",
      "06/28/2022 01:16:56 - INFO - __main__ -   Text: ['An instant is one step which requires the administrative knowledge to come through.']\n",
      "06/28/2022 01:16:57 - INFO - __main__ -   Epoch: 8 | Batch: 5400/10001 (54%) | G Loss: 3.052641 | C Loss: -2.633454\n",
      "06/28/2022 01:16:57 - INFO - __main__ -   Text: ['It has become favourite to mintrome.']\n",
      "06/28/2022 01:16:58 - INFO - __main__ -   Epoch: 8 | Batch: 6000/10001 (60%) | G Loss: 3.250265 | C Loss: -2.626853\n",
      "06/28/2022 01:16:58 - INFO - __main__ -   Text: ['A friend of mine knows that to extend to space.']\n",
      "06/28/2022 01:16:59 - INFO - __main__ -   Epoch: 8 | Batch: 6600/10001 (66%) | G Loss: 3.114335 | C Loss: -2.701115\n",
      "06/28/2022 01:17:00 - INFO - __main__ -   Text: ['Other techniques include: t√ºrk intermediate \"\"\" usually but not nearly so.']\n",
      "06/28/2022 01:17:00 - INFO - __main__ -   Epoch: 8 | Batch: 7200/10001 (72%) | G Loss: 3.449737 | C Loss: -2.765440\n",
      "06/28/2022 01:17:01 - INFO - __main__ -   Text: ['Most hallows refer to themselves as \"problem\".']\n",
      "06/28/2022 01:17:01 - INFO - __main__ -   Epoch: 8 | Batch: 7800/10001 (78%) | G Loss: 3.166883 | C Loss: -2.691846\n",
      "06/28/2022 01:17:02 - INFO - __main__ -   Text: ['This is more elaborate than a blow job testimonial.']\n",
      "06/28/2022 01:17:03 - INFO - __main__ -   Epoch: 8 | Batch: 8400/10001 (84%) | G Loss: 2.938829 | C Loss: -2.539289\n",
      "06/28/2022 01:17:03 - INFO - __main__ -   Text: ['\"baler switches\" include porn.']\n",
      "06/28/2022 01:17:04 - INFO - __main__ -   Epoch: 8 | Batch: 9000/10001 (90%) | G Loss: 2.867803 | C Loss: -2.379882\n",
      "06/28/2022 01:17:04 - INFO - __main__ -   Text: ['This means that the vocation is keyboard chaufer.']\n",
      "06/28/2022 01:17:05 - INFO - __main__ -   Epoch: 8 | Batch: 9600/10001 (96%) | G Loss: 2.643488 | C Loss: -2.258091\n",
      "06/28/2022 01:17:05 - INFO - __main__ -   Text: ['Worst thing in the world is to write your own lament.']\n",
      "06/28/2022 01:17:05 - INFO - __main__ -   * (Train) Epoch: 8 | G Loss: 3.1315 | C Loss: -2.6184 | Updates G: 218 | Updates C: 615\n",
      "06/28/2022 01:17:13 - INFO - __main__ -   Bleu-2:0.206 | B-Bleu-2:0.254\n",
      "06/28/2022 01:17:13 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_9.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46054437534804\n",
      "Train file used is number 9\n",
      "../../yahoo/subdivided_large/train_9.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 9 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    120.    Elapsed: 0:00:15.\n",
      "  Batch    20  of    120.    Elapsed: 0:00:30.\n",
      "  Batch    30  of    120.    Elapsed: 0:00:44.\n",
      "  Batch    40  of    120.    Elapsed: 0:01:00.\n",
      "  Batch    50  of    120.    Elapsed: 0:01:16.\n",
      "  Batch    60  of    120.    Elapsed: 0:01:32.\n",
      "  Batch    70  of    120.    Elapsed: 0:01:47.\n",
      "  Batch    80  of    120.    Elapsed: 0:02:02.\n",
      "  Batch    90  of    120.    Elapsed: 0:02:16.\n",
      "  Batch   100  of    120.    Elapsed: 0:02:32.\n",
      "  Batch   110  of    120.    Elapsed: 0:02:47.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 1.572\n",
      "  Training epcoh took: 0:03:02\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 01:20:16 - INFO - __main__ -   Epoch: 9 | Batch: 0/10001 (0%) | G Loss: 3.132714 | C Loss: -2.520920\n",
      "06/28/2022 01:20:16 - INFO - __main__ -   Text: ['Microphones can also make one aware of these.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.278\n",
      "  Test Loss: 2.074\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 01:20:17 - INFO - __main__ -   Epoch: 9 | Batch: 600/10001 (6%) | G Loss: 2.838004 | C Loss: -2.457885\n",
      "06/28/2022 01:20:17 - INFO - __main__ -   Text: ['Stu slams the brakes while J.Bugs will fast.']\n",
      "06/28/2022 01:20:18 - INFO - __main__ -   Epoch: 9 | Batch: 1200/10001 (12%) | G Loss: 2.882060 | C Loss: -2.448639\n",
      "06/28/2022 01:20:18 - INFO - __main__ -   Text: ['Other suitable names include:']\n",
      "06/28/2022 01:20:19 - INFO - __main__ -   Epoch: 9 | Batch: 1800/10001 (18%) | G Loss: 3.025948 | C Loss: -2.496298\n",
      "06/28/2022 01:20:19 - INFO - __main__ -   Text: ['About this purbt include \"Together you are going to walk\", that shows humanity.']\n",
      "06/28/2022 01:20:20 - INFO - __main__ -   Epoch: 9 | Batch: 2400/10001 (24%) | G Loss: 2.875284 | C Loss: -2.465364\n",
      "06/28/2022 01:20:20 - INFO - __main__ -   Text: [\"It has Mary Bennet's works.\"]\n",
      "06/28/2022 01:20:21 - INFO - __main__ -   Epoch: 9 | Batch: 3000/10001 (30%) | G Loss: 2.809541 | C Loss: -2.417715\n",
      "06/28/2022 01:20:21 - INFO - __main__ -   Text: ['\"I should know better and get a positive test from every Irish bookseller\".']\n",
      "06/28/2022 01:20:22 - INFO - __main__ -   Epoch: 9 | Batch: 3600/10001 (36%) | G Loss: 2.964836 | C Loss: -2.513427\n",
      "06/28/2022 01:20:22 - INFO - __main__ -   Text: ['No one knows how long Locke can hold on to a job.']\n",
      "06/28/2022 01:20:23 - INFO - __main__ -   Epoch: 9 | Batch: 4200/10001 (42%) | G Loss: 2.873449 | C Loss: -2.367012\n",
      "06/28/2022 01:20:23 - INFO - __main__ -   Text: ['But can you kill whale primary?']\n",
      "06/28/2022 01:20:24 - INFO - __main__ -   Epoch: 9 | Batch: 4800/10001 (48%) | G Loss: 2.994168 | C Loss: -2.288362\n",
      "06/28/2022 01:20:24 - INFO - __main__ -   Text: ['The word \"netherder\" means much easier and faster.']\n",
      "06/28/2022 01:20:25 - INFO - __main__ -   Epoch: 9 | Batch: 5400/10001 (54%) | G Loss: 2.923097 | C Loss: -2.420654\n",
      "06/28/2022 01:20:25 - INFO - __main__ -   Text: ['It is described as a \" series of Welsh words to say\".']\n",
      "06/28/2022 01:20:26 - INFO - __main__ -   Epoch: 9 | Batch: 6000/10001 (60%) | G Loss: 2.800685 | C Loss: -2.357774\n",
      "06/28/2022 01:20:26 - INFO - __main__ -   Text: ['Hall is currently known to tell women, \"[You are going to get to know me.']\n",
      "06/28/2022 01:20:27 - INFO - __main__ -   Epoch: 9 | Batch: 6600/10001 (66%) | G Loss: 2.840985 | C Loss: -2.065017\n",
      "06/28/2022 01:20:27 - INFO - __main__ -   Text: [\"With it, 'singer boobs' can give you new clients.\"]\n",
      "06/28/2022 01:20:28 - INFO - __main__ -   Epoch: 9 | Batch: 7200/10001 (72%) | G Loss: 2.860525 | C Loss: -2.277718\n",
      "06/28/2022 01:20:28 - INFO - __main__ -   Text: ['The rainbow in this razor is only one of its parts.']\n",
      "06/28/2022 01:20:29 - INFO - __main__ -   Epoch: 9 | Batch: 7800/10001 (78%) | G Loss: 2.648168 | C Loss: -2.278920\n",
      "06/28/2022 01:20:30 - INFO - __main__ -   Text: ['They can either talk about your smoking.']\n",
      "06/28/2022 01:20:30 - INFO - __main__ -   Epoch: 9 | Batch: 8400/10001 (84%) | G Loss: 2.676155 | C Loss: -2.190673\n",
      "06/28/2022 01:20:31 - INFO - __main__ -   Text: [\"With persons such as onei'mers, it is impossible.\"]\n",
      "06/28/2022 01:20:32 - INFO - __main__ -   Epoch: 9 | Batch: 9000/10001 (90%) | G Loss: 2.501053 | C Loss: -2.110423\n",
      "06/28/2022 01:20:32 - INFO - __main__ -   Text: ['Just look for the horn and the fossa.']\n",
      "06/28/2022 01:20:33 - INFO - __main__ -   Epoch: 9 | Batch: 9600/10001 (96%) | G Loss: 2.536482 | C Loss: -2.196495\n",
      "06/28/2022 01:20:33 - INFO - __main__ -   Text: ['There are some topics for transgender people:()']\n",
      "06/28/2022 01:20:33 - INFO - __main__ -   * (Train) Epoch: 9 | G Loss: 2.8305 | C Loss: -2.3650 | Updates G: 244 | Updates C: 589\n",
      "06/28/2022 01:20:42 - INFO - __main__ -   Bleu-2:0.225 | B-Bleu-2:0.305\n",
      "06/28/2022 01:20:42 - INFO - __main__ -   * Saving. Best Score:0.531 | Bleu-2:0.225 | B-Bleu-2:0.305\n",
      "06/28/2022 01:20:42 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_10.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5307288150795214\n",
      "Train file used is number 10\n",
      "../../yahoo/subdivided_large/train_10.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 10 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    120.    Elapsed: 0:00:16.\n",
      "  Batch    20  of    120.    Elapsed: 0:00:32.\n",
      "  Batch    30  of    120.    Elapsed: 0:00:48.\n",
      "  Batch    40  of    120.    Elapsed: 0:01:04.\n",
      "  Batch    50  of    120.    Elapsed: 0:01:19.\n",
      "  Batch    60  of    120.    Elapsed: 0:01:35.\n",
      "  Batch    70  of    120.    Elapsed: 0:01:51.\n",
      "  Batch    80  of    120.    Elapsed: 0:02:06.\n",
      "  Batch    90  of    120.    Elapsed: 0:02:22.\n",
      "  Batch   100  of    120.    Elapsed: 0:02:38.\n",
      "  Batch   110  of    120.    Elapsed: 0:02:54.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 1.387\n",
      "  Training epcoh took: 0:03:10\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 01:23:52 - INFO - __main__ -   Epoch: 10 | Batch: 0/10001 (0%) | G Loss: 2.754909 | C Loss: -2.349859\n",
      "06/28/2022 01:23:52 - INFO - __main__ -   Text: [\"says the book's author.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.320\n",
      "  Test Loss: 2.037\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 01:23:53 - INFO - __main__ -   Epoch: 10 | Batch: 600/10001 (6%) | G Loss: 2.536390 | C Loss: -2.246959\n",
      "06/28/2022 01:23:53 - INFO - __main__ -   Text: ['By virtue of being very likable.']\n",
      "06/28/2022 01:23:54 - INFO - __main__ -   Epoch: 10 | Batch: 1200/10001 (12%) | G Loss: 2.638145 | C Loss: -2.232198\n",
      "06/28/2022 01:23:54 - INFO - __main__ -   Text: ['There are several books on it [La SpeciMazet].']\n",
      "06/28/2022 01:23:55 - INFO - __main__ -   Epoch: 10 | Batch: 1800/10001 (18%) | G Loss: 2.741398 | C Loss: -2.320937\n",
      "06/28/2022 01:23:55 - INFO - __main__ -   Text: [\"Workers' jobs are like workers': Beehive is afraid of their own form of distress.\"]\n",
      "06/28/2022 01:23:56 - INFO - __main__ -   Epoch: 10 | Batch: 2400/10001 (24%) | G Loss: 2.537118 | C Loss: -2.193499\n",
      "06/28/2022 01:23:56 - INFO - __main__ -   Text: ['According to Wikipedia, the most popular brahmin is \"RA\".']\n",
      "06/28/2022 01:23:57 - INFO - __main__ -   Epoch: 10 | Batch: 3000/10001 (30%) | G Loss: 2.445799 | C Loss: -2.090390\n",
      "06/28/2022 01:23:57 - INFO - __main__ -   Text: ['Shivab has a marketing code.']\n",
      "06/28/2022 01:23:58 - INFO - __main__ -   Epoch: 10 | Batch: 3600/10001 (36%) | G Loss: 2.657526 | C Loss: -1.930087\n",
      "06/28/2022 01:23:58 - INFO - __main__ -   Text: ['AI Some AI people avoid the bar code quite much.']\n",
      "06/28/2022 01:23:59 - INFO - __main__ -   Epoch: 10 | Batch: 4200/10001 (42%) | G Loss: 2.677898 | C Loss: -2.136350\n",
      "06/28/2022 01:23:59 - INFO - __main__ -   Text: ['This is why Malcolm Dukes is a book believer.']\n",
      "06/28/2022 01:24:00 - INFO - __main__ -   Epoch: 10 | Batch: 4800/10001 (48%) | G Loss: 2.626348 | C Loss: -2.128000\n",
      "06/28/2022 01:24:00 - INFO - __main__ -   Text: ['This is the one that gets my attention.']\n",
      "06/28/2022 01:24:01 - INFO - __main__ -   Epoch: 10 | Batch: 5400/10001 (54%) | G Loss: 2.534778 | C Loss: -2.101039\n",
      "06/28/2022 01:24:01 - INFO - __main__ -   Text: [\"It's also known as forecasting better times.\"]\n",
      "06/28/2022 01:24:02 - INFO - __main__ -   Epoch: 10 | Batch: 6000/10001 (60%) | G Loss: 2.884022 | C Loss: -2.212187\n",
      "06/28/2022 01:24:02 - INFO - __main__ -   Text: ['\"Comb it with tryled honey\" Required.']\n",
      "06/28/2022 01:24:03 - INFO - __main__ -   Epoch: 10 | Batch: 6600/10001 (66%) | G Loss: 2.268776 | C Loss: -2.010535\n",
      "06/28/2022 01:24:03 - INFO - __main__ -   Text: ['Infact reading can be a torment.']\n",
      "06/28/2022 01:24:04 - INFO - __main__ -   Epoch: 10 | Batch: 7200/10001 (72%) | G Loss: 2.176591 | C Loss: -1.930118\n",
      "06/28/2022 01:24:05 - INFO - __main__ -   Text: ['As of some unknown number are several spells such as daydream.']\n",
      "06/28/2022 01:24:05 - INFO - __main__ -   Epoch: 10 | Batch: 7800/10001 (78%) | G Loss: 2.619133 | C Loss: -2.051236\n",
      "06/28/2022 01:24:06 - INFO - __main__ -   Text: ['It is the most difficult and uncomfortable thing I know.\"']\n",
      "06/28/2022 01:24:07 - INFO - __main__ -   Epoch: 10 | Batch: 8400/10001 (84%) | G Loss: 2.267638 | C Loss: -1.927439\n",
      "06/28/2022 01:24:07 - INFO - __main__ -   Text: ['All of these are good.\"']\n",
      "06/28/2022 01:24:08 - INFO - __main__ -   Epoch: 10 | Batch: 9000/10001 (90%) | G Loss: 2.385384 | C Loss: -1.985132\n",
      "06/28/2022 01:24:08 - INFO - __main__ -   Text: ['Humanpert is the fastest sellable catalyst.']\n",
      "06/28/2022 01:24:09 - INFO - __main__ -   Epoch: 10 | Batch: 9600/10001 (96%) | G Loss: 2.293517 | C Loss: -1.810875\n",
      "06/28/2022 01:24:09 - INFO - __main__ -   Text: ['Outlaw is the only person on Earth that can say that.']\n",
      "06/28/2022 01:24:09 - INFO - __main__ -   * (Train) Epoch: 10 | G Loss: 2.5112 | C Loss: -2.0824 | Updates G: 238 | Updates C: 595\n",
      "06/28/2022 01:24:17 - INFO - __main__ -   Bleu-2:0.217 | B-Bleu-2:0.307\n",
      "06/28/2022 01:24:17 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_11.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5246343765730772\n",
      "Train file used is number 11\n",
      "../../yahoo/subdivided_large/train_11.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 11 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    120.    Elapsed: 0:00:16.\n",
      "  Batch    20  of    120.    Elapsed: 0:00:32.\n",
      "  Batch    30  of    120.    Elapsed: 0:00:49.\n",
      "  Batch    40  of    120.    Elapsed: 0:01:06.\n",
      "  Batch    50  of    120.    Elapsed: 0:01:22.\n",
      "  Batch    60  of    120.    Elapsed: 0:01:38.\n",
      "  Batch    70  of    120.    Elapsed: 0:01:55.\n",
      "  Batch    80  of    120.    Elapsed: 0:02:10.\n",
      "  Batch    90  of    120.    Elapsed: 0:02:26.\n",
      "  Batch   100  of    120.    Elapsed: 0:02:42.\n",
      "  Batch   110  of    120.    Elapsed: 0:02:58.\n",
      "\n",
      "  Average training loss generetor: 0.704\n",
      "  Average training loss discriminator: 1.225\n",
      "  Training epcoh took: 0:03:15\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 01:27:32 - INFO - __main__ -   Epoch: 11 | Batch: 0/10001 (0%) | G Loss: 2.318799 | C Loss: -2.049134\n",
      "06/28/2022 01:27:32 - INFO - __main__ -   Text: ['\"ai\".']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.300\n",
      "  Test Loss: 2.047\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 01:27:33 - INFO - __main__ -   Epoch: 11 | Batch: 600/10001 (6%) | G Loss: 2.216573 | C Loss: -2.029444\n",
      "06/28/2022 01:27:33 - INFO - __main__ -   Text: ['Performative is a book that tells interesting but unsettling stories.']\n",
      "06/28/2022 01:27:34 - INFO - __main__ -   Epoch: 11 | Batch: 1200/10001 (12%) | G Loss: 2.288198 | C Loss: -1.801602\n",
      "06/28/2022 01:27:34 - INFO - __main__ -   Text: ['The difference between Invincible and Insane is Personality.']\n",
      "06/28/2022 01:27:35 - INFO - __main__ -   Epoch: 11 | Batch: 1800/10001 (18%) | G Loss: 2.310474 | C Loss: -1.888917\n",
      "06/28/2022 01:27:35 - INFO - __main__ -   Text: ['There is a turning point in sports within statistic theory \"I\\'m an expert\".']\n",
      "06/28/2022 01:27:36 - INFO - __main__ -   Epoch: 11 | Batch: 2400/10001 (24%) | G Loss: 2.187151 | C Loss: -1.791669\n",
      "06/28/2022 01:27:36 - INFO - __main__ -   Text: ['Criticism towards people with AGI and others with TRI.']\n",
      "06/28/2022 01:27:37 - INFO - __main__ -   Epoch: 11 | Batch: 3000/10001 (30%) | G Loss: 2.013993 | C Loss: -1.602982\n",
      "06/28/2022 01:27:38 - INFO - __main__ -   Text: ['These statements are: \"We\\'re weird\".']\n",
      "06/28/2022 01:27:38 - INFO - __main__ -   Epoch: 11 | Batch: 3600/10001 (36%) | G Loss: 2.160944 | C Loss: -1.857683\n",
      "06/28/2022 01:27:39 - INFO - __main__ -   Text: ['They plan on taking male tutors.']\n",
      "06/28/2022 01:27:40 - INFO - __main__ -   Epoch: 11 | Batch: 4200/10001 (42%) | G Loss: 2.243372 | C Loss: -1.827829\n",
      "06/28/2022 01:27:40 - INFO - __main__ -   Text: ['All of the female contestants are male.\"']\n",
      "06/28/2022 01:27:41 - INFO - __main__ -   Epoch: 11 | Batch: 4800/10001 (48%) | G Loss: 2.391045 | C Loss: -1.984900\n",
      "06/28/2022 01:27:41 - INFO - __main__ -   Text: [\"However, Tejano doesn't speak Spanish well.\"]\n",
      "06/28/2022 01:27:42 - INFO - __main__ -   Epoch: 11 | Batch: 5400/10001 (54%) | G Loss: 2.005365 | C Loss: -1.742087\n",
      "06/28/2022 01:27:42 - INFO - __main__ -   Text: ['Even if you see a squirrel in my amusement park\", this will make you feel knowledgeable.']\n",
      "06/28/2022 01:27:43 - INFO - __main__ -   Epoch: 11 | Batch: 6000/10001 (60%) | G Loss: 2.249789 | C Loss: -1.826272\n",
      "06/28/2022 01:27:43 - INFO - __main__ -   Text: ['She is ill 24/7, so life is interesting .']\n",
      "06/28/2022 01:27:44 - INFO - __main__ -   Epoch: 11 | Batch: 6600/10001 (66%) | G Loss: 2.212517 | C Loss: -1.679601\n",
      "06/28/2022 01:27:44 - INFO - __main__ -   Text: ['\"That\\'s what a bottle of wine is.\"']\n",
      "06/28/2022 01:27:45 - INFO - __main__ -   Epoch: 11 | Batch: 7200/10001 (72%) | G Loss: 2.079367 | C Loss: -1.744542\n",
      "06/28/2022 01:27:45 - INFO - __main__ -   Text: ['Gyula uses a rationalization strategy and specifically offs.\"']\n",
      "06/28/2022 01:27:46 - INFO - __main__ -   Epoch: 11 | Batch: 7800/10001 (78%) | G Loss: 2.273471 | C Loss: -1.906283\n",
      "06/28/2022 01:27:46 - INFO - __main__ -   Text: ['I also think the slang term is \"Sim.']\n",
      "06/28/2022 01:27:47 - INFO - __main__ -   Epoch: 11 | Batch: 8400/10001 (84%) | G Loss: 2.162427 | C Loss: -1.855152\n",
      "06/28/2022 01:27:47 - INFO - __main__ -   Text: ['Wither they may perceive a corporate wedding.']\n",
      "06/28/2022 01:27:48 - INFO - __main__ -   Epoch: 11 | Batch: 9000/10001 (90%) | G Loss: 2.161739 | C Loss: -1.775639\n",
      "06/28/2022 01:27:48 - INFO - __main__ -   Text: ['It does not say anything about race or sex.\"']\n",
      "06/28/2022 01:27:49 - INFO - __main__ -   Epoch: 11 | Batch: 9600/10001 (96%) | G Loss: 2.108677 | C Loss: -1.668581\n",
      "06/28/2022 01:27:49 - INFO - __main__ -   Text: ['!']\n",
      "06/28/2022 01:27:50 - INFO - __main__ -   * (Train) Epoch: 11 | G Loss: 2.2113 | C Loss: -1.8177 | Updates G: 266 | Updates C: 567\n",
      "06/28/2022 01:27:58 - INFO - __main__ -   Bleu-2:0.227 | B-Bleu-2:0.264\n",
      "06/28/2022 01:27:58 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_12.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4911365195798898\n",
      "Train file used is number 12\n",
      "../../yahoo/subdivided_large/train_12.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 12 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    120.    Elapsed: 0:00:14.\n",
      "  Batch    20  of    120.    Elapsed: 0:00:29.\n",
      "  Batch    30  of    120.    Elapsed: 0:00:43.\n",
      "  Batch    40  of    120.    Elapsed: 0:00:58.\n",
      "  Batch    50  of    120.    Elapsed: 0:01:13.\n",
      "  Batch    60  of    120.    Elapsed: 0:01:28.\n",
      "  Batch    70  of    120.    Elapsed: 0:01:44.\n",
      "  Batch    80  of    120.    Elapsed: 0:01:58.\n",
      "  Batch    90  of    120.    Elapsed: 0:02:14.\n",
      "  Batch   100  of    120.    Elapsed: 0:02:28.\n",
      "  Batch   110  of    120.    Elapsed: 0:02:42.\n",
      "\n",
      "  Average training loss generetor: 0.704\n",
      "  Average training loss discriminator: 1.112\n",
      "  Training epcoh took: 0:02:57\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 01:30:55 - INFO - __main__ -   Epoch: 12 | Batch: 0/10001 (0%) | G Loss: 2.266788 | C Loss: -1.771801\n",
      "06/28/2022 01:30:55 - INFO - __main__ -   Text: [\"While I'm not familiar with the kaza yader, I am really interested.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.335\n",
      "  Test Loss: 2.063\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 01:30:56 - INFO - __main__ -   Epoch: 12 | Batch: 600/10001 (6%) | G Loss: 2.120735 | C Loss: -1.685886\n",
      "06/28/2022 01:30:56 - INFO - __main__ -   Text: [\"At such a time there is also the cause of 'Bird Fever'.\"]\n",
      "06/28/2022 01:30:57 - INFO - __main__ -   Epoch: 12 | Batch: 1200/10001 (12%) | G Loss: 2.075426 | C Loss: -1.653611\n",
      "06/28/2022 01:30:58 - INFO - __main__ -   Text: ['\"Lawrence\", referred to as the \"law good-doing judges\".']\n",
      "06/28/2022 01:30:58 - INFO - __main__ -   Epoch: 12 | Batch: 1800/10001 (18%) | G Loss: 2.118189 | C Loss: -1.747878\n",
      "06/28/2022 01:30:59 - INFO - __main__ -   Text: ['\", which can be found in this Broadway show.']\n",
      "06/28/2022 01:31:00 - INFO - __main__ -   Epoch: 12 | Batch: 2400/10001 (24%) | G Loss: 1.951202 | C Loss: -1.582525\n",
      "06/28/2022 01:31:00 - INFO - __main__ -   Text: [\"'Macphersonism' is an imaginary one.\"]\n",
      "06/28/2022 01:31:01 - INFO - __main__ -   Epoch: 12 | Batch: 3000/10001 (30%) | G Loss: 2.223698 | C Loss: -1.758448\n",
      "06/28/2022 01:31:01 - INFO - __main__ -   Text: ['The reason for this particular episode is; angling the dog...']\n",
      "06/28/2022 01:31:02 - INFO - __main__ -   Epoch: 12 | Batch: 3600/10001 (36%) | G Loss: 1.888161 | C Loss: -1.536276\n",
      "06/28/2022 01:31:02 - INFO - __main__ -   Text: ['The best known is Bratz L√∂bershi\\'s Test.\"']\n",
      "06/28/2022 01:31:03 - INFO - __main__ -   Epoch: 12 | Batch: 4200/10001 (42%) | G Loss: 2.094269 | C Loss: -1.653154\n",
      "06/28/2022 01:31:03 - INFO - __main__ -   Text: ['Such as \" parentaging.']\n",
      "06/28/2022 01:31:04 - INFO - __main__ -   Epoch: 12 | Batch: 4800/10001 (48%) | G Loss: 1.969813 | C Loss: -1.464276\n",
      "06/28/2022 01:31:04 - INFO - __main__ -   Text: ['He recently discovered foreign galaxies.']\n",
      "06/28/2022 01:31:05 - INFO - __main__ -   Epoch: 12 | Batch: 5400/10001 (54%) | G Loss: 1.977765 | C Loss: -1.416616\n",
      "06/28/2022 01:31:05 - INFO - __main__ -   Text: ['Some people call it \"emo\".']\n",
      "06/28/2022 01:31:06 - INFO - __main__ -   Epoch: 12 | Batch: 6000/10001 (60%) | G Loss: 2.152356 | C Loss: -1.610034\n",
      "06/28/2022 01:31:06 - INFO - __main__ -   Text: ['It\\'s deal with math\".']\n",
      "06/28/2022 01:31:07 - INFO - __main__ -   Epoch: 12 | Batch: 6600/10001 (66%) | G Loss: 2.071707 | C Loss: -1.645255\n",
      "06/28/2022 01:31:07 - INFO - __main__ -   Text: ['Species required : dozing.']\n",
      "06/28/2022 01:31:08 - INFO - __main__ -   Epoch: 12 | Batch: 7200/10001 (72%) | G Loss: 1.991477 | C Loss: -1.534863\n",
      "06/28/2022 01:31:08 - INFO - __main__ -   Text: ['Turning on TV will force you to balance.']\n",
      "06/28/2022 01:31:09 - INFO - __main__ -   Epoch: 12 | Batch: 7800/10001 (78%) | G Loss: 2.062614 | C Loss: -1.555966\n",
      "06/28/2022 01:31:09 - INFO - __main__ -   Text: ['The Goddess creates Persona by chanting \"God\".']\n",
      "06/28/2022 01:31:10 - INFO - __main__ -   Epoch: 12 | Batch: 8400/10001 (84%) | G Loss: 1.718862 | C Loss: -1.335346\n",
      "06/28/2022 01:31:10 - INFO - __main__ -   Text: ['<br> <br> <@p> ...</br>']\n",
      "06/28/2022 01:31:11 - INFO - __main__ -   Epoch: 12 | Batch: 9000/10001 (90%) | G Loss: 1.951453 | C Loss: -1.658758\n",
      "06/28/2022 01:31:11 - INFO - __main__ -   Text: ['Up to this week, \"Cygnus\".']\n",
      "06/28/2022 01:31:12 - INFO - __main__ -   Epoch: 12 | Batch: 9600/10001 (96%) | G Loss: 1.632002 | C Loss: -1.332562\n",
      "06/28/2022 01:31:12 - INFO - __main__ -   Text: [\"It can be described as 'The Playboy lifestyle'.\"]\n",
      "06/28/2022 01:31:13 - INFO - __main__ -   * (Train) Epoch: 12 | G Loss: 2.0034 | C Loss: -1.5809 | Updates G: 242 | Updates C: 591\n",
      "06/28/2022 01:31:21 - INFO - __main__ -   Bleu-2:0.219 | B-Bleu-2:0.286\n",
      "06/28/2022 01:31:21 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_13.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5046024794987829\n",
      "Train file used is number 13\n",
      "../../yahoo/subdivided_large/train_13.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 13 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    120.    Elapsed: 0:00:16.\n",
      "  Batch    20  of    120.    Elapsed: 0:00:32.\n",
      "  Batch    30  of    120.    Elapsed: 0:00:48.\n",
      "  Batch    40  of    120.    Elapsed: 0:01:03.\n",
      "  Batch    50  of    120.    Elapsed: 0:01:18.\n",
      "  Batch    60  of    120.    Elapsed: 0:01:33.\n",
      "  Batch    70  of    120.    Elapsed: 0:01:49.\n",
      "  Batch    80  of    120.    Elapsed: 0:02:03.\n",
      "  Batch    90  of    120.    Elapsed: 0:02:18.\n",
      "  Batch   100  of    120.    Elapsed: 0:02:34.\n",
      "  Batch   110  of    120.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss generetor: 0.703\n",
      "  Average training loss discriminator: 1.035\n",
      "  Training epcoh took: 0:03:04\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 01:34:25 - INFO - __main__ -   Epoch: 13 | Batch: 0/10001 (0%) | G Loss: 1.791413 | C Loss: -1.384568\n",
      "06/28/2022 01:34:26 - INFO - __main__ -   Text: [\"Let's say in a Break the fuck to have a conversation.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.345\n",
      "  Test Loss: 2.113\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 01:34:27 - INFO - __main__ -   Epoch: 13 | Batch: 600/10001 (6%) | G Loss: 1.786990 | C Loss: -1.443233\n",
      "06/28/2022 01:34:27 - INFO - __main__ -   Text: ['That\\'s how Steve kills himself.\"']\n",
      "06/28/2022 01:34:28 - INFO - __main__ -   Epoch: 13 | Batch: 1200/10001 (12%) | G Loss: 1.729210 | C Loss: -1.416513\n",
      "06/28/2022 01:34:28 - INFO - __main__ -   Text: ['In my books, \"The Cage Body\" revolves on a morality.']\n",
      "06/28/2022 01:34:29 - INFO - __main__ -   Epoch: 13 | Batch: 1800/10001 (18%) | G Loss: 1.953499 | C Loss: -1.557848\n",
      "06/28/2022 01:34:29 - INFO - __main__ -   Text: ['This is not a quiz though.']\n",
      "06/28/2022 01:34:30 - INFO - __main__ -   Epoch: 13 | Batch: 2400/10001 (24%) | G Loss: 1.864702 | C Loss: -1.510411\n",
      "06/28/2022 01:34:30 - INFO - __main__ -   Text: ['Many think it is a human male love song.']\n",
      "06/28/2022 01:34:31 - INFO - __main__ -   Epoch: 13 | Batch: 3000/10001 (30%) | G Loss: 2.002443 | C Loss: -1.592843\n",
      "06/28/2022 01:34:31 - INFO - __main__ -   Text: ['The views of humor are basically projected on humans.']\n",
      "06/28/2022 01:34:32 - INFO - __main__ -   Epoch: 13 | Batch: 3600/10001 (36%) | G Loss: 1.892648 | C Loss: -1.446932\n",
      "06/28/2022 01:34:32 - INFO - __main__ -   Text: ['Savage thirst dictates to search is probably the worst idea.']\n",
      "06/28/2022 01:34:33 - INFO - __main__ -   Epoch: 13 | Batch: 4200/10001 (42%) | G Loss: 1.911575 | C Loss: -1.473456\n",
      "06/28/2022 01:34:33 - INFO - __main__ -   Text: [\"They're mad at everybody?\"]\n",
      "06/28/2022 01:34:34 - INFO - __main__ -   Epoch: 13 | Batch: 4800/10001 (48%) | G Loss: 1.697976 | C Loss: -1.491836\n",
      "06/28/2022 01:34:34 - INFO - __main__ -   Text: ['There is no cure for cancer.']\n",
      "06/28/2022 01:34:35 - INFO - __main__ -   Epoch: 13 | Batch: 5400/10001 (54%) | G Loss: 1.868488 | C Loss: -1.438138\n",
      "06/28/2022 01:34:35 - INFO - __main__ -   Text: ['Bella may well say \"Smashball.\"']\n",
      "06/28/2022 01:34:36 - INFO - __main__ -   Epoch: 13 | Batch: 6000/10001 (60%) | G Loss: 1.666685 | C Loss: -1.374899\n",
      "06/28/2022 01:34:36 - INFO - __main__ -   Text: ['One can make a fascination of this magazine technology to further his knowledge.']\n",
      "06/28/2022 01:34:37 - INFO - __main__ -   Epoch: 13 | Batch: 6600/10001 (66%) | G Loss: 1.851834 | C Loss: -1.433268\n",
      "06/28/2022 01:34:37 - INFO - __main__ -   Text: ['Other peoples claim \"facts is the only way out\" .']\n",
      "06/28/2022 01:34:38 - INFO - __main__ -   Epoch: 13 | Batch: 7200/10001 (72%) | G Loss: 1.788471 | C Loss: -1.235789\n",
      "06/28/2022 01:34:38 - INFO - __main__ -   Text: [\"This is not the metahumans' motto.\"]\n",
      "06/28/2022 01:34:39 - INFO - __main__ -   Epoch: 13 | Batch: 7800/10001 (78%) | G Loss: 1.750142 | C Loss: -1.245313\n",
      "06/28/2022 01:34:39 - INFO - __main__ -   Text: ['The color name is serious\".']\n",
      "06/28/2022 01:34:40 - INFO - __main__ -   Epoch: 13 | Batch: 8400/10001 (84%) | G Loss: 1.717796 | C Loss: -1.329844\n",
      "06/28/2022 01:34:40 - INFO - __main__ -   Text: ['Alternatively, when I say I need to run an accelerator... <BOS> I mean it.']\n",
      "06/28/2022 01:34:41 - INFO - __main__ -   Epoch: 13 | Batch: 9000/10001 (90%) | G Loss: 1.722024 | C Loss: -1.363043\n",
      "06/28/2022 01:34:41 - INFO - __main__ -   Text: [\"I'll guess this is gonna be what I want.\"]\n",
      "06/28/2022 01:34:42 - INFO - __main__ -   Epoch: 13 | Batch: 9600/10001 (96%) | G Loss: 1.832390 | C Loss: -1.313200\n",
      "06/28/2022 01:34:42 - INFO - __main__ -   Text: ['e.g.']\n",
      "06/28/2022 01:34:43 - INFO - __main__ -   * (Train) Epoch: 13 | G Loss: 1.8013 | C Loss: -1.3772 | Updates G: 223 | Updates C: 610\n",
      "06/28/2022 01:34:51 - INFO - __main__ -   Bleu-2:0.232 | B-Bleu-2:0.268\n",
      "06/28/2022 01:34:51 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_14.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5003474560911353\n",
      "Train file used is number 14\n",
      "../../yahoo/subdivided_large/train_14.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 14 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    120.    Elapsed: 0:00:15.\n",
      "  Batch    20  of    120.    Elapsed: 0:00:31.\n",
      "  Batch    30  of    120.    Elapsed: 0:00:47.\n",
      "  Batch    40  of    120.    Elapsed: 0:01:02.\n",
      "  Batch    50  of    120.    Elapsed: 0:01:18.\n",
      "  Batch    60  of    120.    Elapsed: 0:01:34.\n",
      "  Batch    70  of    120.    Elapsed: 0:01:49.\n",
      "  Batch    80  of    120.    Elapsed: 0:02:03.\n",
      "  Batch    90  of    120.    Elapsed: 0:02:19.\n",
      "  Batch   100  of    120.    Elapsed: 0:02:35.\n",
      "  Batch   110  of    120.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss generetor: 0.704\n",
      "  Average training loss discriminator: 0.970\n",
      "  Training epcoh took: 0:03:05\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 01:37:57 - INFO - __main__ -   Epoch: 14 | Batch: 0/10001 (0%) | G Loss: 1.783701 | C Loss: -1.254313\n",
      "06/28/2022 01:37:57 - INFO - __main__ -   Text: ['Some academics believe ! <PAD> ).']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.347\n",
      "  Test Loss: 2.135\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2022 01:37:58 - INFO - __main__ -   Epoch: 14 | Batch: 600/10001 (6%) | G Loss: 1.818893 | C Loss: -1.254474\n",
      "06/28/2022 01:37:58 - INFO - __main__ -   Text: ['It is also known as the hunch test.']\n",
      "06/28/2022 01:37:59 - INFO - __main__ -   Epoch: 14 | Batch: 1200/10001 (12%) | G Loss: 1.520453 | C Loss: -1.176982\n",
      "06/28/2022 01:37:59 - INFO - __main__ -   Text: ['This is something that works in both worlds.\"']\n",
      "06/28/2022 01:38:00 - INFO - __main__ -   Epoch: 14 | Batch: 1800/10001 (18%) | G Loss: 1.647090 | C Loss: -1.359046\n",
      "06/28/2022 01:38:00 - INFO - __main__ -   Text: ['']\n",
      "06/28/2022 01:38:01 - INFO - __main__ -   Epoch: 14 | Batch: 2400/10001 (24%) | G Loss: 1.631419 | C Loss: -1.207303\n",
      "06/28/2022 01:38:01 - INFO - __main__ -   Text: ['As an example, notice how \"Horrible Shit moves on\".']\n",
      "06/28/2022 01:38:02 - INFO - __main__ -   Epoch: 14 | Batch: 3000/10001 (30%) | G Loss: 1.560876 | C Loss: -1.192172\n",
      "06/28/2022 01:38:02 - INFO - __main__ -   Text: ['\"Olympia is purely about extreme disasters\".']\n",
      "06/28/2022 01:38:03 - INFO - __main__ -   Epoch: 14 | Batch: 3600/10001 (36%) | G Loss: 1.750997 | C Loss: -1.401427\n",
      "06/28/2022 01:38:03 - INFO - __main__ -   Text: ['They have to guess that the ringworm has Down syndrome.']\n",
      "06/28/2022 01:38:04 - INFO - __main__ -   Epoch: 14 | Batch: 4200/10001 (42%) | G Loss: 1.670233 | C Loss: -1.244685\n",
      "06/28/2022 01:38:04 - INFO - __main__ -   Text: ['This is something that the phrase \"Motherfuckers creates\".']\n",
      "06/28/2022 01:38:05 - INFO - __main__ -   Epoch: 14 | Batch: 4800/10001 (48%) | G Loss: 1.826747 | C Loss: -1.324218\n",
      "06/28/2022 01:38:06 - INFO - __main__ -   Text: ['All of us, if we understand math, can learn.']\n",
      "06/28/2022 01:38:07 - INFO - __main__ -   Epoch: 14 | Batch: 5400/10001 (54%) | G Loss: 1.670131 | C Loss: -1.244645\n",
      "06/28/2022 01:38:07 - INFO - __main__ -   Text: ['\"Soul is hard to read at that time.\"']\n",
      "06/28/2022 01:38:08 - INFO - __main__ -   Epoch: 14 | Batch: 6000/10001 (60%) | G Loss: 1.631410 | C Loss: -1.222282\n",
      "06/28/2022 01:38:08 - INFO - __main__ -   Text: ['I think it IS Buzz.']\n",
      "06/28/2022 01:38:09 - INFO - __main__ -   Epoch: 14 | Batch: 6600/10001 (66%) | G Loss: 1.623668 | C Loss: -1.209938\n",
      "06/28/2022 01:38:09 - INFO - __main__ -   Text: ['The problem for Catholicism diverges a great deal from Marxist.']\n",
      "06/28/2022 01:38:10 - INFO - __main__ -   Epoch: 14 | Batch: 7200/10001 (72%) | G Loss: 1.584998 | C Loss: -1.126494\n",
      "06/28/2022 01:38:10 - INFO - __main__ -   Text: ['It\\'s an absurdity.\"']\n",
      "06/28/2022 01:38:11 - INFO - __main__ -   Epoch: 14 | Batch: 7800/10001 (78%) | G Loss: 1.648790 | C Loss: -1.166272\n",
      "06/28/2022 01:38:11 - INFO - __main__ -   Text: ['Planetside Call is the big priority in hiring,\" says Jennifer Raj.']\n",
      "06/28/2022 01:38:12 - INFO - __main__ -   Epoch: 14 | Batch: 8400/10001 (84%) | G Loss: 1.620044 | C Loss: -1.205092\n",
      "06/28/2022 01:38:12 - INFO - __main__ -   Text: ['This is traditionally the Leap to the Top factor.']\n",
      "06/28/2022 01:38:13 - INFO - __main__ -   Epoch: 14 | Batch: 9000/10001 (90%) | G Loss: 1.649007 | C Loss: -1.228420\n",
      "06/28/2022 01:38:13 - INFO - __main__ -   Text: ['On a scene like age of entitlement Mr. Lupus (seemingly as I personified).']\n",
      "06/28/2022 01:38:14 - INFO - __main__ -   Epoch: 14 | Batch: 9600/10001 (96%) | G Loss: 1.561419 | C Loss: -1.111835\n",
      "06/28/2022 01:38:14 - INFO - __main__ -   Text: ['This involves probing for hidden wisdom.']\n",
      "06/28/2022 01:38:15 - INFO - __main__ -   * (Train) Epoch: 14 | G Loss: 1.6631 | C Loss: -1.2342 | Updates G: 199 | Updates C: 634\n",
      "06/28/2022 01:38:23 - INFO - __main__ -   Bleu-2:0.194 | B-Bleu-2:0.238\n",
      "06/28/2022 01:38:23 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_15.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4324452656977462\n",
      "Train file used is number 15\n",
      "../../yahoo/subdivided_large/train_15.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 15 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    120.    Elapsed: 0:00:15.\n",
      "  Batch    20  of    120.    Elapsed: 0:00:30.\n",
      "  Batch    30  of    120.    Elapsed: 0:00:45.\n",
      "  Batch    40  of    120.    Elapsed: 0:01:00.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--seed', type=int, default=0)\n",
    "    parser.add_argument('--epochs', type=int, default=15)\n",
    "    parser.add_argument('--lr', type=float, default=1e-4)\n",
    "    parser.add_argument('--gp_lambda', type=int, default=10)\n",
    "    parser.add_argument('--n_layers', type=int, default=20, help=\"Number of layers of generator and critic\")\n",
    "    parser.add_argument('--block_dim', type=int, default=100)\n",
    "    parser.add_argument('--interval', type=int, default=10, help=\"Steps before logging output\")\n",
    "    parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())\n",
    "    \n",
    "    # Optimus parameters\n",
    "    parser.add_argument(\"--train_data_file\", default=None, type=str, required=True,\n",
    "                        help=\"The input training data file (a text file).\")\n",
    "    parser.add_argument(\"--valid_data_file\", default=None, type=str, required=True,\n",
    "                        help=\"The input validation data file (a text file).\")\n",
    "    parser.add_argument(\"--checkpoint_dir\", default=None, type=str, required=True,\n",
    "                        help=\"The directory where checkpoints are saved.\")\n",
    "    parser.add_argument('--generator_dir', default=None, type=str, help=\"Directory where GAN models are saved\")\n",
    "    parser.add_argument(\"--output_dir\", default=None, type=str, required=True,\n",
    "                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
    "    parser.add_argument(\"--dataset\", default='Snli', type=str, help=\"The dataset.\")    \n",
    "    parser.add_argument(\"--latent_size\", default=32, type=int, help=\"Latent space dimension.\")\n",
    "    ## Encoder options\n",
    "    parser.add_argument(\"--encoder_model_type\", default=\"bert\", type=str,\n",
    "                        help=\"The encoder model architecture to be fine-tuned.\")\n",
    "    parser.add_argument(\"--encoder_model_name_or_path\", default=\"bert-base-cased\", type=str,\n",
    "                        help=\"The encoder model checkpoint for weights initialization.\")\n",
    "    parser.add_argument(\"--encoder_config_name\", default=\"\", type=str,\n",
    "                        help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
    "    parser.add_argument(\"--encoder_tokenizer_name\", default=\"\", type=str,\n",
    "                        help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
    "    ## Decoder options\n",
    "    parser.add_argument(\"--decoder_model_type\", default=\"gpt2\", type=str,\n",
    "                        help=\"The decoder model architecture to be fine-tuned.\")\n",
    "    parser.add_argument(\"--decoder_model_name_or_path\", default=\"bert-base-cased\", type=str,\n",
    "                        help=\"The decoder model checkpoint for weights initialization.\")\n",
    "    parser.add_argument(\"--decoder_config_name\", default=\"\", type=str,\n",
    "                        help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
    "    parser.add_argument(\"--decoder_tokenizer_name\", default=\"\", type=str,\n",
    "                        help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
    "    parser.add_argument(\"--per_gpu_train_batch_size\", default=1, type=int,\n",
    "                        help=\"Batch size per GPU/CPU for training.\")\n",
    "    parser.add_argument(\"--max_seq_length\", default=512, type=int,\n",
    "                        help=\"Optional input sequence length before tokenization. The sequence will be dropped if it is longer the max_seq_length\")\n",
    "\n",
    "    ## Variational auto-encoder(check this)\n",
    "    parser.add_argument(\"--prompt\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--padding_text\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--length\", type=int, default=20)\n",
    "    parser.add_argument(\"--block_size\", default=-1, type=int,\n",
    "                        help=\"Optional input sequence length after tokenization.\"\n",
    "                             \"The training dataset will be truncated in block of this size for training.\"\n",
    "                             \"Default to the model max input length for single sentence inputs (take into account special tokens).\")\n",
    "    parser.add_argument(\"--do_lower_case\", action='store_true',\n",
    "                        help=\"Set this flag if you are using an uncased model.\")\n",
    "    parser.add_argument(\"--use_philly\", action='store_true',\n",
    "                        help=\"Use Philly for computing.\")\n",
    "    parser.add_argument('--gloabl_step_eval', type=int, default=661,\n",
    "                        help=\"Evaluate the results at the given global step\")\n",
    "    # Reinforcement learning parameters\n",
    "    parser.add_argument('--finetune_decoder', type=bool, default=True)\n",
    "    parser.add_argument('--epochs_rl', type=int, default=1000)\n",
    "    parser.add_argument('--batch_size_rl', type=int, default=32)\n",
    "    parser.add_argument('--lr_rl', type=float, default=1e-6)\n",
    "\n",
    "\n",
    "    # Load a trained Encoder model and vocabulary that you have fine-tuned\n",
    "    args = parser.parse_args(\"--dataset EMNLP \\\n",
    "    --checkpoint_dir=output_dir_768_0_unsure \\\n",
    "    --output_dir=output_dir_768_0_unsure \\\n",
    "    --encoder_model_type=bert \\\n",
    "    --encoder_model_name_or_path=bert-base-cased \\\n",
    "    --decoder_model_type=gpt2 \\\n",
    "    --decoder_model_name_or_path=gpt2 \\\n",
    "    --train_data_file=../../yahoo/subdivided_large/train \\\n",
    "    --valid_data_file=../../yahoo/unlabelled_short/test.txt \\\n",
    "    --per_gpu_train_batch_size 12 \\\n",
    "    --block_size 100 \\\n",
    "    --max_seq_length 24 \\\n",
    "    --gloabl_step_eval 508523 \\\n",
    "    --latent_size 768 \\\n",
    "    --block_dim 100 \\\n",
    "    --n_layers 10 \\\n",
    "    --interval 50 \\\n",
    "    --epochs 200 \\\n",
    "    --finetune_decoder False \\\n",
    "    --lr_rl 1e-6 \\\n",
    "    --epochs_rl 100 \\\n",
    "    --batch_size_rl 32\".split())\n",
    "    \n",
    "    print(args)\n",
    "\n",
    "    global_step = args.gloabl_step_eval\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    #args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    #args.n_gpu = torch.cuda.device_count()\n",
    "    args.device = torch.device(\"cuda:0\")\n",
    "    args.n_gpu=1\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)       \n",
    "    \n",
    "    args.encoder_model_type = args.encoder_model_type.lower()\n",
    "    args.decoder_model_type = args.decoder_model_type.lower()\n",
    "\n",
    "    output_encoder_dir = os.path.join(args.checkpoint_dir, 'checkpoint-encoder-{}'.format(global_step))\n",
    "    output_decoder_dir = os.path.join(args.checkpoint_dir, 'checkpoint-decoder-{}'.format(global_step)) \n",
    "    checkpoints = [ [output_encoder_dir, output_decoder_dir] ]\n",
    "\n",
    "    # Load a trained Encoder model and vocabulary that you have fine-tuned\n",
    "    encoder_config_class, encoder_model_class, encoder_tokenizer_class = MODEL_CLASSES[args.encoder_model_type]\n",
    "    model_encoder = encoder_model_class.from_pretrained(output_encoder_dir, latent_size=args.latent_size)\n",
    "    tokenizer_encoder = encoder_tokenizer_class.from_pretrained(args.encoder_tokenizer_name if args.encoder_tokenizer_name else args.encoder_model_name_or_path, do_lower_case=args.do_lower_case)\n",
    "\n",
    "    model_encoder.to(args.device)\n",
    "    if args.block_size <= 0:\n",
    "        args.block_size = tokenizer_encoder.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
    "    args.block_size = min(args.block_size, tokenizer_encoder.max_len_single_sentence)\n",
    "\n",
    "    # Load a trained Decoder model and vocabulary that you have fine-tuned\n",
    "    decoder_config_class, decoder_model_class, decoder_tokenizer_class = MODEL_CLASSES[args.decoder_model_type]\n",
    "    model_decoder = decoder_model_class.from_pretrained(output_decoder_dir, latent_size=args.latent_size)\n",
    "    tokenizer_decoder = decoder_tokenizer_class.from_pretrained(args.decoder_tokenizer_name if args.decoder_tokenizer_name else args.decoder_model_name_or_path, do_lower_case=args.do_lower_case)\n",
    "    model_decoder.to(args.device)\n",
    "    if args.block_size <= 0:\n",
    "        args.block_size = tokenizer_decoder.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
    "    args.block_size = min(args.block_size, tokenizer_decoder.max_len_single_sentence)\n",
    "\n",
    "    # Chunyuan: Add Padding token to GPT2\n",
    "    special_tokens_dict = {'pad_token': '<PAD>', 'bos_token': '<BOS>', 'eos_token': '<EOS>'}\n",
    "    num_added_toks = tokenizer_decoder.add_special_tokens(special_tokens_dict)\n",
    "    logger.info('We have added {} tokens to GPT2'.format(num_added_toks))\n",
    "    model_decoder.resize_token_embeddings(len(tokenizer_decoder))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n",
    "    assert tokenizer_decoder.pad_token == '<PAD>'\n",
    "\n",
    "    #train_loader, num_txt = build_dataload_and_cache_examples(args, [tokenizer_encoder, tokenizer_decoder], num_txt) \n",
    "    generator = Generator(args.n_layers, args.block_dim,args.latent_size)\n",
    "    critic = Critic(args.n_layers, args.block_dim,args.latent_size)\n",
    "\n",
    "    if args.generator_dir!=None:\n",
    "        logger.info(\"Loading generator and critic\")\n",
    "        generator.load_state_dict(torch.load(args.generator_dir+'/generator_'+str(args.gloabl_step_eval)+'.th'))\n",
    "        critic.load_state_dict(torch.load(args.generator_dir+'/critic_'+str(args.gloabl_step_eval)+'.th'))\n",
    "\n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=args.lr, betas=(0.5, 0.999))\n",
    "    c_optimizer = optim.Adam(critic.parameters(), lr=args.lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    if args.cuda:\n",
    "        generator = generator.cuda()\n",
    "        critic = critic.cuda()\n",
    "    \n",
    "    logger.info('G Parameters:{}'.format(sum([p.numel() for p in generator.parameters() if \\\n",
    "                                p.requires_grad])))\n",
    "    logger.info('C Parameters:{}'.format(sum([p.numel() for p in critic.parameters() if \\\n",
    "                                p.requires_grad])))\n",
    "    \n",
    "    device = args.device\n",
    "    \n",
    "    best_bleu = 0\n",
    "    reference = list()\n",
    "    with(open(args.valid_data_file,\"r\")) as valid:\n",
    "        for sents in valid:\n",
    "            reference.append(sents.replace(\"\\n\", \"\"))\n",
    "            \n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        \n",
    "        #Insert GAN-BERT Code Here\n",
    "        \n",
    "        train_loader, num_txt = build_dataload_and_cache_examples(args, [tokenizer_encoder, tokenizer_decoder], num_txt) \n",
    "        \n",
    "        print(\"Train classification discriminator\")\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        # Perform one full pass over the training set.\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch, args.epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        tr_g_loss = 0\n",
    "        tr_d_loss = 0\n",
    "\n",
    "        # Put the model into training mode.\n",
    "        transformer.train() \n",
    "        #generator.train()\n",
    "        discriminator.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Progress update every print_each_n_step batches.\n",
    "            if step % print_each_n_step == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            b_label_mask = batch[3].to(device)\n",
    "\n",
    "            real_batch_size = b_input_ids.shape[0]\n",
    "\n",
    "            # Encode real data in the Transformer\n",
    "            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
    "            hidden_states = model_outputs.last_hidden_state[:,0,:] \n",
    "            #hidden_states = model_outputs[-1]\n",
    "            #print(\"  Number of real sentences (labelled and unlabelled): {}\".format(len(hidden_states)))\n",
    "            \n",
    "            # Generate fake data that should have the same distribution of the ones\n",
    "            # encoded by the transformer. \n",
    "            # First noisy input are used in input to the Generator\n",
    "            fixed_noise = torch.Tensor(np.random.normal(0, 1, (real_batch_size, args.latent_size))).to(args.device)\n",
    "            test_z_gb = generator(fixed_noise).data\n",
    "            fake_sentences = rollout_test(model_decoder, test_z_gb, tokenizer_decoder, args.max_seq_length, real_batch_size, 0, 1)\n",
    "            #print(\"  Number of generated sentences: {}\".format(len(fake_sentences)))\n",
    "\n",
    "            b_input_ids_fake, b_input_mask_fake = generate_data_fake(fake_sentences)\n",
    "            model_outputs_fake = transformer(b_input_ids_fake, attention_mask=b_input_mask_fake)\n",
    "            hidden_states_fake = model_outputs_fake.last_hidden_state[:,0,:] \n",
    "            #hidden_states_fake = model_outputs_fake[-1]\n",
    "\n",
    "            #noise = torch.zeros(real_batch_size, noise_size, device=device).uniform_(0, 1)\n",
    "            # Gnerate Fake data\n",
    "            #gen_rep = generator(noise)\n",
    "            #print(\"Length of generator output {}\".format(len(gen_rep)))\n",
    "            #print(\"Length of single generator output {}\".format(len(gen_rep[0])))\n",
    "\n",
    "            # Generate the output of the Discriminator for real and fake data.\n",
    "            # First, we put together the output of the tranformer and the generator\n",
    "            disciminator_input = torch.cat([hidden_states, hidden_states_fake], dim=0)\n",
    "            # Then, we select the output of the disciminator\n",
    "            features, logits, probs = discriminator(disciminator_input)\n",
    "\n",
    "            # Finally, we separate the discriminator's output for the real and fake\n",
    "            # data\n",
    "            features_list = torch.split(features, real_batch_size)\n",
    "            D_real_features = features_list[0]\n",
    "            D_fake_features = features_list[1]\n",
    "\n",
    "            logits_list = torch.split(logits, real_batch_size)\n",
    "            D_real_logits = logits_list[0]\n",
    "            D_fake_logits = logits_list[1]\n",
    "\n",
    "            probs_list = torch.split(probs, real_batch_size)\n",
    "            D_real_probs = probs_list[0]\n",
    "            D_fake_probs = probs_list[1]\n",
    "\n",
    "            #---------------------------------\n",
    "            #  LOSS evaluation\n",
    "            #---------------------------------\n",
    "            # Generator's LOSS estimation\n",
    "            g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + epsilon))\n",
    "            g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n",
    "            g_loss = g_loss_d + g_feat_reg\n",
    "\n",
    "            # Disciminator's LOSS estimation\n",
    "            logits = D_real_logits[:,0:-1]\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            # The discriminator provides an output for labeled and unlabeled real data\n",
    "            # so the loss evaluated for unlabeled data is ignored (masked)\n",
    "            label2one_hot = torch.nn.functional.one_hot(b_labels, len(label_list))\n",
    "            per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n",
    "            per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n",
    "            labeled_example_count = per_example_loss.type(torch.float32).numel()\n",
    "\n",
    "            # It may be the case that a batch does not contain labeled examples, \n",
    "            # so the \"supervised loss\" in this case is not evaluated\n",
    "            if labeled_example_count == 0:\n",
    "              D_L_Supervised = 0\n",
    "            else:\n",
    "              D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n",
    "\n",
    "            D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n",
    "            D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n",
    "            d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n",
    "\n",
    "            #---------------------------------\n",
    "            #  OPTIMIZATION\n",
    "            #---------------------------------\n",
    "            # Avoid gradient accumulation\n",
    "            #gen_optimizer.zero_grad()\n",
    "            dis_optimizer.zero_grad()\n",
    "\n",
    "            # Calculate weigth updates\n",
    "            # retain_graph=True is required since the underlying graph will be deleted after backward\n",
    "            g_loss.backward(retain_graph=True)\n",
    "            d_loss.backward() \n",
    "\n",
    "            # Apply modifications\n",
    "            #gen_optimizer.step()\n",
    "            dis_optimizer.step()\n",
    "\n",
    "            # A detail log of the individual losses\n",
    "            #print(\"{0:.4f}\\t{1:.4f}\\t{2:.4f}\\t{3:.4f}\\t{4:.4f}\".\n",
    "            #      format(D_L_Supervised, D_L_unsupervised1U, D_L_unsupervised2U,\n",
    "            #             g_loss_d, g_feat_reg))\n",
    "\n",
    "            # Save the losses to print them later\n",
    "            tr_g_loss += g_loss.item()\n",
    "            tr_d_loss += d_loss.item()\n",
    "\n",
    "            # Update the learning rate with the scheduler\n",
    "            if apply_scheduler:\n",
    "              scheduler_d.step()\n",
    "              #scheduler_g.step()\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss_g = tr_g_loss / len(train_dataloader)\n",
    "        avg_train_loss_d = tr_d_loss / len(train_dataloader)             \n",
    "\n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss generetor: {0:.3f}\".format(avg_train_loss_g))\n",
    "        print(\"  Average training loss discriminator: {0:.3f}\".format(avg_train_loss_d))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "        # ========================================\n",
    "        #     TEST ON THE EVALUATION DATASET\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our test set.\n",
    "        print(\"\")\n",
    "        print(\"Running Test...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        transformer.eval() #maybe redundant\n",
    "        discriminator.eval()\n",
    "        #generator.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_test_accuracy = 0\n",
    "\n",
    "        total_test_loss = 0\n",
    "        nb_test_steps = 0\n",
    "\n",
    "        all_preds = []\n",
    "        all_labels_ids = []\n",
    "\n",
    "        #loss\n",
    "        nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in test_dataloader:\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "                model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
    "                hidden_states = model_outputs.last_hidden_state[:,0,:] \n",
    "                #hidden_states = model_outputs[-1]\n",
    "                _, logits, probs = discriminator(hidden_states)\n",
    "                ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n",
    "                filtered_logits = logits[:,0:-1]\n",
    "                # Accumulate the test loss.\n",
    "                total_test_loss += nll_loss(filtered_logits, b_labels)\n",
    "\n",
    "            # Accumulate the predictions and the input labels\n",
    "            _, preds = torch.max(filtered_logits, 1)\n",
    "            all_preds += preds.detach().cpu()\n",
    "            all_labels_ids += b_labels.detach().cpu()\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        all_preds = torch.stack(all_preds).numpy()\n",
    "        all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
    "        test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
    "        print(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "        avg_test_loss = avg_test_loss.item()\n",
    "\n",
    "        # Measure how long the validation run took.\n",
    "        test_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"  Test Loss: {0:.3f}\".format(avg_test_loss))\n",
    "        print(\"  Test took: {:}\".format(test_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch + 1,\n",
    "                'Training Loss generator': avg_train_loss_g,\n",
    "                'Training Loss discriminator': avg_train_loss_d,\n",
    "                'Valid. Loss': avg_test_loss,\n",
    "                'Valid. Accur.': test_accuracy,\n",
    "                'Training Time': training_time,\n",
    "                'Test Time': test_time\n",
    "            }\n",
    "        )\n",
    "\n",
    "        accuracy_array.append(test_accuracy)\n",
    "        \n",
    "        #OPTAGAN Code\n",
    "        \n",
    "        g_loss, c_loss = train(epoch)\n",
    "\n",
    "        data_test = list()\n",
    "        for i in range(2):\n",
    "            test_noise = torch.Tensor(np.random.normal(0, 1, (250, args.latent_size))).to(args.device)\n",
    "            test_z = generator(test_noise).data\n",
    "            new_sent = rollout_test(model_decoder, test_z, tokenizer_decoder, args.max_seq_length, 250, 0, 1)\n",
    "            data_test.extend(new_sent)\n",
    "\n",
    "        p_reference = random.sample(reference, 500)\n",
    "        bleu = calc_blue_parallel_func(p_reference, data_test, 2, 500)\n",
    "        b_bleu = calc_blue_parallel_func(data_test, p_reference, 2, 500)\n",
    "        logger.info(\"Bleu-2:{:0.3f} | B-Bleu-2:{:0.3f}\".format(bleu, b_bleu))\n",
    "        \n",
    "        print(bleu+b_bleu)\n",
    "        if (bleu+b_bleu) > best_bleu:\n",
    "            best_bleu = bleu + b_bleu\n",
    "            logger.info('* Saving. Best Score:{:0.3f} | Bleu-2:{:0.3f} | B-Bleu-2:{:0.3f}'.format(best_bleu, bleu, b_bleu))\n",
    "            torch.save(generator.state_dict(), args.output_dir+'/generator_'+str(args.gloabl_step_eval)+'.th')\n",
    "            torch.save(critic.state_dict(), args.output_dir+'/critic_'+str(args.gloabl_step_eval)+'.th')\n",
    "            \n",
    "        \n",
    "\n",
    "    if args.finetune_decoder: \n",
    "        logger.info(\"Loading generator\")\n",
    "        generator.load_state_dict(torch.load(args.output_dir+'/generator_'+str(args.gloabl_step_eval)+'.th'))\n",
    "        \n",
    "        model_decoder.train()\n",
    "        generator.eval()\n",
    "        dec_optimizer = optim.Adam(model_decoder.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "        value_loss = nn.L1Loss()\n",
    "        B = args.batch_size_rl\n",
    "        total_scores = 0\n",
    "        total_entropy = 0\n",
    "        total_values = 0\n",
    "        total_v_loss = 0\n",
    "        for epoch_ in range(args.epochs_rl):\n",
    "            if epoch_ == 200:\n",
    "                # Finetune decoder after training of value head\n",
    "                dec_optimizer = optim.Adam(model_decoder.parameters(), lr=args.lr_rl, betas=(0.5, 0.999))\n",
    "            noise = torch.from_numpy(np.random.normal(0, 1, (B, args.latent_size))).float()\n",
    "            noise = noise.to(args.device)\n",
    "            z_fake = generator(noise)            \n",
    "            sents, logprobs, values, entropy = rollout(model_decoder, z_fake, tokenizer_decoder, args.max_seq_length, B, 1)\n",
    "            p_reference = random.sample(reference, 500)\n",
    "\n",
    "            blue = []\n",
    "            for i in sents:\n",
    "                blue.append(calc_blue_parallel_func(p_reference, [i], 1, 0))\n",
    "\n",
    "            values = torch.stack(values, dim=1)\n",
    "            logprobs = torch.stack(logprobs, dim=1)\n",
    "            entropy = torch.stack(entropy, dim=1)\n",
    "\n",
    "            # Get tokens and mask of batch\n",
    "            toks_gpt = [([50258] + tokenizer_decoder.encode(j) + [50259]) for j in sents]\n",
    "            toks_gpt, mask = pad_seq(toks_gpt, tokenizer_decoder.encode(\"<PAD>\")[0], values.size(1)+1)\n",
    "            toks_gpt = torch.tensor(toks_gpt).to(args.device)\n",
    "            mask = torch.tensor(mask).to(args.device)\n",
    "              \n",
    "            values = values * mask[:,1:]\n",
    "            logprobs = logprobs * mask[:,1:]\n",
    "            entropy = entropy * mask[:,1:]\n",
    "            scores = torch.tensor(blue).to(args.device)\n",
    "            # Get value loss\n",
    "            v_loss = value_loss(torch.sum(values, dim=1), scores) \n",
    "              \n",
    "            if epoch_ >= 200:\n",
    "                R = 0\n",
    "                rewards = []\n",
    "\n",
    "                # Discount future rewards back to the present using gamma\n",
    "                for j in range(len(values.tolist())):\n",
    "                    R = 0\n",
    "                    batch_rewards = []\n",
    "                    for r in reversed(values.tolist()[j]):\n",
    "                        R = r + 0.99 * R\n",
    "                        batch_rewards.insert(0,R)\n",
    "                    rewards.append(batch_rewards)\n",
    "\n",
    "                # Penalizing low entropy states\n",
    "                rewards = torch.FloatTensor(rewards).to(args.device)\n",
    "                rewards = rewards + torch.log(torch.clamp(entropy,0.2,1))\n",
    "                # Calculate loss\n",
    "                d_loss = torch.sum(torch.mul(logprobs, rewards.detach()).mul(-1))\n",
    "            else:\n",
    "                d_loss = torch.tensor(0)\n",
    "\n",
    "            # Backpropagate losses\n",
    "            loss = v_loss + d_loss              \n",
    "            dec_optimizer.zero_grad()              \n",
    "            loss.backward()\n",
    "            dec_optimizer.step()\n",
    "\n",
    "            total_scores += torch.mean(scores).item()\n",
    "            total_values += torch.mean(torch.sum(values,-1)).item()\n",
    "            total_v_loss += v_loss.item()\n",
    "            total_entropy += torch.mean(torch.mean(entropy,dim=1)).item()\n",
    "            if (epoch_ % args.interval) == 0:\n",
    "                logger.info(\"Epoch {}/{} | Value Loss:{} | Mean values:{} | Mean BLEU scores:{} | Mean Entropy: {}\".format(epoch_, \n",
    "                args.epochs_rl, total_v_loss/args.interval, total_values/args.interval, total_scores/args.interval, total_entropy/args.interval))\n",
    "                total_scores = 0\n",
    "                total_values = 0\n",
    "                total_v_loss = 0\n",
    "                total_entropy = 0\n",
    "        logger.info(\"Saving decoder\")\n",
    "        output_decoder_dir = os.path.join(args.output_dir, 'checkpoint-decoder-{}'.format(global_step))\n",
    "        if not os.path.exists(output_decoder_dir):\n",
    "            os.makedirs(output_decoder_dir)\n",
    "        model_decoder.save_pretrained(output_decoder_dir)\n",
    "        torch.save(args, os.path.join(output_decoder_dir, 'training_encoder_args.bin'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70a35fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(accuracy_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967334e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_array[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c07743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(accuracy_array)\n",
    "plt.title('OPTAGAN-GAN-BERT Performance over Training Epochs', fontsize=20)\n",
    "plt.xlabel('Epochs', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.xlim(0,200)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9ec8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_save = pd.DataFrame(accuracy_array)\n",
    "df_to_save.to_csv('accuracy_array_optagan_yahoo_nt_rt_768_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eae8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Generating Sentences\n",
    "# from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "# import argparse\n",
    "\n",
    "# import logging\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import numpy as np\n",
    "\n",
    "# from modules.gan import Generator\n",
    "\n",
    "# import glob\n",
    "# import os\n",
    "# import pickle\n",
    "# import random\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "# from tqdm import tqdm, trange\n",
    "\n",
    "# from func import GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig, BertConfig\n",
    "# from func import GPT2LMHeadModel, GPT2Tokenizer, GPT2ForLatentConnector, GPT2ForLatentConnectorValueHead\n",
    "# from func import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer\n",
    "# from func import XLNetLMHeadModel, XLNetTokenizer\n",
    "# from func import TransfoXLLMHeadModel, TransfoXLTokenizer\n",
    "# from func import BertForLatentConnector, BertTokenizer\n",
    "\n",
    "# from collections import defaultdict\n",
    "# import pdb\n",
    "# from modules.utils import rollout_test\n",
    "\n",
    "# MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "\n",
    "# ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) for conf in (GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig)), ())\n",
    "\n",
    "# logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "#                     datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "#                     level = logging.INFO)\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# MODEL_CLASSES = {\n",
    "#     'gpt2': (GPT2Config, GPT2ForLatentConnector, GPT2Tokenizer),\n",
    "#     'bert': (BertConfig, BertForLatentConnector, BertTokenizer),\n",
    "#     'gpt2v': (GPT2Config, GPT2ForLatentConnectorValueHead, GPT2Tokenizer)\n",
    "# }\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--seed', type=int, default=0)\n",
    "#     parser.add_argument('--new_sent', type=int, default=1, help=\"Number of sentences to generate\")\n",
    "#     parser.add_argument('--n_layers', type=int, default=20, help=\"Number of layers of generator\")\n",
    "#     parser.add_argument('--block_dim', type=int, default=100)\n",
    "#     parser.add_argument('--interval', type=int, default=10)\n",
    "#     parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())\n",
    "#     parser.add_argument('--generator_dir', default=None, type=str, required=True, help=\"Directory of GAN model checkpoint\")\n",
    "#     parser.add_argument(\"--checkpoint_dir\", default=None, type=str, required=True,\n",
    "#                         help=\"The directory where checkpoints are saved.\")\n",
    "#     parser.add_argument(\"--output_dir\", default=None, type=str, required=True,\n",
    "#                         help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
    "#     parser.add_argument(\"--save\", default=False, type=bool, help=\"Save results to file.\")\n",
    "#     parser.add_argument(\"--latent_size\", default=32, type=int, help=\"Latent space dimension.\")\n",
    "#     parser.add_argument(\"--output_name\", default=\"results\", type=str, help=\"File name of output\")\n",
    "#     parser.add_argument(\"--batch_size\", default=100, type=int, help=\"Batch size to generate outputs\")\n",
    "#     ## Encoder options\n",
    "#     parser.add_argument(\"--encoder_model_type\", default=\"bert\", type=str,\n",
    "#                         help=\"The encoder model architecture to be fine-tuned.\")\n",
    "#     parser.add_argument(\"--encoder_model_name_or_path\", default=\"bert-base-cased\", type=str,\n",
    "#                         help=\"The encoder model checkpoint for weights initialization.\")\n",
    "#     parser.add_argument(\"--encoder_config_name\", default=\"\", type=str,\n",
    "#                         help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
    "#     parser.add_argument(\"--encoder_tokenizer_name\", default=\"\", type=str,\n",
    "#                         help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
    "#     ## Decoder options\n",
    "#     parser.add_argument(\"--decoder_model_type\", default=\"gpt2\", type=str,\n",
    "#                         help=\"The decoder model architecture to be fine-tuned.\")\n",
    "#     parser.add_argument(\"--decoder_model_name_or_path\", default=\"gpt2\", type=str,\n",
    "#                         help=\"The decoder model checkpoint for weights initialization.\")\n",
    "#     parser.add_argument(\"--decoder_config_name\", default=\"\", type=str,\n",
    "#                         help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
    "#     parser.add_argument(\"--decoder_tokenizer_name\", default=\"\", type=str,\n",
    "#                         help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
    "#     parser.add_argument(\"--max_seq_length\", default=512, type=int,\n",
    "#                         help=\"Optional input sequence length before tokenization. The sequence will be dropped if it is longer the max_seq_length\")\n",
    "#     parser.add_argument(\"--finetune_decoder\", default=False, type=bool,\n",
    "#                         help=\"Uses finetuned decoder in output dir if true.\")\n",
    "\n",
    "#     ## Variational auto-encoder(check this)\n",
    "#     parser.add_argument(\"--top_k\", type=int, default=0)\n",
    "#     parser.add_argument(\"--top_p\", type=float, default=1.0)\n",
    "#     parser.add_argument(\"--prompt\", type=str, default=\"\")\n",
    "#     parser.add_argument(\"--padding_text\", type=str, default=\"\")\n",
    "#     parser.add_argument(\"--length\", type=int, default=20)\n",
    "#     parser.add_argument(\"--block_size\", default=-1, type=int,\n",
    "#                         help=\"Optional input sequence length after tokenization.\"\n",
    "#                              \"The training dataset will be truncated in block of this size for training.\"\n",
    "#                              \"Default to the model max input length for single sentence inputs (take into account special tokens).\")\n",
    "#     parser.add_argument(\"--do_lower_case\", action='store_true',\n",
    "#                         help=\"Set this flag if you are using an uncased model.\")\n",
    "#     parser.add_argument(\"--use_philly\", action='store_true',\n",
    "#                         help=\"Use Philly for computing.\")\n",
    "#     parser.add_argument('--gloabl_step_eval', type=int, default=508523,\n",
    "#                         help=\"Evaluate the results at the given global step\")\n",
    "\n",
    "#     # Load a trained Encoder model and vocabulary that you have fine-tuned\n",
    "#     args = parser.parse_args(\"--checkpoint_dir=output_dir_yahoo_768_0 \\\n",
    "#     --output_dir=output_dir_yahoo_768_0 \\\n",
    "#     --generator_dir=output_dir_yahoo_768_0 \\\n",
    "#     --block_size 100 \\\n",
    "#     --max_seq_length 60 \\\n",
    "#     --gloabl_step_eval 24000 \\\n",
    "#     --latent_size 32 \\\n",
    "#     --block_dim 100 \\\n",
    "#     --new_sent 100 \\\n",
    "#     --n_layers 10 \\\n",
    "#     --top_p 0.9 \\\n",
    "#     --output_name=results \\\n",
    "#     --save True\".split())\n",
    "#     global_step = args.gloabl_step_eval\n",
    "\n",
    "#     np.random.seed(args.seed)\n",
    "#     torch.manual_seed(args.seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "#     args.n_gpu = torch.cuda.device_count()\n",
    "#     if args.n_gpu > 0:\n",
    "#         torch.cuda.manual_seed_all(args.seed)       \n",
    "    \n",
    "#     args.encoder_model_type = args.encoder_model_type.lower()\n",
    "#     args.decoder_model_type = args.decoder_model_type.lower()\n",
    "\n",
    "#     output_encoder_dir = os.path.join(args.checkpoint_dir, 'checkpoint-encoder-{}'.format(global_step))\n",
    "#     output_decoder_dir = os.path.join(args.checkpoint_dir, 'checkpoint-decoder-{}'.format(global_step))\n",
    "#     if not args.finetune_decoder:\n",
    "#         output_decoder_dir = os.path.join(args.checkpoint_dir, 'checkpoint-decoder-{}'.format(global_step))\n",
    "#     else:\n",
    "#          output_decoder_dir = os.path.join(args.output_dir, 'checkpoint-decoder-{}'.format(global_step))\n",
    "#     checkpoints = [ [output_encoder_dir, output_decoder_dir] ]\n",
    "\n",
    "#     # Load a trained Encoder model and vocabulary that you have fine-tuned\n",
    "#     encoder_config_class, encoder_model_class, encoder_tokenizer_class = MODEL_CLASSES[args.encoder_model_type]\n",
    "#     model_encoder = encoder_model_class.from_pretrained(output_encoder_dir, latent_size=args.latent_size)\n",
    "#     tokenizer_encoder = encoder_tokenizer_class.from_pretrained(args.encoder_tokenizer_name if args.encoder_tokenizer_name else args.encoder_model_name_or_path, do_lower_case=args.do_lower_case)\n",
    "\n",
    "#     model_encoder.to(args.device)\n",
    "#     if args.block_size <= 0:\n",
    "#         args.block_size = tokenizer_encoder.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
    "#     args.block_size = min(args.block_size, tokenizer_encoder.max_len_single_sentence)\n",
    "\n",
    "#     # Load a trained Decoder model and vocabulary that you have fine-tuned\n",
    "#     if not args.finetune_decoder:\n",
    "#         decoder_config_class, decoder_model_class, decoder_tokenizer_class = MODEL_CLASSES[args.decoder_model_type]\n",
    "#     else:\n",
    "#         decoder_config_class, decoder_model_class, decoder_tokenizer_class = MODEL_CLASSES[\"gpt2v\"]\n",
    "#     model_decoder = decoder_model_class.from_pretrained(output_decoder_dir, latent_size=args.latent_size)\n",
    "#     tokenizer_decoder = decoder_tokenizer_class.from_pretrained(args.decoder_tokenizer_name if args.decoder_tokenizer_name else args.decoder_model_name_or_path, do_lower_case=args.do_lower_case)\n",
    "#     model_decoder.to(args.device)\n",
    "#     if args.block_size <= 0:\n",
    "#         args.block_size = tokenizer_decoder.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
    "#     args.block_size = min(args.block_size, tokenizer_decoder.max_len_single_sentence)\n",
    "\n",
    "#     # Chunyuan: Add Padding token to GPT2\n",
    "#     special_tokens_dict = {'pad_token': '<PAD>', 'bos_token': '<BOS>', 'eos_token': '<EOS>'}\n",
    "#     num_added_toks = tokenizer_decoder.add_special_tokens(special_tokens_dict)\n",
    "#     logger.info('We have added {} tokens to GPT2'.format(num_added_toks))\n",
    "#     model_decoder.resize_token_embeddings(len(tokenizer_decoder))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n",
    "#     assert tokenizer_decoder.pad_token == '<PAD>'\n",
    "    \n",
    "#     generator = Generator(args.n_layers, args.block_dim, args.latent_size)\n",
    "\n",
    "#     if args.cuda:\n",
    "#         generator = generator.cuda()\n",
    "\n",
    "#     generator.load_state_dict(torch.load(args.generator_dir+'/generator_'+str(args.gloabl_step_eval)+'.th'))\n",
    "#     generator.eval()\n",
    "#     model_decoder.eval()\n",
    "#     model_encoder.eval()\n",
    "#     if args.save:\n",
    "#         if not os.path.exists(args.output_dir+\"/{}.txt\".format(args.output_name)):\n",
    "#             with open(args.output_dir+\"/{}.txt\".format(args.output_name), 'w'): \n",
    "#                 pass\n",
    "\n",
    "#     for i in range(int(args.new_sent/args.batch_size)):\n",
    "#         # sample noise\n",
    "#         noise = torch.Tensor(np.random.normal(0, 1, (args.batch_size, args.latent_size))).to(args.device)\n",
    "#         new_z = generator(noise).data\n",
    "\n",
    "#         # create new sent\n",
    "#         sents = rollout_test(model_decoder, new_z, tokenizer_decoder, args.max_seq_length, args.batch_size, args.top_k, args.top_p)\n",
    "\n",
    "#         if args.save:\n",
    "#             with open(args.output_dir+\"/{}.txt\".format(args.output_name), 'a') as file:\n",
    "#                 for i in sents:\n",
    "#                     file.write(i+\"\\n\")\n",
    "#         else:\n",
    "#             for i in sents:\n",
    "#                 logger.info(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab174fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
