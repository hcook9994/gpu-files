{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e2e5956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/harry/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import argparse\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from modules.gan import Generator, Critic\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from func import GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig, BertConfig\n",
    "from func import GPT2LMHeadModel, GPT2Tokenizer, GPT2ForLatentConnector, GPT2ForLatentConnectorValueHead\n",
    "from func import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer\n",
    "from func import XLNetLMHeadModel, XLNetTokenizer\n",
    "from func import TransfoXLLMHeadModel, TransfoXLTokenizer\n",
    "from func import BertForLatentConnector, BertTokenizer\n",
    "\n",
    "from collections import defaultdict\n",
    "from utils import (TextDataset_Split, TextDataset_2Tokenizers, BucketingDataLoader)\n",
    "import pdb\n",
    "from modules.utils import (calc_blue_parallel_func, pad_seq, rollout, rollout_test)\n",
    "#from transformers.modeling_utils import top_k_top_p_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cfebdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-27 14:10:52.928230: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-27 14:10:52.928256: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe98b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_l=pd.read_csv(\"../../yahoo/assigned/train_l.csv\", index_col=\"Unnamed: 0\")\n",
    "df_test_l=pd.read_csv(\"../../yahoo/assigned/test_l.csv\", index_col=\"Unnamed: 0\")\n",
    "df_u=pd.read_csv(\"../../yahoo/assigned/u.csv\", index_col=\"Unnamed: 0\")\n",
    "df_train_u=pd.read_csv(\"../../yahoo/assigned/train_u.csv\", index_col=\"Unnamed: 0\")\n",
    "df_test_u=pd.read_csv(\"../../yahoo/assigned/test_u.csv\", index_col=\"Unnamed: 0\")\n",
    "df_all = pd.concat([df_train_l, df_test_l, df_u, df_train_u, df_test_u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56840583",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_l =  list(df_train_l.to_records(index=False))\n",
    "test_l = list(df_test_l.to_records(index=False))\n",
    "u_list = list(df_u.to_records(index=False))\n",
    "test_u = list(df_test_u.to_records(index=False))\n",
    "train_u = list(df_train_u.to_records(index=False))\n",
    "data_all = list(df_all[\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abea150c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('what is rq?', 'UNK')\n"
     ]
    }
   ],
   "source": [
    "print(train_u[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a21ec834",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['UNK',1,2,3,4,5,6,7,8,9,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5afced3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_loader(input_examples, label_masks, label_map, do_shuffle = False, balance_label_examples = False):\n",
    "  '''\n",
    "  Generate a Dataloader given the input examples, eventually masked if they are \n",
    "  to be considered NOT labeled.\n",
    "  '''\n",
    "  examples = []\n",
    "\n",
    "  # Count the percentage of labeled examples  \n",
    "  num_labeled_examples = 0\n",
    "  for label_mask in label_masks:\n",
    "    if label_mask: \n",
    "      num_labeled_examples += 1\n",
    "  label_mask_rate = num_labeled_examples/len(input_examples)\n",
    "\n",
    "  # if required it applies the balance\n",
    "  for index, ex in enumerate(input_examples): \n",
    "    if label_mask_rate == 1 or not balance_label_examples:\n",
    "      examples.append((ex, label_masks[index]))\n",
    "    else:\n",
    "      # IT SIMULATE A LABELED EXAMPLE\n",
    "      if label_masks[index]:\n",
    "        balance = int(1/label_mask_rate)\n",
    "        balance = int(math.log(balance,2))\n",
    "        if balance < 1:\n",
    "          balance = 1\n",
    "        for b in range(0, int(balance)):\n",
    "          examples.append((ex, label_masks[index]))\n",
    "      else:\n",
    "        examples.append((ex, label_masks[index]))\n",
    "  \n",
    "  #-----------------------------------------------\n",
    "  # Generate input examples to the Transformer\n",
    "  #-----------------------------------------------\n",
    "  input_ids = []\n",
    "  input_mask_array = []\n",
    "  label_mask_array = []\n",
    "  label_id_array = []\n",
    "\n",
    "  # Tokenization \n",
    "  for (text, label_mask) in examples:\n",
    "#     encoded_sent = tokenizer_encoder.convert_tokens_to_ids(tokenizer_encoder.tokenize(text[0]))\n",
    "#     if len(encoded_sent) < (args.max_seq_length-2):\n",
    "#         encoded_sent = encoded_sent[0:(args.max_seq_length-2)]\n",
    "#     encoded_sent = tokenizer_encoder.add_special_tokens_single_sentence(encoded_sent)\n",
    "#     while len(encoded_sent) < args.max_seq_length:\n",
    "#         encoded_sent.insert(-1,0)\n",
    "    encoded_sent = tokenizer.encode(text[0], add_special_tokens=True, max_length=args.max_seq_length, padding=\"max_length\", truncation=True)\n",
    "    input_ids.append(encoded_sent)\n",
    "    label_id_array.append(label_map[text[1]])\n",
    "    label_mask_array.append(label_mask)\n",
    "  \n",
    "  # Attention to token (to ignore padded input wordpieces)\n",
    "  for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]                          \n",
    "    input_mask_array.append(att_mask)\n",
    "  # Convertion to Tensor\n",
    "  input_ids = torch.tensor(input_ids) \n",
    "  input_mask_array = torch.tensor(input_mask_array)\n",
    "  label_id_array = torch.tensor(label_id_array, dtype=torch.long)\n",
    "  label_mask_array = torch.tensor(label_mask_array)\n",
    "\n",
    "  # Building the TensorDataset\n",
    "  dataset = TensorDataset(input_ids, input_mask_array, label_id_array, label_mask_array)\n",
    "\n",
    "  if do_shuffle:\n",
    "    sampler = RandomSampler\n",
    "  else:\n",
    "    sampler = SequentialSampler\n",
    "\n",
    "  # Building the DataLoader\n",
    "  return DataLoader(\n",
    "              dataset,  # The training samples.\n",
    "              sampler = sampler(dataset), \n",
    "              batch_size = args.per_gpu_train_batch_size) # Trains with this batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eab98659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_fake(input_examples):\n",
    "  '''\n",
    "  Generate a Dataloader given the input examples, eventually masked if they are \n",
    "  to be considered NOT labeled.\n",
    "  '''\n",
    "  \n",
    "  #-----------------------------------------------\n",
    "  # Generate input examples to the Transformer\n",
    "  #-----------------------------------------------\n",
    "  input_ids = []\n",
    "  input_mask_array = []\n",
    "\n",
    "  # Tokenization \n",
    "  for text in input_examples:\n",
    "    encoded_sent = tokenizer.encode(text, add_special_tokens=True, max_length=args.max_seq_length, padding=\"max_length\", truncation=True)\n",
    "    input_ids.append(encoded_sent)\n",
    "  \n",
    "  # Attention to token (to ignore padded input wordpieces)\n",
    "  for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]                          \n",
    "    input_mask_array.append(att_mask)\n",
    "  # Convertion to Tensor\n",
    "  input_ids = torch.tensor(input_ids) \n",
    "  input_mask_array = torch.tensor(input_mask_array)\n",
    "\n",
    "  # Building the DataLoader\n",
    "  return input_ids, input_mask_array # Trains with this batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29b2ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------\n",
    "#   The Discriminator\n",
    "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
    "#   https://github.com/crux82/ganbert\n",
    "#------------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
    "        layers = []\n",
    "        hidden_sizes = [input_size] + hidden_sizes\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
    "\n",
    "        self.layers = nn.Sequential(*layers) #per il flatten\n",
    "        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1) # +1 for the probability of this sample being fake/real.\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_rep):\n",
    "        input_rep = self.input_dropout(input_rep)\n",
    "        last_rep = self.layers(input_rep)\n",
    "        logits = self.logit(last_rep)\n",
    "        probs = self.softmax(logits)\n",
    "        return last_rep, logits, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3d77e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------\n",
    "#   The Generator as in \n",
    "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
    "#   https://github.com/crux82/ganbert\n",
    "#------------------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_size=100, output_size=512, hidden_sizes=[512], dropout_rate=0.1):\n",
    "        super(Generator, self).__init__()\n",
    "        layers = []\n",
    "        hidden_sizes = [noise_size] + hidden_sizes\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
    "\n",
    "        layers.append(nn.Linear(hidden_sizes[-1],output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, noise):\n",
    "        output_rep = self.layers(noise)\n",
    "        return output_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7db4d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) for conf in (GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig)), ())\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'gpt2': (GPT2Config, GPT2ForLatentConnectorValueHead, GPT2Tokenizer),\n",
    "    'bert': (BertConfig, BertForLatentConnector, BertTokenizer)\n",
    "}\n",
    "\n",
    "def load_and_cache_examples(args, tokenizer):\n",
    "    if isinstance(tokenizer, list):\n",
    "        dataset = TextDataset_2Tokenizers(tokenizer, args, args.train_data_file, block_size=args.block_size)\n",
    "    else:\n",
    "        dataset = TextDataset_Split(tokenizer, args, args.train_data_file, block_size=args.block_size)\n",
    "    return dataset\n",
    "\n",
    "def build_dataload_and_cache_examples(args, tokenizer):\n",
    "    if isinstance(tokenizer, list):\n",
    "        args.batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "        file_path=args.train_data_file\n",
    "        dataloader = BucketingDataLoader(file_path, args.batch_size, args.max_seq_length, tokenizer, args, bucket=100, shuffle=True)\n",
    "        print(dataloader)\n",
    "    else:\n",
    "        pass \n",
    "    return dataloader\n",
    "\n",
    "def compute_grad_penalty(critic, real_data, fake_data):\n",
    "    B = real_data.size(0)\n",
    "    alpha = torch.FloatTensor(np.random.random((B, 1)))\n",
    "    if args.cuda:\n",
    "        alpha = alpha.cuda()\n",
    "    sample = alpha*real_data + (1-alpha)*fake_data\n",
    "    sample.requires_grad_(True)\n",
    "    score = critic(sample)\n",
    "\n",
    "    outputs = torch.FloatTensor(B, 1).fill_(1.0) #args.latent_size\n",
    "    outputs.requires_grad_(False)\n",
    "    if args.cuda:\n",
    "        outputs = outputs.cuda()\n",
    "    grads = autograd.grad(\n",
    "        outputs=score,\n",
    "        inputs=sample,\n",
    "        grad_outputs=outputs,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True)[0]\n",
    "    grad_penalty = ((grads.norm(2, dim=1) - 1.) ** 2).mean()\n",
    "    return grad_penalty\n",
    "\n",
    "def train(epoch, counter):\n",
    "    \n",
    "    tr_d_loss = 0\n",
    "    \n",
    "    #model_encoder.eval()\n",
    "    model_decoder.train()\n",
    "    generator.train()\n",
    "    #critic.train()\n",
    "    discriminator.train()\n",
    "    transformer.train()\n",
    "    #transformer.eval()\n",
    "    c_train_loss = 0.\n",
    "    g_train_loss = 0.\n",
    "    g_batches = 0\n",
    "    c_batches = 0\n",
    "    c_loss_0 = 1\n",
    "    g_loss_0 = 1\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        \n",
    "        #x = x[0]\n",
    "        \n",
    "        args.batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "        \n",
    "        b_input_ids = batch[0].to(args.device)\n",
    "        b_input_mask = batch[1].to(args.device)\n",
    "        b_labels = batch[2].to(args.device)\n",
    "        b_label_mask = batch[3].to(args.device)\n",
    "        \n",
    "        x = b_input_ids\n",
    "        \n",
    "        #print(\"x output is: {}\".format(x))\n",
    "        \n",
    "        if args.cuda:\n",
    "            x = x.cuda()\n",
    "        # Generate noise\n",
    "        B = args.per_gpu_train_batch_size\n",
    "#         noise = torch.from_numpy(np.random.normal(0, 1, (B,\n",
    "#                                  args.latent_size))).float()\n",
    "        noise = torch.zeros(args.batch_size, noise_size, device=args.device).uniform_(0, 1)\n",
    "#         if args.cuda:\n",
    "#             noise = noise.cuda()\n",
    "            \n",
    "        # Get original text latent embeddings\n",
    "#         with torch.no_grad(): \n",
    "#             pooled_hidden_fea = model_encoder(x, attention_mask=(x > 0).float())[1]\n",
    "#             mean, logvar = model_encoder.linear(pooled_hidden_fea).chunk(2, -1)\n",
    "#             z_real = mean.squeeze(1) \n",
    "        model_outputs = transformer(x, attention_mask=(x > 0).float())\n",
    "        z_real = model_outputs.last_hidden_state[:,0,:]\n",
    "\n",
    "        # Evaluate and get losses\n",
    "        z_fake = generator(noise).data\n",
    "        train_z = rollout_test(model_decoder, z_fake, tokenizer_decoder, args.max_seq_length, args.batch_size, 0, 1)\n",
    "        x_g, x_g_mask = generate_data_fake(train_z)\n",
    "        x_g = x_g.to(args.device)\n",
    "        model_outputs_g = transformer(x_g, attention_mask=(x_g > 0).float())\n",
    "        z_fake_g = model_outputs_g.last_hidden_state[:,0,:]\n",
    "        #real_score = critic(z_real)\n",
    "        #fake_score = critic(z_fake)\n",
    "        \n",
    "        D_real_features, D_real_logits, D_real_probs = discriminator(z_real)\n",
    "        D_fake_features, D_fake_logits, D_fake_probs = discriminator(z_fake_g)\n",
    "        \n",
    "        #print(\"Real logits have these numbers: {}\".format(D_real_logits[:,-1]))\n",
    "        #print(\"Fake logits have these numbers: {}\".format(D_fake_logits[:,-1]))\n",
    "#         print(\"real_score critic: {}\".format(real_score))\n",
    "#         print(\"fake_score critic: {}\".format(fake_score))\n",
    "#         grad_penalty = compute_grad_penalty(critic, z_real.data, z_fake.data)\n",
    "#         c_loss = -torch.mean(real_score) + torch.mean(fake_score) + \\\n",
    "#                  args.gp_lambda*grad_penalty\n",
    "\n",
    "        #fake_score = critic(generator(noise))\n",
    "        #fake_score_d = discriminator(generator(noise))\n",
    "        #g_loss_old = -torch.mean(fake_score)\n",
    "        \n",
    "        # Generator's LOSS estimation\n",
    "        g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + epsilon))\n",
    "        g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n",
    "        g_loss = g_loss_d + g_feat_reg\n",
    "        #g_loss = -torch.mean(D_fake_logits[:,-1])\n",
    "        \n",
    "        # Disciminator's LOSS estimation\n",
    "        logits = D_real_logits[:,0:-1]\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        \n",
    "        # The discriminator provides an output for labeled and unlabeled real data\n",
    "        # so the loss evaluated for unlabeled data is ignored (masked)\n",
    "        label2one_hot = torch.nn.functional.one_hot(b_labels, len(label_list))\n",
    "        per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n",
    "        per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(args.device))\n",
    "        labeled_example_count = per_example_loss.type(torch.float32).numel()\n",
    "\n",
    "        # It may be the case that a batch does not contain labeled examples, \n",
    "        # so the \"supervised loss\" in this case is not evaluated\n",
    "        if labeled_example_count == 0:\n",
    "          D_L_Supervised = 0\n",
    "        else:\n",
    "          D_L_Supervised = torch.div(torch.sum(per_example_loss.to(args.device)), labeled_example_count)\n",
    "                 \n",
    "        D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n",
    "        D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n",
    "        d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n",
    "        \n",
    "        r_g = abs(((g_loss.item() - g_loss_0) / (g_loss_0 + 0.001))) \n",
    "        #r_c = abs(((c_loss.item() - c_loss_0) / (c_loss_0 + 0.001))) \n",
    "        \n",
    "        # Update critic or generator\n",
    "        #if ((2 + epoch) / epoch) * r_c > r_g:\n",
    "#             c_optimizer.zero_grad()\n",
    "#             c_batches += 1\n",
    "#             c_train_loss += c_loss.item()\n",
    "#             c_loss.backward(retain_graph=True)\n",
    "#             c_optimizer.step()\n",
    "        #if counter % 2 ==0:\n",
    "        #---------------------------------\n",
    "        #  OPTIMIZATION\n",
    "        #---------------------------------\n",
    "        # Avoid gradient accumulation\n",
    "        gen_optimizer.zero_grad()\n",
    "        dis_optimizer.zero_grad()\n",
    "        # Calculate weigth updates\n",
    "        # retain_graph=True is required since the underlying graph will be deleted after backward\n",
    "        g_loss.backward(retain_graph=True)\n",
    "        d_loss.backward()\n",
    "        # Apply modifications\n",
    "        gen_optimizer.step()\n",
    "        dis_optimizer.step()\n",
    "        #print(\"Training discriminator this time\")\n",
    "        #else:\n",
    "        g_batches += 1\n",
    "        g_train_loss += g_loss.item()\n",
    "        \n",
    "        \n",
    "        #print(\"Training generator this time\")\n",
    "        \n",
    "        counter=counter+1\n",
    "\n",
    "        # Save the losses to print them later\n",
    "        tr_d_loss += d_loss.item()\n",
    "\n",
    "        #c_loss_0 = c_loss.item()\n",
    "        g_loss_0 = g_loss.item()\n",
    "\n",
    "        if args.interval > 0 and i % args.interval == 0:\n",
    "            #logger.info('Epoch: {} | Batch: {}/{} ({:.0f}%) | G Loss: {:.6f} | C Loss: {:.6f}'.format(\n",
    "            logger.info('Epoch: {} | Batch: {}/{} ({:.0f}%) | G Loss: {:.6f}'.format(\n",
    "                epoch, args.batch_size*i, len(train_loader.dataset),\n",
    "                100.*(args.batch_size*i)/len(train_loader.dataset),\n",
    "                g_loss.item()#, c_loss.item()\n",
    "            ))\n",
    "            #test_noise = torch.Tensor(np.random.normal(0, 1, (1, args.latent_size))).to(args.device)\n",
    "            test_noise = torch.zeros(1, noise_size, device=args.device).uniform_(0, 1)\n",
    "            test_new_z = generator(test_noise).data\n",
    "            # create new sent\n",
    "            test_z = rollout_test(model_decoder, test_new_z, tokenizer_decoder, args.max_seq_length, 1, 0, 1)\n",
    "            logger.info(\"Text: {}\".format(test_z))\n",
    "        \n",
    "    avg_train_loss_d = tr_d_loss / len(train_loader)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"  Average training loss discriminator: {0:.3f}\".format(avg_train_loss_d))\n",
    "    \n",
    "    discriminator.eval()\n",
    "    transformer.eval()\n",
    "    model_decoder.eval()\n",
    "    generatore.eval()\n",
    "    \n",
    "    # Tracking variables \n",
    "    total_test_accuracy = 0\n",
    "   \n",
    "    total_test_loss = 0\n",
    "    nb_test_steps = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels_ids = []\n",
    "\n",
    "    #loss\n",
    "    nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    \n",
    "    # Evaluate data for one epoch\n",
    "    for batch in test_loader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        b_input_ids = batch[0].to(args.device)\n",
    "        b_input_mask = batch[1].to(args.device)\n",
    "        b_labels = batch[2].to(args.device)\n",
    "        \n",
    "        x = b_input_ids\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():\n",
    "#             pooled_hidden_fea = model_encoder(x, attention_mask=(x > 0).float())[1]\n",
    "#             mean, logvar = model_encoder.linear(pooled_hidden_fea).chunk(2, -1)\n",
    "#             z_real = mean.squeeze(1)\n",
    "            model_outputs = transformer(x, attention_mask=(x > 0).float())\n",
    "            z_real = model_outputs.last_hidden_state[:,0,:]\n",
    "            _, logits, probs = discriminator(z_real)\n",
    "            ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n",
    "            filtered_logits = logits[:,0:-1]\n",
    "            # Accumulate the test loss.\n",
    "            total_test_loss += nll_loss(filtered_logits, b_labels)\n",
    "            \n",
    "        # Accumulate the predictions and the input labels\n",
    "        _, preds = torch.max(filtered_logits, 1)\n",
    "        all_preds += preds.detach().cpu()\n",
    "        all_labels_ids += b_labels.detach().cpu()\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    all_preds = torch.stack(all_preds).numpy()\n",
    "    all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
    "    test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
    "    print(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_test_loss = total_test_loss / len(test_loader)\n",
    "    avg_test_loss = avg_test_loss.item()\n",
    "    \n",
    "    print(\"  Test Loss: {0:.3f}\".format(avg_test_loss))\n",
    "    \n",
    "    c_train_loss /= c_batches + 1\n",
    "    g_train_loss /= g_batches + 1\n",
    "    logger.info('* (Train) Epoch: {} | G Loss: {:.4f} | Updates G: {} | Updates C: {}'.format(\n",
    "        #epoch, g_train_loss, c_train_loss, g_batches, c_batches\n",
    "        epoch, g_train_loss, g_batches, c_batches\n",
    "    ))\n",
    "    return (g_train_loss, test_accuracy, counter)\n",
    "    #return (g_train_loss, c_train_loss, test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02224e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--seed', type=int, default=0)\n",
    "parser.add_argument('--epochs', type=int, default=15)\n",
    "parser.add_argument('--lr', type=float, default=1e-4)\n",
    "parser.add_argument('--gp_lambda', type=int, default=10)\n",
    "parser.add_argument('--n_layers', type=int, default=20, help=\"Number of layers of generator and critic\")\n",
    "parser.add_argument('--block_dim', type=int, default=100)\n",
    "parser.add_argument('--interval', type=int, default=10, help=\"Steps before logging output\")\n",
    "parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())\n",
    "\n",
    "# Optimus parameters\n",
    "parser.add_argument(\"--train_data_file\", default=None, type=str, required=True,\n",
    "                    help=\"The input training data file (a text file).\")\n",
    "parser.add_argument(\"--valid_data_file\", default=None, type=str, required=True,\n",
    "                    help=\"The input validation data file (a text file).\")\n",
    "parser.add_argument(\"--checkpoint_dir\", default=None, type=str, required=True,\n",
    "                    help=\"The directory where checkpoints are saved.\")\n",
    "parser.add_argument('--generator_dir', default=None, type=str, help=\"Directory where GAN models are saved\")\n",
    "parser.add_argument(\"--output_dir\", default=None, type=str, required=True,\n",
    "                    help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
    "parser.add_argument(\"--dataset\", default='Snli', type=str, help=\"The dataset.\")    \n",
    "parser.add_argument(\"--latent_size\", default=32, type=int, help=\"Latent space dimension.\")\n",
    "## Encoder options\n",
    "parser.add_argument(\"--encoder_model_type\", default=\"bert\", type=str,\n",
    "                    help=\"The encoder model architecture to be fine-tuned.\")\n",
    "parser.add_argument(\"--encoder_model_name_or_path\", default=\"bert-base-cased\", type=str,\n",
    "                    help=\"The encoder model checkpoint for weights initialization.\")\n",
    "parser.add_argument(\"--encoder_config_name\", default=\"\", type=str,\n",
    "                    help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
    "parser.add_argument(\"--encoder_tokenizer_name\", default=\"\", type=str,\n",
    "                    help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
    "## Decoder options\n",
    "parser.add_argument(\"--decoder_model_type\", default=\"gpt2\", type=str,\n",
    "                    help=\"The decoder model architecture to be fine-tuned.\")\n",
    "parser.add_argument(\"--decoder_model_name_or_path\", default=\"bert-base-cased\", type=str,\n",
    "                    help=\"The decoder model checkpoint for weights initialization.\")\n",
    "parser.add_argument(\"--decoder_config_name\", default=\"\", type=str,\n",
    "                    help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
    "parser.add_argument(\"--decoder_tokenizer_name\", default=\"\", type=str,\n",
    "                    help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
    "parser.add_argument(\"--per_gpu_train_batch_size\", default=1, type=int,\n",
    "                    help=\"Batch size per GPU/CPU for training.\")\n",
    "parser.add_argument(\"--max_seq_length\", default=512, type=int,\n",
    "                    help=\"Optional input sequence length before tokenization. The sequence will be dropped if it is longer the max_seq_length\")\n",
    "\n",
    "## Variational auto-encoder(check this)\n",
    "parser.add_argument(\"--prompt\", type=str, default=\"\")\n",
    "parser.add_argument(\"--padding_text\", type=str, default=\"\")\n",
    "parser.add_argument(\"--length\", type=int, default=20)\n",
    "parser.add_argument(\"--block_size\", default=-1, type=int,\n",
    "                    help=\"Optional input sequence length after tokenization.\"\n",
    "                         \"The training dataset will be truncated in block of this size for training.\"\n",
    "                         \"Default to the model max input length for single sentence inputs (take into account special tokens).\")\n",
    "parser.add_argument(\"--do_lower_case\", action='store_true',\n",
    "                    help=\"Set this flag if you are using an uncased model.\")\n",
    "parser.add_argument(\"--use_philly\", action='store_true',\n",
    "                    help=\"Use Philly for computing.\")\n",
    "parser.add_argument('--gloabl_step_eval', type=int, default=661,\n",
    "                    help=\"Evaluate the results at the given global step\")\n",
    "# Reinforcement learning parameters\n",
    "parser.add_argument('--finetune_decoder', type=bool, default=True)\n",
    "parser.add_argument('--epochs_rl', type=int, default=1000)\n",
    "parser.add_argument('--batch_size_rl', type=int, default=32)\n",
    "parser.add_argument('--lr_rl', type=float, default=1e-6)\n",
    "\n",
    "\n",
    "# Load a trained Encoder model and vocabulary that you have fine-tuned\n",
    "args = parser.parse_args(\"--dataset EMNLP \\\n",
    "--checkpoint_dir=checkpoint-508523-768-0 \\\n",
    "--output_dir=checkpoint-508523-768-0 \\\n",
    "--encoder_model_type=bert \\\n",
    "--encoder_model_name_or_path=bert-base-cased \\\n",
    "--decoder_model_type=gpt2 \\\n",
    "--decoder_model_name_or_path=gpt2 \\\n",
    "--train_data_file=../../yahoo/unlabelled/train.txt \\\n",
    "--valid_data_file=../../yahoo/unlabelled/test.txt \\\n",
    "--per_gpu_train_batch_size 6 \\\n",
    "--block_size 100 \\\n",
    "--max_seq_length 24 \\\n",
    "--gloabl_step_eval 508523 \\\n",
    "--latent_size 768 \\\n",
    "--block_dim 100 \\\n",
    "--n_layers 10 \\\n",
    "--interval 50 \\\n",
    "--epochs 200 \\\n",
    "--finetune_decoder True \\\n",
    "--lr_rl 1e-6 \\\n",
    "--epochs_rl 100 \\\n",
    "--batch_size_rl 32\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f2f8acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = args.gloabl_step_eval\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "#args.n_gpu = torch.cuda.device_count()\n",
    "args.n_gpu=1\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(args.seed)       \n",
    "\n",
    "args.encoder_model_type = args.encoder_model_type.lower()\n",
    "args.decoder_model_type = args.decoder_model_type.lower()\n",
    "\n",
    "output_encoder_dir = os.path.join(args.checkpoint_dir, 'checkpoint-encoder-{}'.format(global_step))\n",
    "output_decoder_dir = os.path.join(args.checkpoint_dir, 'checkpoint-decoder-{}'.format(global_step)) \n",
    "checkpoints = [ [output_encoder_dir, output_decoder_dir] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "359a63bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-cased'\n",
    "transformer = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "354371f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/27/2022 14:11:08 - INFO - func.configuration_utils -   loading configuration file checkpoint-508523-768-0/checkpoint-decoder-508523/config.json\n",
      "06/27/2022 14:11:08 - INFO - func.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"latent_size\": 768,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"vocab_size\": 50260\n",
      "}\n",
      "\n",
      "06/27/2022 14:11:08 - INFO - func.modeling_utils -   loading weights file checkpoint-508523-768-0/checkpoint-decoder-508523/pytorch_model.bin\n",
      "06/27/2022 14:11:11 - INFO - func.modeling_utils -   Weights of GPT2ForLatentConnectorValueHead not initialized from pretrained model: ['v_head.linear1.weight', 'v_head.linear1.bias', 'v_head.linear2.weight', 'v_head.linear2.bias']\n",
      "06/27/2022 14:11:13 - INFO - func.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/harry/.cache/torch/pytorch_transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "06/27/2022 14:11:13 - INFO - func.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/harry/.cache/torch/pytorch_transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "06/27/2022 14:11:16 - INFO - func.tokenization_utils -   Adding <PAD> to the vocabulary\n",
      "06/27/2022 14:11:16 - INFO - func.tokenization_utils -   Assigning <PAD> to the pad_token key of the tokenizer\n",
      "06/27/2022 14:11:16 - INFO - func.tokenization_utils -   Adding <BOS> to the vocabulary\n",
      "06/27/2022 14:11:16 - INFO - func.tokenization_utils -   Assigning <BOS> to the bos_token key of the tokenizer\n",
      "06/27/2022 14:11:16 - INFO - func.tokenization_utils -   Adding <EOS> to the vocabulary\n",
      "06/27/2022 14:11:16 - INFO - func.tokenization_utils -   Assigning <EOS> to the eos_token key of the tokenizer\n",
      "06/27/2022 14:11:16 - INFO - __main__ -   We have added 3 tokens to GPT2\n"
     ]
    }
   ],
   "source": [
    "# Load a trained Encoder model and vocabulary that you have fine-tuned\n",
    "# encoder_config_class, encoder_model_class, encoder_tokenizer_class = MODEL_CLASSES[args.encoder_model_type]\n",
    "# model_encoder = encoder_model_class.from_pretrained(output_encoder_dir, latent_size=args.latent_size)\n",
    "# tokenizer_encoder = encoder_tokenizer_class.from_pretrained(args.encoder_tokenizer_name if args.encoder_tokenizer_name else args.encoder_model_name_or_path, do_lower_case=args.do_lower_case)\n",
    "\n",
    "#model_encoder.to(args.device)\n",
    "# if args.block_size <= 0:\n",
    "#     args.block_size = tokenizer_encoder.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
    "# args.block_size = min(args.block_size, tokenizer_encoder.max_len_single_sentence)\n",
    "\n",
    "# Load a trained Decoder model and vocabulary that you have fine-tuned\n",
    "decoder_config_class, decoder_model_class, decoder_tokenizer_class = MODEL_CLASSES[args.decoder_model_type]\n",
    "model_decoder = decoder_model_class.from_pretrained(output_decoder_dir, latent_size=args.latent_size)\n",
    "tokenizer_decoder = decoder_tokenizer_class.from_pretrained(args.decoder_tokenizer_name if args.decoder_tokenizer_name else args.decoder_model_name_or_path, do_lower_case=args.do_lower_case)\n",
    "model_decoder.to(args.device)\n",
    "# if args.block_size <= 0:\n",
    "#     args.block_size = tokenizer_decoder.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
    "# args.block_size = min(args.block_size, tokenizer_decoder.max_len_single_sentence)\n",
    "\n",
    "# Chunyuan: Add Padding token to GPT2\n",
    "special_tokens_dict = {'pad_token': '<PAD>', 'bos_token': '<BOS>', 'eos_token': '<EOS>'}\n",
    "num_added_toks = tokenizer_decoder.add_special_tokens(special_tokens_dict)\n",
    "logger.info('We have added {} tokens to GPT2'.format(num_added_toks))\n",
    "model_decoder.resize_token_embeddings(len(tokenizer_decoder))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n",
    "assert tokenizer_decoder.pad_token == '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f12c55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1269918/4156125318.py:60: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  label_mask_array = torch.tensor(label_mask_array)\n"
     ]
    }
   ],
   "source": [
    "label_map = {}\n",
    "for (i, label) in enumerate(label_list):\n",
    "  label_map[label] = i\n",
    "\n",
    "train_examples = train_l\n",
    "test_examples = test_l\n",
    "\n",
    "train_label_masks = np.ones(len(train_l), dtype=bool)\n",
    "\n",
    "if u_list:\n",
    "  train_examples = train_l + u_list\n",
    "  #The unlabeled (train) dataset is assigned with a mask set to False\n",
    "  tmp_masks = np.zeros(len(u_list), dtype=bool)\n",
    "  train_label_masks = np.concatenate([train_label_masks,tmp_masks])\n",
    "    \n",
    "test_u_label_masks = np.zeros(len(test_u), dtype=bool)\n",
    "train_u_label_masks = np.zeros(len(train_u), dtype=bool)\n",
    "\n",
    "train_loader = generate_data_loader(train_examples, train_label_masks, label_map, do_shuffle = True, balance_label_examples = False)\n",
    "\n",
    "test_label_masks = np.ones(len(test_examples), dtype=bool)\n",
    "test_loader = generate_data_loader(test_examples, test_label_masks, label_map, do_shuffle = True, balance_label_examples = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "794c3094",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dropout_rate=0.2\n",
    "num_hidden_layers_d = 1\n",
    "hidden_levels_d = [args.latent_size for i in range(0, num_hidden_layers_d)]\n",
    "discriminator = Discriminator(input_size=args.latent_size, hidden_sizes=hidden_levels_d, num_labels=len(label_list), dropout_rate=out_dropout_rate)\n",
    "\n",
    "transformer_vars = [i for i in transformer.parameters()]\n",
    "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
    "#d_vars = [v for v in discriminator.parameters()]\n",
    "\n",
    "epsilon = 1e-8\n",
    "learning_rate_discriminator = 5e-6\n",
    "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
    "\n",
    "discriminator = discriminator.cuda()\n",
    "transformer = transformer.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35ec7797",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_layers_g = 1\n",
    "hidden_levels_g = [args.latent_size for i in range(0, num_hidden_layers_g)]\n",
    "noise_size =100\n",
    "\n",
    "generator = Generator(noise_size=noise_size, output_size=args.latent_size, hidden_sizes=hidden_levels_g, dropout_rate=out_dropout_rate)\n",
    "\n",
    "g_vars = [v for v in generator.parameters()]\n",
    "decoder_vars = g_vars + [i for i in model_decoder.parameters()]\n",
    "\n",
    "learning_rate_generator = 5e-5\n",
    "gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator)\n",
    "\n",
    "generator = generator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da27ee7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/27/2022 14:11:17 - INFO - __main__ -   Epoch: 1 | Batch: 0/10200 (0%) | G Loss: 0.118153\n",
      "06/27/2022 14:11:17 - INFO - __main__ -   Text: ['Soviet Soviet Soviet Recep Recep Liber']\n",
      "06/27/2022 14:11:32 - INFO - __main__ -   Epoch: 1 | Batch: 300/10200 (3%) | G Loss: 0.435581\n",
      "06/27/2022 14:11:32 - INFO - __main__ -   Text: ['by Walter']\n",
      "06/27/2022 14:11:46 - INFO - __main__ -   Epoch: 1 | Batch: 600/10200 (6%) | G Loss: 0.744098\n",
      "06/27/2022 14:11:46 - INFO - __main__ -   Text: ['dating.']\n",
      "06/27/2022 14:12:01 - INFO - __main__ -   Epoch: 1 | Batch: 900/10200 (9%) | G Loss: 0.770248\n",
      "06/27/2022 14:12:01 - INFO - __main__ -   Text: ['to Poverty <BOS> ( <PAD> ,']\n",
      "06/27/2022 14:12:16 - INFO - __main__ -   Epoch: 1 | Batch: 1200/10200 (12%) | G Loss: 0.699463\n",
      "06/27/2022 14:12:16 - INFO - __main__ -   Text: ['(2:3) )']\n",
      "06/27/2022 14:12:31 - INFO - __main__ -   Epoch: 1 | Batch: 1500/10200 (15%) | G Loss: 0.669069\n",
      "06/27/2022 14:12:31 - INFO - __main__ -   Text: ['and have adapted']\n",
      "06/27/2022 14:12:45 - INFO - __main__ -   Epoch: 1 | Batch: 1800/10200 (18%) | G Loss: 0.800124\n",
      "06/27/2022 14:12:46 - INFO - __main__ -   Text: ['and costumes, and jewel, and chains in two']\n",
      "06/27/2022 14:13:01 - INFO - __main__ -   Epoch: 1 | Batch: 2100/10200 (21%) | G Loss: 0.728054\n",
      "06/27/2022 14:13:01 - INFO - __main__ -   Text: ['is the standard DD-h of the Sri Bassi era, before']\n",
      "06/27/2022 14:13:16 - INFO - __main__ -   Epoch: 1 | Batch: 2400/10200 (24%) | G Loss: 0.700889\n",
      "06/27/2022 14:13:16 - INFO - __main__ -   Text: [\"class inequality, and class of their inspire them certainly, strong a set, ferabiddy'-'com\"]\n",
      "06/27/2022 14:13:30 - INFO - __main__ -   Epoch: 1 | Batch: 2700/10200 (26%) | G Loss: 0.650404\n",
      "06/27/2022 14:13:31 - INFO - __main__ -   Text: ['']\n",
      "06/27/2022 14:13:45 - INFO - __main__ -   Epoch: 1 | Batch: 3000/10200 (29%) | G Loss: 0.760462\n",
      "06/27/2022 14:13:45 - INFO - __main__ -   Text: ['changed and renamed in renukulled list and renun ren <PAD> renun renun\" in renun\"']\n",
      "06/27/2022 14:14:00 - INFO - __main__ -   Epoch: 1 | Batch: 3300/10200 (32%) | G Loss: 0.708030\n",
      "06/27/2022 14:14:00 - INFO - __main__ -   Text: ['rendered\" <BOS>aciites.']\n",
      "06/27/2022 14:14:14 - INFO - __main__ -   Epoch: 1 | Batch: 3600/10200 (35%) | G Loss: 0.736004\n",
      "06/27/2022 14:14:14 - INFO - __main__ -   Text: ['Publishing']\n",
      "06/27/2022 14:14:29 - INFO - __main__ -   Epoch: 1 | Batch: 3900/10200 (38%) | G Loss: 0.762728\n",
      "06/27/2022 14:14:29 - INFO - __main__ -   Text: ['of the many <BOS> of the many Italian and Italian talk']\n",
      "06/27/2022 14:14:44 - INFO - __main__ -   Epoch: 1 | Batch: 4200/10200 (41%) | G Loss: 0.767800\n",
      "06/27/2022 14:14:44 - INFO - __main__ -   Text: ['hw <BOS> in the huwagon']\n",
      "06/27/2022 14:14:59 - INFO - __main__ -   Epoch: 1 | Batch: 4500/10200 (44%) | G Loss: 0.742477\n",
      "06/27/2022 14:14:59 - INFO - __main__ -   Text: ['Nickoni takes place in the epicites.']\n",
      "06/27/2022 14:15:14 - INFO - __main__ -   Epoch: 1 | Batch: 4800/10200 (47%) | G Loss: 0.672392\n",
      "06/27/2022 14:15:14 - INFO - __main__ -   Text: ['the most un']\n",
      "06/27/2022 14:15:28 - INFO - __main__ -   Epoch: 1 | Batch: 5100/10200 (50%) | G Loss: 0.726728\n",
      "06/27/2022 14:15:28 - INFO - __main__ -   Text: ['is synonymous with doing <PAD> is not synonymous with doing of having']\n",
      "06/27/2022 14:15:43 - INFO - __main__ -   Epoch: 1 | Batch: 5400/10200 (53%) | G Loss: 0.668551\n",
      "06/27/2022 14:15:43 - INFO - __main__ -   Text: ['game <BOS>ical']\n",
      "06/27/2022 14:15:58 - INFO - __main__ -   Epoch: 1 | Batch: 5700/10200 (56%) | G Loss: 0.774295\n",
      "06/27/2022 14:15:58 - INFO - __main__ -   Text: ['mix <PAD>-crotic mix and use context']\n",
      "06/27/2022 14:16:12 - INFO - __main__ -   Epoch: 1 | Batch: 6000/10200 (59%) | G Loss: 0.754372\n",
      "06/27/2022 14:16:12 - INFO - __main__ -   Text: ['all craft are not']\n",
      "06/27/2022 14:16:27 - INFO - __main__ -   Epoch: 1 | Batch: 6300/10200 (62%) | G Loss: 0.708883\n",
      "06/27/2022 14:16:27 - INFO - __main__ -   Text: ['<BOS>.com. aer.']\n",
      "06/27/2022 14:16:42 - INFO - __main__ -   Epoch: 1 | Batch: 6600/10200 (65%) | G Loss: 0.685915\n",
      "06/27/2022 14:16:42 - INFO - __main__ -   Text: ['scale scale scale scale reaching scale growing in scale grow in scale in scale in scale in a scale']\n",
      "06/27/2022 14:16:57 - INFO - __main__ -   Epoch: 1 | Batch: 6900/10200 (68%) | G Loss: 0.729026\n",
      "06/27/2022 14:16:57 - INFO - __main__ -   Text: ['work <BOS>ized form of form of work']\n",
      "06/27/2022 14:17:12 - INFO - __main__ -   Epoch: 1 | Batch: 7200/10200 (71%) | G Loss: 0.728066\n",
      "06/27/2022 14:17:12 - INFO - __main__ -   Text: [\"– accumulation –Inv. from from to to from to to -' the plight –Inv.Inv. From\"]\n",
      "06/27/2022 14:17:27 - INFO - __main__ -   Epoch: 1 | Batch: 7500/10200 (74%) | G Loss: 0.719209\n",
      "06/27/2022 14:17:27 - INFO - __main__ -   Text: ['<PAD>']\n",
      "06/27/2022 14:17:42 - INFO - __main__ -   Epoch: 1 | Batch: 7800/10200 (76%) | G Loss: 0.709587\n",
      "06/27/2022 14:17:42 - INFO - __main__ -   Text: ['29thst centy%']\n",
      "06/27/2022 14:17:57 - INFO - __main__ -   Epoch: 1 | Batch: 8100/10200 (79%) | G Loss: 0.702567\n",
      "06/27/2022 14:17:57 - INFO - __main__ -   Text: ['<PAD>']\n",
      "06/27/2022 14:18:12 - INFO - __main__ -   Epoch: 1 | Batch: 8400/10200 (82%) | G Loss: 0.731073\n",
      "06/27/2022 14:18:12 - INFO - __main__ -   Text: ['standard, while Martinism standard, and for Britain standard']\n",
      "06/27/2022 14:18:27 - INFO - __main__ -   Epoch: 1 | Batch: 8700/10200 (85%) | G Loss: 0.678584\n",
      "06/27/2022 14:18:27 - INFO - __main__ -   Text: ['uniform of the Ot <PAD>.']\n",
      "06/27/2022 14:18:41 - INFO - __main__ -   Epoch: 1 | Batch: 9000/10200 (88%) | G Loss: 0.719104\n",
      "06/27/2022 14:18:42 - INFO - __main__ -   Text: ['is, \" like, \" cheaper, \" rise, \" values, \" \" attack, \" \\'s Frank']\n",
      "06/27/2022 14:18:56 - INFO - __main__ -   Epoch: 1 | Batch: 9300/10200 (91%) | G Loss: 0.739285\n",
      "06/27/2022 14:18:57 - INFO - __main__ -   Text: ['Hunters of Hallucinum *: rock *: typical rock *: Hillic *: normally known']\n",
      "06/27/2022 14:19:11 - INFO - __main__ -   Epoch: 1 | Batch: 9600/10200 (94%) | G Loss: 0.740812\n",
      "06/27/2022 14:19:11 - INFO - __main__ -   Text: ['then, that by untargu']\n",
      "06/27/2022 14:19:26 - INFO - __main__ -   Epoch: 1 | Batch: 9900/10200 (97%) | G Loss: 0.690748\n",
      "06/27/2022 14:19:26 - INFO - __main__ -   Text: ['sent to-edged s <PAD>um-treme h 3 0 0']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss discriminator: 1.092\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generatore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, args\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m#g_loss, c_loss, test_accuracy = train(epoch)\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     g_loss, test_accuracy, counter \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     accuracy_array\u001b[38;5;241m.\u001b[39mappend(test_accuracy)\n\u001b[1;32m     41\u001b[0m     data_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, counter)\u001b[0m\n\u001b[1;32m    217\u001b[0m transformer\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    218\u001b[0m model_decoder\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 219\u001b[0m \u001b[43mgeneratore\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# Tracking variables \u001b[39;00m\n\u001b[1;32m    222\u001b[0m total_test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generatore' is not defined"
     ]
    }
   ],
   "source": [
    "#train_loader = build_dataload_and_cache_examples(args, [tokenizer_encoder, tokenizer_decoder]) \n",
    "#generator = Generator(args.n_layers, args.block_dim,args.latent_size)\n",
    "#critic = Critic(args.n_layers, args.block_dim, args.latent_size)\n",
    "\n",
    "# if args.generator_dir!=None:\n",
    "#     logger.info(\"Loading generator and critic\")\n",
    "#     generator.load_state_dict(torch.load(args.generator_dir+'/generator_'+str(args.gloabl_step_eval)+'.th'))\n",
    "    #critic.load_state_dict(torch.load(args.generator_dir+'/critic_'+str(args.gloabl_step_eval)+'.th'))\n",
    "    #discriminator.load_state_dict(torch.load(args.generator_dir+'/discriminator_'+str(args.gloabl_step_eval)+'.th'))\n",
    "\n",
    "#g_optimizer = optim.Adam(generator.parameters(), lr=args.lr, betas=(0.5, 0.999))\n",
    "#c_optimizer = optim.Adam(critic.parameters(), lr=args.lr, betas=(0.5, 0.999))\n",
    "#d_optimimizer = optim.Adam(discriminator.parameters(), lr=args.lr, betas=(0.5, 0.999))\n",
    "\n",
    "# if args.cuda:\n",
    "#     generator = generator.cuda()\n",
    "    #critic = critic.cuda()\n",
    "\n",
    "# logger.info('G Parameters:{}'.format(sum([p.numel() for p in generator.parameters() if \\\n",
    "#                             p.requires_grad])))\n",
    "# logger.info('C Parameters:{}'.format(sum([p.numel() for p in critic.parameters() if \\\n",
    "#                             p.requires_grad])))\n",
    "#     logger.info('C Parameters:{}'.format(sum([p.numel() for p in discriminator.parameters() if \\\n",
    "#                                 p.requires_grad])))\n",
    "\n",
    "best_bleu = 0\n",
    "reference = list()\n",
    "with(open(args.valid_data_file,\"r\")) as valid:\n",
    "    for sents in valid:\n",
    "        reference.append(sents.replace(\"\\n\", \"\"))\n",
    "        \n",
    "accuracy_array = []\n",
    "counter = 1\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    #g_loss, c_loss, test_accuracy = train(epoch)\n",
    "    g_loss, test_accuracy, counter = train(epoch, counter)\n",
    "    \n",
    "    accuracy_array.append(test_accuracy)\n",
    "    \n",
    "    data_test = list()\n",
    "    for i in range(2):\n",
    "        #test_noise = torch.Tensor(np.random.normal(0, 1, (250, args.latent_size))).to(args.device)\n",
    "        test_noise = torch.zeros(250, noise_size, device=args.device).uniform_(0, 1)\n",
    "        test_z = generator(test_noise).data\n",
    "        new_sent = rollout_test(model_decoder, test_z, tokenizer_decoder, args.max_seq_length, 250, 0, 1)\n",
    "        data_test.extend(new_sent)\n",
    "\n",
    "    p_reference = random.sample(reference, 500)\n",
    "    bleu = calc_blue_parallel_func(p_reference, data_test, 2, 500)\n",
    "    b_bleu = calc_blue_parallel_func(data_test, p_reference, 2, 500)\n",
    "    logger.info(\"Bleu-2:{:0.3f} | B-Bleu-2:{:0.3f}\".format(bleu, b_bleu))\n",
    "\n",
    "    print(bleu+b_bleu)\n",
    "    if (bleu+b_bleu) > best_bleu:\n",
    "        best_bleu = bleu + b_bleu\n",
    "        logger.info('* Saving. Best Score:{:0.3f} | Bleu-2:{:0.3f} | B-Bleu-2:{:0.3f}'.format(best_bleu, bleu, b_bleu))\n",
    "        torch.save(generator.state_dict(), args.output_dir+'/generator_'+str(args.gloabl_step_eval)+'.th')\n",
    "        #torch.save(critic.state_dict(), args.output_dir+'/critic_'+str(args.gloabl_step_eval)+'.th')\n",
    "        #torch.save(discriminator.state_dict(), args.output_dir+'/discriminator_'+str(args.gloabl_step_eval)+'.th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdb3b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(accuracy_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f349ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_array[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb57bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(accuracy_array)\n",
    "plt.title('HARRYOPTAGAN Yahoo Failed Performance over 200 Training Epochs', fontsize=20)\n",
    "plt.xlabel('Epochs', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.xlim(0,200)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca19afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_save = pd.DataFrame(accuracy_array)\n",
    "df_to_save.to_csv('accuracy_array_optagan_yahoo_no_critic_no_encoder_no_agent_transformer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d5cd37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
