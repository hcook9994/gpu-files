{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55271308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import datetime\n",
    "now_time = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18e4c623",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 11:23:47.638201: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-30 11:23:47.638223: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17b69f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['UNK',1,2,3,4,5,6,7,8,9,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac22c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_l=pd.read_csv(\"../../yahoo/assigned/train_l.csv\", index_col=\"Unnamed: 0\")\n",
    "df_test_l=pd.read_csv(\"../../yahoo/assigned/test_l.csv\", index_col=\"Unnamed: 0\")\n",
    "df_u=pd.read_csv(\"../../yahoo/assigned/u.csv\", index_col=\"Unnamed: 0\")\n",
    "df_train_u=pd.read_csv(\"../../yahoo/assigned/train_u.csv\", index_col=\"Unnamed: 0\")#.head(10000)\n",
    "df_test_u=pd.read_csv(\"../../yahoo/assigned/test_u.csv\", index_col=\"Unnamed: 0\")#.head(5000)\n",
    "df_all = pd.concat([df_train_l, df_test_l, df_u, df_train_u, df_test_u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e52d4083",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_l =  list(df_train_l.to_records(index=False))\n",
    "test_l = list(df_test_l.to_records(index=False))\n",
    "u_list = list(df_u.to_records(index=False))\n",
    "test_u = list(df_test_u.to_records(index=False))\n",
    "train_u = list(df_train_u.to_records(index=False))\n",
    "data_all = list(df_all[\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f5408f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "#  Transformer parameters\n",
    "#--------------------------------\n",
    "max_seq_length = 24\n",
    "batch_size = 64\n",
    "\n",
    "#--------------------------------\n",
    "#  GAN-BERT specific parameters\n",
    "#--------------------------------\n",
    "# number of hidden layers in the generator, \n",
    "# each of the size of the output space\n",
    "#num_hidden_layers_g = 1; \n",
    "# number of hidden layers in the discriminator, \n",
    "# each of the size of the input space\n",
    "num_hidden_layers_d = 1; \n",
    "# size of the generator's input noisy vectors\n",
    "noise_size = 100\n",
    "# dropout to be applied to discriminator's input vectors\n",
    "out_dropout_rate = 0.2\n",
    "\n",
    "# Replicate labeled data to balance poorly represented datasets, \n",
    "# e.g., less than 1% of labeled material\n",
    "apply_balance = True\n",
    "\n",
    "#--------------------------------\n",
    "#  Optimization parameters\n",
    "#--------------------------------\n",
    "learning_rate_discriminator = 5e-6 #5e-6?\n",
    "#learning_rate_generator = 5e-5\n",
    "epsilon = 1e-8\n",
    "num_train_epochs = 200\n",
    "multi_gpu = True\n",
    "# Scheduler\n",
    "apply_scheduler = False\n",
    "warmup_proportion = 0.1\n",
    "# Print\n",
    "print_each_n_step = 10\n",
    "\n",
    "#--------------------------------\n",
    "#  Adopted Tranformer model\n",
    "#--------------------------------\n",
    "# Since this version is compatible with Huggingface transformers, you can uncomment\n",
    "# (or add) transformer models compatible with GAN\n",
    "\n",
    "model_name = \"bert-base-cased\"\n",
    "#model_name = \"bert-base-uncased\"\n",
    "#model_name = \"roberta-base\"\n",
    "#model_name = \"albert-base-v2\"\n",
    "#model_name = \"xlm-roberta-base\"\n",
    "#model_name = \"amazon/bort\"\n",
    "#model_name=\"google/electra-large-discriminator\"\n",
    "#model_name=\"google/electra-small-discriminator\"\n",
    "#model_name=\"microsoft/deberta-v2-xxlarge\"\n",
    "#model_name=\"microsoft/deberta-v3-base\"\n",
    "#model_name = \"google/electra-base-discriminator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4758bf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "transformer = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7beb0062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_loader(input_examples, label_masks, label_map, do_shuffle = False, balance_label_examples = False):\n",
    "  '''\n",
    "  Generate a Dataloader given the input examples, eventually masked if they are \n",
    "  to be considered NOT labeled.\n",
    "  '''\n",
    "  examples = []\n",
    "\n",
    "  # Count the percentage of labeled examples  \n",
    "  num_labeled_examples = 0\n",
    "  for label_mask in label_masks:\n",
    "    if label_mask: \n",
    "      num_labeled_examples += 1\n",
    "  label_mask_rate = num_labeled_examples/len(input_examples)\n",
    "\n",
    "  # if required it applies the balance\n",
    "  for index, ex in enumerate(input_examples): \n",
    "    if label_mask_rate == 1 or not balance_label_examples:\n",
    "      examples.append((ex, label_masks[index]))\n",
    "    else:\n",
    "      # IT SIMULATE A LABELED EXAMPLE\n",
    "      if label_masks[index]:\n",
    "        balance = int(1/label_mask_rate)\n",
    "        balance = int(math.log(balance,2))\n",
    "        if balance < 1:\n",
    "          balance = 1\n",
    "        for b in range(0, int(balance)):\n",
    "          examples.append((ex, label_masks[index]))\n",
    "      else:\n",
    "        examples.append((ex, label_masks[index]))\n",
    "  \n",
    "  #-----------------------------------------------\n",
    "  # Generate input examples to the Transformer\n",
    "  #-----------------------------------------------\n",
    "  input_ids = []\n",
    "  input_mask_array = []\n",
    "  label_mask_array = []\n",
    "  label_id_array = []\n",
    "\n",
    "  # Tokenization \n",
    "  for (text, label_mask) in examples:\n",
    "    encoded_sent = tokenizer.encode(text[0], add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
    "    input_ids.append(encoded_sent)\n",
    "    label_id_array.append(label_map[text[1]])\n",
    "    label_mask_array.append(label_mask)\n",
    "  \n",
    "  # Attention to token (to ignore padded input wordpieces)\n",
    "  for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]                          \n",
    "    input_mask_array.append(att_mask)\n",
    "  # Convertion to Tensor\n",
    "  input_ids = torch.tensor(input_ids) \n",
    "  input_mask_array = torch.tensor(input_mask_array)\n",
    "  label_id_array = torch.tensor(label_id_array, dtype=torch.long)\n",
    "  label_mask_array = torch.tensor(label_mask_array)\n",
    "\n",
    "  # Building the TensorDataset\n",
    "  dataset = TensorDataset(input_ids, input_mask_array, label_id_array, label_mask_array)\n",
    "\n",
    "  if do_shuffle:\n",
    "    sampler = RandomSampler\n",
    "  else:\n",
    "    sampler = SequentialSampler\n",
    "\n",
    "  # Building the DataLoader\n",
    "  return DataLoader(\n",
    "              dataset,  # The training samples.\n",
    "              sampler = sampler(dataset), \n",
    "              batch_size = batch_size) # Trains with this batch size.\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76f98a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_fake(input_examples):\n",
    "  '''\n",
    "  Generate a Dataloader given the input examples, eventually masked if they are \n",
    "  to be considered NOT labeled.\n",
    "  '''\n",
    "  \n",
    "  #-----------------------------------------------\n",
    "  # Generate input examples to the Transformer\n",
    "  #-----------------------------------------------\n",
    "  input_ids = []\n",
    "  input_mask_array = []\n",
    "\n",
    "  # Tokenization \n",
    "  for text in input_examples:\n",
    "    encoded_sent = tokenizer.encode(text, add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
    "    input_ids.append(encoded_sent)\n",
    "  \n",
    "  # Attention to token (to ignore padded input wordpieces)\n",
    "  for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]                          \n",
    "    input_mask_array.append(att_mask)\n",
    "  # Convertion to Tensor\n",
    "  input_ids = torch.tensor(input_ids) \n",
    "  input_mask_array = torch.tensor(input_mask_array)\n",
    "\n",
    "  # Building the DataLoader\n",
    "  return input_ids, input_mask_array # Trains with this batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfd53b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the examples\n",
    "labeled_examples = train_l\n",
    "unlabeled_examples = u_list\n",
    "test_examples = test_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c169846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1651697/1314345009.py:54: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  label_mask_array = torch.tensor(label_mask_array)\n"
     ]
    }
   ],
   "source": [
    "label_map = {}\n",
    "for (i, label) in enumerate(label_list):\n",
    "  label_map[label] = i\n",
    "#------------------------------\n",
    "#   Load the train dataset\n",
    "#------------------------------\n",
    "train_examples = labeled_examples\n",
    "#The labeled (train) dataset is assigned with a mask set to True\n",
    "train_label_masks = np.ones(len(labeled_examples), dtype=bool)\n",
    "#If unlabel examples are available\n",
    "if unlabeled_examples:\n",
    "  train_examples = train_examples + unlabeled_examples\n",
    "  #The unlabeled (train) dataset is assigned with a mask set to False\n",
    "  tmp_masks = np.zeros(len(unlabeled_examples), dtype=bool)\n",
    "  train_label_masks = np.concatenate([train_label_masks,tmp_masks])\n",
    "\n",
    "train_dataloader = generate_data_loader(train_examples, train_label_masks, label_map, do_shuffle = True, balance_label_examples = apply_balance)\n",
    "\n",
    "#------------------------------\n",
    "#   Load the test dataset\n",
    "#------------------------------\n",
    "#The labeled (test) dataset is assigned with a mask set to True\n",
    "test_label_masks = np.ones(len(test_examples), dtype=bool)\n",
    "\n",
    "test_dataloader = generate_data_loader(test_examples, test_label_masks, label_map, do_shuffle = False, balance_label_examples = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bcab6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------\n",
    "#   The Discriminator\n",
    "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
    "#   https://github.com/crux82/ganbert\n",
    "#------------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
    "        layers = []\n",
    "        hidden_sizes = [input_size] + hidden_sizes\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
    "\n",
    "        self.layers = nn.Sequential(*layers) #per il flatten\n",
    "        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1) # +1 for the probability of this sample being fake/real.\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_rep):\n",
    "        input_rep = self.input_dropout(input_rep)\n",
    "        last_rep = self.layers(input_rep)\n",
    "        logits = self.logit(last_rep)\n",
    "        probs = self.softmax(logits)\n",
    "        return last_rep, logits, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d197ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The config file is required to get the dimension of the vector produced by \n",
    "# the underlying transformer\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "hidden_size = int(config.hidden_size)*4\n",
    "# Define the number and width of hidden layers\n",
    "#hidden_levels_g = [hidden_size for i in range(0, num_hidden_layers_g)]\n",
    "hidden_levels_d = [hidden_size for i in range(0, num_hidden_layers_d)]\n",
    "\n",
    "#-------------------------------------------------\n",
    "#   Instantiate the Generator and Discriminator\n",
    "#-------------------------------------------------\n",
    "#generator = Generator(noise_size=noise_size, output_size=hidden_size, hidden_sizes=hidden_levels_g, dropout_rate=out_dropout_rate)\n",
    "discriminator = Discriminator(input_size=hidden_size, hidden_sizes=hidden_levels_d, num_labels=len(label_list), dropout_rate=out_dropout_rate)\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Put everything in the GPU if available\n",
    "if torch.cuda.is_available():    \n",
    "  #generator.cuda()\n",
    "  discriminator.cuda(device)\n",
    "  transformer.cuda(device)\n",
    "#   if multi_gpu:\n",
    "#     transformer = torch.nn.DataParallel(transformer)\n",
    "\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "800d0186",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stats = []\n",
    "\n",
    "accuracy_array=[]\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "#models parameters\n",
    "transformer_vars = [i for i in transformer.parameters()]\n",
    "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
    "#g_vars = [v for v in generator.parameters()]\n",
    "\n",
    "#optimizer\n",
    "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
    "#gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator) \n",
    "\n",
    "#scheduler\n",
    "if apply_scheduler:\n",
    "  num_train_examples = len(train_examples)\n",
    "  num_train_steps = int(num_train_examples / batch_size * num_train_epochs)\n",
    "  num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
    "\n",
    "  scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n",
    "                                           num_warmup_steps = num_warmup_steps)\n",
    "  scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n",
    "                                           num_warmup_steps = num_warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7db4d007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/harry/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#OPTAGAN\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import argparse\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from modules.gan import Generator, Critic\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from func import GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig, BertConfig\n",
    "from func import GPT2LMHeadModel, GPT2Tokenizer, GPT2ForLatentConnector, GPT2ForLatentConnectorValueHead\n",
    "from func import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer\n",
    "from func import XLNetLMHeadModel, XLNetTokenizer\n",
    "from func import TransfoXLLMHeadModel, TransfoXLTokenizer\n",
    "from func import BertForLatentConnector, BertTokenizer\n",
    "\n",
    "from collections import defaultdict\n",
    "from utils import (TextDataset_Split, TextDataset_2Tokenizers, BucketingDataLoader)\n",
    "import pdb\n",
    "from modules.utils import (calc_blue_parallel_func, pad_seq, rollout, rollout_test)\n",
    "#from transformers.modeling_utils import top_k_top_p_filtering\n",
    "\n",
    "\n",
    "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) for conf in (GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig)), ())\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'gpt2': (GPT2Config, GPT2ForLatentConnectorValueHead, GPT2Tokenizer),\n",
    "    'bert': (BertConfig, BertForLatentConnector, BertTokenizer)\n",
    "}\n",
    "\n",
    "num_txt = 1\n",
    "\n",
    "def load_and_cache_examples(args, tokenizer):\n",
    "    if isinstance(tokenizer, list):\n",
    "        dataset = TextDataset_2Tokenizers(tokenizer, args, args.train_data_file, block_size=args.block_size)\n",
    "    else:\n",
    "        dataset = TextDataset_Split(tokenizer, args, args.train_data_file, block_size=args.block_size)\n",
    "    return dataset\n",
    "\n",
    "def build_dataload_and_cache_examples(args, tokenizer, num_txt):\n",
    "    if isinstance(tokenizer, list):\n",
    "        args.batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "        if num_txt<=20:\n",
    "            concatenation=\"{}_{}{}\".format(args.train_data_file, num_txt, \".txt\")\n",
    "            file_path=concatenation\n",
    "            print(\"Train file used is number {}\".format(num_txt))\n",
    "            print(concatenation)\n",
    "            num_txt=num_txt+1\n",
    "        else:\n",
    "            num_txt=1\n",
    "            concatenation=\"{}_{}{}\".format(args.train_data_file, num_txt, \".txt\")\n",
    "            file_path=concatenation\n",
    "            print(\"Train file used is number {}\".format(num_txt))\n",
    "        dataloader = BucketingDataLoader(file_path, args.batch_size, args.max_seq_length, tokenizer, args, bucket=100, shuffle=True)\n",
    "    else:\n",
    "        pass \n",
    "    return dataloader, num_txt\n",
    "\n",
    "def compute_grad_penalty(critic, real_data, fake_data):\n",
    "    B = real_data.size(0)\n",
    "    alpha = torch.FloatTensor(np.random.random((B, 1)))\n",
    "    if args.cuda:\n",
    "        alpha = alpha.cuda(device)\n",
    "    sample = alpha*real_data + (1-alpha)*fake_data\n",
    "    sample.requires_grad_(True)\n",
    "    score = critic(sample)\n",
    "\n",
    "    outputs = torch.FloatTensor(B, 1).fill_(1.0) #args.latent_size\n",
    "    outputs.requires_grad_(False)\n",
    "    if args.cuda:\n",
    "        outputs = outputs.cuda(device)\n",
    "    grads = autograd.grad(\n",
    "        outputs=score,\n",
    "        inputs=sample,\n",
    "        grad_outputs=outputs,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True)[0]\n",
    "    grad_penalty = ((grads.norm(2, dim=1) - 1.) ** 2).mean()\n",
    "    return grad_penalty\n",
    "\n",
    "def train(epoch):\n",
    "    model_encoder.eval()\n",
    "    model_decoder.eval()\n",
    "    generator.train()\n",
    "    critic.train()\n",
    "    c_train_loss = 0.\n",
    "    g_train_loss = 0.\n",
    "    g_batches = 0\n",
    "    c_batches = 0\n",
    "    c_loss_0 = 1\n",
    "    g_loss_0 = 1\n",
    "    for i, x in enumerate(train_loader):\n",
    "        x = x[0]\n",
    "        if args.cuda:\n",
    "            x = x.cuda(device)\n",
    "        # Generate noise\n",
    "        B = args.per_gpu_train_batch_size\n",
    "        noise = torch.from_numpy(np.random.normal(0, 1, (B,\n",
    "                                 args.latent_size))).float()\n",
    "        if args.cuda:\n",
    "            noise = noise.cuda(device)\n",
    "        # Get original text latent embeddings\n",
    "        with torch.no_grad(): \n",
    "            pooled_hidden_fea = model_encoder(x, attention_mask=(x > 0).float())[1]\n",
    "            mean, logvar = model_encoder.linear(pooled_hidden_fea).chunk(2, -1)\n",
    "            z_real = mean.squeeze(1) \n",
    "\n",
    "        # Evaluate and get losses\n",
    "        z_fake = generator(noise)\n",
    "        real_score = critic(z_real)\n",
    "        fake_score = critic(z_fake)\n",
    "        grad_penalty = compute_grad_penalty(critic, z_real.data, z_fake.data)\n",
    "        c_loss = -torch.mean(real_score) + torch.mean(fake_score) + \\\n",
    "                 args.gp_lambda*grad_penalty\n",
    "\n",
    "        fake_score = critic(generator(noise))\n",
    "        g_loss = -torch.mean(fake_score)\n",
    "        \n",
    "        r_g = abs(((g_loss.item() - g_loss_0) / (g_loss_0 + 0.001))) \n",
    "        r_c = abs(((c_loss.item() - c_loss_0) / (c_loss_0 + 0.001))) \n",
    "        \n",
    "        # Update critic or generator\n",
    "        if ((2 + epoch) / epoch) * r_c > r_g:\n",
    "            c_optimizer.zero_grad()\n",
    "            c_batches += 1\n",
    "            c_train_loss += c_loss.item()\n",
    "            c_loss.backward()\n",
    "            c_optimizer.step()\n",
    "        else:\n",
    "            g_optimizer.zero_grad()\n",
    "            g_batches += 1\n",
    "            g_train_loss += g_loss.item()\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "        c_loss_0 = c_loss.item()\n",
    "        g_loss_0 = g_loss.item()\n",
    "\n",
    "        if args.interval > 0 and i % args.interval == 0:\n",
    "            logger.info('Epoch: {} | Batch: {}/{} ({:.0f}%) | G Loss: {:.6f} | C Loss: {:.6f}'.format(\n",
    "                epoch, args.batch_size*i, len(train_loader.dataset),\n",
    "                100.*(args.batch_size*i)/len(train_loader.dataset),\n",
    "                g_loss.item(), c_loss.item()\n",
    "            ))\n",
    "            test_noise = torch.Tensor(np.random.normal(0, 1, (1, args.latent_size))).to(args.device)\n",
    "            test_new_z = generator(test_noise).data\n",
    "            # create new sent\n",
    "            test_z = rollout_test(model_decoder, test_new_z, tokenizer_decoder, args.max_seq_length, 1, 0, 1)\n",
    "            logger.info(\"Text: {}\".format(test_z))\n",
    "\n",
    "    c_train_loss /= c_batches + 1\n",
    "    g_train_loss /= g_batches + 1\n",
    "    logger.info('* (Train) Epoch: {} | G Loss: {:.4f} | C Loss: {:.4f} | Updates G: {} | Updates C: {}'.format(\n",
    "        epoch, g_train_loss, c_train_loss, g_batches, c_batches\n",
    "    ))\n",
    "    return (g_train_loss, c_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eef4b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:24:07 - INFO - func.configuration_utils -   loading configuration file checkpoint-508523-768-0/checkpoint-encoder-508523/config.json\n",
      "06/30/2022 11:24:07 - INFO - func.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "06/30/2022 11:24:07 - INFO - func.modeling_utils -   loading weights file checkpoint-508523-768-0/checkpoint-encoder-508523/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size_rl=32, block_dim=100, block_size=100, checkpoint_dir='checkpoint-508523-768-0', cuda=True, dataset='EMNLP', decoder_config_name='', decoder_model_name_or_path='gpt2', decoder_model_type='gpt2', decoder_tokenizer_name='', do_lower_case=False, encoder_config_name='', encoder_model_name_or_path='bert-base-cased', encoder_model_type='bert', encoder_tokenizer_name='', epochs=200, epochs_rl=100, finetune_decoder=True, generator_dir=None, gloabl_step_eval=508523, gp_lambda=10, interval=50, latent_size=768, length=20, lr=0.0001, lr_rl=1e-06, max_seq_length=24, n_layers=10, output_dir='checkpoint-508523-768-0', padding_text='', per_gpu_train_batch_size=10, prompt='', seed=0, train_data_file='../../yahoo/subdivided_large/train', use_philly=False, valid_data_file='../../yahoo/unlabelled_short/test.txt')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:24:10 - INFO - func.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/harry/.cache/torch/pytorch_transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "06/30/2022 11:24:10 - INFO - func.configuration_utils -   loading configuration file checkpoint-508523-768-0/checkpoint-decoder-508523/config.json\n",
      "06/30/2022 11:24:10 - INFO - func.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"latent_size\": 768,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"vocab_size\": 50260\n",
      "}\n",
      "\n",
      "06/30/2022 11:24:10 - INFO - func.modeling_utils -   loading weights file checkpoint-508523-768-0/checkpoint-decoder-508523/pytorch_model.bin\n",
      "06/30/2022 11:24:12 - INFO - func.modeling_utils -   Weights of GPT2ForLatentConnectorValueHead not initialized from pretrained model: ['v_head.linear1.weight', 'v_head.linear1.bias', 'v_head.linear2.weight', 'v_head.linear2.bias']\n",
      "06/30/2022 11:24:15 - INFO - func.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/harry/.cache/torch/pytorch_transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "06/30/2022 11:24:15 - INFO - func.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/harry/.cache/torch/pytorch_transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "06/30/2022 11:24:15 - INFO - func.tokenization_utils -   Adding <PAD> to the vocabulary\n",
      "06/30/2022 11:24:15 - INFO - func.tokenization_utils -   Assigning <PAD> to the pad_token key of the tokenizer\n",
      "06/30/2022 11:24:15 - INFO - func.tokenization_utils -   Adding <BOS> to the vocabulary\n",
      "06/30/2022 11:24:15 - INFO - func.tokenization_utils -   Assigning <BOS> to the bos_token key of the tokenizer\n",
      "06/30/2022 11:24:15 - INFO - func.tokenization_utils -   Adding <EOS> to the vocabulary\n",
      "06/30/2022 11:24:15 - INFO - func.tokenization_utils -   Assigning <EOS> to the eos_token key of the tokenizer\n",
      "06/30/2022 11:24:15 - INFO - __main__ -   We have added 3 tokens to GPT2\n",
      "06/30/2022 11:24:15 - INFO - __main__ -   G Parameters:255468\n",
      "06/30/2022 11:24:15 - INFO - __main__ -   C Parameters:178001\n",
      "06/30/2022 11:24:15 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_1.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file used is number 1\n",
      "../../yahoo/subdivided_large/train_1.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 1 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:13.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:29.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:37.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:43.\n",
      "  Batch    70  of    172.    Elapsed: 0:00:50.\n",
      "  Batch    80  of    172.    Elapsed: 0:00:57.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:03.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:12.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:18.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:23.\n",
      "  Batch   130  of    172.    Elapsed: 0:01:31.\n",
      "  Batch   140  of    172.    Elapsed: 0:01:40.\n",
      "  Batch   150  of    172.    Elapsed: 0:01:47.\n",
      "  Batch   160  of    172.    Elapsed: 0:01:51.\n",
      "  Batch   170  of    172.    Elapsed: 0:01:56.\n",
      "\n",
      "  Average training loss discriminator: 2.543\n",
      "  Training epcoh took: 0:01:57\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:26:12 - INFO - __main__ -   Epoch: 1 | Batch: 0/10000 (0%) | G Loss: 0.198025 | C Loss: 1.785758\n",
      "06/30/2022 11:26:12 - INFO - __main__ -   Text: ['']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.230\n",
      "  Test Loss: 2.192\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:26:13 - INFO - __main__ -   Epoch: 1 | Batch: 500/10000 (5%) | G Loss: 165.133804 | C Loss: -107.741547\n",
      "06/30/2022 11:26:13 - INFO - __main__ -   Text: ['']\n",
      "06/30/2022 11:26:15 - INFO - __main__ -   Epoch: 1 | Batch: 1000/10000 (10%) | G Loss: 143.557938 | C Loss: -94.116470\n",
      "06/30/2022 11:26:15 - INFO - __main__ -   Text: ['']\n",
      "06/30/2022 11:26:16 - INFO - __main__ -   Epoch: 1 | Batch: 1500/10000 (15%) | G Loss: 124.540504 | C Loss: -77.979843\n",
      "06/30/2022 11:26:16 - INFO - __main__ -   Text: ['']\n",
      "06/30/2022 11:26:18 - INFO - __main__ -   Epoch: 1 | Batch: 2000/10000 (20%) | G Loss: 96.772186 | C Loss: -65.074715\n",
      "06/30/2022 11:26:18 - INFO - __main__ -   Text: ['season....burn. 35']\n",
      "06/30/2022 11:26:19 - INFO - __main__ -   Epoch: 1 | Batch: 2500/10000 (25%) | G Loss: 97.257828 | C Loss: -68.079773\n",
      "06/30/2022 11:26:19 - INFO - __main__ -   Text: ['']\n",
      "06/30/2022 11:26:21 - INFO - __main__ -   Epoch: 1 | Batch: 3000/10000 (30%) | G Loss: 82.774925 | C Loss: -60.342960\n",
      "06/30/2022 11:26:21 - INFO - __main__ -   Text: ['On Buckingham Bell Th. in. Lels. Dr Bond Parliamentary or was., and held..']\n",
      "06/30/2022 11:26:22 - INFO - __main__ -   Epoch: 1 | Batch: 3500/10000 (35%) | G Loss: 71.902580 | C Loss: -53.366158\n",
      "06/30/2022 11:26:22 - INFO - __main__ -   Text: ['']\n",
      "06/30/2022 11:26:24 - INFO - __main__ -   Epoch: 1 | Batch: 4000/10000 (40%) | G Loss: 67.922562 | C Loss: -50.828922\n",
      "06/30/2022 11:26:24 - INFO - __main__ -   Text: ['']\n",
      "06/30/2022 11:26:25 - INFO - __main__ -   Epoch: 1 | Batch: 4500/10000 (45%) | G Loss: 49.377647 | C Loss: -38.321365\n",
      "06/30/2022 11:26:25 - INFO - __main__ -   Text: ['of Ride Nutly G Mr Shine Pierce Pass. History. above. Pick. St. just. Theodore.']\n",
      "06/30/2022 11:26:27 - INFO - __main__ -   Epoch: 1 | Batch: 5000/10000 (50%) | G Loss: 50.369228 | C Loss: -35.712097\n",
      "06/30/2022 11:26:27 - INFO - __main__ -   Text: ['<BOS> <BOS>']\n",
      "06/30/2022 11:26:28 - INFO - __main__ -   Epoch: 1 | Batch: 5500/10000 (55%) | G Loss: 43.785545 | C Loss: -34.743217\n",
      "06/30/2022 11:26:28 - INFO - __main__ -   Text: ['Pirates Rumurance onodeam Dodge Prof. <PAD> I debates.']\n",
      "06/30/2022 11:26:30 - INFO - __main__ -   Epoch: 1 | Batch: 6000/10000 (60%) | G Loss: 36.459694 | C Loss: -28.994633\n",
      "06/30/2022 11:26:30 - INFO - __main__ -   Text: ['']\n",
      "06/30/2022 11:26:31 - INFO - __main__ -   Epoch: 1 | Batch: 6500/10000 (65%) | G Loss: 38.966991 | C Loss: -30.787443\n",
      "06/30/2022 11:26:31 - INFO - __main__ -   Text: ['']\n",
      "06/30/2022 11:26:33 - INFO - __main__ -   Epoch: 1 | Batch: 7000/10000 (70%) | G Loss: 34.812542 | C Loss: -27.694727\n",
      "06/30/2022 11:26:33 - INFO - __main__ -   Text: ['a..\",.,. make. Cairo. in.rying.,. in history North and']\n",
      "06/30/2022 11:26:34 - INFO - __main__ -   Epoch: 1 | Batch: 7500/10000 (75%) | G Loss: 27.756470 | C Loss: -22.646416\n",
      "06/30/2022 11:26:34 - INFO - __main__ -   Text: ['<PAD> <PAD> <PAD> Dis. Kenya Edit. ( end won.']\n",
      "06/30/2022 11:26:36 - INFO - __main__ -   Epoch: 1 | Batch: 8000/10000 (80%) | G Loss: 29.780806 | C Loss: -24.142967\n",
      "06/30/2022 11:26:36 - INFO - __main__ -   Text: ['Dale.']\n",
      "06/30/2022 11:26:37 - INFO - __main__ -   Epoch: 1 | Batch: 8500/10000 (85%) | G Loss: 27.115995 | C Loss: -22.165119\n",
      "06/30/2022 11:26:38 - INFO - __main__ -   Text: ['In Commercial For L... L........\"...)...']\n",
      "06/30/2022 11:26:39 - INFO - __main__ -   Epoch: 1 | Batch: 9000/10000 (90%) | G Loss: 25.845884 | C Loss: -21.198334\n",
      "06/30/2022 11:26:39 - INFO - __main__ -   Text: ['']\n",
      "06/30/2022 11:26:41 - INFO - __main__ -   Epoch: 1 | Batch: 9500/10000 (95%) | G Loss: 27.343550 | C Loss: -22.753040\n",
      "06/30/2022 11:26:41 - INFO - __main__ -   Text: ['rac.. Total. Global Cal. 158 / 7 National. most. L. Gl Lang. M.']\n",
      "06/30/2022 11:26:42 - INFO - __main__ -   * (Train) Epoch: 1 | G Loss: 60.9381 | C Loss: -44.5088 | Updates G: 49 | Updates C: 951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:26:52 - INFO - __main__ -   Bleu-2:0.114 | B-Bleu-2:0.116\n",
      "06/30/2022 11:26:52 - INFO - __main__ -   * Saving. Best Score:0.230 | Bleu-2:0.114 | B-Bleu-2:0.116\n",
      "06/30/2022 11:26:52 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_2.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23005238243093828\n",
      "Train file used is number 2\n",
      "../../yahoo/subdivided_large/train_2.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 2 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:12.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:24.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:36.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:48.\n",
      "  Batch    50  of    172.    Elapsed: 0:01:00.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:12.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:24.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:36.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:48.\n",
      "  Batch   100  of    172.    Elapsed: 0:02:00.\n",
      "  Batch   110  of    172.    Elapsed: 0:02:12.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:24.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:36.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:48.\n",
      "  Batch   150  of    172.    Elapsed: 0:03:00.\n",
      "  Batch   160  of    172.    Elapsed: 0:03:12.\n",
      "  Batch   170  of    172.    Elapsed: 0:03:24.\n",
      "\n",
      "  Average training loss discriminator: 1.731\n",
      "  Training epcoh took: 0:03:26\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:30:18 - INFO - __main__ -   Epoch: 2 | Batch: 0/10001 (0%) | G Loss: 23.098394 | C Loss: -18.858589\n",
      "06/30/2022 11:30:18 - INFO - __main__ -   Text: ['WW My.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.350\n",
      "  Test Loss: 1.891\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:30:19 - INFO - __main__ -   Epoch: 2 | Batch: 500/10001 (5%) | G Loss: 21.244526 | C Loss: -18.285929\n",
      "06/30/2022 11:30:19 - INFO - __main__ -   Text: [\"human... and fluats.'),. the name. at Iy.. F. official.\"]\n",
      "06/30/2022 11:30:21 - INFO - __main__ -   Epoch: 2 | Batch: 1000/10001 (10%) | G Loss: 18.311890 | C Loss: -16.008207\n",
      "06/30/2022 11:30:21 - INFO - __main__ -   Text: ['snakes.\" in the. .. Frley tears. pathopter... target. Elan..']\n",
      "06/30/2022 11:30:22 - INFO - __main__ -   Epoch: 2 | Batch: 1500/10001 (15%) | G Loss: 17.182837 | C Loss: -15.079105\n",
      "06/30/2022 11:30:23 - INFO - __main__ -   Text: ['This images.\".. operating available. from were. Gal.. With).. \". It. and.']\n",
      "06/30/2022 11:30:24 - INFO - __main__ -   Epoch: 2 | Batch: 2000/10001 (20%) | G Loss: 16.018772 | C Loss: -14.354933\n",
      "06/30/2022 11:30:24 - INFO - __main__ -   Text: ['which A Godless who .\".']\n",
      "06/30/2022 11:30:26 - INFO - __main__ -   Epoch: 2 | Batch: 2500/10001 (25%) | G Loss: 16.664881 | C Loss: -14.898582\n",
      "06/30/2022 11:30:26 - INFO - __main__ -   Text: ['Seekle Thiso to Hon Beatkin Port Marks First Hat Off. . Head.train.. to']\n",
      "06/30/2022 11:30:27 - INFO - __main__ -   Epoch: 2 | Batch: 3000/10001 (30%) | G Loss: 12.765788 | C Loss: -12.214834\n",
      "06/30/2022 11:30:27 - INFO - __main__ -   Text: ['Man, may raise a public by hee\" <BOS>.']\n",
      "06/30/2022 11:30:29 - INFO - __main__ -   Epoch: 2 | Batch: 3500/10001 (35%) | G Loss: 11.207479 | C Loss: -10.541281\n",
      "06/30/2022 11:30:29 - INFO - __main__ -   Text: ['V Mary The Bond The They Knock It The B !']\n",
      "06/30/2022 11:30:30 - INFO - __main__ -   Epoch: 2 | Batch: 4000/10001 (40%) | G Loss: 9.465363 | C Loss: -9.571609\n",
      "06/30/2022 11:30:30 - INFO - __main__ -   Text: [') .']\n",
      "06/30/2022 11:30:31 - INFO - __main__ -   Epoch: 2 | Batch: 4500/10001 (45%) | G Loss: 9.229304 | C Loss: -9.191293\n",
      "06/30/2022 11:30:31 - INFO - __main__ -   Text: ['\" Voting They!']\n",
      "06/30/2022 11:30:33 - INFO - __main__ -   Epoch: 2 | Batch: 5000/10001 (50%) | G Loss: 8.599711 | C Loss: -8.270650\n",
      "06/30/2022 11:30:33 - INFO - __main__ -   Text: ['Hug Whitman writes \" short notes my profile \".']\n",
      "06/30/2022 11:30:34 - INFO - __main__ -   Epoch: 2 | Batch: 5500/10001 (55%) | G Loss: 6.757656 | C Loss: -6.727028\n",
      "06/30/2022 11:30:34 - INFO - __main__ -   Text: ['Offer from An Offer to the Part For Those, a The But As.']\n",
      "06/30/2022 11:30:36 - INFO - __main__ -   Epoch: 2 | Batch: 6000/10001 (60%) | G Loss: 6.568805 | C Loss: -6.382906\n",
      "06/30/2022 11:30:36 - INFO - __main__ -   Text: ['His home in Michigan is a buyer.']\n",
      "06/30/2022 11:30:37 - INFO - __main__ -   Epoch: 2 | Batch: 6500/10001 (65%) | G Loss: 6.387315 | C Loss: -6.302121\n",
      "06/30/2022 11:30:37 - INFO - __main__ -   Text: ['is a Credit Code\" According to many en Rappers.']\n",
      "06/30/2022 11:30:39 - INFO - __main__ -   Epoch: 2 | Batch: 7000/10001 (70%) | G Loss: 5.935838 | C Loss: -5.743687\n",
      "06/30/2022 11:30:39 - INFO - __main__ -   Text: ['Eventually may you have yours?']\n",
      "06/30/2022 11:30:40 - INFO - __main__ -   Epoch: 2 | Batch: 7500/10001 (75%) | G Loss: 4.510571 | C Loss: -4.545758\n",
      "06/30/2022 11:30:40 - INFO - __main__ -   Text: ['Others want these \" Their want even field.']\n",
      "06/30/2022 11:30:42 - INFO - __main__ -   Epoch: 2 | Batch: 8000/10001 (80%) | G Loss: 4.598644 | C Loss: -4.267575\n",
      "06/30/2022 11:30:42 - INFO - __main__ -   Text: ['\"If you wish them a good car, I is it hardy.']\n",
      "06/30/2022 11:30:43 - INFO - __main__ -   Epoch: 2 | Batch: 8500/10001 (85%) | G Loss: 5.071875 | C Loss: -4.701849\n",
      "06/30/2022 11:30:43 - INFO - __main__ -   Text: ['jealous jealousy!']\n",
      "06/30/2022 11:30:44 - INFO - __main__ -   Epoch: 2 | Batch: 9000/10001 (90%) | G Loss: 4.491385 | C Loss: -3.972384\n",
      "06/30/2022 11:30:44 - INFO - __main__ -   Text: ['The message?']\n",
      "06/30/2022 11:30:46 - INFO - __main__ -   Epoch: 2 | Batch: 9500/10001 (95%) | G Loss: 5.152290 | C Loss: -4.604696\n",
      "06/30/2022 11:30:46 - INFO - __main__ -   Text: ['Ritual is 2-1 for the gold) and a heat.']\n",
      "06/30/2022 11:30:47 - INFO - __main__ -   * (Train) Epoch: 2 | G Loss: 9.1296 | C Loss: -9.3195 | Updates G: 132 | Updates C: 868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:30:57 - INFO - __main__ -   Bleu-2:0.178 | B-Bleu-2:0.229\n",
      "06/30/2022 11:30:57 - INFO - __main__ -   * Saving. Best Score:0.407 | Bleu-2:0.178 | B-Bleu-2:0.229\n",
      "06/30/2022 11:30:57 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_3.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4074562209734345\n",
      "Train file used is number 3\n",
      "../../yahoo/subdivided_large/train_3.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 3 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:30.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:40.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:50.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:00.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:10.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:21.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:30.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:41.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:51.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:01.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:12.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:21.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:31.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:41.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:51.\n",
      "\n",
      "  Average training loss discriminator: 0.785\n",
      "  Training epcoh took: 0:02:52\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:33:49 - INFO - __main__ -   Epoch: 3 | Batch: 0/10001 (0%) | G Loss: 4.309246 | C Loss: -3.552221\n",
      "06/30/2022 11:33:49 - INFO - __main__ -   Text: ['and science,\" where does Waldo belong to?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.407\n",
      "  Test Loss: 1.828\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:33:50 - INFO - __main__ -   Epoch: 3 | Batch: 500/10001 (5%) | G Loss: 4.994123 | C Loss: -4.078777\n",
      "06/30/2022 11:33:50 - INFO - __main__ -   Text: ['Ingets Everybody does the packing on a table.']\n",
      "06/30/2022 11:33:52 - INFO - __main__ -   Epoch: 3 | Batch: 1000/10001 (10%) | G Loss: 3.905934 | C Loss: -3.570907\n",
      "06/30/2022 11:33:52 - INFO - __main__ -   Text: ['He knows that insects-and-kay.']\n",
      "06/30/2022 11:33:53 - INFO - __main__ -   Epoch: 3 | Batch: 1500/10001 (15%) | G Loss: 4.738429 | C Loss: -3.904589\n",
      "06/30/2022 11:33:53 - INFO - __main__ -   Text: ['CNN says is .']\n",
      "06/30/2022 11:33:54 - INFO - __main__ -   Epoch: 3 | Batch: 2000/10001 (20%) | G Loss: 5.971350 | C Loss: -4.678089\n",
      "06/30/2022 11:33:54 - INFO - __main__ -   Text: ['The heart rate isn\\'t affected!\"']\n",
      "06/30/2022 11:33:55 - INFO - __main__ -   Epoch: 3 | Batch: 2500/10001 (25%) | G Loss: 5.317640 | C Loss: -4.261834\n",
      "06/30/2022 11:33:55 - INFO - __main__ -   Text: ['Rush mode himself into his wish!']\n",
      "06/30/2022 11:33:57 - INFO - __main__ -   Epoch: 3 | Batch: 3000/10001 (30%) | G Loss: 5.488830 | C Loss: -4.105955\n",
      "06/30/2022 11:33:57 - INFO - __main__ -   Text: ['Cobain protein famous is definitely patronosis for cannabis.']\n",
      "06/30/2022 11:33:58 - INFO - __main__ -   Epoch: 3 | Batch: 3500/10001 (35%) | G Loss: 5.907498 | C Loss: -4.958885\n",
      "06/30/2022 11:33:58 - INFO - __main__ -   Text: ['Architect covering it is a book afair.']\n",
      "06/30/2022 11:34:00 - INFO - __main__ -   Epoch: 3 | Batch: 4000/10001 (40%) | G Loss: 5.078088 | C Loss: -4.334650\n",
      "06/30/2022 11:34:00 - INFO - __main__ -   Text: ['Thai formula.']\n",
      "06/30/2022 11:34:01 - INFO - __main__ -   Epoch: 3 | Batch: 4500/10001 (45%) | G Loss: 5.182598 | C Loss: -4.451852\n",
      "06/30/2022 11:34:01 - INFO - __main__ -   Text: ['parrot.\"']\n",
      "06/30/2022 11:34:02 - INFO - __main__ -   Epoch: 3 | Batch: 5000/10001 (50%) | G Loss: 5.627130 | C Loss: -4.772646\n",
      "06/30/2022 11:34:03 - INFO - __main__ -   Text: ['... Then neut voting someone.\"']\n",
      "06/30/2022 11:34:04 - INFO - __main__ -   Epoch: 3 | Batch: 5500/10001 (55%) | G Loss: 5.447237 | C Loss: -4.403783\n",
      "06/30/2022 11:34:04 - INFO - __main__ -   Text: ['Dial IP science!']\n",
      "06/30/2022 11:34:05 - INFO - __main__ -   Epoch: 3 | Batch: 6000/10001 (60%) | G Loss: 6.022892 | C Loss: -4.977739\n",
      "06/30/2022 11:34:05 - INFO - __main__ -   Text: ['Mack is not trying to remedy any objections across weather.']\n",
      "06/30/2022 11:34:07 - INFO - __main__ -   Epoch: 3 | Batch: 6500/10001 (65%) | G Loss: 5.946341 | C Loss: -4.881607\n",
      "06/30/2022 11:34:07 - INFO - __main__ -   Text: ['(Sometimes though) It is also called a hairless!']\n",
      "06/30/2022 11:34:08 - INFO - __main__ -   Epoch: 3 | Batch: 7000/10001 (70%) | G Loss: 4.916690 | C Loss: -4.185829\n",
      "06/30/2022 11:34:08 - INFO - __main__ -   Text: ['pony Betty is recommended.']\n",
      "06/30/2022 11:34:10 - INFO - __main__ -   Epoch: 3 | Batch: 7500/10001 (75%) | G Loss: 5.620440 | C Loss: -4.599382\n",
      "06/30/2022 11:34:10 - INFO - __main__ -   Text: ['While fast usually leads to a rap.']\n",
      "06/30/2022 11:34:11 - INFO - __main__ -   Epoch: 3 | Batch: 8000/10001 (80%) | G Loss: 5.733124 | C Loss: -4.786844\n",
      "06/30/2022 11:34:11 - INFO - __main__ -   Text: ['']\n",
      "06/30/2022 11:34:13 - INFO - __main__ -   Epoch: 3 | Batch: 8500/10001 (85%) | G Loss: 4.818783 | C Loss: -3.970418\n",
      "06/30/2022 11:34:13 - INFO - __main__ -   Text: ['A very varied list of verterance...']\n",
      "06/30/2022 11:34:14 - INFO - __main__ -   Epoch: 3 | Batch: 9000/10001 (90%) | G Loss: 5.000401 | C Loss: -4.265058\n",
      "06/30/2022 11:34:14 - INFO - __main__ -   Text: ['These are better mongculus.']\n",
      "06/30/2022 11:34:15 - INFO - __main__ -   Epoch: 3 | Batch: 9500/10001 (95%) | G Loss: 5.948709 | C Loss: -4.997493\n",
      "06/30/2022 11:34:15 - INFO - __main__ -   Text: [\"Earth's needs to head all around our pains.\"]\n",
      "06/30/2022 11:34:17 - INFO - __main__ -   * (Train) Epoch: 3 | G Loss: 5.2576 | C Loss: -4.4107 | Updates G: 160 | Updates C: 840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:34:25 - INFO - __main__ -   Bleu-2:0.183 | B-Bleu-2:0.248\n",
      "06/30/2022 11:34:25 - INFO - __main__ -   * Saving. Best Score:0.432 | Bleu-2:0.183 | B-Bleu-2:0.248\n",
      "06/30/2022 11:34:25 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_4.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4316620933383662\n",
      "Train file used is number 4\n",
      "../../yahoo/subdivided_large/train_4.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 4 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:09.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:17.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:27.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:37.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:45.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:54.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:03.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:12.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:21.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:31.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:39.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:49.\n",
      "  Batch   130  of    172.    Elapsed: 0:01:57.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:05.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:14.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:24.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:32.\n",
      "\n",
      "  Average training loss discriminator: 0.274\n",
      "  Training epcoh took: 0:02:34\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:37:00 - INFO - __main__ -   Epoch: 4 | Batch: 0/10001 (0%) | G Loss: 5.482580 | C Loss: -4.827225\n",
      "06/30/2022 11:37:00 - INFO - __main__ -   Text: ['As well as we want to take care of the animals.\"']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.420\n",
      "  Test Loss: 1.975\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:37:01 - INFO - __main__ -   Epoch: 4 | Batch: 500/10001 (5%) | G Loss: 5.672466 | C Loss: -4.588940\n",
      "06/30/2022 11:37:01 - INFO - __main__ -   Text: ['now suggests.)']\n",
      "06/30/2022 11:37:02 - INFO - __main__ -   Epoch: 4 | Batch: 1000/10001 (10%) | G Loss: 5.242168 | C Loss: -4.529867\n",
      "06/30/2022 11:37:02 - INFO - __main__ -   Text: ['Marry tells it is 8 of the 21.']\n",
      "06/30/2022 11:37:03 - INFO - __main__ -   Epoch: 4 | Batch: 1500/10001 (15%) | G Loss: 5.127385 | C Loss: -4.558949\n",
      "06/30/2022 11:37:03 - INFO - __main__ -   Text: ['A little interference of Rachabhava.']\n",
      "06/30/2022 11:37:05 - INFO - __main__ -   Epoch: 4 | Batch: 2000/10001 (20%) | G Loss: 6.162306 | C Loss: -4.810852\n",
      "06/30/2022 11:37:05 - INFO - __main__ -   Text: ['Gun knows why H will AN understands Find another it.. Socket Then Find syntax .']\n",
      "06/30/2022 11:37:06 - INFO - __main__ -   Epoch: 4 | Batch: 2500/10001 (25%) | G Loss: 5.316992 | C Loss: -4.414975\n",
      "06/30/2022 11:37:06 - INFO - __main__ -   Text: ['\"What I Am Not About\".']\n",
      "06/30/2022 11:37:07 - INFO - __main__ -   Epoch: 4 | Batch: 3000/10001 (30%) | G Loss: 5.470304 | C Loss: -4.320871\n",
      "06/30/2022 11:37:07 - INFO - __main__ -   Text: ['birds will become predipiful to little.\"']\n",
      "06/30/2022 11:37:08 - INFO - __main__ -   Epoch: 4 | Batch: 3500/10001 (35%) | G Loss: 4.516907 | C Loss: -3.764960\n",
      "06/30/2022 11:37:08 - INFO - __main__ -   Text: ['It is also often heard to call Bab to the Help.']\n",
      "06/30/2022 11:37:09 - INFO - __main__ -   Epoch: 4 | Batch: 4000/10001 (40%) | G Loss: 5.469005 | C Loss: -4.555129\n",
      "06/30/2022 11:37:10 - INFO - __main__ -   Text: ['Other players may be confused by it!']\n",
      "06/30/2022 11:37:11 - INFO - __main__ -   Epoch: 4 | Batch: 4500/10001 (45%) | G Loss: 5.404914 | C Loss: -4.414881\n",
      "06/30/2022 11:37:11 - INFO - __main__ -   Text: ['The sentiment is that invertebrate have not been successful in this space.']\n",
      "06/30/2022 11:37:12 - INFO - __main__ -   Epoch: 4 | Batch: 5000/10001 (50%) | G Loss: 5.367531 | C Loss: -4.483166\n",
      "06/30/2022 11:37:12 - INFO - __main__ -   Text: ['Note in they care, science.']\n",
      "06/30/2022 11:37:13 - INFO - __main__ -   Epoch: 4 | Batch: 5500/10001 (55%) | G Loss: 5.305739 | C Loss: -4.772731\n",
      "06/30/2022 11:37:13 - INFO - __main__ -   Text: [\"New York barbers can be John C. Lewis's version of hell.\"]\n",
      "06/30/2022 11:37:15 - INFO - __main__ -   Epoch: 4 | Batch: 6000/10001 (60%) | G Loss: 5.348575 | C Loss: -4.222363\n",
      "06/30/2022 11:37:15 - INFO - __main__ -   Text: [\"('Don't I be too tall for you').\"]\n",
      "06/30/2022 11:37:16 - INFO - __main__ -   Epoch: 4 | Batch: 6500/10001 (65%) | G Loss: 5.425618 | C Loss: -4.586976\n",
      "06/30/2022 11:37:16 - INFO - __main__ -   Text: ['The point of view on non-Asian tigers park.']\n",
      "06/30/2022 11:37:17 - INFO - __main__ -   Epoch: 4 | Batch: 7000/10001 (70%) | G Loss: 5.457704 | C Loss: -4.667295\n",
      "06/30/2022 11:37:17 - INFO - __main__ -   Text: ['jrabaceer.']\n",
      "06/30/2022 11:37:18 - INFO - __main__ -   Epoch: 4 | Batch: 7500/10001 (75%) | G Loss: 5.622611 | C Loss: -4.580776\n",
      "06/30/2022 11:37:18 - INFO - __main__ -   Text: ['things for defense is you.']\n",
      "06/30/2022 11:37:19 - INFO - __main__ -   Epoch: 4 | Batch: 8000/10001 (80%) | G Loss: 5.114910 | C Loss: -4.334330\n",
      "06/30/2022 11:37:19 - INFO - __main__ -   Text: ['Brillently Customer Grunt.']\n",
      "06/30/2022 11:37:20 - INFO - __main__ -   Epoch: 4 | Batch: 8500/10001 (85%) | G Loss: 5.241020 | C Loss: -4.424819\n",
      "06/30/2022 11:37:21 - INFO - __main__ -   Text: ['However \"\" has \"\" is more likely.']\n",
      "06/30/2022 11:37:22 - INFO - __main__ -   Epoch: 4 | Batch: 9000/10001 (90%) | G Loss: 5.229600 | C Loss: -4.356837\n",
      "06/30/2022 11:37:22 - INFO - __main__ -   Text: ['He asserts to spend most of his time trying to pixity.']\n",
      "06/30/2022 11:37:23 - INFO - __main__ -   Epoch: 4 | Batch: 9500/10001 (95%) | G Loss: 5.082873 | C Loss: -4.316816\n",
      "06/30/2022 11:37:23 - INFO - __main__ -   Text: ['He is concerned with general English language skills.']\n",
      "06/30/2022 11:37:24 - INFO - __main__ -   * (Train) Epoch: 4 | G Loss: 5.3355 | C Loss: -4.4474 | Updates G: 208 | Updates C: 792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:37:32 - INFO - __main__ -   Bleu-2:0.204 | B-Bleu-2:0.252\n",
      "06/30/2022 11:37:32 - INFO - __main__ -   * Saving. Best Score:0.456 | Bleu-2:0.204 | B-Bleu-2:0.252\n",
      "06/30/2022 11:37:32 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4559313456793346\n",
      "Train file used is number 5\n",
      "../../yahoo/subdivided_large/train_5.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 5 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:09.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:17.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:25.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:34.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:42.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:50.\n",
      "  Batch    70  of    172.    Elapsed: 0:00:59.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:07.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:16.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:23.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:31.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:40.\n",
      "  Batch   130  of    172.    Elapsed: 0:01:48.\n",
      "  Batch   140  of    172.    Elapsed: 0:01:57.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:06.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:14.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:23.\n",
      "\n",
      "  Average training loss discriminator: 0.133\n",
      "  Training epcoh took: 0:02:25\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:39:57 - INFO - __main__ -   Epoch: 5 | Batch: 0/10001 (0%) | G Loss: 4.741425 | C Loss: -3.991138\n",
      "06/30/2022 11:39:57 - INFO - __main__ -   Text: ['\"[Mortgage] is a professional.\"']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.417\n",
      "  Test Loss: 2.126\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:39:59 - INFO - __main__ -   Epoch: 5 | Batch: 500/10001 (5%) | G Loss: 5.375140 | C Loss: -4.377096\n",
      "06/30/2022 11:39:59 - INFO - __main__ -   Text: ['They are comparatively devious overts.']\n",
      "06/30/2022 11:40:00 - INFO - __main__ -   Epoch: 5 | Batch: 1000/10001 (10%) | G Loss: 5.548887 | C Loss: -4.616294\n",
      "06/30/2022 11:40:00 - INFO - __main__ -   Text: ['Drama is called for an extra boost.']\n",
      "06/30/2022 11:40:01 - INFO - __main__ -   Epoch: 5 | Batch: 1500/10001 (15%) | G Loss: 4.441598 | C Loss: -4.041420\n",
      "06/30/2022 11:40:01 - INFO - __main__ -   Text: ['Miss765 has an unorthodox idea about Conan.']\n",
      "06/30/2022 11:40:03 - INFO - __main__ -   Epoch: 5 | Batch: 2000/10001 (20%) | G Loss: 4.699568 | C Loss: -4.055166\n",
      "06/30/2022 11:40:03 - INFO - __main__ -   Text: ['viviously Modi.']\n",
      "06/30/2022 11:40:04 - INFO - __main__ -   Epoch: 5 | Batch: 2500/10001 (25%) | G Loss: 4.975516 | C Loss: -4.025387\n",
      "06/30/2022 11:40:04 - INFO - __main__ -   Text: ['It teaches you how to make text messages send.']\n",
      "06/30/2022 11:40:06 - INFO - __main__ -   Epoch: 5 | Batch: 3000/10001 (30%) | G Loss: 5.105340 | C Loss: -4.338366\n",
      "06/30/2022 11:40:06 - INFO - __main__ -   Text: ['some \"send the troops down\".']\n",
      "06/30/2022 11:40:07 - INFO - __main__ -   Epoch: 5 | Batch: 3500/10001 (35%) | G Loss: 4.610634 | C Loss: -3.760745\n",
      "06/30/2022 11:40:07 - INFO - __main__ -   Text: ['jobs versus consequence.']\n",
      "06/30/2022 11:40:08 - INFO - __main__ -   Epoch: 5 | Batch: 4000/10001 (40%) | G Loss: 4.983557 | C Loss: -3.988626\n",
      "06/30/2022 11:40:08 - INFO - __main__ -   Text: ['Manufact the Sunk! <PAD>']\n",
      "06/30/2022 11:40:10 - INFO - __main__ -   Epoch: 5 | Batch: 4500/10001 (45%) | G Loss: 5.066486 | C Loss: -4.088455\n",
      "06/30/2022 11:40:10 - INFO - __main__ -   Text: ['When she thinks under her main name  This is the place.']\n",
      "06/30/2022 11:40:11 - INFO - __main__ -   Epoch: 5 | Batch: 5000/10001 (50%) | G Loss: 4.402347 | C Loss: -3.735750\n",
      "06/30/2022 11:40:11 - INFO - __main__ -   Text: ['It is also useful to see your heart rate.']\n",
      "06/30/2022 11:40:12 - INFO - __main__ -   Epoch: 5 | Batch: 5500/10001 (55%) | G Loss: 4.731288 | C Loss: -3.576129\n",
      "06/30/2022 11:40:12 - INFO - __main__ -   Text: ['Jiro gets dangerous if he wants a pinch.']\n",
      "06/30/2022 11:40:14 - INFO - __main__ -   Epoch: 5 | Batch: 6000/10001 (60%) | G Loss: 5.017382 | C Loss: -4.001304\n",
      "06/30/2022 11:40:14 - INFO - __main__ -   Text: ['\"Boom!\"']\n",
      "06/30/2022 11:40:15 - INFO - __main__ -   Epoch: 5 | Batch: 6500/10001 (65%) | G Loss: 4.927584 | C Loss: -3.882739\n",
      "06/30/2022 11:40:15 - INFO - __main__ -   Text: [\"critic's blind can see.\"]\n",
      "06/30/2022 11:40:17 - INFO - __main__ -   Epoch: 5 | Batch: 7000/10001 (70%) | G Loss: 5.107712 | C Loss: -4.246943\n",
      "06/30/2022 11:40:17 - INFO - __main__ -   Text: ['North is superior in this by being the fastest species.']\n",
      "06/30/2022 11:40:18 - INFO - __main__ -   Epoch: 5 | Batch: 7500/10001 (75%) | G Loss: 4.695456 | C Loss: -3.973894\n",
      "06/30/2022 11:40:18 - INFO - __main__ -   Text: ['These dogs have a bad black colour.']\n",
      "06/30/2022 11:40:19 - INFO - __main__ -   Epoch: 5 | Batch: 8000/10001 (80%) | G Loss: 4.832548 | C Loss: -4.076912\n",
      "06/30/2022 11:40:20 - INFO - __main__ -   Text: ['What can I say... .. -1950 cruelty?\"']\n",
      "06/30/2022 11:40:21 - INFO - __main__ -   Epoch: 5 | Batch: 8500/10001 (85%) | G Loss: 4.750348 | C Loss: -3.816502\n",
      "06/30/2022 11:40:21 - INFO - __main__ -   Text: ['Previously I think that this may well interest me.']\n",
      "06/30/2022 11:40:22 - INFO - __main__ -   Epoch: 5 | Batch: 9000/10001 (90%) | G Loss: 4.465086 | C Loss: -3.740621\n",
      "06/30/2022 11:40:22 - INFO - __main__ -   Text: ['Rhino may refer to not ship.']\n",
      "06/30/2022 11:40:24 - INFO - __main__ -   Epoch: 5 | Batch: 9500/10001 (95%) | G Loss: 4.648093 | C Loss: -3.672388\n",
      "06/30/2022 11:40:24 - INFO - __main__ -   Text: ['The GM can also ally with them, or use liang.']\n",
      "06/30/2022 11:40:25 - INFO - __main__ -   * (Train) Epoch: 5 | G Loss: 4.8908 | C Loss: -4.0216 | Updates G: 228 | Updates C: 772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:40:33 - INFO - __main__ -   Bleu-2:0.190 | B-Bleu-2:0.243\n",
      "06/30/2022 11:40:33 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_6.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4331146158697022\n",
      "Train file used is number 6\n",
      "../../yahoo/subdivided_large/train_6.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 6 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:08.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:17.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:26.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:35.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:43.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:52.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:00.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:09.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:17.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:26.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:34.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:42.\n",
      "  Batch   130  of    172.    Elapsed: 0:01:51.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:00.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:08.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:17.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:26.\n",
      "\n",
      "  Average training loss discriminator: 0.103\n",
      "  Training epcoh took: 0:02:27\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:43:00 - INFO - __main__ -   Epoch: 6 | Batch: 0/10001 (0%) | G Loss: 4.805538 | C Loss: -4.143377\n",
      "06/30/2022 11:43:00 - INFO - __main__ -   Text: ['Past and future is not the key).']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.422\n",
      "  Test Loss: 2.232\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:43:02 - INFO - __main__ -   Epoch: 6 | Batch: 500/10001 (5%) | G Loss: 4.600390 | C Loss: -3.988561\n",
      "06/30/2022 11:43:02 - INFO - __main__ -   Text: ['It is called \"fucking taxes\".']\n",
      "06/30/2022 11:43:03 - INFO - __main__ -   Epoch: 6 | Batch: 1000/10001 (10%) | G Loss: 4.689137 | C Loss: -3.729098\n",
      "06/30/2022 11:43:03 - INFO - __main__ -   Text: ['If the culprit is ignited, the \"critter\".']\n",
      "06/30/2022 11:43:04 - INFO - __main__ -   Epoch: 6 | Batch: 1500/10001 (15%) | G Loss: 4.699380 | C Loss: -3.799588\n",
      "06/30/2022 11:43:04 - INFO - __main__ -   Text: ['(This is because everyone knows janda is scam.)']\n",
      "06/30/2022 11:43:05 - INFO - __main__ -   Epoch: 6 | Batch: 2000/10001 (20%) | G Loss: 4.788892 | C Loss: -3.802551\n",
      "06/30/2022 11:43:05 - INFO - __main__ -   Text: [\"Murder's only yield of error (k).\"]\n",
      "06/30/2022 11:43:06 - INFO - __main__ -   Epoch: 6 | Batch: 2500/10001 (25%) | G Loss: 4.322745 | C Loss: -3.531069\n",
      "06/30/2022 11:43:06 - INFO - __main__ -   Text: ['Low is a divination of traditional baccose running.']\n",
      "06/30/2022 11:43:08 - INFO - __main__ -   Epoch: 6 | Batch: 3000/10001 (30%) | G Loss: 4.498153 | C Loss: -3.498635\n",
      "06/30/2022 11:43:08 - INFO - __main__ -   Text: ['It is a battle well and forever !']\n",
      "06/30/2022 11:43:09 - INFO - __main__ -   Epoch: 6 | Batch: 3500/10001 (35%) | G Loss: 4.674010 | C Loss: -3.745797\n",
      "06/30/2022 11:43:09 - INFO - __main__ -   Text: ['The question is, Which brewery should drink this opiate?\"']\n",
      "06/30/2022 11:43:10 - INFO - __main__ -   Epoch: 6 | Batch: 4000/10001 (40%) | G Loss: 4.393142 | C Loss: -3.599168\n",
      "06/30/2022 11:43:10 - INFO - __main__ -   Text: ['Experience Transfer by Crow should become a learning tool.']\n",
      "06/30/2022 11:43:11 - INFO - __main__ -   Epoch: 6 | Batch: 4500/10001 (45%) | G Loss: 4.468278 | C Loss: -3.559050\n",
      "06/30/2022 11:43:11 - INFO - __main__ -   Text: ['niamy.\"']\n",
      "06/30/2022 11:43:12 - INFO - __main__ -   Epoch: 6 | Batch: 5000/10001 (50%) | G Loss: 4.526014 | C Loss: -3.406312\n",
      "06/30/2022 11:43:12 - INFO - __main__ -   Text: ['The virus is too HIV positive to be transmitted from people.']\n",
      "06/30/2022 11:43:13 - INFO - __main__ -   Epoch: 6 | Batch: 5500/10001 (55%) | G Loss: 4.290692 | C Loss: -3.408567\n",
      "06/30/2022 11:43:13 - INFO - __main__ -   Text: ['He has quite a technical thirst to find exactly what is going on.']\n",
      "06/30/2022 11:43:14 - INFO - __main__ -   Epoch: 6 | Batch: 6000/10001 (60%) | G Loss: 4.847805 | C Loss: -3.908496\n",
      "06/30/2022 11:43:14 - INFO - __main__ -   Text: ['Also some of the jingle the cars are in.']\n",
      "06/30/2022 11:43:16 - INFO - __main__ -   Epoch: 6 | Batch: 6500/10001 (65%) | G Loss: 4.562038 | C Loss: -3.721252\n",
      "06/30/2022 11:43:16 - INFO - __main__ -   Text: ['In fact, only the Drunk Master can explain how a Wham!']\n",
      "06/30/2022 11:43:17 - INFO - __main__ -   Epoch: 6 | Batch: 7000/10001 (70%) | G Loss: 4.506082 | C Loss: -3.387790\n",
      "06/30/2022 11:43:17 - INFO - __main__ -   Text: ['This make wut on how much her character can do.']\n",
      "06/30/2022 11:43:19 - INFO - __main__ -   Epoch: 6 | Batch: 7500/10001 (75%) | G Loss: 4.010514 | C Loss: -3.299880\n",
      "06/30/2022 11:43:19 - INFO - __main__ -   Text: ['The words appear on tricks of speech.']\n",
      "06/30/2022 11:43:20 - INFO - __main__ -   Epoch: 6 | Batch: 8000/10001 (80%) | G Loss: 3.983473 | C Loss: -3.462327\n",
      "06/30/2022 11:43:20 - INFO - __main__ -   Text: ['Habitation is spacious, happy and peaceful.']\n",
      "06/30/2022 11:43:21 - INFO - __main__ -   Epoch: 6 | Batch: 8500/10001 (85%) | G Loss: 4.022036 | C Loss: -3.251711\n",
      "06/30/2022 11:43:22 - INFO - __main__ -   Text: ['\", is weak.']\n",
      "06/30/2022 11:43:23 - INFO - __main__ -   Epoch: 6 | Batch: 9000/10001 (90%) | G Loss: 4.348704 | C Loss: -3.644624\n",
      "06/30/2022 11:43:23 - INFO - __main__ -   Text: ['The salesman can tell you exactly what messages they have sent.']\n",
      "06/30/2022 11:43:24 - INFO - __main__ -   Epoch: 6 | Batch: 9500/10001 (95%) | G Loss: 4.472589 | C Loss: -3.761567\n",
      "06/30/2022 11:43:24 - INFO - __main__ -   Text: ['It is called Joystick.\"']\n",
      "06/30/2022 11:43:26 - INFO - __main__ -   * (Train) Epoch: 6 | G Loss: 4.5316 | C Loss: -3.6598 | Updates G: 238 | Updates C: 762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:43:34 - INFO - __main__ -   Bleu-2:0.190 | B-Bleu-2:0.258\n",
      "06/30/2022 11:43:34 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_7.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44787392229887174\n",
      "Train file used is number 7\n",
      "../../yahoo/subdivided_large/train_7.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 7 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:09.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:18.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:27.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:36.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:45.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:55.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:05.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:13.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:23.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:33.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:42.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:52.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:00.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:10.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:19.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:29.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:38.\n",
      "\n",
      "  Average training loss discriminator: 0.088\n",
      "  Training epcoh took: 0:02:40\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:46:15 - INFO - __main__ -   Epoch: 7 | Batch: 0/10001 (0%) | G Loss: 3.918416 | C Loss: -3.225816\n",
      "06/30/2022 11:46:15 - INFO - __main__ -   Text: ['An imagine guy is obviously assumed to play the spectator game).']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.415\n",
      "  Test Loss: 2.321\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:46:16 - INFO - __main__ -   Epoch: 7 | Batch: 500/10001 (5%) | G Loss: 4.531611 | C Loss: -3.625335\n",
      "06/30/2022 11:46:16 - INFO - __main__ -   Text: ['How do they induce reading?']\n",
      "06/30/2022 11:46:18 - INFO - __main__ -   Epoch: 7 | Batch: 1000/10001 (10%) | G Loss: 4.249739 | C Loss: -3.538882\n",
      "06/30/2022 11:46:18 - INFO - __main__ -   Text: ['Peter writes \"How to become a Pirate!\"']\n",
      "06/30/2022 11:46:19 - INFO - __main__ -   Epoch: 7 | Batch: 1500/10001 (15%) | G Loss: 4.482125 | C Loss: -3.561132\n",
      "06/30/2022 11:46:19 - INFO - __main__ -   Text: ['Ditto Comics.']\n",
      "06/30/2022 11:46:20 - INFO - __main__ -   Epoch: 7 | Batch: 2000/10001 (20%) | G Loss: 4.455854 | C Loss: -3.574908\n",
      "06/30/2022 11:46:20 - INFO - __main__ -   Text: ['The Thai mind than is its standard.']\n",
      "06/30/2022 11:46:22 - INFO - __main__ -   Epoch: 7 | Batch: 2500/10001 (25%) | G Loss: 4.116321 | C Loss: -3.223588\n",
      "06/30/2022 11:46:22 - INFO - __main__ -   Text: ['Shweah it\\'s more science than fiction\".']\n",
      "06/30/2022 11:46:23 - INFO - __main__ -   Epoch: 7 | Batch: 3000/10001 (30%) | G Loss: 3.939222 | C Loss: -3.189972\n",
      "06/30/2022 11:46:23 - INFO - __main__ -   Text: ['Peter has some better know best when to force meat.']\n",
      "06/30/2022 11:46:25 - INFO - __main__ -   Epoch: 7 | Batch: 3500/10001 (35%) | G Loss: 4.456848 | C Loss: -3.603807\n",
      "06/30/2022 11:46:25 - INFO - __main__ -   Text: ['To ELIOR you may either be wise or delirious!\"']\n",
      "06/30/2022 11:46:26 - INFO - __main__ -   Epoch: 7 | Batch: 4000/10001 (40%) | G Loss: 4.058523 | C Loss: -3.292311\n",
      "06/30/2022 11:46:26 - INFO - __main__ -   Text: ['To know it is impossible to use 10 arrows.\"']\n",
      "06/30/2022 11:46:27 - INFO - __main__ -   Epoch: 7 | Batch: 4500/10001 (45%) | G Loss: 4.077252 | C Loss: -3.465792\n",
      "06/30/2022 11:46:27 - INFO - __main__ -   Text: ['Type I sympathy and a dream can be achieved even if iPhone III.']\n",
      "06/30/2022 11:46:29 - INFO - __main__ -   Epoch: 7 | Batch: 5000/10001 (50%) | G Loss: 4.035002 | C Loss: -3.431807\n",
      "06/30/2022 11:46:29 - INFO - __main__ -   Text: ['The only way that I have to introduce these terms to you is \"Hau\".']\n",
      "06/30/2022 11:46:30 - INFO - __main__ -   Epoch: 7 | Batch: 5500/10001 (55%) | G Loss: 4.180364 | C Loss: -3.381300\n",
      "06/30/2022 11:46:30 - INFO - __main__ -   Text: ['Chinmams tell jokes instead of Valentine chocolate.']\n",
      "06/30/2022 11:46:32 - INFO - __main__ -   Epoch: 7 | Batch: 6000/10001 (60%) | G Loss: 4.159679 | C Loss: -3.352351\n",
      "06/30/2022 11:46:32 - INFO - __main__ -   Text: ['Not even app?']\n",
      "06/30/2022 11:46:33 - INFO - __main__ -   Epoch: 7 | Batch: 6500/10001 (65%) | G Loss: 4.279746 | C Loss: -3.391290\n",
      "06/30/2022 11:46:33 - INFO - __main__ -   Text: ['The reason why is not just high level accuracy\".']\n",
      "06/30/2022 11:46:34 - INFO - __main__ -   Epoch: 7 | Batch: 7000/10001 (70%) | G Loss: 4.284569 | C Loss: -3.414030\n",
      "06/30/2022 11:46:34 - INFO - __main__ -   Text: ['They are way too busy to spam.\"']\n",
      "06/30/2022 11:46:36 - INFO - __main__ -   Epoch: 7 | Batch: 7500/10001 (75%) | G Loss: 4.232913 | C Loss: -3.388779\n",
      "06/30/2022 11:46:36 - INFO - __main__ -   Text: ['Discrimination is to criticize someone\\'s behavior.\"']\n",
      "06/30/2022 11:46:37 - INFO - __main__ -   Epoch: 7 | Batch: 8000/10001 (80%) | G Loss: 3.987951 | C Loss: -3.296476\n",
      "06/30/2022 11:46:37 - INFO - __main__ -   Text: ['Cheese is always good.']\n",
      "06/30/2022 11:46:39 - INFO - __main__ -   Epoch: 7 | Batch: 8500/10001 (85%) | G Loss: 3.744700 | C Loss: -2.951985\n",
      "06/30/2022 11:46:39 - INFO - __main__ -   Text: ['Cohickatter is a useless guy who ignores science?']\n",
      "06/30/2022 11:46:40 - INFO - __main__ -   Epoch: 7 | Batch: 9000/10001 (90%) | G Loss: 3.954308 | C Loss: -3.163118\n",
      "06/30/2022 11:46:40 - INFO - __main__ -   Text: ['Since \"\" gets hit, \"\" loots\".']\n",
      "06/30/2022 11:46:41 - INFO - __main__ -   Epoch: 7 | Batch: 9500/10001 (95%) | G Loss: 3.766575 | C Loss: -3.112987\n",
      "06/30/2022 11:46:41 - INFO - __main__ -   Text: ['He is probably the only one who knows how to expressigraphy.']\n",
      "06/30/2022 11:46:43 - INFO - __main__ -   * (Train) Epoch: 7 | G Loss: 4.1309 | C Loss: -3.3438 | Updates G: 226 | Updates C: 774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:46:52 - INFO - __main__ -   Bleu-2:0.200 | B-Bleu-2:0.264\n",
      "06/30/2022 11:46:52 - INFO - __main__ -   * Saving. Best Score:0.464 | Bleu-2:0.200 | B-Bleu-2:0.264\n",
      "06/30/2022 11:46:52 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_8.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4641250514552133\n",
      "Train file used is number 8\n",
      "../../yahoo/subdivided_large/train_8.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 8 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:19.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:29.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:38.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:47.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:56.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:04.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:14.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:23.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:32.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:42.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:51.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:00.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:09.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:18.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:28.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:37.\n",
      "\n",
      "  Average training loss discriminator: 0.074\n",
      "  Training epcoh took: 0:02:39\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:49:31 - INFO - __main__ -   Epoch: 8 | Batch: 0/10001 (0%) | G Loss: 4.264320 | C Loss: -3.226911\n",
      "06/30/2022 11:49:31 - INFO - __main__ -   Text: ['This path exposes to immoral behavior in the adult human society.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.417\n",
      "  Test Loss: 2.352\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:49:32 - INFO - __main__ -   Epoch: 8 | Batch: 500/10001 (5%) | G Loss: 3.876499 | C Loss: -3.318099\n",
      "06/30/2022 11:49:32 - INFO - __main__ -   Text: ['By \"personality\" that means you.\"']\n",
      "06/30/2022 11:49:34 - INFO - __main__ -   Epoch: 8 | Batch: 1000/10001 (10%) | G Loss: 3.933834 | C Loss: -3.253555\n",
      "06/30/2022 11:49:34 - INFO - __main__ -   Text: ['Nat tells us why boys English is a bad word.']\n",
      "06/30/2022 11:49:35 - INFO - __main__ -   Epoch: 8 | Batch: 1500/10001 (15%) | G Loss: 4.134632 | C Loss: -3.065179\n",
      "06/30/2022 11:49:35 - INFO - __main__ -   Text: ['Neurotic thinking is nothing but subject mind.']\n",
      "06/30/2022 11:49:37 - INFO - __main__ -   Epoch: 8 | Batch: 2000/10001 (20%) | G Loss: 4.040706 | C Loss: -3.287549\n",
      "06/30/2022 11:49:37 - INFO - __main__ -   Text: ['It is never a lie, you can\\'t imagine it.\"']\n",
      "06/30/2022 11:49:38 - INFO - __main__ -   Epoch: 8 | Batch: 2500/10001 (25%) | G Loss: 3.848289 | C Loss: -3.227030\n",
      "06/30/2022 11:49:38 - INFO - __main__ -   Text: ['\", referring to Dan.']\n",
      "06/30/2022 11:49:39 - INFO - __main__ -   Epoch: 8 | Batch: 3000/10001 (30%) | G Loss: 3.725226 | C Loss: -3.084496\n",
      "06/30/2022 11:49:39 - INFO - __main__ -   Text: ['They are Modern Time Towers!']\n",
      "06/30/2022 11:49:41 - INFO - __main__ -   Epoch: 8 | Batch: 3500/10001 (35%) | G Loss: 3.859028 | C Loss: -2.956742\n",
      "06/30/2022 11:49:41 - INFO - __main__ -   Text: ['There is an explanation of her lawlessness!']\n",
      "06/30/2022 11:49:42 - INFO - __main__ -   Epoch: 8 | Batch: 4000/10001 (40%) | G Loss: 3.551817 | C Loss: -2.872381\n",
      "06/30/2022 11:49:42 - INFO - __main__ -   Text: ['A well-qualified human can also be talking about this planet.']\n",
      "06/30/2022 11:49:43 - INFO - __main__ -   Epoch: 8 | Batch: 4500/10001 (45%) | G Loss: 3.795213 | C Loss: -3.097847\n",
      "06/30/2022 11:49:43 - INFO - __main__ -   Text: ['It is called the jester on the internet.']\n",
      "06/30/2022 11:49:45 - INFO - __main__ -   Epoch: 8 | Batch: 5000/10001 (50%) | G Loss: 3.902487 | C Loss: -2.986448\n",
      "06/30/2022 11:49:45 - INFO - __main__ -   Text: ['Utilizing the idea of infinite clean frags.']\n",
      "06/30/2022 11:49:46 - INFO - __main__ -   Epoch: 8 | Batch: 5500/10001 (55%) | G Loss: 3.853704 | C Loss: -3.022860\n",
      "06/30/2022 11:49:46 - INFO - __main__ -   Text: ['\"calm me, when will Supreme rise?\"']\n",
      "06/30/2022 11:49:48 - INFO - __main__ -   Epoch: 8 | Batch: 6000/10001 (60%) | G Loss: 3.787366 | C Loss: -2.852541\n",
      "06/30/2022 11:49:48 - INFO - __main__ -   Text: ['Believes it a \"data call\"\"']\n",
      "06/30/2022 11:49:49 - INFO - __main__ -   Epoch: 8 | Batch: 6500/10001 (65%) | G Loss: 3.758652 | C Loss: -3.079165\n",
      "06/30/2022 11:49:49 - INFO - __main__ -   Text: ['\"antah\", this indicates that.']\n",
      "06/30/2022 11:49:50 - INFO - __main__ -   Epoch: 8 | Batch: 7000/10001 (70%) | G Loss: 3.782041 | C Loss: -3.177853\n",
      "06/30/2022 11:49:50 - INFO - __main__ -   Text: ['It displays a pathological attitude towards body math.']\n",
      "06/30/2022 11:49:52 - INFO - __main__ -   Epoch: 8 | Batch: 7500/10001 (75%) | G Loss: 3.763213 | C Loss: -3.082004\n",
      "06/30/2022 11:49:52 - INFO - __main__ -   Text: ['His wife has become very cynical about this fact.']\n",
      "06/30/2022 11:49:53 - INFO - __main__ -   Epoch: 8 | Batch: 8000/10001 (80%) | G Loss: 3.930640 | C Loss: -2.933278\n",
      "06/30/2022 11:49:53 - INFO - __main__ -   Text: ['The lack of the autoer nichats is an addiction.']\n",
      "06/30/2022 11:49:54 - INFO - __main__ -   Epoch: 8 | Batch: 8500/10001 (85%) | G Loss: 3.788883 | C Loss: -2.887856\n",
      "06/30/2022 11:49:54 - INFO - __main__ -   Text: ['The cowboy might even term some copycats.\"']\n",
      "06/30/2022 11:49:56 - INFO - __main__ -   Epoch: 8 | Batch: 9000/10001 (90%) | G Loss: 4.006094 | C Loss: -3.203970\n",
      "06/30/2022 11:49:56 - INFO - __main__ -   Text: [\"'Never Knots up the game.'\"]\n",
      "06/30/2022 11:49:57 - INFO - __main__ -   Epoch: 8 | Batch: 9500/10001 (95%) | G Loss: 3.583315 | C Loss: -2.779632\n",
      "06/30/2022 11:49:57 - INFO - __main__ -   Text: ['Miller writes personal books on mathematics: One theory is that Americans have some food insecurity.']\n",
      "06/30/2022 11:49:59 - INFO - __main__ -   * (Train) Epoch: 8 | G Loss: 3.7518 | C Loss: -3.0099 | Updates G: 246 | Updates C: 754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:50:07 - INFO - __main__ -   Bleu-2:0.207 | B-Bleu-2:0.268\n",
      "06/30/2022 11:50:07 - INFO - __main__ -   * Saving. Best Score:0.475 | Bleu-2:0.207 | B-Bleu-2:0.268\n",
      "06/30/2022 11:50:07 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_9.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4745108375776034\n",
      "Train file used is number 9\n",
      "../../yahoo/subdivided_large/train_9.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 9 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:09.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:18.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:27.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:35.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:45.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:54.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:03.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:11.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:20.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:28.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:37.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:47.\n",
      "  Batch   130  of    172.    Elapsed: 0:01:55.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:04.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:14.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:24.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:32.\n",
      "\n",
      "  Average training loss discriminator: 0.062\n",
      "  Training epcoh took: 0:02:33\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:52:41 - INFO - __main__ -   Epoch: 9 | Batch: 0/10001 (0%) | G Loss: 3.362345 | C Loss: -2.757501\n",
      "06/30/2022 11:52:41 - INFO - __main__ -   Text: ['Four gates is better than five gates.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.412\n",
      "  Test Loss: 2.448\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:52:42 - INFO - __main__ -   Epoch: 9 | Batch: 500/10001 (5%) | G Loss: 3.374963 | C Loss: -2.837275\n",
      "06/30/2022 11:52:42 - INFO - __main__ -   Text: ['Despite fitting description, Barney tends to be arrogant.']\n",
      "06/30/2022 11:52:43 - INFO - __main__ -   Epoch: 9 | Batch: 1000/10001 (10%) | G Loss: 3.653122 | C Loss: -2.846082\n",
      "06/30/2022 11:52:43 - INFO - __main__ -   Text: ['Ferrel the miners are a stinker there.']\n",
      "06/30/2022 11:52:45 - INFO - __main__ -   Epoch: 9 | Batch: 1500/10001 (15%) | G Loss: 3.387557 | C Loss: -2.787097\n",
      "06/30/2022 11:52:45 - INFO - __main__ -   Text: ['The pseudocite is mostly nonsense.\"']\n",
      "06/30/2022 11:52:46 - INFO - __main__ -   Epoch: 9 | Batch: 2000/10001 (20%) | G Loss: 3.668082 | C Loss: -2.916860\n",
      "06/30/2022 11:52:46 - INFO - __main__ -   Text: [\"This spell probably doesn't break just yet.\"]\n",
      "06/30/2022 11:52:47 - INFO - __main__ -   Epoch: 9 | Batch: 2500/10001 (25%) | G Loss: 3.551186 | C Loss: -2.814125\n",
      "06/30/2022 11:52:47 - INFO - __main__ -   Text: ['The perfect solution to this is the marten cutthroat.']\n",
      "06/30/2022 11:52:49 - INFO - __main__ -   Epoch: 9 | Batch: 3000/10001 (30%) | G Loss: 3.201802 | C Loss: -2.510508\n",
      "06/30/2022 11:52:49 - INFO - __main__ -   Text: ['Craig used to call it satire.']\n",
      "06/30/2022 11:52:50 - INFO - __main__ -   Epoch: 9 | Batch: 3500/10001 (35%) | G Loss: 3.516598 | C Loss: -2.680345\n",
      "06/30/2022 11:52:50 - INFO - __main__ -   Text: ['The best things to do is write and write as Hebrew.']\n",
      "06/30/2022 11:52:51 - INFO - __main__ -   Epoch: 9 | Batch: 4000/10001 (40%) | G Loss: 3.010118 | C Loss: -2.624502\n",
      "06/30/2022 11:52:52 - INFO - __main__ -   Text: ['Sports sports are not edition like sports depend all ayurt.']\n",
      "06/30/2022 11:52:53 - INFO - __main__ -   Epoch: 9 | Batch: 4500/10001 (45%) | G Loss: 3.270828 | C Loss: -2.533387\n",
      "06/30/2022 11:52:53 - INFO - __main__ -   Text: ['Shiny times come from Bharat.']\n",
      "06/30/2022 11:52:54 - INFO - __main__ -   Epoch: 9 | Batch: 5000/10001 (50%) | G Loss: 3.366075 | C Loss: -2.682216\n",
      "06/30/2022 11:52:54 - INFO - __main__ -   Text: ['Spell check is a ability to get you some speed.']\n",
      "06/30/2022 11:52:56 - INFO - __main__ -   Epoch: 9 | Batch: 5500/10001 (55%) | G Loss: 3.332996 | C Loss: -2.587677\n",
      "06/30/2022 11:52:56 - INFO - __main__ -   Text: ['This person will refer refer to the \"Flopjack\" scale.']\n",
      "06/30/2022 11:52:57 - INFO - __main__ -   Epoch: 9 | Batch: 6000/10001 (60%) | G Loss: 3.336119 | C Loss: -2.728324\n",
      "06/30/2022 11:52:57 - INFO - __main__ -   Text: ['Love coveting is extremely easy.']\n",
      "06/30/2022 11:52:58 - INFO - __main__ -   Epoch: 9 | Batch: 6500/10001 (65%) | G Loss: 3.221011 | C Loss: -2.457649\n",
      "06/30/2022 11:52:58 - INFO - __main__ -   Text: ['What if I go somewhere crazy?']\n",
      "06/30/2022 11:53:00 - INFO - __main__ -   Epoch: 9 | Batch: 7000/10001 (70%) | G Loss: 3.626249 | C Loss: -2.793527\n",
      "06/30/2022 11:53:00 - INFO - __main__ -   Text: ['Through the magic Purge, I can do a lot of things.']\n",
      "06/30/2022 11:53:01 - INFO - __main__ -   Epoch: 9 | Batch: 7500/10001 (75%) | G Loss: 3.410158 | C Loss: -2.583592\n",
      "06/30/2022 11:53:01 - INFO - __main__ -   Text: ['\", \\'Maybe you cannot rule the land\\'.']\n",
      "06/30/2022 11:53:02 - INFO - __main__ -   Epoch: 9 | Batch: 8000/10001 (80%) | G Loss: 3.559796 | C Loss: -2.689039\n",
      "06/30/2022 11:53:03 - INFO - __main__ -   Text: ['\"Kavuna You might be wicked\".']\n",
      "06/30/2022 11:53:04 - INFO - __main__ -   Epoch: 9 | Batch: 8500/10001 (85%) | G Loss: 3.482730 | C Loss: -2.632404\n",
      "06/30/2022 11:53:04 - INFO - __main__ -   Text: ['The Jvy Soc is hugely pointless.']\n",
      "06/30/2022 11:53:05 - INFO - __main__ -   Epoch: 9 | Batch: 9000/10001 (90%) | G Loss: 3.187058 | C Loss: -2.480495\n",
      "06/30/2022 11:53:05 - INFO - __main__ -   Text: ['Some people react in the same way as Menon says .']\n",
      "06/30/2022 11:53:07 - INFO - __main__ -   Epoch: 9 | Batch: 9500/10001 (95%) | G Loss: 3.283001 | C Loss: -2.525028\n",
      "06/30/2022 11:53:07 - INFO - __main__ -   Text: [\"This may give a false sense of a 'numbers game.'\"]\n",
      "06/30/2022 11:53:08 - INFO - __main__ -   * (Train) Epoch: 9 | G Loss: 3.3366 | C Loss: -2.6730 | Updates G: 258 | Updates C: 742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:53:16 - INFO - __main__ -   Bleu-2:0.216 | B-Bleu-2:0.259\n",
      "06/30/2022 11:53:16 - INFO - __main__ -   * Saving. Best Score:0.475 | Bleu-2:0.216 | B-Bleu-2:0.259\n",
      "06/30/2022 11:53:16 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_10.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4751852827248273\n",
      "Train file used is number 10\n",
      "../../yahoo/subdivided_large/train_10.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 10 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:19.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:28.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:37.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:46.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:56.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:05.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:14.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:23.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:33.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:42.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:51.\n",
      "  Batch   130  of    172.    Elapsed: 0:01:59.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:08.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:17.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:26.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:35.\n",
      "\n",
      "  Average training loss discriminator: 0.064\n",
      "  Training epcoh took: 0:02:37\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:55:53 - INFO - __main__ -   Epoch: 10 | Batch: 0/10001 (0%) | G Loss: 2.882911 | C Loss: -2.402076\n",
      "06/30/2022 11:55:53 - INFO - __main__ -   Text: ['Comic conniptions mean this man.]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.415\n",
      "  Test Loss: 2.480\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:55:55 - INFO - __main__ -   Epoch: 10 | Batch: 500/10001 (5%) | G Loss: 3.371243 | C Loss: -2.588562\n",
      "06/30/2022 11:55:55 - INFO - __main__ -   Text: ['\"Won\\'t you do?\"']\n",
      "06/30/2022 11:55:56 - INFO - __main__ -   Epoch: 10 | Batch: 1000/10001 (10%) | G Loss: 2.985229 | C Loss: -2.415268\n",
      "06/30/2022 11:55:56 - INFO - __main__ -   Text: ['Customers are called gene changers if they may help.']\n",
      "06/30/2022 11:55:58 - INFO - __main__ -   Epoch: 10 | Batch: 1500/10001 (15%) | G Loss: 3.431109 | C Loss: -2.671074\n",
      "06/30/2022 11:55:58 - INFO - __main__ -   Text: ['Duk Jyopadok thinks sex is open access at reaching outside levels.']\n",
      "06/30/2022 11:55:59 - INFO - __main__ -   Epoch: 10 | Batch: 2000/10001 (20%) | G Loss: 3.219281 | C Loss: -2.520916\n",
      "06/30/2022 11:55:59 - INFO - __main__ -   Text: [\"television isn't a pleasant habit to talk about!\"]\n",
      "06/30/2022 11:56:00 - INFO - __main__ -   Epoch: 10 | Batch: 2500/10001 (25%) | G Loss: 3.249369 | C Loss: -2.484682\n",
      "06/30/2022 11:56:00 - INFO - __main__ -   Text: ['They specialize in combining any cosmetics household material may have.']\n",
      "06/30/2022 11:56:02 - INFO - __main__ -   Epoch: 10 | Batch: 3000/10001 (30%) | G Loss: 2.881461 | C Loss: -2.427939\n",
      "06/30/2022 11:56:02 - INFO - __main__ -   Text: ['\"\"The Sphiloboom may read all these days.\"']\n",
      "06/30/2022 11:56:03 - INFO - __main__ -   Epoch: 10 | Batch: 3500/10001 (35%) | G Loss: 2.985428 | C Loss: -2.258530\n",
      "06/30/2022 11:56:03 - INFO - __main__ -   Text: ['Like Aunt Michelle stands out.']\n",
      "06/30/2022 11:56:04 - INFO - __main__ -   Epoch: 10 | Batch: 4000/10001 (40%) | G Loss: 3.117368 | C Loss: -2.439909\n",
      "06/30/2022 11:56:04 - INFO - __main__ -   Text: ['The movie is very silly given how Spain did it.']\n",
      "06/30/2022 11:56:06 - INFO - __main__ -   Epoch: 10 | Batch: 4500/10001 (45%) | G Loss: 3.035048 | C Loss: -2.322117\n",
      "06/30/2022 11:56:06 - INFO - __main__ -   Text: ['This way you understand what it is.']\n",
      "06/30/2022 11:56:07 - INFO - __main__ -   Epoch: 10 | Batch: 5000/10001 (50%) | G Loss: 2.846835 | C Loss: -2.171055\n",
      "06/30/2022 11:56:07 - INFO - __main__ -   Text: ['It\\'s... \"Mystery...\"']\n",
      "06/30/2022 11:56:08 - INFO - __main__ -   Epoch: 10 | Batch: 5500/10001 (55%) | G Loss: 3.055604 | C Loss: -2.445549\n",
      "06/30/2022 11:56:08 - INFO - __main__ -   Text: ['Jensen instructors that exercise through smarter papers.']\n",
      "06/30/2022 11:56:10 - INFO - __main__ -   Epoch: 10 | Batch: 6000/10001 (60%) | G Loss: 2.831072 | C Loss: -2.185790\n",
      "06/30/2022 11:56:10 - INFO - __main__ -   Text: ['There many products out there like turp.']\n",
      "06/30/2022 11:56:11 - INFO - __main__ -   Epoch: 10 | Batch: 6500/10001 (65%) | G Loss: 3.122309 | C Loss: -2.316386\n",
      "06/30/2022 11:56:11 - INFO - __main__ -   Text: ['Noticing is the opposite of help.']\n",
      "06/30/2022 11:56:12 - INFO - __main__ -   Epoch: 10 | Batch: 7000/10001 (70%) | G Loss: 2.793884 | C Loss: -2.222972\n",
      "06/30/2022 11:56:13 - INFO - __main__ -   Text: ['Woibhacker!']\n",
      "06/30/2022 11:56:14 - INFO - __main__ -   Epoch: 10 | Batch: 7500/10001 (75%) | G Loss: 2.907751 | C Loss: -2.270581\n",
      "06/30/2022 11:56:14 - INFO - __main__ -   Text: ['It is even called Heartlessness!\"']\n",
      "06/30/2022 11:56:15 - INFO - __main__ -   Epoch: 10 | Batch: 8000/10001 (80%) | G Loss: 2.985839 | C Loss: -2.173875\n",
      "06/30/2022 11:56:15 - INFO - __main__ -   Text: ['The calculator results to \"Beat Me the ham!\".']\n",
      "06/30/2022 11:56:17 - INFO - __main__ -   Epoch: 10 | Batch: 8500/10001 (85%) | G Loss: 2.904047 | C Loss: -2.186506\n",
      "06/30/2022 11:56:17 - INFO - __main__ -   Text: ['How many years <PAD> can go after a primate?\"']\n",
      "06/30/2022 11:56:18 - INFO - __main__ -   Epoch: 10 | Batch: 9000/10001 (90%) | G Loss: 2.803175 | C Loss: -2.193750\n",
      "06/30/2022 11:56:18 - INFO - __main__ -   Text: ['This is true if you think of bowling.']\n",
      "06/30/2022 11:56:19 - INFO - __main__ -   Epoch: 10 | Batch: 9500/10001 (95%) | G Loss: 2.652371 | C Loss: -2.167107\n",
      "06/30/2022 11:56:19 - INFO - __main__ -   Text: ['Supreme is very random now.']\n",
      "06/30/2022 11:56:21 - INFO - __main__ -   * (Train) Epoch: 10 | G Loss: 2.9859 | C Loss: -2.3499 | Updates G: 270 | Updates C: 730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:56:27 - INFO - __main__ -   Bleu-2:0.210 | B-Bleu-2:0.260\n",
      "06/30/2022 11:56:27 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_11.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4700079384279097\n",
      "Train file used is number 11\n",
      "../../yahoo/subdivided_large/train_11.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 11 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:09.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:19.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:28.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:38.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:47.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:56.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:05.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:15.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:24.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:33.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:43.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:53.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:03.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:12.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:21.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:31.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:40.\n",
      "\n",
      "  Average training loss discriminator: 0.064\n",
      "  Training epcoh took: 0:02:42\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:59:10 - INFO - __main__ -   Epoch: 11 | Batch: 0/10001 (0%) | G Loss: 2.998063 | C Loss: -1.917301\n",
      "06/30/2022 11:59:10 - INFO - __main__ -   Text: ['They call it a \"smaller stage song\".']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.417\n",
      "  Test Loss: 2.515\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:59:12 - INFO - __main__ -   Epoch: 11 | Batch: 500/10001 (5%) | G Loss: 2.920982 | C Loss: -2.211493\n",
      "06/30/2022 11:59:12 - INFO - __main__ -   Text: ['This type of miss is an English way of writing on e-mail.']\n",
      "06/30/2022 11:59:13 - INFO - __main__ -   Epoch: 11 | Batch: 1000/10001 (10%) | G Loss: 2.780583 | C Loss: -2.179813\n",
      "06/30/2022 11:59:13 - INFO - __main__ -   Text: ['There are some clothing intolerant fans in the trenches\".']\n",
      "06/30/2022 11:59:15 - INFO - __main__ -   Epoch: 11 | Batch: 1500/10001 (15%) | G Loss: 3.131815 | C Loss: -2.449577\n",
      "06/30/2022 11:59:15 - INFO - __main__ -   Text: ['The reason for this is the presence of shreds.']\n",
      "06/30/2022 11:59:16 - INFO - __main__ -   Epoch: 11 | Batch: 2000/10001 (20%) | G Loss: 2.641733 | C Loss: -2.239055\n",
      "06/30/2022 11:59:16 - INFO - __main__ -   Text: ['Their affair is like mean courts, \"The devil gives judges\".']\n",
      "06/30/2022 11:59:17 - INFO - __main__ -   Epoch: 11 | Batch: 2500/10001 (25%) | G Loss: 2.482116 | C Loss: -2.091573\n",
      "06/30/2022 11:59:17 - INFO - __main__ -   Text: ['Yourself better prolong it my disguise?\" <PAD> More.']\n",
      "06/30/2022 11:59:19 - INFO - __main__ -   Epoch: 11 | Batch: 3000/10001 (30%) | G Loss: 2.566559 | C Loss: -2.011889\n",
      "06/30/2022 11:59:19 - INFO - __main__ -   Text: ['Our responses are generally predictable happening later.']\n",
      "06/30/2022 11:59:20 - INFO - __main__ -   Epoch: 11 | Batch: 3500/10001 (35%) | G Loss: 2.686672 | C Loss: -1.984641\n",
      "06/30/2022 11:59:20 - INFO - __main__ -   Text: [\"It's basically why I've got a job.\"]\n",
      "06/30/2022 11:59:21 - INFO - __main__ -   Epoch: 11 | Batch: 4000/10001 (40%) | G Loss: 2.607836 | C Loss: -1.982325\n",
      "06/30/2022 11:59:22 - INFO - __main__ -   Text: ['Comic book lovers will grope you with that letter from the Z.']\n",
      "06/30/2022 11:59:23 - INFO - __main__ -   Epoch: 11 | Batch: 4500/10001 (45%) | G Loss: 3.010648 | C Loss: -2.241627\n",
      "06/30/2022 11:59:23 - INFO - __main__ -   Text: ['\"Kaumont\" may also be classified as an ND.']\n",
      "06/30/2022 11:59:24 - INFO - __main__ -   Epoch: 11 | Batch: 5000/10001 (50%) | G Loss: 2.584796 | C Loss: -2.011613\n",
      "06/30/2022 11:59:24 - INFO - __main__ -   Text: ['Those letters are called energy letters.']\n",
      "06/30/2022 11:59:26 - INFO - __main__ -   Epoch: 11 | Batch: 5500/10001 (55%) | G Loss: 2.729284 | C Loss: -2.077762\n",
      "06/30/2022 11:59:26 - INFO - __main__ -   Text: ['Mind your mind!']\n",
      "06/30/2022 11:59:27 - INFO - __main__ -   Epoch: 11 | Batch: 6000/10001 (60%) | G Loss: 2.730089 | C Loss: -2.107356\n",
      "06/30/2022 11:59:27 - INFO - __main__ -   Text: ['This book has interests on the topic.']\n",
      "06/30/2022 11:59:28 - INFO - __main__ -   Epoch: 11 | Batch: 6500/10001 (65%) | G Loss: 2.610197 | C Loss: -2.031161\n",
      "06/30/2022 11:59:28 - INFO - __main__ -   Text: ['It works better conversation, or its \"().']\n",
      "06/30/2022 11:59:30 - INFO - __main__ -   Epoch: 11 | Batch: 7000/10001 (70%) | G Loss: 2.724607 | C Loss: -2.131041\n",
      "06/30/2022 11:59:30 - INFO - __main__ -   Text: ['Ranger is doing pretty well at school Without Joker really happening batshit.\"']\n",
      "06/30/2022 11:59:31 - INFO - __main__ -   Epoch: 11 | Batch: 7500/10001 (75%) | G Loss: 2.739435 | C Loss: -2.065125\n",
      "06/30/2022 11:59:31 - INFO - __main__ -   Text: ['It doesn\\'t talk to money but to food.\"']\n",
      "06/30/2022 11:59:32 - INFO - __main__ -   Epoch: 11 | Batch: 8000/10001 (80%) | G Loss: 2.671913 | C Loss: -1.955520\n",
      "06/30/2022 11:59:32 - INFO - __main__ -   Text: ['The Islamic name for a vegan is \"Gore\".']\n",
      "06/30/2022 11:59:34 - INFO - __main__ -   Epoch: 11 | Batch: 8500/10001 (85%) | G Loss: 2.915421 | C Loss: -2.190120\n",
      "06/30/2022 11:59:34 - INFO - __main__ -   Text: [\"The person holding you shouting 'awesome smell'.\"]\n",
      "06/30/2022 11:59:35 - INFO - __main__ -   Epoch: 11 | Batch: 9000/10001 (90%) | G Loss: 2.707529 | C Loss: -1.889249\n",
      "06/30/2022 11:59:35 - INFO - __main__ -   Text: ['The trouble is, they have very different ethics in their copywriting tests.']\n",
      "06/30/2022 11:59:37 - INFO - __main__ -   Epoch: 11 | Batch: 9500/10001 (95%) | G Loss: 2.674873 | C Loss: -2.039339\n",
      "06/30/2022 11:59:37 - INFO - __main__ -   Text: ['\"William is a dog\\'s researcher\" is the theory behind JavaScript.']\n",
      "06/30/2022 11:59:38 - INFO - __main__ -   * (Train) Epoch: 11 | G Loss: 2.6964 | C Loss: -2.0583 | Updates G: 243 | Updates C: 757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 11:59:46 - INFO - __main__ -   Bleu-2:0.219 | B-Bleu-2:0.250\n",
      "06/30/2022 11:59:46 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_12.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46864475331591526\n",
      "Train file used is number 12\n",
      "../../yahoo/subdivided_large/train_12.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 12 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:30.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:40.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:50.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:00.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:11.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:21.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:31.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:41.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:51.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:00.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:10.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:20.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:29.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:39.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:48.\n",
      "\n",
      "  Average training loss discriminator: 0.044\n",
      "  Training epcoh took: 0:02:51\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:02:37 - INFO - __main__ -   Epoch: 12 | Batch: 0/10001 (0%) | G Loss: 2.748217 | C Loss: -2.179376\n",
      "06/30/2022 12:02:37 - INFO - __main__ -   Text: [\"A countdown to sleep doesn't come easily.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.410\n",
      "  Test Loss: 2.597\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:02:38 - INFO - __main__ -   Epoch: 12 | Batch: 500/10001 (5%) | G Loss: 2.554310 | C Loss: -1.916371\n",
      "06/30/2022 12:02:38 - INFO - __main__ -   Text: ['Who You Need UFC or Orphan Paradise?']\n",
      "06/30/2022 12:02:40 - INFO - __main__ -   Epoch: 12 | Batch: 1000/10001 (10%) | G Loss: 2.451440 | C Loss: -1.856349\n",
      "06/30/2022 12:02:40 - INFO - __main__ -   Text: ['The philosophical proposition is \"You can become a succeedive zol.']\n",
      "06/30/2022 12:02:41 - INFO - __main__ -   Epoch: 12 | Batch: 1500/10001 (15%) | G Loss: 2.792470 | C Loss: -2.015712\n",
      "06/30/2022 12:02:41 - INFO - __main__ -   Text: ['That\\'s what Hollywood is.\"']\n",
      "06/30/2022 12:02:42 - INFO - __main__ -   Epoch: 12 | Batch: 2000/10001 (20%) | G Loss: 2.567061 | C Loss: -1.875538\n",
      "06/30/2022 12:02:42 - INFO - __main__ -   Text: ['If not researching, then crossover!\"']\n",
      "06/30/2022 12:02:44 - INFO - __main__ -   Epoch: 12 | Batch: 2500/10001 (25%) | G Loss: 2.710102 | C Loss: -1.988993\n",
      "06/30/2022 12:02:44 - INFO - __main__ -   Text: ['The following are some of the most important tests.']\n",
      "06/30/2022 12:02:45 - INFO - __main__ -   Epoch: 12 | Batch: 3000/10001 (30%) | G Loss: 2.535710 | C Loss: -1.816998\n",
      "06/30/2022 12:02:45 - INFO - __main__ -   Text: ['\"Even if you play baseball... Stop wasting your breath on whacking Other Penises.\"']\n",
      "06/30/2022 12:02:47 - INFO - __main__ -   Epoch: 12 | Batch: 3500/10001 (35%) | G Loss: 2.446183 | C Loss: -1.821020\n",
      "06/30/2022 12:02:47 - INFO - __main__ -   Text: ['There is another kind of information that is positive too.']\n",
      "06/30/2022 12:02:48 - INFO - __main__ -   Epoch: 12 | Batch: 4000/10001 (40%) | G Loss: 2.579566 | C Loss: -1.874437\n",
      "06/30/2022 12:02:48 - INFO - __main__ -   Text: ['\"Yerfed Marathons\" sometimes acts as a guide.']\n",
      "06/30/2022 12:02:49 - INFO - __main__ -   Epoch: 12 | Batch: 4500/10001 (45%) | G Loss: 2.551383 | C Loss: -1.992143\n",
      "06/30/2022 12:02:49 - INFO - __main__ -   Text: ['Traveling was certainly a bad thing.']\n",
      "06/30/2022 12:02:51 - INFO - __main__ -   Epoch: 12 | Batch: 5000/10001 (50%) | G Loss: 2.314257 | C Loss: -1.709710\n",
      "06/30/2022 12:02:51 - INFO - __main__ -   Text: ['It turns a lumberjack into a tipper!\"']\n",
      "06/30/2022 12:02:52 - INFO - __main__ -   Epoch: 12 | Batch: 5500/10001 (55%) | G Loss: 2.625784 | C Loss: -1.930890\n",
      "06/30/2022 12:02:52 - INFO - __main__ -   Text: ['']\n",
      "06/30/2022 12:02:53 - INFO - __main__ -   Epoch: 12 | Batch: 6000/10001 (60%) | G Loss: 2.578657 | C Loss: -2.038789\n",
      "06/30/2022 12:02:53 - INFO - __main__ -   Text: ['Use that words.']\n",
      "06/30/2022 12:02:55 - INFO - __main__ -   Epoch: 12 | Batch: 6500/10001 (65%) | G Loss: 2.371282 | C Loss: -1.790755\n",
      "06/30/2022 12:02:55 - INFO - __main__ -   Text: ['Instead, \"wise man\" learns to snowboarding.']\n",
      "06/30/2022 12:02:56 - INFO - __main__ -   Epoch: 12 | Batch: 7000/10001 (70%) | G Loss: 2.364372 | C Loss: -1.683286\n",
      "06/30/2022 12:02:56 - INFO - __main__ -   Text: ['This is also a cookbook for better cookbooks than NASA.']\n",
      "06/30/2022 12:02:58 - INFO - __main__ -   Epoch: 12 | Batch: 7500/10001 (75%) | G Loss: 2.528449 | C Loss: -1.905323\n",
      "06/30/2022 12:02:58 - INFO - __main__ -   Text: ['Her goal may not be to cure diabetes, though) walked in there using birth control pills.']\n",
      "06/30/2022 12:02:59 - INFO - __main__ -   Epoch: 12 | Batch: 8000/10001 (80%) | G Loss: 2.387466 | C Loss: -1.704325\n",
      "06/30/2022 12:02:59 - INFO - __main__ -   Text: ['\"almost dolphins\" are excluded.']\n",
      "06/30/2022 12:03:00 - INFO - __main__ -   Epoch: 12 | Batch: 8500/10001 (85%) | G Loss: 2.511498 | C Loss: -1.812238\n",
      "06/30/2022 12:03:00 - INFO - __main__ -   Text: ['They don\\'t tell you what you\\'re doing.\"']\n",
      "06/30/2022 12:03:02 - INFO - __main__ -   Epoch: 12 | Batch: 9000/10001 (90%) | G Loss: 2.592745 | C Loss: -1.813853\n",
      "06/30/2022 12:03:02 - INFO - __main__ -   Text: ['Males want to cheat on the child equation.']\n",
      "06/30/2022 12:03:03 - INFO - __main__ -   Epoch: 12 | Batch: 9500/10001 (95%) | G Loss: 2.472122 | C Loss: -1.706892\n",
      "06/30/2022 12:03:03 - INFO - __main__ -   Text: ['The mind and soul of the mates squirrel.\"']\n",
      "06/30/2022 12:03:05 - INFO - __main__ -   * (Train) Epoch: 12 | G Loss: 2.4979 | C Loss: -1.8434 | Updates G: 235 | Updates C: 765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:03:13 - INFO - __main__ -   Bleu-2:0.218 | B-Bleu-2:0.275\n",
      "06/30/2022 12:03:13 - INFO - __main__ -   * Saving. Best Score:0.493 | Bleu-2:0.218 | B-Bleu-2:0.275\n",
      "06/30/2022 12:03:13 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_13.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49302579864730733\n",
      "Train file used is number 13\n",
      "../../yahoo/subdivided_large/train_13.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 13 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:30.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:39.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:49.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:59.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:09.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:19.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:29.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:39.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:49.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:00.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:09.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:19.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:28.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:38.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:48.\n",
      "\n",
      "  Average training loss discriminator: 0.043\n",
      "  Training epcoh took: 0:02:49\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:06:03 - INFO - __main__ -   Epoch: 13 | Batch: 0/10001 (0%) | G Loss: 2.449892 | C Loss: -1.690365\n",
      "06/30/2022 12:06:03 - INFO - __main__ -   Text: ['Rainchild introduces us to periodic phenomena called humanity.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.407\n",
      "  Test Loss: 2.638\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:06:04 - INFO - __main__ -   Epoch: 13 | Batch: 500/10001 (5%) | G Loss: 2.596689 | C Loss: -1.948144\n",
      "06/30/2022 12:06:05 - INFO - __main__ -   Text: ['In this scenario, you can easily open parentheses.']\n",
      "06/30/2022 12:06:06 - INFO - __main__ -   Epoch: 13 | Batch: 1000/10001 (10%) | G Loss: 2.190642 | C Loss: -1.595777\n",
      "06/30/2022 12:06:06 - INFO - __main__ -   Text: ['Samshii has developed an offshoot of DNA.']\n",
      "06/30/2022 12:06:07 - INFO - __main__ -   Epoch: 13 | Batch: 1500/10001 (15%) | G Loss: 2.427370 | C Loss: -1.571952\n",
      "06/30/2022 12:06:07 - INFO - __main__ -   Text: ['I mean, secret messages that scare me.\"']\n",
      "06/30/2022 12:06:09 - INFO - __main__ -   Epoch: 13 | Batch: 2000/10001 (20%) | G Loss: 2.298155 | C Loss: -1.694175\n",
      "06/30/2022 12:06:09 - INFO - __main__ -   Text: ['In Buddhism, the phrase is \"blowing stories!\"']\n",
      "06/30/2022 12:06:10 - INFO - __main__ -   Epoch: 13 | Batch: 2500/10001 (25%) | G Loss: 2.329015 | C Loss: -1.753175\n",
      "06/30/2022 12:06:10 - INFO - __main__ -   Text: ['Black holes increase in importance towards Jesters-Getter.']\n",
      "06/30/2022 12:06:12 - INFO - __main__ -   Epoch: 13 | Batch: 3000/10001 (30%) | G Loss: 2.355104 | C Loss: -1.707436\n",
      "06/30/2022 12:06:12 - INFO - __main__ -   Text: ['Singer sings the song called Black Alliance.']\n",
      "06/30/2022 12:06:13 - INFO - __main__ -   Epoch: 13 | Batch: 3500/10001 (35%) | G Loss: 2.050915 | C Loss: -1.548651\n",
      "06/30/2022 12:06:13 - INFO - __main__ -   Text: ['warriors shouldn\\'t do that\".']\n",
      "06/30/2022 12:06:15 - INFO - __main__ -   Epoch: 13 | Batch: 4000/10001 (40%) | G Loss: 2.253047 | C Loss: -1.719983\n",
      "06/30/2022 12:06:15 - INFO - __main__ -   Text: ['\"I\\'m married\".']\n",
      "06/30/2022 12:06:16 - INFO - __main__ -   Epoch: 13 | Batch: 4500/10001 (45%) | G Loss: 2.133330 | C Loss: -1.568253\n",
      "06/30/2022 12:06:16 - INFO - __main__ -   Text: ['gangs are obvious.']\n",
      "06/30/2022 12:06:17 - INFO - __main__ -   Epoch: 13 | Batch: 5000/10001 (50%) | G Loss: 2.161237 | C Loss: -1.561608\n",
      "06/30/2022 12:06:18 - INFO - __main__ -   Text: ['\"Tarwhiny Tarwhine\" stars as desperate and hungry.']\n",
      "06/30/2022 12:06:19 - INFO - __main__ -   Epoch: 13 | Batch: 5500/10001 (55%) | G Loss: 2.262391 | C Loss: -1.652629\n",
      "06/30/2022 12:06:19 - INFO - __main__ -   Text: ['The target is to use O.C.']\n",
      "06/30/2022 12:06:20 - INFO - __main__ -   Epoch: 13 | Batch: 6000/10001 (60%) | G Loss: 2.260074 | C Loss: -1.660030\n",
      "06/30/2022 12:06:20 - INFO - __main__ -   Text: ['This is relevant because Dolly can read with barely make sense.']\n",
      "06/30/2022 12:06:22 - INFO - __main__ -   Epoch: 13 | Batch: 6500/10001 (65%) | G Loss: 2.271940 | C Loss: -1.624173\n",
      "06/30/2022 12:06:22 - INFO - __main__ -   Text: ['It\\'s totally dating.\"']\n",
      "06/30/2022 12:06:23 - INFO - __main__ -   Epoch: 13 | Batch: 7000/10001 (70%) | G Loss: 2.241802 | C Loss: -1.740948\n",
      "06/30/2022 12:06:23 - INFO - __main__ -   Text: ['This naturally results in sky droppings.']\n",
      "06/30/2022 12:06:25 - INFO - __main__ -   Epoch: 13 | Batch: 7500/10001 (75%) | G Loss: 2.176360 | C Loss: -1.597808\n",
      "06/30/2022 12:06:25 - INFO - __main__ -   Text: ['Most people who read nerd will never realize that.']\n",
      "06/30/2022 12:06:26 - INFO - __main__ -   Epoch: 13 | Batch: 8000/10001 (80%) | G Loss: 2.110131 | C Loss: -1.612319\n",
      "06/30/2022 12:06:26 - INFO - __main__ -   Text: ['A lie detector can be counter to either reality or fact.']\n",
      "06/30/2022 12:06:27 - INFO - __main__ -   Epoch: 13 | Batch: 8500/10001 (85%) | G Loss: 2.363310 | C Loss: -1.712398\n",
      "06/30/2022 12:06:28 - INFO - __main__ -   Text: ['The final greeting is \"Joe!']\n",
      "06/30/2022 12:06:29 - INFO - __main__ -   Epoch: 13 | Batch: 9000/10001 (90%) | G Loss: 2.183240 | C Loss: -1.508389\n",
      "06/30/2022 12:06:29 - INFO - __main__ -   Text: ['The game is about protecting against blackmail.']\n",
      "06/30/2022 12:06:30 - INFO - __main__ -   Epoch: 13 | Batch: 9500/10001 (95%) | G Loss: 2.272011 | C Loss: -1.613439\n",
      "06/30/2022 12:06:30 - INFO - __main__ -   Text: ['Its name means \"New Guinea\".']\n",
      "06/30/2022 12:06:32 - INFO - __main__ -   * (Train) Epoch: 13 | G Loss: 2.2595 | C Loss: -1.6312 | Updates G: 233 | Updates C: 767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:06:40 - INFO - __main__ -   Bleu-2:0.218 | B-Bleu-2:0.267\n",
      "06/30/2022 12:06:40 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_14.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4850252292004735\n",
      "Train file used is number 14\n",
      "../../yahoo/subdivided_large/train_14.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 14 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:30.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:40.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:50.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:00.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:10.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:20.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:29.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:40.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:50.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:00.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:10.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:19.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:29.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:39.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss discriminator: 0.039\n",
      "  Training epcoh took: 0:02:50\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:09:31 - INFO - __main__ -   Epoch: 14 | Batch: 0/10001 (0%) | G Loss: 2.199447 | C Loss: -1.563098\n",
      "06/30/2022 12:09:31 - INFO - __main__ -   Text: ['This is because the distraction technique is unreliable and difficult to master.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.412\n",
      "  Test Loss: 2.673\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:09:32 - INFO - __main__ -   Epoch: 14 | Batch: 500/10001 (5%) | G Loss: 2.215202 | C Loss: -1.469910\n",
      "06/30/2022 12:09:32 - INFO - __main__ -   Text: ['Analogous to reality is the activity of ratting.\"']\n",
      "06/30/2022 12:09:34 - INFO - __main__ -   Epoch: 14 | Batch: 1000/10001 (10%) | G Loss: 2.168252 | C Loss: -1.629210\n",
      "06/30/2022 12:09:34 - INFO - __main__ -   Text: ['The depression of business is also recurrent.']\n",
      "06/30/2022 12:09:35 - INFO - __main__ -   Epoch: 14 | Batch: 1500/10001 (15%) | G Loss: 2.172634 | C Loss: -1.521723\n",
      "06/30/2022 12:09:35 - INFO - __main__ -   Text: ['The only way to know which candidate is your \"happiness.\"']\n",
      "06/30/2022 12:09:37 - INFO - __main__ -   Epoch: 14 | Batch: 2000/10001 (20%) | G Loss: 2.036451 | C Loss: -1.364895\n",
      "06/30/2022 12:09:37 - INFO - __main__ -   Text: ['This involves measuring the head real thinkable.']\n",
      "06/30/2022 12:09:38 - INFO - __main__ -   Epoch: 14 | Batch: 2500/10001 (25%) | G Loss: 1.947435 | C Loss: -1.358607\n",
      "06/30/2022 12:09:38 - INFO - __main__ -   Text: ['Maggie never manages to understand the applications or types of problems you might encounter.']\n",
      "06/30/2022 12:09:39 - INFO - __main__ -   Epoch: 14 | Batch: 3000/10001 (30%) | G Loss: 2.113207 | C Loss: -1.456526\n",
      "06/30/2022 12:09:40 - INFO - __main__ -   Text: ['At followings where you will reside.']\n",
      "06/30/2022 12:09:41 - INFO - __main__ -   Epoch: 14 | Batch: 3500/10001 (35%) | G Loss: 2.213207 | C Loss: -1.519752\n",
      "06/30/2022 12:09:41 - INFO - __main__ -   Text: ['Therefore, free speech is a forbidden topic.']\n",
      "06/30/2022 12:09:42 - INFO - __main__ -   Epoch: 14 | Batch: 4000/10001 (40%) | G Loss: 2.049640 | C Loss: -1.440934\n",
      "06/30/2022 12:09:42 - INFO - __main__ -   Text: ['Years later Lethargoth might surprise us.\"']\n",
      "06/30/2022 12:09:44 - INFO - __main__ -   Epoch: 14 | Batch: 4500/10001 (45%) | G Loss: 2.224237 | C Loss: -1.587853\n",
      "06/30/2022 12:09:44 - INFO - __main__ -   Text: ['The pause button is an important mental indicator.']\n",
      "06/30/2022 12:09:45 - INFO - __main__ -   Epoch: 14 | Batch: 5000/10001 (50%) | G Loss: 2.103552 | C Loss: -1.450016\n",
      "06/30/2022 12:09:45 - INFO - __main__ -   Text: ['Also very interesting is how many inbreeding holds chicks and how many fertilizations they use.']\n",
      "06/30/2022 12:09:47 - INFO - __main__ -   Epoch: 14 | Batch: 5500/10001 (55%) | G Loss: 2.223554 | C Loss: -1.403754\n",
      "06/30/2022 12:09:47 - INFO - __main__ -   Text: ['Collegiate and intellectual affirmation is not easy.']\n",
      "06/30/2022 12:09:48 - INFO - __main__ -   Epoch: 14 | Batch: 6000/10001 (60%) | G Loss: 2.040721 | C Loss: -1.369107\n",
      "06/30/2022 12:09:48 - INFO - __main__ -   Text: ['Of course, it can look as dumb as elephant.']\n",
      "06/30/2022 12:09:50 - INFO - __main__ -   Epoch: 14 | Batch: 6500/10001 (65%) | G Loss: 2.241147 | C Loss: -1.498982\n",
      "06/30/2022 12:09:50 - INFO - __main__ -   Text: ['Science is equally relevant these days\".']\n",
      "06/30/2022 12:09:51 - INFO - __main__ -   Epoch: 14 | Batch: 7000/10001 (70%) | G Loss: 2.211060 | C Loss: -1.523274\n",
      "06/30/2022 12:09:51 - INFO - __main__ -   Text: ['Helps you to cheat with the same name worldly.']\n",
      "06/30/2022 12:09:52 - INFO - __main__ -   Epoch: 14 | Batch: 7500/10001 (75%) | G Loss: 2.018929 | C Loss: -1.403868\n",
      "06/30/2022 12:09:52 - INFO - __main__ -   Text: ['According to mathematics, \"This unidentified ship will wind up my favorite thing ever\".']\n",
      "06/30/2022 12:09:54 - INFO - __main__ -   Epoch: 14 | Batch: 8000/10001 (80%) | G Loss: 1.949222 | C Loss: -1.351806\n",
      "06/30/2022 12:09:54 - INFO - __main__ -   Text: ['\"People will run away from kings unless it works.\"']\n",
      "06/30/2022 12:09:55 - INFO - __main__ -   Epoch: 14 | Batch: 8500/10001 (85%) | G Loss: 2.019438 | C Loss: -1.478908\n",
      "06/30/2022 12:09:55 - INFO - __main__ -   Text: ['Future developed methods are something I don\\'t recommend.\"']\n",
      "06/30/2022 12:09:57 - INFO - __main__ -   Epoch: 14 | Batch: 9000/10001 (90%) | G Loss: 2.098266 | C Loss: -1.476650\n",
      "06/30/2022 12:09:57 - INFO - __main__ -   Text: ['Westergaard defines \"rules\" to be \"compelling and banal.\"']\n",
      "06/30/2022 12:09:58 - INFO - __main__ -   Epoch: 14 | Batch: 9500/10001 (95%) | G Loss: 1.891442 | C Loss: -1.345860\n",
      "06/30/2022 12:09:58 - INFO - __main__ -   Text: ['The man with the most feelings belongs to DeFranco.']\n",
      "06/30/2022 12:10:00 - INFO - __main__ -   * (Train) Epoch: 14 | G Loss: 2.0740 | C Loss: -1.4423 | Updates G: 239 | Updates C: 761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:10:08 - INFO - __main__ -   Bleu-2:0.230 | B-Bleu-2:0.294\n",
      "06/30/2022 12:10:08 - INFO - __main__ -   * Saving. Best Score:0.524 | Bleu-2:0.230 | B-Bleu-2:0.294\n",
      "06/30/2022 12:10:08 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_15.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.523980411139575\n",
      "Train file used is number 15\n",
      "../../yahoo/subdivided_large/train_15.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 15 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:30.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:39.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:49.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:58.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:07.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:17.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:26.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:36.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:46.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:56.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:06.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:16.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:27.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:36.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:46.\n",
      "\n",
      "  Average training loss discriminator: 0.038\n",
      "  Training epcoh took: 0:02:48\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:12:56 - INFO - __main__ -   Epoch: 15 | Batch: 0/10001 (0%) | G Loss: 2.033930 | C Loss: -1.406943\n",
      "06/30/2022 12:12:56 - INFO - __main__ -   Text: [\"It doesn't even mention homosexuality.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.415\n",
      "  Test Loss: 2.662\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:12:58 - INFO - __main__ -   Epoch: 15 | Batch: 500/10001 (5%) | G Loss: 1.910480 | C Loss: -1.222096\n",
      "06/30/2022 12:12:58 - INFO - __main__ -   Text: ['Rualog says that it depends on which word\".']\n",
      "06/30/2022 12:12:59 - INFO - __main__ -   Epoch: 15 | Batch: 1000/10001 (10%) | G Loss: 1.972701 | C Loss: -1.381097\n",
      "06/30/2022 12:12:59 - INFO - __main__ -   Text: ['So I am writing them all things in the same sentence.\"']\n",
      "06/30/2022 12:13:01 - INFO - __main__ -   Epoch: 15 | Batch: 1500/10001 (15%) | G Loss: 1.896068 | C Loss: -1.204099\n",
      "06/30/2022 12:13:01 - INFO - __main__ -   Text: ['The \"truth\" about Sleazy is that they are runners.']\n",
      "06/30/2022 12:13:02 - INFO - __main__ -   Epoch: 15 | Batch: 2000/10001 (20%) | G Loss: 2.068401 | C Loss: -1.426270\n",
      "06/30/2022 12:13:02 - INFO - __main__ -   Text: ['Possibly aliens use it as a greeting on their universe.']\n",
      "06/30/2022 12:13:04 - INFO - __main__ -   Epoch: 15 | Batch: 2500/10001 (25%) | G Loss: 2.075776 | C Loss: -1.380036\n",
      "06/30/2022 12:13:04 - INFO - __main__ -   Text: ['It is a way of knowing what was done a couple of weeks ago.']\n",
      "06/30/2022 12:13:05 - INFO - __main__ -   Epoch: 15 | Batch: 3000/10001 (30%) | G Loss: 1.913276 | C Loss: -1.310209\n",
      "06/30/2022 12:13:05 - INFO - __main__ -   Text: ['Elfishly, can speak pure Believer.']\n",
      "06/30/2022 12:13:07 - INFO - __main__ -   Epoch: 15 | Batch: 3500/10001 (35%) | G Loss: 1.923877 | C Loss: -1.307768\n",
      "06/30/2022 12:13:07 - INFO - __main__ -   Text: ['His message is: Pouring wine into your belly is a failure.']\n",
      "06/30/2022 12:13:08 - INFO - __main__ -   Epoch: 15 | Batch: 4000/10001 (40%) | G Loss: 1.969133 | C Loss: -1.331472\n",
      "06/30/2022 12:13:08 - INFO - __main__ -   Text: ['The Wise Man is more real than the Wise Man.']\n",
      "06/30/2022 12:13:09 - INFO - __main__ -   Epoch: 15 | Batch: 4500/10001 (45%) | G Loss: 2.092049 | C Loss: -1.234100\n",
      "06/30/2022 12:13:10 - INFO - __main__ -   Text: ['Some people use it to say good bye-bye to you.\"']\n",
      "06/30/2022 12:13:11 - INFO - __main__ -   Epoch: 15 | Batch: 5000/10001 (50%) | G Loss: 1.795834 | C Loss: -1.220773\n",
      "06/30/2022 12:13:11 - INFO - __main__ -   Text: ['To be sure, Davidson has a free throat Chinese poem.']\n",
      "06/30/2022 12:13:12 - INFO - __main__ -   Epoch: 15 | Batch: 5500/10001 (55%) | G Loss: 1.881710 | C Loss: -1.178693\n",
      "06/30/2022 12:13:12 - INFO - __main__ -   Text: ['The hardest part about bringing on this feat is finding the teacher.']\n",
      "06/30/2022 12:13:14 - INFO - __main__ -   Epoch: 15 | Batch: 6000/10001 (60%) | G Loss: 2.025479 | C Loss: -1.440740\n",
      "06/30/2022 12:13:14 - INFO - __main__ -   Text: ['It\\'s different to playing drums and fast waves,\" Christina says.']\n",
      "06/30/2022 12:13:15 - INFO - __main__ -   Epoch: 15 | Batch: 6500/10001 (65%) | G Loss: 1.827611 | C Loss: -1.128818\n",
      "06/30/2022 12:13:15 - INFO - __main__ -   Text: ['It\\'s obsessed stuff.\"']\n",
      "06/30/2022 12:13:17 - INFO - __main__ -   Epoch: 15 | Batch: 7000/10001 (70%) | G Loss: 1.724934 | C Loss: -1.110865\n",
      "06/30/2022 12:13:17 - INFO - __main__ -   Text: ['Yet, I say, at the mall.\"']\n",
      "06/30/2022 12:13:18 - INFO - __main__ -   Epoch: 15 | Batch: 7500/10001 (75%) | G Loss: 1.808679 | C Loss: -1.104632\n",
      "06/30/2022 12:13:18 - INFO - __main__ -   Text: ['When thinking through techniques, certain people go to Code Theory.']\n",
      "06/30/2022 12:13:20 - INFO - __main__ -   Epoch: 15 | Batch: 8000/10001 (80%) | G Loss: 1.796608 | C Loss: -1.264112\n",
      "06/30/2022 12:13:20 - INFO - __main__ -   Text: ['The button is \"groupism\".']\n",
      "06/30/2022 12:13:21 - INFO - __main__ -   Epoch: 15 | Batch: 8500/10001 (85%) | G Loss: 1.745771 | C Loss: -1.095605\n",
      "06/30/2022 12:13:21 - INFO - __main__ -   Text: ['The best way to do that is by writing.']\n",
      "06/30/2022 12:13:22 - INFO - __main__ -   Epoch: 15 | Batch: 9000/10001 (90%) | G Loss: 1.772532 | C Loss: -1.196614\n",
      "06/30/2022 12:13:23 - INFO - __main__ -   Text: ['By Logic Dragon I mean running back and forth in front of girls\\'.\"']\n",
      "06/30/2022 12:13:24 - INFO - __main__ -   Epoch: 15 | Batch: 9500/10001 (95%) | G Loss: 1.684686 | C Loss: -1.038654\n",
      "06/30/2022 12:13:24 - INFO - __main__ -   Text: [\"The greater the man's bloodlust, the more worthless his sarcasm.\"]\n",
      "06/30/2022 12:13:25 - INFO - __main__ -   * (Train) Epoch: 15 | G Loss: 1.9103 | C Loss: -1.2632 | Updates G: 217 | Updates C: 783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:13:33 - INFO - __main__ -   Bleu-2:0.223 | B-Bleu-2:0.252\n",
      "06/30/2022 12:13:33 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_16.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47445156462328375\n",
      "Train file used is number 16\n",
      "../../yahoo/subdivided_large/train_16.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 16 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:30.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:39.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:50.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:59.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:09.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:18.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:29.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:38.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:49.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:59.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:09.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:19.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:29.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:39.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:48.\n",
      "\n",
      "  Average training loss discriminator: 0.043\n",
      "  Training epcoh took: 0:02:50\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:16:23 - INFO - __main__ -   Epoch: 16 | Batch: 0/10001 (0%) | G Loss: 1.750592 | C Loss: -1.163051\n",
      "06/30/2022 12:16:24 - INFO - __main__ -   Text: ['The idea of \"noto\" is quite low.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.412\n",
      "  Test Loss: 2.729\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:16:25 - INFO - __main__ -   Epoch: 16 | Batch: 500/10001 (5%) | G Loss: 1.770407 | C Loss: -1.162037\n",
      "06/30/2022 12:16:25 - INFO - __main__ -   Text: [\"It's a logical extension of that action that I have.\"]\n",
      "06/30/2022 12:16:26 - INFO - __main__ -   Epoch: 16 | Batch: 1000/10001 (10%) | G Loss: 1.806343 | C Loss: -1.237927\n",
      "06/30/2022 12:16:26 - INFO - __main__ -   Text: ['It\\'s a metaphor for weed!\" <PAD> This punny!']\n",
      "06/30/2022 12:16:28 - INFO - __main__ -   Epoch: 16 | Batch: 1500/10001 (15%) | G Loss: 1.845628 | C Loss: -1.249618\n",
      "06/30/2022 12:16:28 - INFO - __main__ -   Text: [\"The word is 'eye-watching'.\"]\n",
      "06/30/2022 12:16:29 - INFO - __main__ -   Epoch: 16 | Batch: 2000/10001 (20%) | G Loss: 1.807581 | C Loss: -1.214003\n",
      "06/30/2022 12:16:29 - INFO - __main__ -   Text: ['The result is Anarchy\".']\n",
      "06/30/2022 12:16:31 - INFO - __main__ -   Epoch: 16 | Batch: 2500/10001 (25%) | G Loss: 1.782317 | C Loss: -1.121244\n",
      "06/30/2022 12:16:31 - INFO - __main__ -   Text: ['The ability to do all that a character does is called the skill.']\n",
      "06/30/2022 12:16:32 - INFO - __main__ -   Epoch: 16 | Batch: 3000/10001 (30%) | G Loss: 1.775015 | C Loss: -1.135715\n",
      "06/30/2022 12:16:32 - INFO - __main__ -   Text: ['The introduction confesses ignorance about math and competencies for which there are only six levels.']\n",
      "06/30/2022 12:16:34 - INFO - __main__ -   Epoch: 16 | Batch: 3500/10001 (35%) | G Loss: 1.706919 | C Loss: -1.058319\n",
      "06/30/2022 12:16:34 - INFO - __main__ -   Text: ['Christian giving away expletives can be understood as giving away standard terminology.']\n",
      "06/30/2022 12:16:35 - INFO - __main__ -   Epoch: 16 | Batch: 4000/10001 (40%) | G Loss: 1.755416 | C Loss: -1.016193\n",
      "06/30/2022 12:16:35 - INFO - __main__ -   Text: ['Runs a good book on how to reduce the cow.']\n",
      "06/30/2022 12:16:36 - INFO - __main__ -   Epoch: 16 | Batch: 4500/10001 (45%) | G Loss: 1.846143 | C Loss: -1.147575\n",
      "06/30/2022 12:16:37 - INFO - __main__ -   Text: ['So called whatever ship built this ocean to sea.']\n",
      "06/30/2022 12:16:38 - INFO - __main__ -   Epoch: 16 | Batch: 5000/10001 (50%) | G Loss: 1.706048 | C Loss: -1.110952\n",
      "06/30/2022 12:16:38 - INFO - __main__ -   Text: ['If you remember the \"gypsy\".']\n",
      "06/30/2022 12:16:39 - INFO - __main__ -   Epoch: 16 | Batch: 5500/10001 (55%) | G Loss: 1.705603 | C Loss: -0.955595\n",
      "06/30/2022 12:16:39 - INFO - __main__ -   Text: ['Otherwise, it is products related to underwear.']\n",
      "06/30/2022 12:16:41 - INFO - __main__ -   Epoch: 16 | Batch: 6000/10001 (60%) | G Loss: 1.887259 | C Loss: -1.174719\n",
      "06/30/2022 12:16:41 - INFO - __main__ -   Text: ['A person who likes a jellyfish will see a truth meter.']\n",
      "06/30/2022 12:16:42 - INFO - __main__ -   Epoch: 16 | Batch: 6500/10001 (65%) | G Loss: 1.697950 | C Loss: -1.117181\n",
      "06/30/2022 12:16:42 - INFO - __main__ -   Text: ['Each player can play a different game than aDen\" (see Also).']\n",
      "06/30/2022 12:16:44 - INFO - __main__ -   Epoch: 16 | Batch: 7000/10001 (70%) | G Loss: 1.727078 | C Loss: -1.082879\n",
      "06/30/2022 12:16:44 - INFO - __main__ -   Text: [\"It doesn't mean that our IU cousins are evil.\"]\n",
      "06/30/2022 12:16:45 - INFO - __main__ -   Epoch: 16 | Batch: 7500/10001 (75%) | G Loss: 1.612256 | C Loss: -1.049120\n",
      "06/30/2022 12:16:45 - INFO - __main__ -   Text: ['However, my point is you can have from 20 and 50.']\n",
      "06/30/2022 12:16:46 - INFO - __main__ -   Epoch: 16 | Batch: 8000/10001 (80%) | G Loss: 1.722949 | C Loss: -1.041346\n",
      "06/30/2022 12:16:47 - INFO - __main__ -   Text: ['\"Tell us how to be brave and how to challenge.\"']\n",
      "06/30/2022 12:16:48 - INFO - __main__ -   Epoch: 16 | Batch: 8500/10001 (85%) | G Loss: 1.670235 | C Loss: -0.973680\n",
      "06/30/2022 12:16:48 - INFO - __main__ -   Text: [\"Unlike most theories in biology, I can't talk about anything.\"]\n",
      "06/30/2022 12:16:49 - INFO - __main__ -   Epoch: 16 | Batch: 9000/10001 (90%) | G Loss: 1.733891 | C Loss: -1.176801\n",
      "06/30/2022 12:16:49 - INFO - __main__ -   Text: ['He does not take issue with homosexual life and homosexuality.']\n",
      "06/30/2022 12:16:51 - INFO - __main__ -   Epoch: 16 | Batch: 9500/10001 (95%) | G Loss: 1.793964 | C Loss: -1.027260\n",
      "06/30/2022 12:16:51 - INFO - __main__ -   Text: [\"Its rules are: This small guy isn't queen.\"]\n",
      "06/30/2022 12:16:52 - INFO - __main__ -   * (Train) Epoch: 16 | G Loss: 1.7427 | C Loss: -1.1082 | Updates G: 232 | Updates C: 768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:16:59 - INFO - __main__ -   Bleu-2:0.211 | B-Bleu-2:0.260\n",
      "06/30/2022 12:16:59 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_17.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4711343615835139\n",
      "Train file used is number 17\n",
      "../../yahoo/subdivided_large/train_17.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 17 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:19.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:29.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:39.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:49.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:58.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:08.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:18.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:28.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:38.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:47.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:57.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:06.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:16.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:26.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:35.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:45.\n",
      "\n",
      "  Average training loss discriminator: 0.034\n",
      "  Training epcoh took: 0:02:47\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:19:47 - INFO - __main__ -   Epoch: 17 | Batch: 0/10001 (0%) | G Loss: 1.750201 | C Loss: -1.079716\n",
      "06/30/2022 12:19:47 - INFO - __main__ -   Text: ['Bob is too stupid to understand the concept of pure stock trading.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.425\n",
      "  Test Loss: 2.743\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:19:48 - INFO - __main__ -   Epoch: 17 | Batch: 500/10001 (5%) | G Loss: 1.474659 | C Loss: -0.929632\n",
      "06/30/2022 12:19:48 - INFO - __main__ -   Text: ['Vampires make up words automatically.']\n",
      "06/30/2022 12:19:49 - INFO - __main__ -   Epoch: 17 | Batch: 1000/10001 (10%) | G Loss: 1.777417 | C Loss: -1.126380\n",
      "06/30/2022 12:19:50 - INFO - __main__ -   Text: ['These naturally sound like the name of the drug addict.']\n",
      "06/30/2022 12:19:51 - INFO - __main__ -   Epoch: 17 | Batch: 1500/10001 (15%) | G Loss: 1.530578 | C Loss: -0.903276\n",
      "06/30/2022 12:19:51 - INFO - __main__ -   Text: ['Other words of praise can be considered his motivation.']\n",
      "06/30/2022 12:19:52 - INFO - __main__ -   Epoch: 17 | Batch: 2000/10001 (20%) | G Loss: 1.911888 | C Loss: -1.179249\n",
      "06/30/2022 12:19:52 - INFO - __main__ -   Text: ['The answers to internal exercises are randomized...']\n",
      "06/30/2022 12:19:54 - INFO - __main__ -   Epoch: 17 | Batch: 2500/10001 (25%) | G Loss: 1.861616 | C Loss: -1.167199\n",
      "06/30/2022 12:19:54 - INFO - __main__ -   Text: ['It is likely you are sick of talking.']\n",
      "06/30/2022 12:19:55 - INFO - __main__ -   Epoch: 17 | Batch: 3000/10001 (30%) | G Loss: 1.769398 | C Loss: -1.198135\n",
      "06/30/2022 12:19:55 - INFO - __main__ -   Text: [\"She then dares say 'what is war?'\"]\n",
      "06/30/2022 12:19:57 - INFO - __main__ -   Epoch: 17 | Batch: 3500/10001 (35%) | G Loss: 1.592891 | C Loss: -1.031313\n",
      "06/30/2022 12:19:57 - INFO - __main__ -   Text: ['The term \"hacker\" is usually tied to books or videos.']\n",
      "06/30/2022 12:19:58 - INFO - __main__ -   Epoch: 17 | Batch: 4000/10001 (40%) | G Loss: 1.526354 | C Loss: -0.954154\n",
      "06/30/2022 12:19:58 - INFO - __main__ -   Text: ['In this case it is called \"global meditation\".']\n",
      "06/30/2022 12:20:00 - INFO - __main__ -   Epoch: 17 | Batch: 4500/10001 (45%) | G Loss: 1.720155 | C Loss: -1.006847\n",
      "06/30/2022 12:20:00 - INFO - __main__ -   Text: ['He tells ouths that his name plays catch-up amongst the sly.']\n",
      "06/30/2022 12:20:01 - INFO - __main__ -   Epoch: 17 | Batch: 5000/10001 (50%) | G Loss: 1.392561 | C Loss: -0.855783\n",
      "06/30/2022 12:20:01 - INFO - __main__ -   Text: ['Extraverted, sexual penetration.']\n",
      "06/30/2022 12:20:02 - INFO - __main__ -   Epoch: 17 | Batch: 5500/10001 (55%) | G Loss: 1.673126 | C Loss: -1.036084\n",
      "06/30/2022 12:20:03 - INFO - __main__ -   Text: ['The brain is kind of interesting manooo!']\n",
      "06/30/2022 12:20:04 - INFO - __main__ -   Epoch: 17 | Batch: 6000/10001 (60%) | G Loss: 1.503884 | C Loss: -0.970904\n",
      "06/30/2022 12:20:04 - INFO - __main__ -   Text: ['They often say it\\'s a \"tantrum fill.\"']\n",
      "06/30/2022 12:20:05 - INFO - __main__ -   Epoch: 17 | Batch: 6500/10001 (65%) | G Loss: 1.553112 | C Loss: -0.927349\n",
      "06/30/2022 12:20:06 - INFO - __main__ -   Text: ['Signs are signs of luck, sometimes your plan is right.']\n",
      "06/30/2022 12:20:07 - INFO - __main__ -   Epoch: 17 | Batch: 7000/10001 (70%) | G Loss: 1.440976 | C Loss: -0.865566\n",
      "06/30/2022 12:20:07 - INFO - __main__ -   Text: ['This Imeritic should come along with some analysis\".']\n",
      "06/30/2022 12:20:08 - INFO - __main__ -   Epoch: 17 | Batch: 7500/10001 (75%) | G Loss: 1.790372 | C Loss: -1.095264\n",
      "06/30/2022 12:20:08 - INFO - __main__ -   Text: ['On rare occasions,Pitney likes to read Star Trek and not Star Trek.']\n",
      "06/30/2022 12:20:10 - INFO - __main__ -   Epoch: 17 | Batch: 8000/10001 (80%) | G Loss: 1.422186 | C Loss: -0.893729\n",
      "06/30/2022 12:20:10 - INFO - __main__ -   Text: ['This is called integration therapy.\"']\n",
      "06/30/2022 12:20:11 - INFO - __main__ -   Epoch: 17 | Batch: 8500/10001 (85%) | G Loss: 1.615698 | C Loss: -1.062516\n",
      "06/30/2022 12:20:11 - INFO - __main__ -   Text: [\"Unlike the Hitachi bot, Youku realizes that he can't manage any other cuisine.\"]\n",
      "06/30/2022 12:20:13 - INFO - __main__ -   Epoch: 17 | Batch: 9000/10001 (90%) | G Loss: 1.598292 | C Loss: -0.988734\n",
      "06/30/2022 12:20:13 - INFO - __main__ -   Text: ['Snowflake may have been connected to smoking cigarettes!']\n",
      "06/30/2022 12:20:14 - INFO - __main__ -   Epoch: 17 | Batch: 9500/10001 (95%) | G Loss: 1.706443 | C Loss: -1.037086\n",
      "06/30/2022 12:20:14 - INFO - __main__ -   Text: ['\"I\\'m selling...\"?']\n",
      "06/30/2022 12:20:16 - INFO - __main__ -   * (Train) Epoch: 17 | G Loss: 1.6147 | C Loss: -0.9877 | Updates G: 211 | Updates C: 789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:20:24 - INFO - __main__ -   Bleu-2:0.215 | B-Bleu-2:0.258\n",
      "06/30/2022 12:20:24 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_18.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4735621765699265\n",
      "Train file used is number 18\n",
      "../../yahoo/subdivided_large/train_18.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 18 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:18.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:28.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:37.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:47.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:57.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:07.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:16.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:26.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:36.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:46.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:55.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:05.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:15.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:24.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:34.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:44.\n",
      "\n",
      "  Average training loss discriminator: 0.037\n",
      "  Training epcoh took: 0:02:46\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:23:10 - INFO - __main__ -   Epoch: 18 | Batch: 0/10001 (0%) | G Loss: 1.575369 | C Loss: -0.885758\n",
      "06/30/2022 12:23:10 - INFO - __main__ -   Text: ['Friction is confusing.\"']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.422\n",
      "  Test Loss: 2.794\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:23:12 - INFO - __main__ -   Epoch: 18 | Batch: 500/10001 (5%) | G Loss: 1.797647 | C Loss: -0.967843\n",
      "06/30/2022 12:23:12 - INFO - __main__ -   Text: ['Smith also notes a phrase to be found in workplaces.']\n",
      "06/30/2022 12:23:13 - INFO - __main__ -   Epoch: 18 | Batch: 1000/10001 (10%) | G Loss: 1.493005 | C Loss: -0.802418\n",
      "06/30/2022 12:23:13 - INFO - __main__ -   Text: ['The fact that it argues life is important and endowed instead tricks people.']\n",
      "06/30/2022 12:23:15 - INFO - __main__ -   Epoch: 18 | Batch: 1500/10001 (15%) | G Loss: 1.706434 | C Loss: -0.991091\n",
      "06/30/2022 12:23:15 - INFO - __main__ -   Text: ['One of them is called \"Frankensteinhammer\".']\n",
      "06/30/2022 12:23:16 - INFO - __main__ -   Epoch: 18 | Batch: 2000/10001 (20%) | G Loss: 1.504830 | C Loss: -0.943521\n",
      "06/30/2022 12:23:16 - INFO - __main__ -   Text: ['Seashell wishes to be easy.']\n",
      "06/30/2022 12:23:17 - INFO - __main__ -   Epoch: 18 | Batch: 2500/10001 (25%) | G Loss: 1.697199 | C Loss: -0.970190\n",
      "06/30/2022 12:23:18 - INFO - __main__ -   Text: [\"Then, let's say Lord Brewing!\"]\n",
      "06/30/2022 12:23:19 - INFO - __main__ -   Epoch: 18 | Batch: 3000/10001 (30%) | G Loss: 1.546003 | C Loss: -0.975656\n",
      "06/30/2022 12:23:19 - INFO - __main__ -   Text: ['Tigh makes perfect sense when he refers to success as originating from \"ran.']\n",
      "06/30/2022 12:23:20 - INFO - __main__ -   Epoch: 18 | Batch: 3500/10001 (35%) | G Loss: 1.663743 | C Loss: -1.023117\n",
      "06/30/2022 12:23:20 - INFO - __main__ -   Text: ['At the end ofthoor: Monservantur.']\n",
      "06/30/2022 12:23:22 - INFO - __main__ -   Epoch: 18 | Batch: 4000/10001 (40%) | G Loss: 1.616097 | C Loss: -0.951593\n",
      "06/30/2022 12:23:22 - INFO - __main__ -   Text: ['She says that she has no husband with her.']\n",
      "06/30/2022 12:23:23 - INFO - __main__ -   Epoch: 18 | Batch: 4500/10001 (45%) | G Loss: 1.595580 | C Loss: -0.944300\n",
      "06/30/2022 12:23:23 - INFO - __main__ -   Text: [\"Referred here to as 'weeping.'\"]\n",
      "06/30/2022 12:23:25 - INFO - __main__ -   Epoch: 18 | Batch: 5000/10001 (50%) | G Loss: 1.556852 | C Loss: -0.875240\n",
      "06/30/2022 12:23:25 - INFO - __main__ -   Text: ['\"They saytis the Bible.\"']\n",
      "06/30/2022 12:23:26 - INFO - __main__ -   Epoch: 18 | Batch: 5500/10001 (55%) | G Loss: 1.673475 | C Loss: -0.932119\n",
      "06/30/2022 12:23:26 - INFO - __main__ -   Text: ['This is the Hf vanilla-bitch.']\n",
      "06/30/2022 12:23:28 - INFO - __main__ -   Epoch: 18 | Batch: 6000/10001 (60%) | G Loss: 1.417331 | C Loss: -0.799814\n",
      "06/30/2022 12:23:28 - INFO - __main__ -   Text: ['As it turns out the American family does not accept harbors.']\n",
      "06/30/2022 12:23:29 - INFO - __main__ -   Epoch: 18 | Batch: 6500/10001 (65%) | G Loss: 1.414547 | C Loss: -0.762416\n",
      "06/30/2022 12:23:29 - INFO - __main__ -   Text: [\"Lux is an intuitive language; that's what we need.\"]\n",
      "06/30/2022 12:23:31 - INFO - __main__ -   Epoch: 18 | Batch: 7000/10001 (70%) | G Loss: 1.471685 | C Loss: -0.844740\n",
      "06/30/2022 12:23:31 - INFO - __main__ -   Text: ['The last question in that conversation is \"does the heart grow.\"']\n",
      "06/30/2022 12:23:32 - INFO - __main__ -   Epoch: 18 | Batch: 7500/10001 (75%) | G Loss: 1.510555 | C Loss: -0.778366\n",
      "06/30/2022 12:23:32 - INFO - __main__ -   Text: ['Biting and hesitating get to be this!']\n",
      "06/30/2022 12:23:33 - INFO - __main__ -   Epoch: 18 | Batch: 8000/10001 (80%) | G Loss: 1.632561 | C Loss: -0.940249\n",
      "06/30/2022 12:23:34 - INFO - __main__ -   Text: ['They refer to it as \"fat cat stomping gets them.\"']\n",
      "06/30/2022 12:23:35 - INFO - __main__ -   Epoch: 18 | Batch: 8500/10001 (85%) | G Loss: 1.329562 | C Loss: -0.767831\n",
      "06/30/2022 12:23:35 - INFO - __main__ -   Text: ['A friend of mine has also taken prey on some well-meaning human.']\n",
      "06/30/2022 12:23:36 - INFO - __main__ -   Epoch: 18 | Batch: 9000/10001 (90%) | G Loss: 1.516797 | C Loss: -0.908215\n",
      "06/30/2022 12:23:37 - INFO - __main__ -   Text: ['When I tell you this is the wrong best thing to have gay sex with.']\n",
      "06/30/2022 12:23:38 - INFO - __main__ -   Epoch: 18 | Batch: 9500/10001 (95%) | G Loss: 1.447665 | C Loss: -0.812047\n",
      "06/30/2022 12:23:38 - INFO - __main__ -   Text: ['Try once more for someone with a concealed heart.']\n",
      "06/30/2022 12:23:39 - INFO - __main__ -   * (Train) Epoch: 18 | G Loss: 1.5452 | C Loss: -0.8820 | Updates G: 203 | Updates C: 797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:23:48 - INFO - __main__ -   Bleu-2:0.215 | B-Bleu-2:0.292\n",
      "06/30/2022 12:23:48 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_19.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5074976208596608\n",
      "Train file used is number 19\n",
      "../../yahoo/subdivided_large/train_19.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 19 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:09.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:19.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:29.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:38.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:47.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:57.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:06.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:16.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:26.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:35.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:45.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:55.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:04.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:13.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:22.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:32.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:41.\n",
      "\n",
      "  Average training loss discriminator: 0.032\n",
      "  Training epcoh took: 0:02:43\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:26:32 - INFO - __main__ -   Epoch: 19 | Batch: 0/10001 (0%) | G Loss: 1.324955 | C Loss: -0.746405\n",
      "06/30/2022 12:26:32 - INFO - __main__ -   Text: ['She has no English.\"']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.440\n",
      "  Test Loss: 2.703\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:26:34 - INFO - __main__ -   Epoch: 19 | Batch: 500/10001 (5%) | G Loss: 1.417214 | C Loss: -0.855539\n",
      "06/30/2022 12:26:34 - INFO - __main__ -   Text: ['Master Nowbar is a great predictor because your life will be better if you are a master (Phaser.).']\n",
      "06/30/2022 12:26:35 - INFO - __main__ -   Epoch: 19 | Batch: 1000/10001 (10%) | G Loss: 1.539146 | C Loss: -0.872483\n",
      "06/30/2022 12:26:35 - INFO - __main__ -   Text: ['if you have a drawing account, then close all eyes.']\n",
      "06/30/2022 12:26:37 - INFO - __main__ -   Epoch: 19 | Batch: 1500/10001 (15%) | G Loss: 1.399231 | C Loss: -0.802406\n",
      "06/30/2022 12:26:37 - INFO - __main__ -   Text: ['...']\n",
      "06/30/2022 12:26:38 - INFO - __main__ -   Epoch: 19 | Batch: 2000/10001 (20%) | G Loss: 1.599258 | C Loss: -0.873849\n",
      "06/30/2022 12:26:38 - INFO - __main__ -   Text: ['For example a spicy buffet for women.\"']\n",
      "06/30/2022 12:26:39 - INFO - __main__ -   Epoch: 19 | Batch: 2500/10001 (25%) | G Loss: 1.449760 | C Loss: -0.830384\n",
      "06/30/2022 12:26:40 - INFO - __main__ -   Text: ['Some people are more adept with their computer.\"']\n",
      "06/30/2022 12:26:41 - INFO - __main__ -   Epoch: 19 | Batch: 3000/10001 (30%) | G Loss: 1.532513 | C Loss: -0.615364\n",
      "06/30/2022 12:26:41 - INFO - __main__ -   Text: ['They might also note the name of the only type of Parkway bike.']\n",
      "06/30/2022 12:26:42 - INFO - __main__ -   Epoch: 19 | Batch: 3500/10001 (35%) | G Loss: 1.372687 | C Loss: -0.778010\n",
      "06/30/2022 12:26:42 - INFO - __main__ -   Text: ['The title can also refer to mypersonal health.']\n",
      "06/30/2022 12:26:44 - INFO - __main__ -   Epoch: 19 | Batch: 4000/10001 (40%) | G Loss: 1.473979 | C Loss: -0.689658\n",
      "06/30/2022 12:26:44 - INFO - __main__ -   Text: ['That doesn\\'t mean we\\'re spiritually disconnected.\"']\n",
      "06/30/2022 12:26:45 - INFO - __main__ -   Epoch: 19 | Batch: 4500/10001 (45%) | G Loss: 1.600638 | C Loss: -0.926023\n",
      "06/30/2022 12:26:45 - INFO - __main__ -   Text: ['A lot of people call that movie plus\"  <PAD>\".']\n",
      "06/30/2022 12:26:47 - INFO - __main__ -   Epoch: 19 | Batch: 5000/10001 (50%) | G Loss: 1.374952 | C Loss: -0.830047\n",
      "06/30/2022 12:26:47 - INFO - __main__ -   Text: ['A grim day for the speaker may also root for bread pudding.']\n",
      "06/30/2022 12:26:48 - INFO - __main__ -   Epoch: 19 | Batch: 5500/10001 (55%) | G Loss: 1.445001 | C Loss: -0.673340\n",
      "06/30/2022 12:26:48 - INFO - __main__ -   Text: ['This also bolsters the theory of omnidirectional boobs.\"']\n",
      "06/30/2022 12:26:50 - INFO - __main__ -   Epoch: 19 | Batch: 6000/10001 (60%) | G Loss: 1.466245 | C Loss: -0.798277\n",
      "06/30/2022 12:26:50 - INFO - __main__ -   Text: ['Unlike \"the foolproof girls\", \"hackers go.\"']\n",
      "06/30/2022 12:26:51 - INFO - __main__ -   Epoch: 19 | Batch: 6500/10001 (65%) | G Loss: 1.373724 | C Loss: -0.759683\n",
      "06/30/2022 12:26:51 - INFO - __main__ -   Text: ['They have incredibly valid thoughts.']\n",
      "06/30/2022 12:26:53 - INFO - __main__ -   Epoch: 19 | Batch: 7000/10001 (70%) | G Loss: 1.258419 | C Loss: -0.641454\n",
      "06/30/2022 12:26:53 - INFO - __main__ -   Text: ['Next page: What couple eats dogfood?']\n",
      "06/30/2022 12:26:54 - INFO - __main__ -   Epoch: 19 | Batch: 7500/10001 (75%) | G Loss: 1.563254 | C Loss: -0.850485\n",
      "06/30/2022 12:26:54 - INFO - __main__ -   Text: ['Relatively, I may be living in Stockholm.\"']\n",
      "06/30/2022 12:26:55 - INFO - __main__ -   Epoch: 19 | Batch: 8000/10001 (80%) | G Loss: 1.348334 | C Loss: -0.689368\n",
      "06/30/2022 12:26:56 - INFO - __main__ -   Text: ['Its requires memory (what would a \"good roommate be\";\").']\n",
      "06/30/2022 12:26:57 - INFO - __main__ -   Epoch: 19 | Batch: 8500/10001 (85%) | G Loss: 1.709281 | C Loss: -0.966518\n",
      "06/30/2022 12:26:57 - INFO - __main__ -   Text: ['The opposite of Stooges is used by contentment artists.']\n",
      "06/30/2022 12:26:58 - INFO - __main__ -   Epoch: 19 | Batch: 9000/10001 (90%) | G Loss: 1.414782 | C Loss: -0.773808\n",
      "06/30/2022 12:26:59 - INFO - __main__ -   Text: [\"Although the audience says the Courier's name instead of a name, that's true for Pound.\"]\n",
      "06/30/2022 12:27:00 - INFO - __main__ -   Epoch: 19 | Batch: 9500/10001 (95%) | G Loss: 1.302889 | C Loss: -0.836208\n",
      "06/30/2022 12:27:00 - INFO - __main__ -   Text: ['\"Arp kacter la libert\" write.']\n",
      "06/30/2022 12:27:01 - INFO - __main__ -   * (Train) Epoch: 19 | G Loss: 1.4501 | C Loss: -0.8153 | Updates G: 181 | Updates C: 819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:27:08 - INFO - __main__ -   Bleu-2:0.236 | B-Bleu-2:0.266\n",
      "06/30/2022 12:27:08 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_20.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5017104599807264\n",
      "Train file used is number 20\n",
      "../../yahoo/subdivided_large/train_20.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 20 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:09.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:16.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:25.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:34.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:42.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:50.\n",
      "  Batch    70  of    172.    Elapsed: 0:00:58.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:07.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:16.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:24.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:32.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:40.\n",
      "  Batch   130  of    172.    Elapsed: 0:01:48.\n",
      "  Batch   140  of    172.    Elapsed: 0:01:56.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:05.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:13.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:21.\n",
      "\n",
      "  Average training loss discriminator: 0.025\n",
      "  Training epcoh took: 0:02:23\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:29:32 - INFO - __main__ -   Epoch: 20 | Batch: 0/10001 (0%) | G Loss: 2.006514 | C Loss: -1.160885\n",
      "06/30/2022 12:29:32 - INFO - __main__ -   Text: ['Max has no sense of drive 2.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.438\n",
      "  Test Loss: 2.775\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:29:33 - INFO - __main__ -   Epoch: 20 | Batch: 500/10001 (5%) | G Loss: 1.170761 | C Loss: -0.569928\n",
      "06/30/2022 12:29:33 - INFO - __main__ -   Text: ['It\\'s a combination of metaphor and politics.\"']\n",
      "06/30/2022 12:29:35 - INFO - __main__ -   Epoch: 20 | Batch: 1000/10001 (10%) | G Loss: 1.569864 | C Loss: -0.999765\n",
      "06/30/2022 12:29:35 - INFO - __main__ -   Text: ['On a more logical level, there is \"Buddha Yoga\".']\n",
      "06/30/2022 12:29:36 - INFO - __main__ -   Epoch: 20 | Batch: 1500/10001 (15%) | G Loss: 1.414941 | C Loss: -0.865394\n",
      "06/30/2022 12:29:36 - INFO - __main__ -   Text: ['The more fundamental questions are: What are you doing?']\n",
      "06/30/2022 12:29:37 - INFO - __main__ -   Epoch: 20 | Batch: 2000/10001 (20%) | G Loss: 1.117085 | C Loss: -0.595644\n",
      "06/30/2022 12:29:38 - INFO - __main__ -   Text: [\"If it's doable, then you can do breakfast.\"]\n",
      "06/30/2022 12:29:39 - INFO - __main__ -   Epoch: 20 | Batch: 2500/10001 (25%) | G Loss: 1.566638 | C Loss: -0.907211\n",
      "06/30/2022 12:29:39 - INFO - __main__ -   Text: ['In fact, if the Wild is very strong, it is called \"the disease\".']\n",
      "06/30/2022 12:29:40 - INFO - __main__ -   Epoch: 20 | Batch: 3000/10001 (30%) | G Loss: 1.428822 | C Loss: -0.822222\n",
      "06/30/2022 12:29:41 - INFO - __main__ -   Text: ['Unix Owners (only) love tasks even more.']\n",
      "06/30/2022 12:29:42 - INFO - __main__ -   Epoch: 20 | Batch: 3500/10001 (35%) | G Loss: 1.358982 | C Loss: -0.716882\n",
      "06/30/2022 12:29:42 - INFO - __main__ -   Text: ['It\\'s prophetic to call it \"Good Luck\"!']\n",
      "06/30/2022 12:29:43 - INFO - __main__ -   Epoch: 20 | Batch: 4000/10001 (40%) | G Loss: 1.226550 | C Loss: -0.570790\n",
      "06/30/2022 12:29:43 - INFO - __main__ -   Text: ['It is tedious and dangerous to have a double agent in a brokerage account.']\n",
      "06/30/2022 12:29:45 - INFO - __main__ -   Epoch: 20 | Batch: 4500/10001 (45%) | G Loss: 1.306591 | C Loss: -0.686979\n",
      "06/30/2022 12:29:45 - INFO - __main__ -   Text: ['The exact reason why the game is called \"hybrid murder\".']\n",
      "06/30/2022 12:29:46 - INFO - __main__ -   Epoch: 20 | Batch: 5000/10001 (50%) | G Loss: 1.484240 | C Loss: -0.825888\n",
      "06/30/2022 12:29:46 - INFO - __main__ -   Text: ['The term \"Batman\" best in class.']\n",
      "06/30/2022 12:29:48 - INFO - __main__ -   Epoch: 20 | Batch: 5500/10001 (55%) | G Loss: 1.432432 | C Loss: -0.835294\n",
      "06/30/2022 12:29:48 - INFO - __main__ -   Text: ['In theory, fake states are common.']\n",
      "06/30/2022 12:29:49 - INFO - __main__ -   Epoch: 20 | Batch: 6000/10001 (60%) | G Loss: 1.269167 | C Loss: -0.685389\n",
      "06/30/2022 12:29:49 - INFO - __main__ -   Text: ['When asked to describe a nail, they are weak.']\n",
      "06/30/2022 12:29:51 - INFO - __main__ -   Epoch: 20 | Batch: 6500/10001 (65%) | G Loss: 1.319344 | C Loss: -0.653508\n",
      "06/30/2022 12:29:51 - INFO - __main__ -   Text: ['The enemy can be tricked simply or thought that way.']\n",
      "06/30/2022 12:29:52 - INFO - __main__ -   Epoch: 20 | Batch: 7000/10001 (70%) | G Loss: 1.401035 | C Loss: -0.721171\n",
      "06/30/2022 12:29:52 - INFO - __main__ -   Text: ['Wiskers is a quietname.']\n",
      "06/30/2022 12:29:54 - INFO - __main__ -   Epoch: 20 | Batch: 7500/10001 (75%) | G Loss: 1.368273 | C Loss: -0.775901\n",
      "06/30/2022 12:29:54 - INFO - __main__ -   Text: ['To learn how to get a finger transplant, read Walder.']\n",
      "06/30/2022 12:29:55 - INFO - __main__ -   Epoch: 20 | Batch: 8000/10001 (80%) | G Loss: 1.298430 | C Loss: -0.642335\n",
      "06/30/2022 12:29:55 - INFO - __main__ -   Text: ['When Katie gets pregnant, \"It should be my baby\".']\n",
      "06/30/2022 12:29:57 - INFO - __main__ -   Epoch: 20 | Batch: 8500/10001 (85%) | G Loss: 1.357928 | C Loss: -0.728172\n",
      "06/30/2022 12:29:57 - INFO - __main__ -   Text: ['He rarely writes true quotes.']\n",
      "06/30/2022 12:29:58 - INFO - __main__ -   Epoch: 20 | Batch: 9000/10001 (90%) | G Loss: 1.772143 | C Loss: -0.871504\n",
      "06/30/2022 12:29:58 - INFO - __main__ -   Text: ['Not far above, peodled and ready to be quoted.\"']\n",
      "06/30/2022 12:29:59 - INFO - __main__ -   Epoch: 20 | Batch: 9500/10001 (95%) | G Loss: 1.308828 | C Loss: -0.724042\n",
      "06/30/2022 12:30:00 - INFO - __main__ -   Text: [\"Buried in the Will is Will's rational focus.\"]\n",
      "06/30/2022 12:30:01 - INFO - __main__ -   * (Train) Epoch: 20 | G Loss: 1.3988 | C Loss: -0.7573 | Updates G: 173 | Updates C: 827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:30:09 - INFO - __main__ -   Bleu-2:0.225 | B-Bleu-2:0.280\n",
      "06/30/2022 12:30:09 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_1.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5044702666167674\n",
      "Train file used is number 1\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 21 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:30.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:40.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:50.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:00.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:10.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:20.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:30.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:40.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:50.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:00.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:11.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:20.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:30.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:40.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss discriminator: 0.030\n",
      "  Training epcoh took: 0:02:52\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:33:02 - INFO - __main__ -   Epoch: 21 | Batch: 0/10000 (0%) | G Loss: 1.388993 | C Loss: -0.726569\n",
      "06/30/2022 12:33:02 - INFO - __main__ -   Text: ['If you have interests in mathematics, you will end up accosting fat.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.453\n",
      "  Test Loss: 2.812\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:33:03 - INFO - __main__ -   Epoch: 21 | Batch: 500/10000 (5%) | G Loss: 1.468707 | C Loss: -0.685750\n",
      "06/30/2022 12:33:03 - INFO - __main__ -   Text: ['The name \"magnification threat\" is not appropriate.']\n",
      "06/30/2022 12:33:05 - INFO - __main__ -   Epoch: 21 | Batch: 1000/10000 (10%) | G Loss: 1.436041 | C Loss: -0.810039\n",
      "06/30/2022 12:33:05 - INFO - __main__ -   Text: ['Aonal comedy does poorly in a beauty contest.']\n",
      "06/30/2022 12:33:06 - INFO - __main__ -   Epoch: 21 | Batch: 1500/10000 (15%) | G Loss: 1.407303 | C Loss: -0.681857\n",
      "06/30/2022 12:33:06 - INFO - __main__ -   Text: ['Picasso says \"double shanks\".']\n",
      "06/30/2022 12:33:08 - INFO - __main__ -   Epoch: 21 | Batch: 2000/10000 (20%) | G Loss: 1.415480 | C Loss: -0.760157\n",
      "06/30/2022 12:33:08 - INFO - __main__ -   Text: ['Why not animate wildlife albeit with all mammals in sight?']\n",
      "06/30/2022 12:33:09 - INFO - __main__ -   Epoch: 21 | Batch: 2500/10000 (25%) | G Loss: 1.349266 | C Loss: -0.751335\n",
      "06/30/2022 12:33:09 - INFO - __main__ -   Text: ['Another important thing is that Delphi is a science fiction author.']\n",
      "06/30/2022 12:33:10 - INFO - __main__ -   Epoch: 21 | Batch: 3000/10000 (30%) | G Loss: 1.186236 | C Loss: -0.639898\n",
      "06/30/2022 12:33:11 - INFO - __main__ -   Text: ['\"For me, the easier it is to write the novel. <PAD>']\n",
      "06/30/2022 12:33:12 - INFO - __main__ -   Epoch: 21 | Batch: 3500/10000 (35%) | G Loss: 1.607280 | C Loss: -0.917307\n",
      "06/30/2022 12:33:12 - INFO - __main__ -   Text: ['The object is a play study.']\n",
      "06/30/2022 12:33:13 - INFO - __main__ -   Epoch: 21 | Batch: 4000/10000 (40%) | G Loss: 1.134055 | C Loss: -0.626357\n",
      "06/30/2022 12:33:13 - INFO - __main__ -   Text: ['Polly will do her homework for you in a book.']\n",
      "06/30/2022 12:33:15 - INFO - __main__ -   Epoch: 21 | Batch: 4500/10000 (45%) | G Loss: 1.404751 | C Loss: -0.740261\n",
      "06/30/2022 12:33:15 - INFO - __main__ -   Text: ['The ideal: ultimately non-dangerous.']\n",
      "06/30/2022 12:33:16 - INFO - __main__ -   Epoch: 21 | Batch: 5000/10000 (50%) | G Loss: 1.374587 | C Loss: -0.615712\n",
      "06/30/2022 12:33:16 - INFO - __main__ -   Text: ['They may refer to: Alan Simons by preference.']\n",
      "06/30/2022 12:33:18 - INFO - __main__ -   Epoch: 21 | Batch: 5500/10000 (55%) | G Loss: 1.509633 | C Loss: -0.693504\n",
      "06/30/2022 12:33:18 - INFO - __main__ -   Text: ['It apparently asks for money.']\n",
      "06/30/2022 12:33:19 - INFO - __main__ -   Epoch: 21 | Batch: 6000/10000 (60%) | G Loss: 1.226357 | C Loss: -0.564566\n",
      "06/30/2022 12:33:19 - INFO - __main__ -   Text: ['The Cyclops is usually called a cyclops because it commands no attention.']\n",
      "06/30/2022 12:33:20 - INFO - __main__ -   Epoch: 21 | Batch: 6500/10000 (65%) | G Loss: 1.252524 | C Loss: -0.601178\n",
      "06/30/2022 12:33:21 - INFO - __main__ -   Text: ['The book makes a direct comparison to egg farming.']\n",
      "06/30/2022 12:33:22 - INFO - __main__ -   Epoch: 21 | Batch: 7000/10000 (70%) | G Loss: 1.539899 | C Loss: -0.782816\n",
      "06/30/2022 12:33:22 - INFO - __main__ -   Text: ['The only thing that irritates me is genetically read results.\"']\n",
      "06/30/2022 12:33:23 - INFO - __main__ -   Epoch: 21 | Batch: 7500/10000 (75%) | G Loss: 1.190803 | C Loss: -0.620029\n",
      "06/30/2022 12:33:23 - INFO - __main__ -   Text: ['These are \"ulcrum testicles\".']\n",
      "06/30/2022 12:33:25 - INFO - __main__ -   Epoch: 21 | Batch: 8000/10000 (80%) | G Loss: 1.255326 | C Loss: -0.631894\n",
      "06/30/2022 12:33:25 - INFO - __main__ -   Text: ['The one I am most read on is Pet Lord.']\n",
      "06/30/2022 12:33:26 - INFO - __main__ -   Epoch: 21 | Batch: 8500/10000 (85%) | G Loss: 1.156830 | C Loss: -0.512961\n",
      "06/30/2022 12:33:26 - INFO - __main__ -   Text: [\"The WWD can't write anything!\"]\n",
      "06/30/2022 12:33:28 - INFO - __main__ -   Epoch: 21 | Batch: 9000/10000 (90%) | G Loss: 1.372244 | C Loss: -0.739288\n",
      "06/30/2022 12:33:28 - INFO - __main__ -   Text: ['Also  ?']\n",
      "06/30/2022 12:33:29 - INFO - __main__ -   Epoch: 21 | Batch: 9500/10000 (95%) | G Loss: 1.281931 | C Loss: -0.536840\n",
      "06/30/2022 12:33:29 - INFO - __main__ -   Text: ['He says that it is Irish because the ghost walks on the moon.']\n",
      "06/30/2022 12:33:31 - INFO - __main__ -   * (Train) Epoch: 21 | G Loss: 1.3357 | C Loss: -0.6776 | Updates G: 185 | Updates C: 815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:33:40 - INFO - __main__ -   Bleu-2:0.231 | B-Bleu-2:0.294\n",
      "06/30/2022 12:33:40 - INFO - __main__ -   * Saving. Best Score:0.525 | Bleu-2:0.231 | B-Bleu-2:0.294\n",
      "06/30/2022 12:33:40 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_1.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5250203491189821\n",
      "Train file used is number 1\n",
      "../../yahoo/subdivided_large/train_1.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 22 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:30.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:40.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:51.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:01.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:11.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:21.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:30.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:40.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:50.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:00.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:09.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:19.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:29.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:39.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss discriminator: 0.024\n",
      "  Training epcoh took: 0:02:51\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:36:31 - INFO - __main__ -   Epoch: 22 | Batch: 0/10000 (0%) | G Loss: 1.299446 | C Loss: -0.689254\n",
      "06/30/2022 12:36:31 - INFO - __main__ -   Text: ['Meeks doesn\\'t describe it as \"therapy\".']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.425\n",
      "  Test Loss: 2.781\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:36:32 - INFO - __main__ -   Epoch: 22 | Batch: 500/10000 (5%) | G Loss: 1.218720 | C Loss: -0.591605\n",
      "06/30/2022 12:36:32 - INFO - __main__ -   Text: ['We find that basically all geeks are stupid.']\n",
      "06/30/2022 12:36:34 - INFO - __main__ -   Epoch: 22 | Batch: 1000/10000 (10%) | G Loss: 1.440436 | C Loss: -0.694416\n",
      "06/30/2022 12:36:34 - INFO - __main__ -   Text: ['But if coincidentally his clan is apex model..']\n",
      "06/30/2022 12:36:35 - INFO - __main__ -   Epoch: 22 | Batch: 1500/10000 (15%) | G Loss: 1.263394 | C Loss: -0.647380\n",
      "06/30/2022 12:36:35 - INFO - __main__ -   Text: [\"All this suggests that it's not against the law.\"]\n",
      "06/30/2022 12:36:37 - INFO - __main__ -   Epoch: 22 | Batch: 2000/10000 (20%) | G Loss: 1.421975 | C Loss: -0.682940\n",
      "06/30/2022 12:36:37 - INFO - __main__ -   Text: ['Graff has described Denizens of Gilead as \"short and light.\"']\n",
      "06/30/2022 12:36:38 - INFO - __main__ -   Epoch: 22 | Batch: 2500/10000 (25%) | G Loss: 1.221419 | C Loss: -0.592316\n",
      "06/30/2022 12:36:38 - INFO - __main__ -   Text: [\"So we call it 'rat negation'.\"]\n",
      "06/30/2022 12:36:40 - INFO - __main__ -   Epoch: 22 | Batch: 3000/10000 (30%) | G Loss: 1.283686 | C Loss: -0.707372\n",
      "06/30/2022 12:36:40 - INFO - __main__ -   Text: [\"This is an example of whether philanthropy actually isn't good.\"]\n",
      "06/30/2022 12:36:41 - INFO - __main__ -   Epoch: 22 | Batch: 3500/10000 (35%) | G Loss: 1.206953 | C Loss: -0.596558\n",
      "06/30/2022 12:36:41 - INFO - __main__ -   Text: [\"There's even some scientific pretos that 'better unpoisoned crum.'\"]\n",
      "06/30/2022 12:36:42 - INFO - __main__ -   Epoch: 22 | Batch: 4000/10000 (40%) | G Loss: 1.314800 | C Loss: -0.589375\n",
      "06/30/2022 12:36:42 - INFO - __main__ -   Text: ['this object is usually the \"Hello!\"']\n",
      "06/30/2022 12:36:44 - INFO - __main__ -   Epoch: 22 | Batch: 4500/10000 (45%) | G Loss: 1.150576 | C Loss: -0.574226\n",
      "06/30/2022 12:36:44 - INFO - __main__ -   Text: [\"The more an 'overt[ing] up' kid wants his perverted thoughts and ideas.\"]\n",
      "06/30/2022 12:36:45 - INFO - __main__ -   Epoch: 22 | Batch: 5000/10000 (50%) | G Loss: 1.138138 | C Loss: -0.532443\n",
      "06/30/2022 12:36:45 - INFO - __main__ -   Text: ['One of the most important things that I can teach you is the sense that the short change isperfect.']\n",
      "06/30/2022 12:36:47 - INFO - __main__ -   Epoch: 22 | Batch: 5500/10000 (55%) | G Loss: 1.258705 | C Loss: -0.637751\n",
      "06/30/2022 12:36:47 - INFO - __main__ -   Text: ['Pictures of the day include: One crazy adventure is funny!']\n",
      "06/30/2022 12:36:48 - INFO - __main__ -   Epoch: 22 | Batch: 6000/10000 (60%) | G Loss: 1.159977 | C Loss: -0.549990\n",
      "06/30/2022 12:36:48 - INFO - __main__ -   Text: ['The quarterback will ask \"Do my ideas sound good?']\n",
      "06/30/2022 12:36:50 - INFO - __main__ -   Epoch: 22 | Batch: 6500/10000 (65%) | G Loss: 1.323219 | C Loss: -0.666586\n",
      "06/30/2022 12:36:50 - INFO - __main__ -   Text: ['Have I?\"']\n",
      "06/30/2022 12:36:51 - INFO - __main__ -   Epoch: 22 | Batch: 7000/10000 (70%) | G Loss: 1.137840 | C Loss: -0.586589\n",
      "06/30/2022 12:36:51 - INFO - __main__ -   Text: ['The term hurricane ant is used to describe paying attention to a hurricane.']\n",
      "06/30/2022 12:36:53 - INFO - __main__ -   Epoch: 22 | Batch: 7500/10000 (75%) | G Loss: 1.273916 | C Loss: -0.627322\n",
      "06/30/2022 12:36:53 - INFO - __main__ -   Text: ['My dear coworker has barely studied English.']\n",
      "06/30/2022 12:36:54 - INFO - __main__ -   Epoch: 22 | Batch: 8000/10000 (80%) | G Loss: 1.129245 | C Loss: -0.537323\n",
      "06/30/2022 12:36:54 - INFO - __main__ -   Text: ['It squares with the proverbial [[Oh, I am sure !]]']\n",
      "06/30/2022 12:36:55 - INFO - __main__ -   Epoch: 22 | Batch: 8500/10000 (85%) | G Loss: 1.279789 | C Loss: -0.664654\n",
      "06/30/2022 12:36:56 - INFO - __main__ -   Text: ['In this situation I guess it is fake advice\".']\n",
      "06/30/2022 12:36:57 - INFO - __main__ -   Epoch: 22 | Batch: 9000/10000 (90%) | G Loss: 1.131545 | C Loss: -0.596215\n",
      "06/30/2022 12:36:57 - INFO - __main__ -   Text: ['When given a challenge, it can be consideraoned.\"']\n",
      "06/30/2022 12:36:58 - INFO - __main__ -   Epoch: 22 | Batch: 9500/10000 (95%) | G Loss: 1.302935 | C Loss: -0.669398\n",
      "06/30/2022 12:36:58 - INFO - __main__ -   Text: ['Much like the viewer, bounty hunters must play a role in creation.']\n",
      "06/30/2022 12:37:00 - INFO - __main__ -   * (Train) Epoch: 22 | G Loss: 1.2567 | C Loss: -0.6136 | Updates G: 165 | Updates C: 835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:37:08 - INFO - __main__ -   Bleu-2:0.213 | B-Bleu-2:0.269\n",
      "06/30/2022 12:37:08 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_2.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48129116983016274\n",
      "Train file used is number 2\n",
      "../../yahoo/subdivided_large/train_2.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 23 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:29.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:39.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:48.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:59.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:08.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:18.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:28.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:38.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:48.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:58.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:08.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:18.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:27.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:36.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:47.\n",
      "\n",
      "  Average training loss discriminator: 0.024\n",
      "  Training epcoh took: 0:02:49\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:39:57 - INFO - __main__ -   Epoch: 23 | Batch: 0/10001 (0%) | G Loss: 1.068438 | C Loss: -0.375573\n",
      "06/30/2022 12:39:58 - INFO - __main__ -   Text: ['Batman loves to watch anime.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.432\n",
      "  Test Loss: 2.779\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:39:58 - INFO - __main__ -   Epoch: 23 | Batch: 500/10001 (5%) | G Loss: 1.150495 | C Loss: -0.494131\n",
      "06/30/2022 12:39:58 - INFO - __main__ -   Text: ['\"I have no idea how to fight without you.\"']\n",
      "06/30/2022 12:39:59 - INFO - __main__ -   Epoch: 23 | Batch: 1000/10001 (10%) | G Loss: 1.244906 | C Loss: -0.619865\n",
      "06/30/2022 12:39:59 - INFO - __main__ -   Text: ['Of course, there are some paedophile books on the market.\"']\n",
      "06/30/2022 12:40:00 - INFO - __main__ -   Epoch: 23 | Batch: 1500/10001 (15%) | G Loss: 1.084252 | C Loss: -0.489215\n",
      "06/30/2022 12:40:00 - INFO - __main__ -   Text: ['The hamster clucks on its video screen: Whip it.\"']\n",
      "06/30/2022 12:40:01 - INFO - __main__ -   Epoch: 23 | Batch: 2000/10001 (20%) | G Loss: 1.448406 | C Loss: -0.659813\n",
      "06/30/2022 12:40:01 - INFO - __main__ -   Text: ['He says * nothing  nothing fun happens here !']\n",
      "06/30/2022 12:40:02 - INFO - __main__ -   Epoch: 23 | Batch: 2500/10001 (25%) | G Loss: 1.252202 | C Loss: -0.546469\n",
      "06/30/2022 12:40:02 - INFO - __main__ -   Text: ['This web show aims to campaign the pretext of services. <PAD>']\n",
      "06/30/2022 12:40:02 - INFO - __main__ -   Epoch: 23 | Batch: 3000/10001 (30%) | G Loss: 1.238932 | C Loss: -0.578918\n",
      "06/30/2022 12:40:02 - INFO - __main__ -   Text: ['The next is giddle and impatient.']\n",
      "06/30/2022 12:40:03 - INFO - __main__ -   Epoch: 23 | Batch: 3500/10001 (35%) | G Loss: 1.087689 | C Loss: -0.488578\n",
      "06/30/2022 12:40:03 - INFO - __main__ -   Text: ['When someone is pretending to be the tip of the iceberg.']\n",
      "06/30/2022 12:40:04 - INFO - __main__ -   Epoch: 23 | Batch: 4000/10001 (40%) | G Loss: 1.314412 | C Loss: -0.608368\n",
      "06/30/2022 12:40:04 - INFO - __main__ -   Text: ['This alone is why I think Low.\"']\n",
      "06/30/2022 12:40:05 - INFO - __main__ -   Epoch: 23 | Batch: 4500/10001 (45%) | G Loss: 1.136294 | C Loss: -0.530629\n",
      "06/30/2022 12:40:05 - INFO - __main__ -   Text: ['The sport accordingly protests us gratitude for most of our inventions.']\n",
      "06/30/2022 12:40:06 - INFO - __main__ -   Epoch: 23 | Batch: 5000/10001 (50%) | G Loss: 1.151563 | C Loss: -0.590595\n",
      "06/30/2022 12:40:06 - INFO - __main__ -   Text: ['Look at it like a martial art combination.\"']\n",
      "06/30/2022 12:40:06 - INFO - __main__ -   Epoch: 23 | Batch: 5500/10001 (55%) | G Loss: 1.331598 | C Loss: -0.727962\n",
      "06/30/2022 12:40:06 - INFO - __main__ -   Text: ['With A Pink Kaye, a sport.']\n",
      "06/30/2022 12:40:07 - INFO - __main__ -   Epoch: 23 | Batch: 6000/10001 (60%) | G Loss: 1.038666 | C Loss: -0.440672\n",
      "06/30/2022 12:40:07 - INFO - __main__ -   Text: [\"A song about science is like a fairy tale; draw.'\"]\n",
      "06/30/2022 12:40:08 - INFO - __main__ -   Epoch: 23 | Batch: 6500/10001 (65%) | G Loss: 1.154254 | C Loss: -0.540719\n",
      "06/30/2022 12:40:08 - INFO - __main__ -   Text: ['Kara must know how to count numbers.']\n",
      "06/30/2022 12:40:09 - INFO - __main__ -   Epoch: 23 | Batch: 7000/10001 (70%) | G Loss: 1.160053 | C Loss: -0.548537\n",
      "06/30/2022 12:40:09 - INFO - __main__ -   Text: ['There are some victims, wanting some kind of mouthpiece known as \"Slob.\"']\n",
      "06/30/2022 12:40:10 - INFO - __main__ -   Epoch: 23 | Batch: 7500/10001 (75%) | G Loss: 1.049206 | C Loss: -0.451854\n",
      "06/30/2022 12:40:10 - INFO - __main__ -   Text: ['Even if it\\'s just about them, jokes about god.\"']\n",
      "06/30/2022 12:40:10 - INFO - __main__ -   Epoch: 23 | Batch: 8000/10001 (80%) | G Loss: 1.274303 | C Loss: -0.575892\n",
      "06/30/2022 12:40:11 - INFO - __main__ -   Text: ['\"Wizard\" has human peer pressure or psychiatric pull.\"']\n",
      "06/30/2022 12:40:11 - INFO - __main__ -   Epoch: 23 | Batch: 8500/10001 (85%) | G Loss: 1.223226 | C Loss: -0.644503\n",
      "06/30/2022 12:40:11 - INFO - __main__ -   Text: ['It is termed all-in-one way,\" Says AVWalks.']\n",
      "06/30/2022 12:40:12 - INFO - __main__ -   Epoch: 23 | Batch: 9000/10001 (90%) | G Loss: 1.427915 | C Loss: -0.622561\n",
      "06/30/2022 12:40:12 - INFO - __main__ -   Text: ['This is probably what drives Corywebs into swing.']\n",
      "06/30/2022 12:40:13 - INFO - __main__ -   Epoch: 23 | Batch: 9500/10001 (95%) | G Loss: 1.155654 | C Loss: -0.598902\n",
      "06/30/2022 12:40:13 - INFO - __main__ -   Text: ['To enter the gym is to learn to search for food.']\n",
      "06/30/2022 12:40:14 - INFO - __main__ -   * (Train) Epoch: 23 | G Loss: 1.1965 | C Loss: -0.5505 | Updates G: 168 | Updates C: 832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:40:22 - INFO - __main__ -   Bleu-2:0.225 | B-Bleu-2:0.288\n",
      "06/30/2022 12:40:22 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_3.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5124616015378666\n",
      "Train file used is number 3\n",
      "../../yahoo/subdivided_large/train_3.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 24 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:28.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:38.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:48.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:57.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:07.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:17.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:26.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:36.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:45.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:55.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:04.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:14.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:23.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:33.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:41.\n",
      "\n",
      "  Average training loss discriminator: 0.029\n",
      "  Training epcoh took: 0:02:43\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:43:05 - INFO - __main__ -   Epoch: 24 | Batch: 0/10001 (0%) | G Loss: 1.097910 | C Loss: -0.489490\n",
      "06/30/2022 12:43:05 - INFO - __main__ -   Text: ['Gong has an arguing style that is similar to \"spending 10 minutes.\"']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.440\n",
      "  Test Loss: 2.789\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:43:06 - INFO - __main__ -   Epoch: 24 | Batch: 500/10001 (5%) | G Loss: 1.305550 | C Loss: -0.683471\n",
      "06/30/2022 12:43:07 - INFO - __main__ -   Text: [\"It's a less terrifying event than motorcyclists.\"]\n",
      "06/30/2022 12:43:08 - INFO - __main__ -   Epoch: 24 | Batch: 1000/10001 (10%) | G Loss: 1.114658 | C Loss: -0.562611\n",
      "06/30/2022 12:43:08 - INFO - __main__ -   Text: ['Randomly the notation of this magic act can also refer to these marriages.']\n",
      "06/30/2022 12:43:09 - INFO - __main__ -   Epoch: 24 | Batch: 1500/10001 (15%) | G Loss: 1.170444 | C Loss: -0.544907\n",
      "06/30/2022 12:43:10 - INFO - __main__ -   Text: ['It\\'s not perfect but it will do well if used correctly.\"']\n",
      "06/30/2022 12:43:11 - INFO - __main__ -   Epoch: 24 | Batch: 2000/10001 (20%) | G Loss: 1.142430 | C Loss: -0.581793\n",
      "06/30/2022 12:43:11 - INFO - __main__ -   Text: ['Other terms are \"wildling\".']\n",
      "06/30/2022 12:43:12 - INFO - __main__ -   Epoch: 24 | Batch: 2500/10001 (25%) | G Loss: 0.976057 | C Loss: -0.419474\n",
      "06/30/2022 12:43:12 - INFO - __main__ -   Text: ['The haziest thing you can do with Teenager is Ricky.']\n",
      "06/30/2022 12:43:14 - INFO - __main__ -   Epoch: 24 | Batch: 3000/10001 (30%) | G Loss: 1.244538 | C Loss: -0.596721\n",
      "06/30/2022 12:43:14 - INFO - __main__ -   Text: [\"Prugh doesn't shoot shit to death.\"]\n",
      "06/30/2022 12:43:15 - INFO - __main__ -   Epoch: 24 | Batch: 3500/10001 (35%) | G Loss: 1.121647 | C Loss: -0.509589\n",
      "06/30/2022 12:43:15 - INFO - __main__ -   Text: ['Hocus pocus!']\n",
      "06/30/2022 12:43:17 - INFO - __main__ -   Epoch: 24 | Batch: 4000/10001 (40%) | G Loss: 1.006555 | C Loss: -0.439179\n",
      "06/30/2022 12:43:17 - INFO - __main__ -   Text: ['This is not surprising since Shimadzu has the magic skills of Magic Eagles.']\n",
      "06/30/2022 12:43:18 - INFO - __main__ -   Epoch: 24 | Batch: 4500/10001 (45%) | G Loss: 1.235676 | C Loss: -0.549898\n",
      "06/30/2022 12:43:18 - INFO - __main__ -   Text: ['This isn\\'t science fiction too\"']\n",
      "06/30/2022 12:43:19 - INFO - __main__ -   Epoch: 24 | Batch: 5000/10001 (50%) | G Loss: 0.880937 | C Loss: -0.301328\n",
      "06/30/2022 12:43:19 - INFO - __main__ -   Text: ['It is the logical way to look at cures.']\n",
      "06/30/2022 12:43:21 - INFO - __main__ -   Epoch: 24 | Batch: 5500/10001 (55%) | G Loss: 1.172308 | C Loss: -0.445194\n",
      "06/30/2022 12:43:21 - INFO - __main__ -   Text: ['The images you ask for are images and lyrics.']\n",
      "06/30/2022 12:43:22 - INFO - __main__ -   Epoch: 24 | Batch: 6000/10001 (60%) | G Loss: 1.192591 | C Loss: -0.459414\n",
      "06/30/2022 12:43:22 - INFO - __main__ -   Text: [\"Eleanor seems like a good ol' guy for you.\"]\n",
      "06/30/2022 12:43:24 - INFO - __main__ -   Epoch: 24 | Batch: 6500/10001 (65%) | G Loss: 1.013583 | C Loss: -0.445515\n",
      "06/30/2022 12:43:24 - INFO - __main__ -   Text: ['Formerly more important is finding the name dog.']\n",
      "06/30/2022 12:43:25 - INFO - __main__ -   Epoch: 24 | Batch: 7000/10001 (70%) | G Loss: 1.141019 | C Loss: -0.574803\n",
      "06/30/2022 12:43:25 - INFO - __main__ -   Text: [\"Takes notice of what I've written before.\"]\n",
      "06/30/2022 12:43:26 - INFO - __main__ -   Epoch: 24 | Batch: 7500/10001 (75%) | G Loss: 1.072425 | C Loss: -0.477247\n",
      "06/30/2022 12:43:26 - INFO - __main__ -   Text: ['saves gay people money.']\n",
      "06/30/2022 12:43:28 - INFO - __main__ -   Epoch: 24 | Batch: 8000/10001 (80%) | G Loss: 0.905759 | C Loss: -0.283222\n",
      "06/30/2022 12:43:28 - INFO - __main__ -   Text: ['A guy named \"Soda Buildings\" shows me how to do it.']\n",
      "06/30/2022 12:43:29 - INFO - __main__ -   Epoch: 24 | Batch: 8500/10001 (85%) | G Loss: 1.264559 | C Loss: -0.559818\n",
      "06/30/2022 12:43:29 - INFO - __main__ -   Text: ['This is a really blurry concept.']\n",
      "06/30/2022 12:43:31 - INFO - __main__ -   Epoch: 24 | Batch: 9000/10001 (90%) | G Loss: 1.039845 | C Loss: -0.458687\n",
      "06/30/2022 12:43:31 - INFO - __main__ -   Text: ['Bachelor\\'s coding is totally out there for you.\"']\n",
      "06/30/2022 12:43:32 - INFO - __main__ -   Epoch: 24 | Batch: 9500/10001 (95%) | G Loss: 0.989194 | C Loss: -0.366869\n",
      "06/30/2022 12:43:32 - INFO - __main__ -   Text: ['it is a bee tale.']\n",
      "06/30/2022 12:43:33 - INFO - __main__ -   * (Train) Epoch: 24 | G Loss: 1.1317 | C Loss: -0.5084 | Updates G: 158 | Updates C: 842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:43:41 - INFO - __main__ -   Bleu-2:0.215 | B-Bleu-2:0.273\n",
      "06/30/2022 12:43:41 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_4.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4871707448686986\n",
      "Train file used is number 4\n",
      "../../yahoo/subdivided_large/train_4.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 25 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:30.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:40.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:50.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:00.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:09.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:19.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:29.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:39.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:48.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:58.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:08.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:17.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:27.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:37.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:46.\n",
      "\n",
      "  Average training loss discriminator: 0.027\n",
      "  Training epcoh took: 0:02:49\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:46:30 - INFO - __main__ -   Epoch: 25 | Batch: 0/10001 (0%) | G Loss: 1.157474 | C Loss: -0.464005\n",
      "06/30/2022 12:46:30 - INFO - __main__ -   Text: ['Also read Qubec Biscuits licensed to readers only.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.440\n",
      "  Test Loss: 2.813\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:46:31 - INFO - __main__ -   Epoch: 25 | Batch: 500/10001 (5%) | G Loss: 1.135066 | C Loss: -0.435285\n",
      "06/30/2022 12:46:31 - INFO - __main__ -   Text: ['\"Recreative trolls\".']\n",
      "06/30/2022 12:46:33 - INFO - __main__ -   Epoch: 25 | Batch: 1000/10001 (10%) | G Loss: 1.119726 | C Loss: -0.482029\n",
      "06/30/2022 12:46:33 - INFO - __main__ -   Text: ['\"Sweet ranch is just a gay man is always waiting for the happy one.\"']\n",
      "06/30/2022 12:46:34 - INFO - __main__ -   Epoch: 25 | Batch: 1500/10001 (15%) | G Loss: 1.174988 | C Loss: -0.610447\n",
      "06/30/2022 12:46:34 - INFO - __main__ -   Text: ['They are pirates. <PAD> pirates are pirates.\"']\n",
      "06/30/2022 12:46:36 - INFO - __main__ -   Epoch: 25 | Batch: 2000/10001 (20%) | G Loss: 1.086719 | C Loss: -0.476675\n",
      "06/30/2022 12:46:36 - INFO - __main__ -   Text: ['Hate Can To Hate is an antidote to the unfairness by which he is.']\n",
      "06/30/2022 12:46:37 - INFO - __main__ -   Epoch: 25 | Batch: 2500/10001 (25%) | G Loss: 1.028456 | C Loss: -0.460514\n",
      "06/30/2022 12:46:37 - INFO - __main__ -   Text: [\"Succesfully plant pages on people's backs.\"]\n",
      "06/30/2022 12:46:39 - INFO - __main__ -   Epoch: 25 | Batch: 3000/10001 (30%) | G Loss: 1.063999 | C Loss: -0.474780\n",
      "06/30/2022 12:46:39 - INFO - __main__ -   Text: ['Let us say Italy is Cunnapp, who in turn gets Cunnapp.']\n",
      "06/30/2022 12:46:40 - INFO - __main__ -   Epoch: 25 | Batch: 3500/10001 (35%) | G Loss: 1.001020 | C Loss: -0.407581\n",
      "06/30/2022 12:46:40 - INFO - __main__ -   Text: ['The class scores PSR-level world-view for being too cool.']\n",
      "06/30/2022 12:46:42 - INFO - __main__ -   Epoch: 25 | Batch: 4000/10001 (40%) | G Loss: 1.245063 | C Loss: -0.573303\n",
      "06/30/2022 12:46:42 - INFO - __main__ -   Text: ['Sometimes I\\'m not so impressive that all I\\'m doing is searching.\"']\n",
      "06/30/2022 12:46:43 - INFO - __main__ -   Epoch: 25 | Batch: 4500/10001 (45%) | G Loss: 1.110816 | C Loss: -0.500612\n",
      "06/30/2022 12:46:43 - INFO - __main__ -   Text: ['\"Psychologically, mentally\" is a tough one.']\n",
      "06/30/2022 12:46:45 - INFO - __main__ -   Epoch: 25 | Batch: 5000/10001 (50%) | G Loss: 0.945769 | C Loss: -0.387830\n",
      "06/30/2022 12:46:45 - INFO - __main__ -   Text: ['This is why I see the Vinodab bluff.']\n",
      "06/30/2022 12:46:46 - INFO - __main__ -   Epoch: 25 | Batch: 5500/10001 (55%) | G Loss: 1.096605 | C Loss: -0.503150\n",
      "06/30/2022 12:46:46 - INFO - __main__ -   Text: ['When someone is talking to you, you mean more than US.']\n",
      "06/30/2022 12:46:47 - INFO - __main__ -   Epoch: 25 | Batch: 6000/10001 (60%) | G Loss: 1.286601 | C Loss: -0.591489\n",
      "06/30/2022 12:46:47 - INFO - __main__ -   Text: ['']\n",
      "06/30/2022 12:46:49 - INFO - __main__ -   Epoch: 25 | Batch: 6500/10001 (65%) | G Loss: 1.021769 | C Loss: -0.464504\n",
      "06/30/2022 12:46:49 - INFO - __main__ -   Text: ['A rather improbable grade who does not have to \"escape.\"']\n",
      "06/30/2022 12:46:50 - INFO - __main__ -   Epoch: 25 | Batch: 7000/10001 (70%) | G Loss: 0.918262 | C Loss: -0.375388\n",
      "06/30/2022 12:46:50 - INFO - __main__ -   Text: ['One only has to look at the Blitz of words in North American.']\n",
      "06/30/2022 12:46:52 - INFO - __main__ -   Epoch: 25 | Batch: 7500/10001 (75%) | G Loss: 1.077703 | C Loss: -0.547973\n",
      "06/30/2022 12:46:52 - INFO - __main__ -   Text: ['Enemy prey is commonly to see how snakes can attack.']\n",
      "06/30/2022 12:46:53 - INFO - __main__ -   Epoch: 25 | Batch: 8000/10001 (80%) | G Loss: 1.113583 | C Loss: -0.556872\n",
      "06/30/2022 12:46:53 - INFO - __main__ -   Text: ['The idea of a fish fertilizing the body in this way is tough to understand.']\n",
      "06/30/2022 12:46:55 - INFO - __main__ -   Epoch: 25 | Batch: 8500/10001 (85%) | G Loss: 1.146325 | C Loss: -0.532916\n",
      "06/30/2022 12:46:55 - INFO - __main__ -   Text: ['His own tricks are sometimes... Nonsense!']\n",
      "06/30/2022 12:46:56 - INFO - __main__ -   Epoch: 25 | Batch: 9000/10001 (90%) | G Loss: 1.022414 | C Loss: -0.380797\n",
      "06/30/2022 12:46:56 - INFO - __main__ -   Text: [\"SNOTRO's verses are like the words of God.\"]\n",
      "06/30/2022 12:46:58 - INFO - __main__ -   Epoch: 25 | Batch: 9500/10001 (95%) | G Loss: 0.936375 | C Loss: -0.336541\n",
      "06/30/2022 12:46:58 - INFO - __main__ -   Text: [\"When I say, 'Lawyer' I always say 'Lawyer'.\"]\n",
      "06/30/2022 12:46:59 - INFO - __main__ -   * (Train) Epoch: 25 | G Loss: 1.0780 | C Loss: -0.4489 | Updates G: 148 | Updates C: 852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:47:07 - INFO - __main__ -   Bleu-2:0.231 | B-Bleu-2:0.263\n",
      "06/30/2022 12:47:07 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49371389953475964\n",
      "Train file used is number 5\n",
      "../../yahoo/subdivided_large/train_5.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 26 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:19.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:29.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:39.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:48.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:57.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:08.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:18.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:28.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:37.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:46.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:56.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:06.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:16.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:25.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:34.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:44.\n",
      "\n",
      "  Average training loss discriminator: 0.025\n",
      "  Training epcoh took: 0:02:46\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:49:54 - INFO - __main__ -   Epoch: 26 | Batch: 0/10001 (0%) | G Loss: 1.106889 | C Loss: -0.469989\n",
      "06/30/2022 12:49:54 - INFO - __main__ -   Text: ['\"Rhyee\\'s stuff is old\".']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.432\n",
      "  Test Loss: 2.817\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:49:56 - INFO - __main__ -   Epoch: 26 | Batch: 500/10001 (5%) | G Loss: 1.122207 | C Loss: -0.461342\n",
      "06/30/2022 12:49:56 - INFO - __main__ -   Text: ['If one is retarded, give this one a try.']\n",
      "06/30/2022 12:49:57 - INFO - __main__ -   Epoch: 26 | Batch: 1000/10001 (10%) | G Loss: 0.922398 | C Loss: -0.383640\n",
      "06/30/2022 12:49:57 - INFO - __main__ -   Text: ['The \"Podestian\" can be caught either by bleeding.']\n",
      "06/30/2022 12:49:59 - INFO - __main__ -   Epoch: 26 | Batch: 1500/10001 (15%) | G Loss: 0.986217 | C Loss: -0.367743\n",
      "06/30/2022 12:49:59 - INFO - __main__ -   Text: ['Officers will usually call such foundry high treason and therefore a high treason legal term.']\n",
      "06/30/2022 12:50:00 - INFO - __main__ -   Epoch: 26 | Batch: 2000/10001 (20%) | G Loss: 1.109742 | C Loss: -0.497682\n",
      "06/30/2022 12:50:00 - INFO - __main__ -   Text: ['\"Byakcrease\" also ignores the condescending Chinese term.']\n",
      "06/30/2022 12:50:02 - INFO - __main__ -   Epoch: 26 | Batch: 2500/10001 (25%) | G Loss: 1.056478 | C Loss: -0.387000\n",
      "06/30/2022 12:50:02 - INFO - __main__ -   Text: ['There are 1s games in which women gain really high scores.']\n",
      "06/30/2022 12:50:03 - INFO - __main__ -   Epoch: 26 | Batch: 3000/10001 (30%) | G Loss: 0.969387 | C Loss: -0.300636\n",
      "06/30/2022 12:50:03 - INFO - __main__ -   Text: ['This Arkham-level solemnity is positively insane!\"']\n",
      "06/30/2022 12:50:04 - INFO - __main__ -   Epoch: 26 | Batch: 3500/10001 (35%) | G Loss: 1.052060 | C Loss: -0.436153\n",
      "06/30/2022 12:50:05 - INFO - __main__ -   Text: ['Only something can make me sick.\"']\n",
      "06/30/2022 12:50:06 - INFO - __main__ -   Epoch: 26 | Batch: 4000/10001 (40%) | G Loss: 1.006230 | C Loss: -0.411058\n",
      "06/30/2022 12:50:06 - INFO - __main__ -   Text: ['\", franchises \" Throughout because they are culture Cruisinburg.\"']\n",
      "06/30/2022 12:50:07 - INFO - __main__ -   Epoch: 26 | Batch: 4500/10001 (45%) | G Loss: 1.181632 | C Loss: -0.516317\n",
      "06/30/2022 12:50:07 - INFO - __main__ -   Text: ['It is a hypnotist before the public.']\n",
      "06/30/2022 12:50:09 - INFO - __main__ -   Epoch: 26 | Batch: 5000/10001 (50%) | G Loss: 0.965649 | C Loss: -0.253067\n",
      "06/30/2022 12:50:09 - INFO - __main__ -   Text: ['Most likely it is Poison Ivy who are completely ignorant of the term \"Daylight\".']\n",
      "06/30/2022 12:50:10 - INFO - __main__ -   Epoch: 26 | Batch: 5500/10001 (55%) | G Loss: 1.109592 | C Loss: -0.466504\n",
      "06/30/2022 12:50:10 - INFO - __main__ -   Text: ['I preach anti-\"Sexy\" endorsement preferences.\"']\n",
      "06/30/2022 12:50:12 - INFO - __main__ -   Epoch: 26 | Batch: 6000/10001 (60%) | G Loss: 1.022206 | C Loss: -0.390435\n",
      "06/30/2022 12:50:12 - INFO - __main__ -   Text: [\"It's really amazing that Ulysses is a phrase that describes him.\"]\n",
      "06/30/2022 12:50:13 - INFO - __main__ -   Epoch: 26 | Batch: 6500/10001 (65%) | G Loss: 0.991569 | C Loss: -0.394638\n",
      "06/30/2022 12:50:13 - INFO - __main__ -   Text: ['The first reason I am interpreting \"Van Em!\"']\n",
      "06/30/2022 12:50:15 - INFO - __main__ -   Epoch: 26 | Batch: 7000/10001 (70%) | G Loss: 0.974347 | C Loss: -0.280104\n",
      "06/30/2022 12:50:15 - INFO - __main__ -   Text: ['Storm is a flipside to fearlessness.']\n",
      "06/30/2022 12:50:16 - INFO - __main__ -   Epoch: 26 | Batch: 7500/10001 (75%) | G Loss: 1.268134 | C Loss: -0.533992\n",
      "06/30/2022 12:50:16 - INFO - __main__ -   Text: ['Sal has the problemyou have to be that stupid.']\n",
      "06/30/2022 12:50:18 - INFO - __main__ -   Epoch: 26 | Batch: 8000/10001 (80%) | G Loss: 0.934478 | C Loss: -0.361533\n",
      "06/30/2022 12:50:18 - INFO - __main__ -   Text: ['No wonder that one of the most famous philosophers of the 21st century.']\n",
      "06/30/2022 12:50:19 - INFO - __main__ -   Epoch: 26 | Batch: 8500/10001 (85%) | G Loss: 1.004984 | C Loss: -0.342247\n",
      "06/30/2022 12:50:19 - INFO - __main__ -   Text: ['The goal of the game is to see what I actually look like.\"']\n",
      "06/30/2022 12:50:21 - INFO - __main__ -   Epoch: 26 | Batch: 9000/10001 (90%) | G Loss: 0.995300 | C Loss: -0.386214\n",
      "06/30/2022 12:50:21 - INFO - __main__ -   Text: ['Little encircles means rigorous exercise.\"']\n",
      "06/30/2022 12:50:22 - INFO - __main__ -   Epoch: 26 | Batch: 9500/10001 (95%) | G Loss: 0.986089 | C Loss: -0.383016\n",
      "06/30/2022 12:50:22 - INFO - __main__ -   Text: [\"Unfortunately he's not considered 'Athena.'\"]\n",
      "06/30/2022 12:50:23 - INFO - __main__ -   * (Train) Epoch: 26 | G Loss: 1.0477 | C Loss: -0.4036 | Updates G: 153 | Updates C: 847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:50:32 - INFO - __main__ -   Bleu-2:0.226 | B-Bleu-2:0.255\n",
      "06/30/2022 12:50:32 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_6.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4809244622974872\n",
      "Train file used is number 6\n",
      "../../yahoo/subdivided_large/train_6.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 27 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:09.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:19.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:28.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:37.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:46.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:54.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:03.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:12.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:21.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:30.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:39.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:49.\n",
      "  Batch   130  of    172.    Elapsed: 0:01:57.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:06.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:15.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:24.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:33.\n",
      "\n",
      "  Average training loss discriminator: 0.024\n",
      "  Training epcoh took: 0:02:35\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:53:07 - INFO - __main__ -   Epoch: 27 | Batch: 0/10001 (0%) | G Loss: 1.157957 | C Loss: -0.518591\n",
      "06/30/2022 12:53:07 - INFO - __main__ -   Text: ['Believing in God is correct.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.398\n",
      "  Test Loss: 2.914\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:53:08 - INFO - __main__ -   Epoch: 27 | Batch: 500/10001 (5%) | G Loss: 0.929140 | C Loss: -0.315116\n",
      "06/30/2022 12:53:08 - INFO - __main__ -   Text: ['They grow speeches or meditation blocks.']\n",
      "06/30/2022 12:53:09 - INFO - __main__ -   Epoch: 27 | Batch: 1000/10001 (10%) | G Loss: 0.968188 | C Loss: -0.361545\n",
      "06/30/2022 12:53:09 - INFO - __main__ -   Text: ['Aeither to you!\".']\n",
      "06/30/2022 12:53:10 - INFO - __main__ -   Epoch: 27 | Batch: 1500/10001 (15%) | G Loss: 0.977541 | C Loss: -0.322642\n",
      "06/30/2022 12:53:10 - INFO - __main__ -   Text: ['There is no certainty the T .300 force will search for me.']\n",
      "06/30/2022 12:53:10 - INFO - __main__ -   Epoch: 27 | Batch: 2000/10001 (20%) | G Loss: 1.105427 | C Loss: -0.434336\n",
      "06/30/2022 12:53:10 - INFO - __main__ -   Text: ['More often than not, that one foot sofa has two feet of motion.']\n",
      "06/30/2022 12:53:11 - INFO - __main__ -   Epoch: 27 | Batch: 2500/10001 (25%) | G Loss: 1.186056 | C Loss: -0.423788\n",
      "06/30/2022 12:53:11 - INFO - __main__ -   Text: ['This spell can be used for code-related tasks.']\n",
      "06/30/2022 12:53:12 - INFO - __main__ -   Epoch: 27 | Batch: 3000/10001 (30%) | G Loss: 0.892487 | C Loss: -0.320799\n",
      "06/30/2022 12:53:12 - INFO - __main__ -   Text: ['If you agree with it, say hello!\"']\n",
      "06/30/2022 12:53:13 - INFO - __main__ -   Epoch: 27 | Batch: 3500/10001 (35%) | G Loss: 0.984736 | C Loss: -0.332788\n",
      "06/30/2022 12:53:13 - INFO - __main__ -   Text: [\"There's a tendency among many knitters to call eye-lettering floss.\"]\n",
      "06/30/2022 12:53:14 - INFO - __main__ -   Epoch: 27 | Batch: 4000/10001 (40%) | G Loss: 1.075043 | C Loss: -0.410508\n",
      "06/30/2022 12:53:14 - INFO - __main__ -   Text: ['The metaphor is not tolerant of drinking.']\n",
      "06/30/2022 12:53:15 - INFO - __main__ -   Epoch: 27 | Batch: 4500/10001 (45%) | G Loss: 0.952959 | C Loss: -0.345544\n",
      "06/30/2022 12:53:15 - INFO - __main__ -   Text: [\"Peter knows his name is 'extra'.\"]\n",
      "06/30/2022 12:53:15 - INFO - __main__ -   Epoch: 27 | Batch: 5000/10001 (50%) | G Loss: 0.948539 | C Loss: -0.309435\n",
      "06/30/2022 12:53:15 - INFO - __main__ -   Text: ['They keep hearing bad phonic experimentally\".']\n",
      "06/30/2022 12:53:16 - INFO - __main__ -   Epoch: 27 | Batch: 5500/10001 (55%) | G Loss: 0.971991 | C Loss: -0.414453\n",
      "06/30/2022 12:53:16 - INFO - __main__ -   Text: [\"She's basically saying, 'Volunteers (I'm not doing anything).\"]\n",
      "06/30/2022 12:53:17 - INFO - __main__ -   Epoch: 27 | Batch: 6000/10001 (60%) | G Loss: 1.034868 | C Loss: -0.314319\n",
      "06/30/2022 12:53:17 - INFO - __main__ -   Text: ['The customer expects it to be too cold.\"']\n",
      "06/30/2022 12:53:18 - INFO - __main__ -   Epoch: 27 | Batch: 6500/10001 (65%) | G Loss: 0.885699 | C Loss: -0.347681\n",
      "06/30/2022 12:53:18 - INFO - __main__ -   Text: ['Sleazy is often mistaken for a knowledge-based business persuasion method.']\n",
      "06/30/2022 12:53:19 - INFO - __main__ -   Epoch: 27 | Batch: 7000/10001 (70%) | G Loss: 1.129701 | C Loss: -0.523791\n",
      "06/30/2022 12:53:19 - INFO - __main__ -   Text: ['The question is if he knows what \"soup\" is like.']\n",
      "06/30/2022 12:53:19 - INFO - __main__ -   Epoch: 27 | Batch: 7500/10001 (75%) | G Loss: 0.961438 | C Loss: -0.415160\n",
      "06/30/2022 12:53:19 - INFO - __main__ -   Text: ['The acronym cycle is a great predictor.']\n",
      "06/30/2022 12:53:20 - INFO - __main__ -   Epoch: 27 | Batch: 8000/10001 (80%) | G Loss: 0.978043 | C Loss: -0.363857\n",
      "06/30/2022 12:53:20 - INFO - __main__ -   Text: ['A fetish is a good sign.']\n",
      "06/30/2022 12:53:21 - INFO - __main__ -   Epoch: 27 | Batch: 8500/10001 (85%) | G Loss: 0.894722 | C Loss: -0.343140\n",
      "06/30/2022 12:53:21 - INFO - __main__ -   Text: ['It\\'s funny that the term \"Mormon\" is commonly used by Mormons.']\n",
      "06/30/2022 12:53:22 - INFO - __main__ -   Epoch: 27 | Batch: 9000/10001 (90%) | G Loss: 0.886428 | C Loss: -0.303205\n",
      "06/30/2022 12:53:22 - INFO - __main__ -   Text: ['They need someone who knows how to speak.']\n",
      "06/30/2022 12:53:23 - INFO - __main__ -   Epoch: 27 | Batch: 9500/10001 (95%) | G Loss: 1.194967 | C Loss: -0.485576\n",
      "06/30/2022 12:53:23 - INFO - __main__ -   Text: ['Meaning \"a funny guy goes mad\".']\n",
      "06/30/2022 12:53:23 - INFO - __main__ -   * (Train) Epoch: 27 | G Loss: 0.9915 | C Loss: -0.3695 | Updates G: 149 | Updates C: 851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:53:32 - INFO - __main__ -   Bleu-2:0.238 | B-Bleu-2:0.259\n",
      "06/30/2022 12:53:32 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_7.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4971730599808024\n",
      "Train file used is number 7\n",
      "../../yahoo/subdivided_large/train_7.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 28 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:29.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:38.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:48.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:57.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:06.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:16.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:26.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:35.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:44.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:55.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:04.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:29.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:38.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:48.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:58.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:07.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:17.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:27.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:36.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:46.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:56.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:05.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:14.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:24.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:34.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:43.\n",
      "\n",
      "  Average training loss discriminator: 0.023\n",
      "  Training epcoh took: 0:02:45\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:59:40 - INFO - __main__ -   Epoch: 29 | Batch: 0/10001 (0%) | G Loss: 0.866868 | C Loss: -0.291564\n",
      "06/30/2022 12:59:40 - INFO - __main__ -   Text: ['Or at least some superhuman meta-thing!\"']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.412\n",
      "  Test Loss: 2.991\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 12:59:42 - INFO - __main__ -   Epoch: 29 | Batch: 500/10001 (5%) | G Loss: 1.015357 | C Loss: -0.396960\n",
      "06/30/2022 12:59:42 - INFO - __main__ -   Text: ['Superpredator is always an equation to be made.']\n",
      "06/30/2022 12:59:43 - INFO - __main__ -   Epoch: 29 | Batch: 1000/10001 (10%) | G Loss: 0.687791 | C Loss: -0.155905\n",
      "06/30/2022 12:59:43 - INFO - __main__ -   Text: ['Temporary superstition is now called B-Dog or Mercury.']\n",
      "06/30/2022 12:59:45 - INFO - __main__ -   Epoch: 29 | Batch: 1500/10001 (15%) | G Loss: 0.922725 | C Loss: -0.348659\n",
      "06/30/2022 12:59:45 - INFO - __main__ -   Text: ['An iMystery plays unique roles in the wishes category.']\n",
      "06/30/2022 12:59:46 - INFO - __main__ -   Epoch: 29 | Batch: 2000/10001 (20%) | G Loss: 0.926301 | C Loss: -0.390699\n",
      "06/30/2022 12:59:46 - INFO - __main__ -   Text: ['This film requires next to none even close to science.']\n",
      "06/30/2022 12:59:48 - INFO - __main__ -   Epoch: 29 | Batch: 2500/10001 (25%) | G Loss: 0.763636 | C Loss: -0.243738\n",
      "06/30/2022 12:59:48 - INFO - __main__ -   Text: ['Some people think that there is another life form called areps.']\n",
      "06/30/2022 12:59:49 - INFO - __main__ -   Epoch: 29 | Batch: 3000/10001 (30%) | G Loss: 1.214351 | C Loss: -0.477077\n",
      "06/30/2022 12:59:49 - INFO - __main__ -   Text: ['There is a lot of math there.']\n",
      "06/30/2022 12:59:51 - INFO - __main__ -   Epoch: 29 | Batch: 3500/10001 (35%) | G Loss: 0.880632 | C Loss: -0.286935\n",
      "06/30/2022 12:59:51 - INFO - __main__ -   Text: ['In fact, this word is more apt to describe me.']\n",
      "06/30/2022 12:59:52 - INFO - __main__ -   Epoch: 29 | Batch: 4000/10001 (40%) | G Loss: 0.772926 | C Loss: -0.290282\n",
      "06/30/2022 12:59:52 - INFO - __main__ -   Text: ['Pig tells me that he really needs someone to teach him.']\n",
      "06/30/2022 12:59:53 - INFO - __main__ -   Epoch: 29 | Batch: 4500/10001 (45%) | G Loss: 0.948265 | C Loss: -0.292076\n",
      "06/30/2022 12:59:54 - INFO - __main__ -   Text: ['You will learn to use the godswood that you have.\"']\n",
      "06/30/2022 12:59:55 - INFO - __main__ -   Epoch: 29 | Batch: 5000/10001 (50%) | G Loss: 0.734512 | C Loss: -0.208329\n",
      "06/30/2022 12:59:55 - INFO - __main__ -   Text: ['In fact, does good Science even look like magic?\"']\n",
      "06/30/2022 12:59:56 - INFO - __main__ -   Epoch: 29 | Batch: 5500/10001 (55%) | G Loss: 0.835725 | C Loss: -0.204289\n",
      "06/30/2022 12:59:56 - INFO - __main__ -   Text: ['The goal always is to reverse the addict phenomenon.\"']\n",
      "06/30/2022 12:59:58 - INFO - __main__ -   Epoch: 29 | Batch: 6000/10001 (60%) | G Loss: 0.731021 | C Loss: -0.237960\n",
      "06/30/2022 12:59:58 - INFO - __main__ -   Text: ['It is ever finding something interesting.']\n",
      "06/30/2022 12:59:59 - INFO - __main__ -   Epoch: 29 | Batch: 6500/10001 (65%) | G Loss: 0.857670 | C Loss: -0.342838\n",
      "06/30/2022 12:59:59 - INFO - __main__ -   Text: ['About conquering ghosts.']\n",
      "06/30/2022 13:00:01 - INFO - __main__ -   Epoch: 29 | Batch: 7000/10001 (70%) | G Loss: 0.836401 | C Loss: -0.259572\n",
      "06/30/2022 13:00:01 - INFO - __main__ -   Text: ['A college freshman who expects to read this song may have trouble.']\n",
      "06/30/2022 13:00:02 - INFO - __main__ -   Epoch: 29 | Batch: 7500/10001 (75%) | G Loss: 0.737178 | C Loss: -0.224944\n",
      "06/30/2022 13:00:02 - INFO - __main__ -   Text: ['Berry-Sense does just that what Berry-Sense is not.']\n",
      "06/30/2022 13:00:04 - INFO - __main__ -   Epoch: 29 | Batch: 8000/10001 (80%) | G Loss: 0.739363 | C Loss: -0.205312\n",
      "06/30/2022 13:00:04 - INFO - __main__ -   Text: ['The concept he calls \"the pearl bilbo\".']\n",
      "06/30/2022 13:00:05 - INFO - __main__ -   Epoch: 29 | Batch: 8500/10001 (85%) | G Loss: 1.024084 | C Loss: -0.390118\n",
      "06/30/2022 13:00:05 - INFO - __main__ -   Text: [\"Here is Warbuckle's list of favorite books.\"]\n",
      "06/30/2022 13:00:06 - INFO - __main__ -   Epoch: 29 | Batch: 9000/10001 (90%) | G Loss: 0.708102 | C Loss: -0.241947\n",
      "06/30/2022 13:00:07 - INFO - __main__ -   Text: ['It can help new people find their Web identity.']\n",
      "06/30/2022 13:00:08 - INFO - __main__ -   Epoch: 29 | Batch: 9500/10001 (95%) | G Loss: 0.778951 | C Loss: -0.224923\n",
      "06/30/2022 13:00:08 - INFO - __main__ -   Text: ['Unfortunately, it is not strictly an affirmative answer.\"']\n",
      "06/30/2022 13:00:09 - INFO - __main__ -   * (Train) Epoch: 29 | G Loss: 0.8795 | C Loss: -0.2989 | Updates G: 134 | Updates C: 866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:00:17 - INFO - __main__ -   Bleu-2:0.214 | B-Bleu-2:0.240\n",
      "06/30/2022 13:00:17 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_9.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4536716912534402\n",
      "Train file used is number 9\n",
      "../../yahoo/subdivided_large/train_9.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 30 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:29.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:38.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:47.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:57.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:06.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:15.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:25.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:35.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:44.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:54.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:03.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:13.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:23.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:32.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:42.\n",
      "\n",
      "  Average training loss discriminator: 0.024\n",
      "  Training epcoh took: 0:02:44\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:03:01 - INFO - __main__ -   Epoch: 30 | Batch: 0/10001 (0%) | G Loss: 1.396030 | C Loss: -0.623793\n",
      "06/30/2022 13:03:01 - INFO - __main__ -   Text: ['This seems to be the natural supplement for most people.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.432\n",
      "  Test Loss: 2.841\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:03:03 - INFO - __main__ -   Epoch: 30 | Batch: 500/10001 (5%) | G Loss: 1.276703 | C Loss: -0.449751\n",
      "06/30/2022 13:03:03 - INFO - __main__ -   Text: ['I wonder if the \"anon\" is a psychological interrogation.']\n",
      "06/30/2022 13:03:04 - INFO - __main__ -   Epoch: 30 | Batch: 1000/10001 (10%) | G Loss: 1.127272 | C Loss: -0.391122\n",
      "06/30/2022 13:03:04 - INFO - __main__ -   Text: ['The medium is gay.']\n",
      "06/30/2022 13:03:06 - INFO - __main__ -   Epoch: 30 | Batch: 1500/10001 (15%) | G Loss: 0.776192 | C Loss: -0.025789\n",
      "06/30/2022 13:03:06 - INFO - __main__ -   Text: [\"One may also use 'Weight Per Minute' for significance.\"]\n",
      "06/30/2022 13:03:07 - INFO - __main__ -   Epoch: 30 | Batch: 2000/10001 (20%) | G Loss: 0.707704 | C Loss: -0.136128\n",
      "06/30/2022 13:03:08 - INFO - __main__ -   Text: ['She is mildly hyperbolic about what type of human is hallucinating.']\n",
      "06/30/2022 13:03:09 - INFO - __main__ -   Epoch: 30 | Batch: 2500/10001 (25%) | G Loss: 0.692619 | C Loss: -0.176653\n",
      "06/30/2022 13:03:09 - INFO - __main__ -   Text: ['Sleuth is one of our most challenging subjects.\"']\n",
      "06/30/2022 13:03:10 - INFO - __main__ -   Epoch: 30 | Batch: 3000/10001 (30%) | G Loss: 0.777038 | C Loss: -0.217657\n",
      "06/30/2022 13:03:10 - INFO - __main__ -   Text: ['The central theme is loneliness.']\n",
      "06/30/2022 13:03:12 - INFO - __main__ -   Epoch: 30 | Batch: 3500/10001 (35%) | G Loss: 0.867538 | C Loss: -0.343629\n",
      "06/30/2022 13:03:12 - INFO - __main__ -   Text: ['KoKo does not do it version by version.']\n",
      "06/30/2022 13:03:13 - INFO - __main__ -   Epoch: 30 | Batch: 4000/10001 (40%) | G Loss: 0.991007 | C Loss: -0.320142\n",
      "06/30/2022 13:03:13 - INFO - __main__ -   Text: ['When they spoof the frog: \"That frog!\"']\n",
      "06/30/2022 13:03:15 - INFO - __main__ -   Epoch: 30 | Batch: 4500/10001 (45%) | G Loss: 0.669868 | C Loss: -0.143933\n",
      "06/30/2022 13:03:15 - INFO - __main__ -   Text: ['This domain name essentially means \"absorb.\"']\n",
      "06/30/2022 13:03:16 - INFO - __main__ -   Epoch: 30 | Batch: 5000/10001 (50%) | G Loss: 0.742930 | C Loss: -0.162277\n",
      "06/30/2022 13:03:16 - INFO - __main__ -   Text: ['They talk to their bern sarahs.']\n",
      "06/30/2022 13:03:18 - INFO - __main__ -   Epoch: 30 | Batch: 5500/10001 (55%) | G Loss: 0.988280 | C Loss: -0.308755\n",
      "06/30/2022 13:03:18 - INFO - __main__ -   Text: ['Structures and structures are good at generalizing.']\n",
      "06/30/2022 13:03:19 - INFO - __main__ -   Epoch: 30 | Batch: 6000/10001 (60%) | G Loss: 0.920781 | C Loss: -0.217149\n",
      "06/30/2022 13:03:19 - INFO - __main__ -   Text: ['They mean: The Blue Man model.']\n",
      "06/30/2022 13:03:21 - INFO - __main__ -   Epoch: 30 | Batch: 6500/10001 (65%) | G Loss: 0.835478 | C Loss: -0.177168\n",
      "06/30/2022 13:03:21 - INFO - __main__ -   Text: ['More seriously, it is called the developmental brink\" for readers.']\n",
      "06/30/2022 13:03:22 - INFO - __main__ -   Epoch: 30 | Batch: 7000/10001 (70%) | G Loss: 0.891686 | C Loss: -0.312428\n",
      "06/30/2022 13:03:22 - INFO - __main__ -   Text: ['You can change your name to musicAPPLIIIII eat monkeys etc.']\n",
      "06/30/2022 13:03:24 - INFO - __main__ -   Epoch: 30 | Batch: 7500/10001 (75%) | G Loss: 0.804466 | C Loss: -0.293339\n",
      "06/30/2022 13:03:24 - INFO - __main__ -   Text: ['The best option is to stab Sarah.\"']\n",
      "06/30/2022 13:03:25 - INFO - __main__ -   Epoch: 30 | Batch: 8000/10001 (80%) | G Loss: 0.706308 | C Loss: -0.102540\n",
      "06/30/2022 13:03:25 - INFO - __main__ -   Text: ['Mohawk may refer to:']\n",
      "06/30/2022 13:03:26 - INFO - __main__ -   Epoch: 30 | Batch: 8500/10001 (85%) | G Loss: 0.772088 | C Loss: -0.209164\n",
      "06/30/2022 13:03:27 - INFO - __main__ -   Text: ['If you have seen a Japanese anime movie, then the fight of Japanese\"']\n",
      "06/30/2022 13:03:28 - INFO - __main__ -   Epoch: 30 | Batch: 9000/10001 (90%) | G Loss: 1.149268 | C Loss: -0.448235\n",
      "06/30/2022 13:03:28 - INFO - __main__ -   Text: ['As with \"Big Island Dog\", Ford is prone to becoming \"faux bully.\"']\n",
      "06/30/2022 13:03:29 - INFO - __main__ -   Epoch: 30 | Batch: 9500/10001 (95%) | G Loss: 0.670244 | C Loss: -0.184588\n",
      "06/30/2022 13:03:30 - INFO - __main__ -   Text: ['Others find life scary.']\n",
      "06/30/2022 13:03:31 - INFO - __main__ -   * (Train) Epoch: 30 | G Loss: 0.8743 | C Loss: -0.2836 | Updates G: 99 | Updates C: 901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:03:39 - INFO - __main__ -   Bleu-2:0.219 | B-Bleu-2:0.252\n",
      "06/30/2022 13:03:39 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_10.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.471000985205179\n",
      "Train file used is number 10\n",
      "../../yahoo/subdivided_large/train_10.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 31 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:29.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:39.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:48.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:59.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:09.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:19.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:30.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:40.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:49.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:59.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:09.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:18.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:28.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:38.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:48.\n",
      "\n",
      "  Average training loss discriminator: 0.025\n",
      "  Training epcoh took: 0:02:50\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:06:29 - INFO - __main__ -   Epoch: 31 | Batch: 0/10001 (0%) | G Loss: 0.771217 | C Loss: -0.277558\n",
      "06/30/2022 13:06:29 - INFO - __main__ -   Text: ['This is the answer to washing your ass.\"']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.420\n",
      "  Test Loss: 2.937\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:06:31 - INFO - __main__ -   Epoch: 31 | Batch: 500/10001 (5%) | G Loss: 0.792880 | C Loss: -0.276128\n",
      "06/30/2022 13:06:31 - INFO - __main__ -   Text: ['Llorentrep explains how he can help bring thunder.']\n",
      "06/30/2022 13:06:32 - INFO - __main__ -   Epoch: 31 | Batch: 1000/10001 (10%) | G Loss: 0.737041 | C Loss: -0.199789\n",
      "06/30/2022 13:06:32 - INFO - __main__ -   Text: ['The term \"Galaxy Mirror\" excited me because I enjoy the flair.']\n",
      "06/30/2022 13:06:34 - INFO - __main__ -   Epoch: 31 | Batch: 1500/10001 (15%) | G Loss: 0.678132 | C Loss: -0.116658\n",
      "06/30/2022 13:06:34 - INFO - __main__ -   Text: ['Buriously, given its name, it is basically a comparative comedy movie.']\n",
      "06/30/2022 13:06:35 - INFO - __main__ -   Epoch: 31 | Batch: 2000/10001 (20%) | G Loss: 0.769662 | C Loss: -0.228152\n",
      "06/30/2022 13:06:35 - INFO - __main__ -   Text: ['The physiology of a Leicester.']\n",
      "06/30/2022 13:06:37 - INFO - __main__ -   Epoch: 31 | Batch: 2500/10001 (25%) | G Loss: 1.111848 | C Loss: -0.369177\n",
      "06/30/2022 13:06:37 - INFO - __main__ -   Text: ['As a plus-emoji prompt, this cheetah defines the sensitivity level.']\n",
      "06/30/2022 13:06:38 - INFO - __main__ -   Epoch: 31 | Batch: 3000/10001 (30%) | G Loss: 0.770024 | C Loss: -0.254269\n",
      "06/30/2022 13:06:38 - INFO - __main__ -   Text: [\"Glasnok is not what I'm here for.\"]\n",
      "06/30/2022 13:06:40 - INFO - __main__ -   Epoch: 31 | Batch: 3500/10001 (35%) | G Loss: 0.698510 | C Loss: -0.151925\n",
      "06/30/2022 13:06:40 - INFO - __main__ -   Text: ['Many describe them as a \"brilliant bass\".']\n",
      "06/30/2022 13:06:41 - INFO - __main__ -   Epoch: 31 | Batch: 4000/10001 (40%) | G Loss: 0.867134 | C Loss: -0.318965\n",
      "06/30/2022 13:06:41 - INFO - __main__ -   Text: ['His is doable but never.']\n",
      "06/30/2022 13:06:42 - INFO - __main__ -   Epoch: 31 | Batch: 4500/10001 (45%) | G Loss: 0.811789 | C Loss: -0.216925\n",
      "06/30/2022 13:06:43 - INFO - __main__ -   Text: ['Since Mink knows he is an \"expert\" he can tell a story.']\n",
      "06/30/2022 13:06:44 - INFO - __main__ -   Epoch: 31 | Batch: 5000/10001 (50%) | G Loss: 0.716117 | C Loss: -0.211950\n",
      "06/30/2022 13:06:44 - INFO - __main__ -   Text: ['This often involves getting food for ILL.']\n",
      "06/30/2022 13:06:45 - INFO - __main__ -   Epoch: 31 | Batch: 5500/10001 (55%) | G Loss: 0.901452 | C Loss: -0.281835\n",
      "06/30/2022 13:06:46 - INFO - __main__ -   Text: ['The influence of incantations is once a visitor is fooled.']\n",
      "06/30/2022 13:06:47 - INFO - __main__ -   Epoch: 31 | Batch: 6000/10001 (60%) | G Loss: 0.956951 | C Loss: -0.245022\n",
      "06/30/2022 13:06:47 - INFO - __main__ -   Text: ['The writing is based on how much power each philosopher has.']\n",
      "06/30/2022 13:06:48 - INFO - __main__ -   Epoch: 31 | Batch: 6500/10001 (65%) | G Loss: 0.824724 | C Loss: -0.186603\n",
      "06/30/2022 13:06:48 - INFO - __main__ -   Text: ['Rod puts it like: quixotic!']\n",
      "06/30/2022 13:06:50 - INFO - __main__ -   Epoch: 31 | Batch: 7000/10001 (70%) | G Loss: 0.797147 | C Loss: -0.300407\n",
      "06/30/2022 13:06:50 - INFO - __main__ -   Text: ['The lowering of probability is about avoiding shell attacks.']\n",
      "06/30/2022 13:06:51 - INFO - __main__ -   Epoch: 31 | Batch: 7500/10001 (75%) | G Loss: 0.725026 | C Loss: -0.215006\n",
      "06/30/2022 13:06:51 - INFO - __main__ -   Text: ['He says that musical success is solely based on popularity.']\n",
      "06/30/2022 13:06:53 - INFO - __main__ -   Epoch: 31 | Batch: 8000/10001 (80%) | G Loss: 0.774666 | C Loss: -0.216624\n",
      "06/30/2022 13:06:53 - INFO - __main__ -   Text: [\"In terms of fanbase, Larry seriously doesn't know who to get hit with.\"]\n",
      "06/30/2022 13:06:54 - INFO - __main__ -   Epoch: 31 | Batch: 8500/10001 (85%) | G Loss: 0.799156 | C Loss: -0.278749\n",
      "06/30/2022 13:06:54 - INFO - __main__ -   Text: ['They can find out whether somebody works at the gym or not.\"']\n",
      "06/30/2022 13:06:56 - INFO - __main__ -   Epoch: 31 | Batch: 9000/10001 (90%) | G Loss: 0.795542 | C Loss: -0.337576\n",
      "06/30/2022 13:06:56 - INFO - __main__ -   Text: [\"When interpreting a phenomenon, we cannot quote 'the LAW'.\"]\n",
      "06/30/2022 13:06:57 - INFO - __main__ -   Epoch: 31 | Batch: 9500/10001 (95%) | G Loss: 0.780135 | C Loss: -0.263997\n",
      "06/30/2022 13:06:57 - INFO - __main__ -   Text: ['They usually quote Monster Manual founder Jerry Seinfeld.']\n",
      "06/30/2022 13:06:59 - INFO - __main__ -   * (Train) Epoch: 31 | G Loss: 0.8201 | C Loss: -0.2471 | Updates G: 109 | Updates C: 891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:07:08 - INFO - __main__ -   Bleu-2:0.234 | B-Bleu-2:0.279\n",
      "06/30/2022 13:07:08 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_11.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5137105147553047\n",
      "Train file used is number 11\n",
      "../../yahoo/subdivided_large/train_11.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 32 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:09.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:18.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:28.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:37.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:46.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:56.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:05.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:15.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:24.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:33.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:43.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:52.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:01.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:11.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:20.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:30.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:40.\n",
      "\n",
      "  Average training loss discriminator: 0.024\n",
      "  Training epcoh took: 0:02:42\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:09:50 - INFO - __main__ -   Epoch: 32 | Batch: 0/10001 (0%) | G Loss: 0.617398 | C Loss: -0.190601\n",
      "06/30/2022 13:09:50 - INFO - __main__ -   Text: ['Ark is saying, \"Yes, it\\'s very interesting.\"']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.438\n",
      "  Test Loss: 2.958\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:09:51 - INFO - __main__ -   Epoch: 32 | Batch: 500/10001 (5%) | G Loss: 0.714141 | C Loss: -0.210874\n",
      "06/30/2022 13:09:51 - INFO - __main__ -   Text: ['It informs people about wolves only: ...']\n",
      "06/30/2022 13:09:53 - INFO - __main__ -   Epoch: 32 | Batch: 1000/10001 (10%) | G Loss: 0.851583 | C Loss: -0.345619\n",
      "06/30/2022 13:09:53 - INFO - __main__ -   Text: ['They sometimes dip into the secrets of magic names.']\n",
      "06/30/2022 13:09:54 - INFO - __main__ -   Epoch: 32 | Batch: 1500/10001 (15%) | G Loss: 0.653649 | C Loss: -0.187673\n",
      "06/30/2022 13:09:54 - INFO - __main__ -   Text: ['As far away as irow produces, their name is sounding like \"Marconi\".']\n",
      "06/30/2022 13:09:56 - INFO - __main__ -   Epoch: 32 | Batch: 2000/10001 (20%) | G Loss: 0.689261 | C Loss: -0.125167\n",
      "06/30/2022 13:09:56 - INFO - __main__ -   Text: ['No more alerting obama no for me.']\n",
      "06/30/2022 13:09:57 - INFO - __main__ -   Epoch: 32 | Batch: 2500/10001 (25%) | G Loss: 0.673561 | C Loss: -0.156474\n",
      "06/30/2022 13:09:57 - INFO - __main__ -   Text: ['\"If you love math, then huge!\"']\n",
      "06/30/2022 13:09:59 - INFO - __main__ -   Epoch: 32 | Batch: 3000/10001 (30%) | G Loss: 0.792530 | C Loss: -0.285210\n",
      "06/30/2022 13:09:59 - INFO - __main__ -   Text: ['As a better-educated person, bees can be of help.']\n",
      "06/30/2022 13:10:00 - INFO - __main__ -   Epoch: 32 | Batch: 3500/10001 (35%) | G Loss: 0.678185 | C Loss: -0.203872\n",
      "06/30/2022 13:10:00 - INFO - __main__ -   Text: ['The most common reason it is difficult to code is the nature of comedians.']\n",
      "06/30/2022 13:10:02 - INFO - __main__ -   Epoch: 32 | Batch: 4000/10001 (40%) | G Loss: 0.732291 | C Loss: -0.150487\n",
      "06/30/2022 13:10:02 - INFO - __main__ -   Text: ['It is strange though since repeated Kantrinks means \"som!\"']\n",
      "06/30/2022 13:10:03 - INFO - __main__ -   Epoch: 32 | Batch: 4500/10001 (45%) | G Loss: 0.787939 | C Loss: -0.174116\n",
      "06/30/2022 13:10:03 - INFO - __main__ -   Text: ['Louise is normally shown having an erotic interest in females.']\n",
      "06/30/2022 13:10:05 - INFO - __main__ -   Epoch: 32 | Batch: 5000/10001 (50%) | G Loss: 0.657451 | C Loss: -0.041214\n",
      "06/30/2022 13:10:05 - INFO - __main__ -   Text: ['School bullying can be problematic if the teacher does not know what \"\"Good Luck\".']\n",
      "06/30/2022 13:10:06 - INFO - __main__ -   Epoch: 32 | Batch: 5500/10001 (55%) | G Loss: 0.632787 | C Loss: -0.061894\n",
      "06/30/2022 13:10:06 - INFO - __main__ -   Text: ['All vampires have a stupid name.']\n",
      "06/30/2022 13:10:08 - INFO - __main__ -   Epoch: 32 | Batch: 6000/10001 (60%) | G Loss: 0.789820 | C Loss: -0.289747\n",
      "06/30/2022 13:10:08 - INFO - __main__ -   Text: [\"Two things are hard for you to say: You're 31.\"]\n",
      "06/30/2022 13:10:09 - INFO - __main__ -   Epoch: 32 | Batch: 6500/10001 (65%) | G Loss: 0.882145 | C Loss: -0.356026\n",
      "06/30/2022 13:10:09 - INFO - __main__ -   Text: ['Dramas are designed to be humorous.']\n",
      "06/30/2022 13:10:11 - INFO - __main__ -   Epoch: 32 | Batch: 7000/10001 (70%) | G Loss: 0.831271 | C Loss: -0.324127\n",
      "06/30/2022 13:10:11 - INFO - __main__ -   Text: ['No actual alarmist should ever create a single alarm every day.']\n",
      "06/30/2022 13:10:12 - INFO - __main__ -   Epoch: 32 | Batch: 7500/10001 (75%) | G Loss: 0.759345 | C Loss: -0.155238\n",
      "06/30/2022 13:10:12 - INFO - __main__ -   Text: [\"Plurals either way usually choose which males you're dealing with.\"]\n",
      "06/30/2022 13:10:14 - INFO - __main__ -   Epoch: 32 | Batch: 8000/10001 (80%) | G Loss: 0.815272 | C Loss: -0.267774\n",
      "06/30/2022 13:10:14 - INFO - __main__ -   Text: ['They are called the well wisher.']\n",
      "06/30/2022 13:10:15 - INFO - __main__ -   Epoch: 32 | Batch: 8500/10001 (85%) | G Loss: 0.675858 | C Loss: -0.086464\n",
      "06/30/2022 13:10:15 - INFO - __main__ -   Text: ['There is no legal definition for \"suicide diet\".']\n",
      "06/30/2022 13:10:17 - INFO - __main__ -   Epoch: 32 | Batch: 9000/10001 (90%) | G Loss: 0.719262 | C Loss: -0.182672\n",
      "06/30/2022 13:10:17 - INFO - __main__ -   Text: ['This is suitable for girls who want to grow up with a language.']\n",
      "06/30/2022 13:10:18 - INFO - __main__ -   Epoch: 32 | Batch: 9500/10001 (95%) | G Loss: 0.707689 | C Loss: -0.170394\n",
      "06/30/2022 13:10:18 - INFO - __main__ -   Text: ['The \"strange dream\" of \"Wilson\".']\n",
      "06/30/2022 13:10:20 - INFO - __main__ -   * (Train) Epoch: 32 | G Loss: 0.7770 | C Loss: -0.2206 | Updates G: 99 | Updates C: 901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:10:28 - INFO - __main__ -   Bleu-2:0.226 | B-Bleu-2:0.281\n",
      "06/30/2022 13:10:28 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_12.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.507503920195577\n",
      "Train file used is number 12\n",
      "../../yahoo/subdivided_large/train_12.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 33 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:30.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:40.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:50.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:59.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:09.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:19.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:29.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:38.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:48.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:58.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:09.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:18.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:27.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:37.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:47.\n",
      "\n",
      "  Average training loss discriminator: 0.017\n",
      "  Training epcoh took: 0:02:49\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:13:18 - INFO - __main__ -   Epoch: 33 | Batch: 0/10001 (0%) | G Loss: 1.011064 | C Loss: -0.345637\n",
      "06/30/2022 13:13:18 - INFO - __main__ -   Text: ['Richer than you in all skill ranks.\"']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.435\n",
      "  Test Loss: 2.963\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:13:19 - INFO - __main__ -   Epoch: 33 | Batch: 500/10001 (5%) | G Loss: 0.680316 | C Loss: -0.191549\n",
      "06/30/2022 13:13:19 - INFO - __main__ -   Text: ['The reason is just that I\\'ve heard this phrase so often.\"']\n",
      "06/30/2022 13:13:20 - INFO - __main__ -   Epoch: 33 | Batch: 1000/10001 (10%) | G Loss: 0.688031 | C Loss: -0.164857\n",
      "06/30/2022 13:13:21 - INFO - __main__ -   Text: ['Adjust it exactly when you want it to be.\"']\n",
      "06/30/2022 13:13:22 - INFO - __main__ -   Epoch: 33 | Batch: 1500/10001 (15%) | G Loss: 0.913240 | C Loss: -0.326041\n",
      "06/30/2022 13:13:22 - INFO - __main__ -   Text: ['It is very easy to understand what it is actually about.']\n",
      "06/30/2022 13:13:23 - INFO - __main__ -   Epoch: 33 | Batch: 2000/10001 (20%) | G Loss: 0.786589 | C Loss: -0.238640\n",
      "06/30/2022 13:13:23 - INFO - __main__ -   Text: ['They see fast food, but not take care.']\n",
      "06/30/2022 13:13:25 - INFO - __main__ -   Epoch: 33 | Batch: 2500/10001 (25%) | G Loss: 0.713798 | C Loss: -0.132434\n",
      "06/30/2022 13:13:25 - INFO - __main__ -   Text: ['It begins with a good idea of how to backup a stranger.']\n",
      "06/30/2022 13:13:26 - INFO - __main__ -   Epoch: 33 | Batch: 3000/10001 (30%) | G Loss: 0.658142 | C Loss: -0.166440\n",
      "06/30/2022 13:13:26 - INFO - __main__ -   Text: ['Choose \"kill me\"!\"']\n",
      "06/30/2022 13:13:28 - INFO - __main__ -   Epoch: 33 | Batch: 3500/10001 (35%) | G Loss: 0.643849 | C Loss: -0.151751\n",
      "06/30/2022 13:13:28 - INFO - __main__ -   Text: ['Returning to another universe, it is called the Three Mirror.']\n",
      "06/30/2022 13:13:29 - INFO - __main__ -   Epoch: 33 | Batch: 4000/10001 (40%) | G Loss: 0.749769 | C Loss: -0.206884\n",
      "06/30/2022 13:13:29 - INFO - __main__ -   Text: ['Women are going to be more skilled at math than men.']\n",
      "06/30/2022 13:13:31 - INFO - __main__ -   Epoch: 33 | Batch: 4500/10001 (45%) | G Loss: 0.751668 | C Loss: -0.205243\n",
      "06/30/2022 13:13:31 - INFO - __main__ -   Text: ['For instance, watching a horror film \"Real Ghost\" far exceeds written state.']\n",
      "06/30/2022 13:13:32 - INFO - __main__ -   Epoch: 33 | Batch: 5000/10001 (50%) | G Loss: 0.976420 | C Loss: -0.407728\n",
      "06/30/2022 13:13:32 - INFO - __main__ -   Text: ['It is, however, called Dorothy\".']\n",
      "06/30/2022 13:13:34 - INFO - __main__ -   Epoch: 33 | Batch: 5500/10001 (55%) | G Loss: 1.247578 | C Loss: -0.556213\n",
      "06/30/2022 13:13:34 - INFO - __main__ -   Text: ['You can read it yourself, but you\\'re not learning.\"']\n",
      "06/30/2022 13:13:35 - INFO - __main__ -   Epoch: 33 | Batch: 6000/10001 (60%) | G Loss: 1.243416 | C Loss: -0.367567\n",
      "06/30/2022 13:13:35 - INFO - __main__ -   Text: ['\"Paranoia\" (\" economic reality\").']\n",
      "06/30/2022 13:13:37 - INFO - __main__ -   Epoch: 33 | Batch: 6500/10001 (65%) | G Loss: 0.708552 | C Loss: -0.314900\n",
      "06/30/2022 13:13:37 - INFO - __main__ -   Text: ['The difference here is when most residential birds misreport the type of dwarf they are.']\n",
      "06/30/2022 13:13:38 - INFO - __main__ -   Epoch: 33 | Batch: 7000/10001 (70%) | G Loss: 0.541738 | C Loss: -0.118264\n",
      "06/30/2022 13:13:38 - INFO - __main__ -   Text: ['\"He knows slang\" is also a sentiment that comes up in every language.']\n",
      "06/30/2022 13:13:40 - INFO - __main__ -   Epoch: 33 | Batch: 7500/10001 (75%) | G Loss: 0.600589 | C Loss: -0.090514\n",
      "06/30/2022 13:13:40 - INFO - __main__ -   Text: ['Charley is picturical of the gigantic supermarket.']\n",
      "06/30/2022 13:13:41 - INFO - __main__ -   Epoch: 33 | Batch: 8000/10001 (80%) | G Loss: 1.101715 | C Loss: -0.331265\n",
      "06/30/2022 13:13:41 - INFO - __main__ -   Text: ['The characters can even say \"immortal\" to others.']\n",
      "06/30/2022 13:13:43 - INFO - __main__ -   Epoch: 33 | Batch: 8500/10001 (85%) | G Loss: 1.501284 | C Loss: -0.309467\n",
      "06/30/2022 13:13:43 - INFO - __main__ -   Text: ['The Travel Methodology is termed \"political science\".']\n",
      "06/30/2022 13:13:44 - INFO - __main__ -   Epoch: 33 | Batch: 9000/10001 (90%) | G Loss: 1.716174 | C Loss: -0.143740\n",
      "06/30/2022 13:13:44 - INFO - __main__ -   Text: ['It\\'s a pull of Joe Rogan.\"']\n",
      "06/30/2022 13:13:46 - INFO - __main__ -   Epoch: 33 | Batch: 9500/10001 (95%) | G Loss: 2.697407 | C Loss: -0.078355\n",
      "06/30/2022 13:13:46 - INFO - __main__ -   Text: ['Gossip is very common when it comes to selling or renting coffee.']\n",
      "06/30/2022 13:13:47 - INFO - __main__ -   * (Train) Epoch: 33 | G Loss: 0.9354 | C Loss: -0.2367 | Updates G: 77 | Updates C: 923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:13:55 - INFO - __main__ -   Bleu-2:0.227 | B-Bleu-2:0.305\n",
      "06/30/2022 13:13:55 - INFO - __main__ -   * Saving. Best Score:0.532 | Bleu-2:0.227 | B-Bleu-2:0.305\n",
      "06/30/2022 13:13:55 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_13.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5316057049425774\n",
      "Train file used is number 13\n",
      "../../yahoo/subdivided_large/train_13.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 34 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:09.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:19.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:29.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:38.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:48.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:57.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:07.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:17.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:27.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:37.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:47.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:57.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:06.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:17.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:26.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:36.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:47.\n",
      "\n",
      "  Average training loss discriminator: 0.017\n",
      "  Training epcoh took: 0:02:49\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:16:44 - INFO - __main__ -   Epoch: 34 | Batch: 0/10001 (0%) | G Loss: 4.090986 | C Loss: -0.235273\n",
      "06/30/2022 13:16:45 - INFO - __main__ -   Text: ['Meanwhile, Daniel II is known for being jealous.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.435\n",
      "  Test Loss: 3.038\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:16:46 - INFO - __main__ -   Epoch: 34 | Batch: 500/10001 (5%) | G Loss: 3.228743 | C Loss: -0.647670\n",
      "06/30/2022 13:16:46 - INFO - __main__ -   Text: ['Those time is very important...']\n",
      "06/30/2022 13:16:47 - INFO - __main__ -   Epoch: 34 | Batch: 1000/10001 (10%) | G Loss: 1.247998 | C Loss: 0.182477\n",
      "06/30/2022 13:16:48 - INFO - __main__ -   Text: ['There is exhausting information in books that remind us of death.']\n",
      "06/30/2022 13:16:49 - INFO - __main__ -   Epoch: 34 | Batch: 1500/10001 (15%) | G Loss: 1.021193 | C Loss: -0.347284\n",
      "06/30/2022 13:16:49 - INFO - __main__ -   Text: ['The potential for nasty things in Ramanish is rare.']\n",
      "06/30/2022 13:16:51 - INFO - __main__ -   Epoch: 34 | Batch: 2000/10001 (20%) | G Loss: 0.885155 | C Loss: -0.002028\n",
      "06/30/2022 13:16:51 - INFO - __main__ -   Text: ['They don\\'t confuse equations with snakes.\"\"']\n",
      "06/30/2022 13:16:52 - INFO - __main__ -   Epoch: 34 | Batch: 2500/10001 (25%) | G Loss: 1.420758 | C Loss: -0.219596\n",
      "06/30/2022 13:16:52 - INFO - __main__ -   Text: ['The dark side are the optimistic warnings that Europeans will not be \"discoverable in their lands.']\n",
      "06/30/2022 13:16:53 - INFO - __main__ -   Epoch: 34 | Batch: 3000/10001 (30%) | G Loss: 0.983411 | C Loss: -0.263831\n",
      "06/30/2022 13:16:53 - INFO - __main__ -   Text: [\"He differs from most other 'TH empiricists'.\"]\n",
      "06/30/2022 13:16:54 - INFO - __main__ -   Epoch: 34 | Batch: 3500/10001 (35%) | G Loss: 0.906941 | C Loss: -0.160677\n",
      "06/30/2022 13:16:54 - INFO - __main__ -   Text: ['Whether this is your stoneman or your leeching algum, dumb.']\n",
      "06/30/2022 13:16:55 - INFO - __main__ -   Epoch: 34 | Batch: 4000/10001 (40%) | G Loss: 0.931346 | C Loss: -0.291435\n",
      "06/30/2022 13:16:55 - INFO - __main__ -   Text: ['They are silly when they say \"Don\\'t Move - It\\'s all\".']\n",
      "06/30/2022 13:16:56 - INFO - __main__ -   Epoch: 34 | Batch: 4500/10001 (45%) | G Loss: 1.133448 | C Loss: -0.003762\n",
      "06/30/2022 13:16:56 - INFO - __main__ -   Text: ['Hey, former Yourguru!, word of no consequence.']\n",
      "06/30/2022 13:16:56 - INFO - __main__ -   Epoch: 34 | Batch: 5000/10001 (50%) | G Loss: 1.417769 | C Loss: -0.382561\n",
      "06/30/2022 13:16:56 - INFO - __main__ -   Text: ['Same mafioso.An Outcast.\"\"']\n",
      "06/30/2022 13:16:57 - INFO - __main__ -   Epoch: 34 | Batch: 5500/10001 (55%) | G Loss: 1.342700 | C Loss: -0.242718\n",
      "06/30/2022 13:16:57 - INFO - __main__ -   Text: ['With no desktop computer, it is imperative they work online\".']\n",
      "06/30/2022 13:16:58 - INFO - __main__ -   Epoch: 34 | Batch: 6000/10001 (60%) | G Loss: 0.958448 | C Loss: -0.125744\n",
      "06/30/2022 13:16:58 - INFO - __main__ -   Text: ['Some theories are that it is because of \"The Book Test.\"']\n",
      "06/30/2022 13:16:59 - INFO - __main__ -   Epoch: 34 | Batch: 6500/10001 (65%) | G Loss: 1.127928 | C Loss: -0.217822\n",
      "06/30/2022 13:16:59 - INFO - __main__ -   Text: ['The feeling here is that mainline is church\".']\n",
      "06/30/2022 13:17:00 - INFO - __main__ -   Epoch: 34 | Batch: 7000/10001 (70%) | G Loss: 0.953399 | C Loss: 0.011906\n",
      "06/30/2022 13:17:00 - INFO - __main__ -   Text: [\"And he's also a homosexual like Rodney Bradshaw.\"]\n",
      "06/30/2022 13:17:01 - INFO - __main__ -   Epoch: 34 | Batch: 7500/10001 (75%) | G Loss: 0.936776 | C Loss: -0.230042\n",
      "06/30/2022 13:17:01 - INFO - __main__ -   Text: ['Aaron lists credit cards, and his can-do attitude.']\n",
      "06/30/2022 13:17:01 - INFO - __main__ -   Epoch: 34 | Batch: 8000/10001 (80%) | G Loss: 0.688926 | C Loss: -0.201036\n",
      "06/30/2022 13:17:01 - INFO - __main__ -   Text: ['Wonder Woman will collect show tickets when rejected.']\n",
      "06/30/2022 13:17:02 - INFO - __main__ -   Epoch: 34 | Batch: 8500/10001 (85%) | G Loss: 0.700779 | C Loss: -0.193140\n",
      "06/30/2022 13:17:02 - INFO - __main__ -   Text: ['OverrunningATIONS suits drummer.']\n",
      "06/30/2022 13:17:03 - INFO - __main__ -   Epoch: 34 | Batch: 9000/10001 (90%) | G Loss: 0.996975 | C Loss: -0.336018\n",
      "06/30/2022 13:17:03 - INFO - __main__ -   Text: ['As a consultant full of teachers there, they should be able to argue about whether or not they are']\n",
      "06/30/2022 13:17:04 - INFO - __main__ -   Epoch: 34 | Batch: 9500/10001 (95%) | G Loss: 0.674728 | C Loss: -0.148313\n",
      "06/30/2022 13:17:04 - INFO - __main__ -   Text: ['Stay away from stupid superstition!']\n",
      "06/30/2022 13:17:05 - INFO - __main__ -   * (Train) Epoch: 34 | G Loss: 1.1281 | C Loss: -0.2188 | Updates G: 50 | Updates C: 950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:17:13 - INFO - __main__ -   Bleu-2:0.215 | B-Bleu-2:0.254\n",
      "06/30/2022 13:17:13 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_14.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46829614065097747\n",
      "Train file used is number 14\n",
      "../../yahoo/subdivided_large/train_14.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 35 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:29.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:39.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:49.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:59.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:08.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:19.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:28.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:38.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:48.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:57.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:07.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:16.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:26.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:36.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:46.\n",
      "\n",
      "  Average training loss discriminator: 0.017\n",
      "  Training epcoh took: 0:02:47\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:20:01 - INFO - __main__ -   Epoch: 35 | Batch: 0/10001 (0%) | G Loss: 0.855262 | C Loss: -0.195640\n",
      "06/30/2022 13:20:01 - INFO - __main__ -   Text: ['Again, it is really only about sex.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.427\n",
      "  Test Loss: 3.101\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:20:02 - INFO - __main__ -   Epoch: 35 | Batch: 500/10001 (5%) | G Loss: 0.897303 | C Loss: -0.267420\n",
      "06/30/2022 13:20:02 - INFO - __main__ -   Text: ['They are basically trying to be me and now I am.\"']\n",
      "06/30/2022 13:20:04 - INFO - __main__ -   Epoch: 35 | Batch: 1000/10001 (10%) | G Loss: 0.739818 | C Loss: -0.177185\n",
      "06/30/2022 13:20:04 - INFO - __main__ -   Text: ['Morality is vacuum-seeming.']\n",
      "06/30/2022 13:20:05 - INFO - __main__ -   Epoch: 35 | Batch: 1500/10001 (15%) | G Loss: 0.523916 | C Loss: -0.132626\n",
      "06/30/2022 13:20:05 - INFO - __main__ -   Text: ['Simultaneously with Bart writing, it might be a bit easier.']\n",
      "06/30/2022 13:20:07 - INFO - __main__ -   Epoch: 35 | Batch: 2000/10001 (20%) | G Loss: 0.482545 | C Loss: -0.176821\n",
      "06/30/2022 13:20:07 - INFO - __main__ -   Text: ['The author of the article responds asking \"what does that means?\".']\n",
      "06/30/2022 13:20:08 - INFO - __main__ -   Epoch: 35 | Batch: 2500/10001 (25%) | G Loss: 0.577258 | C Loss: -0.167570\n",
      "06/30/2022 13:20:08 - INFO - __main__ -   Text: ['Yoggly!']\n",
      "06/30/2022 13:20:10 - INFO - __main__ -   Epoch: 35 | Batch: 3000/10001 (30%) | G Loss: 0.590634 | C Loss: -0.201641\n",
      "06/30/2022 13:20:10 - INFO - __main__ -   Text: ['Perhaps Dodd will prove your worthless speeches.']\n",
      "06/30/2022 13:20:11 - INFO - __main__ -   Epoch: 35 | Batch: 3500/10001 (35%) | G Loss: 0.460432 | C Loss: -0.199317\n",
      "06/30/2022 13:20:11 - INFO - __main__ -   Text: ['Unfazed, they also get infatuated.']\n",
      "06/30/2022 13:20:13 - INFO - __main__ -   Epoch: 35 | Batch: 4000/10001 (40%) | G Loss: 0.473634 | C Loss: -0.106499\n",
      "06/30/2022 13:20:13 - INFO - __main__ -   Text: ['Educated police don\\'t speak English like real people do.\" <PAD>']\n",
      "06/30/2022 13:20:14 - INFO - __main__ -   Epoch: 35 | Batch: 4500/10001 (45%) | G Loss: 0.358629 | C Loss: -0.130165\n",
      "06/30/2022 13:20:14 - INFO - __main__ -   Text: ['The term positive effect is derived from sound effect.']\n",
      "06/30/2022 13:20:15 - INFO - __main__ -   Epoch: 35 | Batch: 5000/10001 (50%) | G Loss: 0.615472 | C Loss: -0.212361\n",
      "06/30/2022 13:20:15 - INFO - __main__ -   Text: ['The Black Monkey certainly does not consider himself a bomber.']\n",
      "06/30/2022 13:20:17 - INFO - __main__ -   Epoch: 35 | Batch: 5500/10001 (55%) | G Loss: 0.611351 | C Loss: -0.306538\n",
      "06/30/2022 13:20:17 - INFO - __main__ -   Text: ['This turned out to be nothing though of any kind.']\n",
      "06/30/2022 13:20:18 - INFO - __main__ -   Epoch: 35 | Batch: 6000/10001 (60%) | G Loss: 0.555837 | C Loss: -0.164911\n",
      "06/30/2022 13:20:19 - INFO - __main__ -   Text: ['I am a warning sign for anyone looking for printed stories.\"']\n",
      "06/30/2022 13:20:20 - INFO - __main__ -   Epoch: 35 | Batch: 6500/10001 (65%) | G Loss: 0.824028 | C Loss: -0.246075\n",
      "06/30/2022 13:20:20 - INFO - __main__ -   Text: ['The word dictionary should be searchable.']\n",
      "06/30/2022 13:20:21 - INFO - __main__ -   Epoch: 35 | Batch: 7000/10001 (70%) | G Loss: 0.652571 | C Loss: -0.203699\n",
      "06/30/2022 13:20:21 - INFO - __main__ -   Text: ['Improvements :)']\n",
      "06/30/2022 13:20:23 - INFO - __main__ -   Epoch: 35 | Batch: 7500/10001 (75%) | G Loss: 0.369900 | C Loss: -0.135496\n",
      "06/30/2022 13:20:23 - INFO - __main__ -   Text: ['They can tell you which rules are correct.']\n",
      "06/30/2022 13:20:24 - INFO - __main__ -   Epoch: 35 | Batch: 8000/10001 (80%) | G Loss: 0.454597 | C Loss: -0.199480\n",
      "06/30/2022 13:20:24 - INFO - __main__ -   Text: ['It is doubtful that that he has invented the beta.\"']\n",
      "06/30/2022 13:20:26 - INFO - __main__ -   Epoch: 35 | Batch: 8500/10001 (85%) | G Loss: 0.579595 | C Loss: -0.212651\n",
      "06/30/2022 13:20:26 - INFO - __main__ -   Text: ['Although not as LGBTQ friendly, it might be useful.']\n",
      "06/30/2022 13:20:27 - INFO - __main__ -   Epoch: 35 | Batch: 9000/10001 (90%) | G Loss: 0.818487 | C Loss: -0.350756\n",
      "06/30/2022 13:20:27 - INFO - __main__ -   Text: ['Statistical tests Therefore, they are Psychic\".']\n",
      "06/30/2022 13:20:29 - INFO - __main__ -   Epoch: 35 | Batch: 9500/10001 (95%) | G Loss: 0.655463 | C Loss: -0.183679\n",
      "06/30/2022 13:20:29 - INFO - __main__ -   Text: ['Robbing people with your name is fun.']\n",
      "06/30/2022 13:20:30 - INFO - __main__ -   * (Train) Epoch: 35 | G Loss: 0.5564 | C Loss: -0.1723 | Updates G: 116 | Updates C: 884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:20:39 - INFO - __main__ -   Bleu-2:0.221 | B-Bleu-2:0.286\n",
      "06/30/2022 13:20:39 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_15.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5069226446647481\n",
      "Train file used is number 15\n",
      "../../yahoo/subdivided_large/train_15.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 36 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:09.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:14.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:24.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:34.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:43.\n",
      "\n",
      "  Average training loss discriminator: 0.021\n",
      "  Training epcoh took: 0:02:45\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:23:24 - INFO - __main__ -   Epoch: 36 | Batch: 0/10001 (0%) | G Loss: 0.475688 | C Loss: -0.241962\n",
      "06/30/2022 13:23:24 - INFO - __main__ -   Text: ['A high level spell is an easy way to rule by charm.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.425\n",
      "  Test Loss: 2.976\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:23:25 - INFO - __main__ -   Epoch: 36 | Batch: 500/10001 (5%) | G Loss: 0.518193 | C Loss: -0.180934\n",
      "06/30/2022 13:23:25 - INFO - __main__ -   Text: ['The magic wand is that people like Moe give it truth.']\n",
      "06/30/2022 13:23:26 - INFO - __main__ -   Epoch: 36 | Batch: 1000/10001 (10%) | G Loss: 0.440846 | C Loss: -0.192720\n",
      "06/30/2022 13:23:26 - INFO - __main__ -   Text: ['It is the worst-selling classically precog.']\n",
      "06/30/2022 13:23:27 - INFO - __main__ -   Epoch: 36 | Batch: 1500/10001 (15%) | G Loss: 0.693576 | C Loss: -0.243022\n",
      "06/30/2022 13:23:27 - INFO - __main__ -   Text: ['Conversely, watch Norton teachers without magic.']\n",
      "06/30/2022 13:23:28 - INFO - __main__ -   Epoch: 36 | Batch: 2000/10001 (20%) | G Loss: 0.499921 | C Loss: -0.213950\n",
      "06/30/2022 13:23:28 - INFO - __main__ -   Text: ['Teen-agers have other children with similar behaviors.']\n",
      "06/30/2022 13:23:29 - INFO - __main__ -   Epoch: 36 | Batch: 2500/10001 (25%) | G Loss: 0.423078 | C Loss: -0.062080\n",
      "06/30/2022 13:23:30 - INFO - __main__ -   Text: ['Since I am not necessarily a savvy shopper I think I must have some sort of accent.']\n",
      "06/30/2022 13:23:31 - INFO - __main__ -   Epoch: 36 | Batch: 3000/10001 (30%) | G Loss: 0.553076 | C Loss: -0.209405\n",
      "06/30/2022 13:23:31 - INFO - __main__ -   Text: ['This may be a reference to marathons.\"']\n",
      "06/30/2022 13:23:32 - INFO - __main__ -   Epoch: 36 | Batch: 3500/10001 (35%) | G Loss: 0.716726 | C Loss: -0.300558\n",
      "06/30/2022 13:23:33 - INFO - __main__ -   Text: ['This thread is the definition of the buttech ...\" <BOS> \"This is an optimizer\".']\n",
      "06/30/2022 13:23:34 - INFO - __main__ -   Epoch: 36 | Batch: 4000/10001 (40%) | G Loss: 0.477890 | C Loss: -0.162756\n",
      "06/30/2022 13:23:34 - INFO - __main__ -   Text: ['DHT won\\'t ever have one anyways.\"']\n",
      "06/30/2022 13:23:35 - INFO - __main__ -   Epoch: 36 | Batch: 4500/10001 (45%) | G Loss: 0.469049 | C Loss: -0.173148\n",
      "06/30/2022 13:23:36 - INFO - __main__ -   Text: ['Hoppiness is this word for many things you don in Hollywood.']\n",
      "06/30/2022 13:23:37 - INFO - __main__ -   Epoch: 36 | Batch: 5000/10001 (50%) | G Loss: 0.600452 | C Loss: -0.128791\n",
      "06/30/2022 13:23:37 - INFO - __main__ -   Text: ['Crystal Dreams is a very unusual spell that helps you become a magic spell.']\n",
      "06/30/2022 13:23:38 - INFO - __main__ -   Epoch: 36 | Batch: 5500/10001 (55%) | G Loss: 0.937148 | C Loss: -0.236383\n",
      "06/30/2022 13:23:39 - INFO - __main__ -   Text: ['wrote in citing \"The frog or marsupial must cause trouble and trouble.\"']\n",
      "06/30/2022 13:23:40 - INFO - __main__ -   Epoch: 36 | Batch: 6000/10001 (60%) | G Loss: 1.190798 | C Loss: -0.384022\n",
      "06/30/2022 13:23:40 - INFO - __main__ -   Text: ['The narrator of Compassionately goes for a testicles or wet dance.']\n",
      "06/30/2022 13:23:41 - INFO - __main__ -   Epoch: 36 | Batch: 6500/10001 (65%) | G Loss: 0.494763 | C Loss: -0.085490\n",
      "06/30/2022 13:23:42 - INFO - __main__ -   Text: ['As crazier sounds then it becomes.']\n",
      "06/30/2022 13:23:43 - INFO - __main__ -   Epoch: 36 | Batch: 7000/10001 (70%) | G Loss: 0.455756 | C Loss: -0.150747\n",
      "06/30/2022 13:23:43 - INFO - __main__ -   Text: ['Instead of a side career, I only want to be a planes player.\"']\n",
      "06/30/2022 13:23:44 - INFO - __main__ -   Epoch: 36 | Batch: 7500/10001 (75%) | G Loss: 0.525924 | C Loss: -0.166603\n",
      "06/30/2022 13:23:44 - INFO - __main__ -   Text: ['Socializing means drinking a lot.']\n",
      "06/30/2022 13:23:46 - INFO - __main__ -   Epoch: 36 | Batch: 8000/10001 (80%) | G Loss: 0.524621 | C Loss: -0.173054\n",
      "06/30/2022 13:23:46 - INFO - __main__ -   Text: ['Saturn is This is the fictional interpretation of the answer.']\n",
      "06/30/2022 13:23:47 - INFO - __main__ -   Epoch: 36 | Batch: 8500/10001 (85%) | G Loss: 0.698591 | C Loss: -0.235521\n",
      "06/30/2022 13:23:47 - INFO - __main__ -   Text: ['It is one of many reasons dating is not necessary for bad marriages.']\n",
      "06/30/2022 13:23:49 - INFO - __main__ -   Epoch: 36 | Batch: 9000/10001 (90%) | G Loss: 0.643405 | C Loss: -0.167895\n",
      "06/30/2022 13:23:49 - INFO - __main__ -   Text: ['The friendliest book on the planet is \"The Mobius Part.\"']\n",
      "06/30/2022 13:23:50 - INFO - __main__ -   Epoch: 36 | Batch: 9500/10001 (95%) | G Loss: 0.721418 | C Loss: -0.267679\n",
      "06/30/2022 13:23:50 - INFO - __main__ -   Text: ['The Ours are jokes, yes.']\n",
      "06/30/2022 13:23:52 - INFO - __main__ -   * (Train) Epoch: 36 | G Loss: 0.5696 | C Loss: -0.1801 | Updates G: 83 | Updates C: 917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:24:01 - INFO - __main__ -   Bleu-2:0.224 | B-Bleu-2:0.267\n",
      "06/30/2022 13:24:01 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_16.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49173462430699655\n",
      "Train file used is number 16\n",
      "../../yahoo/subdivided_large/train_16.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 37 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:30.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:39.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:49.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:58.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:08.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:17.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:27.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:37.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:46.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:56.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:06.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:16.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:25.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:36.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:45.\n",
      "\n",
      "  Average training loss discriminator: 0.025\n",
      "  Training epcoh took: 0:02:47\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:26:49 - INFO - __main__ -   Epoch: 37 | Batch: 0/10001 (0%) | G Loss: 0.760163 | C Loss: -0.169918\n",
      "06/30/2022 13:26:49 - INFO - __main__ -   Text: ['Next term is occiptery nonsense!']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.430\n",
      "  Test Loss: 2.972\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:26:50 - INFO - __main__ -   Epoch: 37 | Batch: 500/10001 (5%) | G Loss: 0.420737 | C Loss: -0.033560\n",
      "06/30/2022 13:26:50 - INFO - __main__ -   Text: ['One would assume that this French word does not have meaning in \"homosexual\".']\n",
      "06/30/2022 13:26:52 - INFO - __main__ -   Epoch: 37 | Batch: 1000/10001 (10%) | G Loss: 0.499564 | C Loss: -0.137767\n",
      "06/30/2022 13:26:52 - INFO - __main__ -   Text: ['To think it is better name locally?']\n",
      "06/30/2022 13:26:53 - INFO - __main__ -   Epoch: 37 | Batch: 1500/10001 (15%) | G Loss: 0.538603 | C Loss: -0.217332\n",
      "06/30/2022 13:26:53 - INFO - __main__ -   Text: ['The name \"Astrologer\" is convenient way to bring in more visitors.']\n",
      "06/30/2022 13:26:55 - INFO - __main__ -   Epoch: 37 | Batch: 2000/10001 (20%) | G Loss: 0.413495 | C Loss: -0.057565\n",
      "06/30/2022 13:26:55 - INFO - __main__ -   Text: ['He tells us that phallus cockies call monkey times\".']\n",
      "06/30/2022 13:26:56 - INFO - __main__ -   Epoch: 37 | Batch: 2500/10001 (25%) | G Loss: 0.422055 | C Loss: -0.120237\n",
      "06/30/2022 13:26:56 - INFO - __main__ -   Text: ['They like aesthetics of ingetters and fallers.']\n",
      "06/30/2022 13:26:58 - INFO - __main__ -   Epoch: 37 | Batch: 3000/10001 (30%) | G Loss: 0.517364 | C Loss: -0.150680\n",
      "06/30/2022 13:26:58 - INFO - __main__ -   Text: ['Jilech may get frustrated with it!']\n",
      "06/30/2022 13:26:59 - INFO - __main__ -   Epoch: 37 | Batch: 3500/10001 (35%) | G Loss: 0.649978 | C Loss: -0.132449\n",
      "06/30/2022 13:26:59 - INFO - __main__ -   Text: ['The impairment gives you the opportunity to speak fluent French.']\n",
      "06/30/2022 13:27:01 - INFO - __main__ -   Epoch: 37 | Batch: 4000/10001 (40%) | G Loss: 0.586318 | C Loss: -0.095734\n",
      "06/30/2022 13:27:01 - INFO - __main__ -   Text: ['The position of vieve is sometimes called a vieve compared to the actual vieve. <PAD>']\n",
      "06/30/2022 13:27:02 - INFO - __main__ -   Epoch: 37 | Batch: 4500/10001 (45%) | G Loss: 0.984701 | C Loss: -0.368972\n",
      "06/30/2022 13:27:02 - INFO - __main__ -   Text: ['\"Goldfish Fever\" is always a cryptic meter.']\n",
      "06/30/2022 13:27:04 - INFO - __main__ -   Epoch: 37 | Batch: 5000/10001 (50%) | G Loss: 0.531354 | C Loss: -0.100941\n",
      "06/30/2022 13:27:04 - INFO - __main__ -   Text: ['A random yet foolhardy name can confuse you.']\n",
      "06/30/2022 13:27:05 - INFO - __main__ -   Epoch: 37 | Batch: 5500/10001 (55%) | G Loss: 0.529198 | C Loss: -0.084501\n",
      "06/30/2022 13:27:05 - INFO - __main__ -   Text: ['The paths to the \"Singaporean Pregnancy Check\".']\n",
      "06/30/2022 13:27:07 - INFO - __main__ -   Epoch: 37 | Batch: 6000/10001 (60%) | G Loss: 0.461782 | C Loss: -0.109274\n",
      "06/30/2022 13:27:07 - INFO - __main__ -   Text: ['The player values her psychic abilities over the \"chiller\".']\n",
      "06/30/2022 13:27:08 - INFO - __main__ -   Epoch: 37 | Batch: 6500/10001 (65%) | G Loss: 0.499667 | C Loss: -0.160356\n",
      "06/30/2022 13:27:08 - INFO - __main__ -   Text: ['Aman, I\\'m just trying to please everybody.\"']\n",
      "06/30/2022 13:27:10 - INFO - __main__ -   Epoch: 37 | Batch: 7000/10001 (70%) | G Loss: 0.535234 | C Loss: -0.198357\n",
      "06/30/2022 13:27:10 - INFO - __main__ -   Text: ['Ha!']\n",
      "06/30/2022 13:27:11 - INFO - __main__ -   Epoch: 37 | Batch: 7500/10001 (75%) | G Loss: 0.496272 | C Loss: 0.001772\n",
      "06/30/2022 13:27:11 - INFO - __main__ -   Text: [\"Students don't realize how much I hate to exercises with Ob Pebbles.\"]\n",
      "06/30/2022 13:27:13 - INFO - __main__ -   Epoch: 37 | Batch: 8000/10001 (80%) | G Loss: 0.451433 | C Loss: -0.129849\n",
      "06/30/2022 13:27:13 - INFO - __main__ -   Text: ['Some goaded in with various pearls.']\n",
      "06/30/2022 13:27:14 - INFO - __main__ -   Epoch: 37 | Batch: 8500/10001 (85%) | G Loss: 0.654444 | C Loss: -0.250681\n",
      "06/30/2022 13:27:14 - INFO - __main__ -   Text: ['Buried in my trousers...not a celebrity.\"']\n",
      "06/30/2022 13:27:16 - INFO - __main__ -   Epoch: 37 | Batch: 9000/10001 (90%) | G Loss: 0.575702 | C Loss: -0.294342\n",
      "06/30/2022 13:27:16 - INFO - __main__ -   Text: ['Isabel does not mind but the idea of vampires\".']\n",
      "06/30/2022 13:27:17 - INFO - __main__ -   Epoch: 37 | Batch: 9500/10001 (95%) | G Loss: 0.601891 | C Loss: -0.137862\n",
      "06/30/2022 13:27:17 - INFO - __main__ -   Text: ['Breast cancer can have profound negative effects on the person.']\n",
      "06/30/2022 13:27:19 - INFO - __main__ -   * (Train) Epoch: 37 | G Loss: 0.5745 | C Loss: -0.1507 | Updates G: 97 | Updates C: 903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:27:27 - INFO - __main__ -   Bleu-2:0.223 | B-Bleu-2:0.262\n",
      "06/30/2022 13:27:27 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_17.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4845954950614164\n",
      "Train file used is number 17\n",
      "../../yahoo/subdivided_large/train_17.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 38 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:30.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:39.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:49.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:58.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:08.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:17.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:27.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:37.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:46.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:56.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:06.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:16.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:25.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:35.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:45.\n",
      "\n",
      "  Average training loss discriminator: 0.017\n",
      "  Training epcoh took: 0:02:47\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:30:14 - INFO - __main__ -   Epoch: 38 | Batch: 0/10001 (0%) | G Loss: 0.475117 | C Loss: -0.091496\n",
      "06/30/2022 13:30:14 - INFO - __main__ -   Text: ['Von Googling produces results which are highly academic.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.445\n",
      "  Test Loss: 3.006\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:30:16 - INFO - __main__ -   Epoch: 38 | Batch: 500/10001 (5%) | G Loss: 0.518774 | C Loss: -0.058763\n",
      "06/30/2022 13:30:16 - INFO - __main__ -   Text: ['The repetition is annoying for me since his mouth is so broad.']\n",
      "06/30/2022 13:30:17 - INFO - __main__ -   Epoch: 38 | Batch: 1000/10001 (10%) | G Loss: 0.434291 | C Loss: -0.089263\n",
      "06/30/2022 13:30:17 - INFO - __main__ -   Text: ['This is a host of tricks used by all human beings.']\n",
      "06/30/2022 13:30:19 - INFO - __main__ -   Epoch: 38 | Batch: 1500/10001 (15%) | G Loss: 0.444056 | C Loss: -0.047257\n",
      "06/30/2022 13:30:19 - INFO - __main__ -   Text: [\"That's because this isn't your basic voice!\"]\n",
      "06/30/2022 13:30:20 - INFO - __main__ -   Epoch: 38 | Batch: 2000/10001 (20%) | G Loss: 0.703016 | C Loss: -0.208158\n",
      "06/30/2022 13:30:20 - INFO - __main__ -   Text: ['Those would sure be discussed... <PAD> if anything.']\n",
      "06/30/2022 13:30:22 - INFO - __main__ -   Epoch: 38 | Batch: 2500/10001 (25%) | G Loss: 0.547749 | C Loss: -0.147943\n",
      "06/30/2022 13:30:22 - INFO - __main__ -   Text: ['\"You actually know the obituary of successful teenagers.\"']\n",
      "06/30/2022 13:30:23 - INFO - __main__ -   Epoch: 38 | Batch: 3000/10001 (30%) | G Loss: 0.460166 | C Loss: -0.030628\n",
      "06/30/2022 13:30:23 - INFO - __main__ -   Text: ['Max is, quite simply, dangerous.\"']\n",
      "06/30/2022 13:30:25 - INFO - __main__ -   Epoch: 38 | Batch: 3500/10001 (35%) | G Loss: 0.424202 | C Loss: -0.074280\n",
      "06/30/2022 13:30:25 - INFO - __main__ -   Text: ['They let me know their choice of food is ketchup.']\n",
      "06/30/2022 13:30:26 - INFO - __main__ -   Epoch: 38 | Batch: 4000/10001 (40%) | G Loss: 0.693412 | C Loss: -0.224030\n",
      "06/30/2022 13:30:26 - INFO - __main__ -   Text: ['\"Rancid\" reports on how to get best from \"It\".']\n",
      "06/30/2022 13:30:28 - INFO - __main__ -   Epoch: 38 | Batch: 4500/10001 (45%) | G Loss: 0.932257 | C Loss: -0.355220\n",
      "06/30/2022 13:30:28 - INFO - __main__ -   Text: ['A human being shouts that, \"High day!\"']\n",
      "06/30/2022 13:30:29 - INFO - __main__ -   Epoch: 38 | Batch: 5000/10001 (50%) | G Loss: 0.425009 | C Loss: -0.026609\n",
      "06/30/2022 13:30:29 - INFO - __main__ -   Text: ['Daar, and not iVek.']\n",
      "06/30/2022 13:30:31 - INFO - __main__ -   Epoch: 38 | Batch: 5500/10001 (55%) | G Loss: 0.607132 | C Loss: -0.164055\n",
      "06/30/2022 13:30:31 - INFO - __main__ -   Text: ['Its goal is to make the \"fucker Jewish.\"']\n",
      "06/30/2022 13:30:32 - INFO - __main__ -   Epoch: 38 | Batch: 6000/10001 (60%) | G Loss: 0.763714 | C Loss: -0.164536\n",
      "06/30/2022 13:30:32 - INFO - __main__ -   Text: ['The story is very : Lethal is jealous because of his talent.']\n",
      "06/30/2022 13:30:34 - INFO - __main__ -   Epoch: 38 | Batch: 6500/10001 (65%) | G Loss: 0.839685 | C Loss: -0.097805\n",
      "06/30/2022 13:30:34 - INFO - __main__ -   Text: ['It is possible that Katherine loves pasta as much as Jennifer.']\n",
      "06/30/2022 13:30:35 - INFO - __main__ -   Epoch: 38 | Batch: 7000/10001 (70%) | G Loss: 1.868067 | C Loss: -0.327292\n",
      "06/30/2022 13:30:35 - INFO - __main__ -   Text: ['United States is a textbook for offroad adventure.']\n",
      "06/30/2022 13:30:37 - INFO - __main__ -   Epoch: 38 | Batch: 7500/10001 (75%) | G Loss: 0.837639 | C Loss: -0.546937\n",
      "06/30/2022 13:30:37 - INFO - __main__ -   Text: ['To the pupil he doesn it: sight may well be the work.']\n",
      "06/30/2022 13:30:38 - INFO - __main__ -   Epoch: 38 | Batch: 8000/10001 (80%) | G Loss: 0.502210 | C Loss: -0.110583\n",
      "06/30/2022 13:30:38 - INFO - __main__ -   Text: ['Other than that, Shealy hears about Average Average people.']\n",
      "06/30/2022 13:30:40 - INFO - __main__ -   Epoch: 38 | Batch: 8500/10001 (85%) | G Loss: 0.609919 | C Loss: -0.201778\n",
      "06/30/2022 13:30:40 - INFO - __main__ -   Text: [\"It's not that many English Brits prefer to read than Mimas.\"]\n",
      "06/30/2022 13:30:41 - INFO - __main__ -   Epoch: 38 | Batch: 9000/10001 (90%) | G Loss: 0.640773 | C Loss: -0.153872\n",
      "06/30/2022 13:30:42 - INFO - __main__ -   Text: ['This friendly company is very dangerous.']\n",
      "06/30/2022 13:30:43 - INFO - __main__ -   Epoch: 38 | Batch: 9500/10001 (95%) | G Loss: 0.413968 | C Loss: -0.075926\n",
      "06/30/2022 13:30:43 - INFO - __main__ -   Text: ['Mein Kampf is an enlightened man.']\n",
      "06/30/2022 13:30:44 - INFO - __main__ -   * (Train) Epoch: 38 | G Loss: 0.6598 | C Loss: -0.1617 | Updates G: 79 | Updates C: 921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:30:53 - INFO - __main__ -   Bleu-2:0.201 | B-Bleu-2:0.263\n",
      "06/30/2022 13:30:53 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_18.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4642586555209507\n",
      "Train file used is number 18\n",
      "../../yahoo/subdivided_large/train_18.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 39 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:19.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:29.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:38.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:48.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:58.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:07.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:17.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:27.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:37.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:47.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:57.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:06.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:16.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:26.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:35.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:45.\n",
      "\n",
      "  Average training loss discriminator: 0.022\n",
      "  Training epcoh took: 0:02:47\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:33:40 - INFO - __main__ -   Epoch: 39 | Batch: 0/10001 (0%) | G Loss: 0.433102 | C Loss: -0.169137\n",
      "06/30/2022 13:33:40 - INFO - __main__ -   Text: ['He is the CEO of Measure.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.435\n",
      "  Test Loss: 3.059\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:33:42 - INFO - __main__ -   Epoch: 39 | Batch: 500/10001 (5%) | G Loss: 0.446216 | C Loss: -0.069446\n",
      "06/30/2022 13:33:42 - INFO - __main__ -   Text: ['!']\n",
      "06/30/2022 13:33:43 - INFO - __main__ -   Epoch: 39 | Batch: 1000/10001 (10%) | G Loss: 0.762646 | C Loss: -0.022844\n",
      "06/30/2022 13:33:43 - INFO - __main__ -   Text: ['Any pronoun that looks like this or similar will be mistaken.']\n",
      "06/30/2022 13:33:45 - INFO - __main__ -   Epoch: 39 | Batch: 1500/10001 (15%) | G Loss: 0.889086 | C Loss: -0.058861\n",
      "06/30/2022 13:33:45 - INFO - __main__ -   Text: ['There is little doubt in my mind that Bender is similar to me.']\n",
      "06/30/2022 13:33:46 - INFO - __main__ -   Epoch: 39 | Batch: 2000/10001 (20%) | G Loss: 0.905911 | C Loss: -0.304974\n",
      "06/30/2022 13:33:46 - INFO - __main__ -   Text: ['Another name for rum being \"wolfberry\".']\n",
      "06/30/2022 13:33:48 - INFO - __main__ -   Epoch: 39 | Batch: 2500/10001 (25%) | G Loss: 0.791787 | C Loss: -0.296698\n",
      "06/30/2022 13:33:48 - INFO - __main__ -   Text: ['As a beauty guide, it is impossible to figure out the people.']\n",
      "06/30/2022 13:33:49 - INFO - __main__ -   Epoch: 39 | Batch: 3000/10001 (30%) | G Loss: 0.753557 | C Loss: 0.078673\n",
      "06/30/2022 13:33:49 - INFO - __main__ -   Text: ['Throughout the lengthsomeness, Hoof is referred to as a beacon.']\n",
      "06/30/2022 13:33:50 - INFO - __main__ -   Epoch: 39 | Batch: 3500/10001 (35%) | G Loss: 0.922567 | C Loss: 0.011689\n",
      "06/30/2022 13:33:50 - INFO - __main__ -   Text: ['The reality on meteorology is that the OI.']\n",
      "06/30/2022 13:33:52 - INFO - __main__ -   Epoch: 39 | Batch: 4000/10001 (40%) | G Loss: 0.533759 | C Loss: -0.209187\n",
      "06/30/2022 13:33:52 - INFO - __main__ -   Text: ['Actually, I joke surely that...\"']\n",
      "06/30/2022 13:33:53 - INFO - __main__ -   Epoch: 39 | Batch: 4500/10001 (45%) | G Loss: 0.264909 | C Loss: -0.122669\n",
      "06/30/2022 13:33:53 - INFO - __main__ -   Text: ['It gives a high percentage of the test.']\n",
      "06/30/2022 13:33:54 - INFO - __main__ -   Epoch: 39 | Batch: 5000/10001 (50%) | G Loss: 0.334809 | C Loss: -0.065345\n",
      "06/30/2022 13:33:54 - INFO - __main__ -   Text: ['But diffidently, Google\"\".']\n",
      "06/30/2022 13:33:55 - INFO - __main__ -   Epoch: 39 | Batch: 5500/10001 (55%) | G Loss: 0.412065 | C Loss: -0.151386\n",
      "06/30/2022 13:33:55 - INFO - __main__ -   Text: [\"He'll tell secrets about us but most people know him as Alan.\"]\n",
      "06/30/2022 13:33:57 - INFO - __main__ -   Epoch: 39 | Batch: 6000/10001 (60%) | G Loss: 0.716870 | C Loss: -0.179815\n",
      "06/30/2022 13:33:57 - INFO - __main__ -   Text: ['The results of programming are sometimes shirking.']\n",
      "06/30/2022 13:33:58 - INFO - __main__ -   Epoch: 39 | Batch: 6500/10001 (65%) | G Loss: 0.684711 | C Loss: -0.104499\n",
      "06/30/2022 13:33:58 - INFO - __main__ -   Text: [\"Unlike many logicians, it doesn't jargon or `do.'\"]\n",
      "06/30/2022 13:33:59 - INFO - __main__ -   Epoch: 39 | Batch: 7000/10001 (70%) | G Loss: 1.015688 | C Loss: 0.178151\n",
      "06/30/2022 13:34:00 - INFO - __main__ -   Text: [\"There's a certain kind of magic because a person's fame does not extend yet.\"]\n",
      "06/30/2022 13:34:01 - INFO - __main__ -   Epoch: 39 | Batch: 7500/10001 (75%) | G Loss: 1.460370 | C Loss: -0.070711\n",
      "06/30/2022 13:34:01 - INFO - __main__ -   Text: ['It is anal for anal sex.']\n",
      "06/30/2022 13:34:02 - INFO - __main__ -   Epoch: 39 | Batch: 8000/10001 (80%) | G Loss: 2.224241 | C Loss: -0.252876\n",
      "06/30/2022 13:34:03 - INFO - __main__ -   Text: ['The gaffe grows it out of his or her belief system.']\n",
      "06/30/2022 13:34:04 - INFO - __main__ -   Epoch: 39 | Batch: 8500/10001 (85%) | G Loss: 2.823795 | C Loss: -0.446697\n",
      "06/30/2022 13:34:04 - INFO - __main__ -   Text: [\"It's an indicator of your athletic prowess.\"]\n",
      "06/30/2022 13:34:06 - INFO - __main__ -   Epoch: 39 | Batch: 9000/10001 (90%) | G Loss: 2.785504 | C Loss: -0.162311\n",
      "06/30/2022 13:34:06 - INFO - __main__ -   Text: ['They can use the methods published by alumnus, Golden Eagle.']\n",
      "06/30/2022 13:34:07 - INFO - __main__ -   Epoch: 39 | Batch: 9500/10001 (95%) | G Loss: 1.296652 | C Loss: -0.146357\n",
      "06/30/2022 13:34:07 - INFO - __main__ -   Text: ['This is what Michael is calling advice.']\n",
      "06/30/2022 13:34:09 - INFO - __main__ -   * (Train) Epoch: 39 | G Loss: 0.9657 | C Loss: -0.1953 | Updates G: 31 | Updates C: 969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:34:17 - INFO - __main__ -   Bleu-2:0.226 | B-Bleu-2:0.286\n",
      "06/30/2022 13:34:17 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_19.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5112790783969992\n",
      "Train file used is number 19\n",
      "../../yahoo/subdivided_large/train_19.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 40 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:30.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:41.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:52.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:02.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:12.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:22.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:32.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:42.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:52.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:02.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:12.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:22.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:32.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:42.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:52.\n",
      "\n",
      "  Average training loss discriminator: 0.018\n",
      "  Training epcoh took: 0:02:54\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:37:11 - INFO - __main__ -   Epoch: 40 | Batch: 0/10001 (0%) | G Loss: 1.628981 | C Loss: -0.161820\n",
      "06/30/2022 13:37:11 - INFO - __main__ -   Text: ['It is similar to the Korean Game of Thrones.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.440\n",
      "  Test Loss: 3.044\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:37:13 - INFO - __main__ -   Epoch: 40 | Batch: 500/10001 (5%) | G Loss: 2.237700 | C Loss: -0.427098\n",
      "06/30/2022 13:37:13 - INFO - __main__ -   Text: ['Since breathing can put anyone near water, Tutore recommends \"Human Breathing.\"']\n",
      "06/30/2022 13:37:14 - INFO - __main__ -   Epoch: 40 | Batch: 1000/10001 (10%) | G Loss: 2.417579 | C Loss: -0.308339\n",
      "06/30/2022 13:37:14 - INFO - __main__ -   Text: ['Sometimes I\\'m actually being asked to try Fox bugs?\"']\n",
      "06/30/2022 13:37:16 - INFO - __main__ -   Epoch: 40 | Batch: 1500/10001 (15%) | G Loss: 2.371376 | C Loss: -0.719986\n",
      "06/30/2022 13:37:16 - INFO - __main__ -   Text: ['The woman who reads this cannot spell \"Tai chi\".']\n",
      "06/30/2022 13:37:17 - INFO - __main__ -   Epoch: 40 | Batch: 2000/10001 (20%) | G Loss: 2.702002 | C Loss: -0.757358\n",
      "06/30/2022 13:37:17 - INFO - __main__ -   Text: ['30 kg l is too much.\"\"']\n",
      "06/30/2022 13:37:19 - INFO - __main__ -   Epoch: 40 | Batch: 2500/10001 (25%) | G Loss: 3.200760 | C Loss: -0.313271\n",
      "06/30/2022 13:37:19 - INFO - __main__ -   Text: ['spaces hangar matter?']\n",
      "06/30/2022 13:37:20 - INFO - __main__ -   Epoch: 40 | Batch: 3000/10001 (30%) | G Loss: 3.359646 | C Loss: -0.882893\n",
      "06/30/2022 13:37:20 - INFO - __main__ -   Text: ['\", wrote Carlee.']\n",
      "06/30/2022 13:37:22 - INFO - __main__ -   Epoch: 40 | Batch: 3500/10001 (35%) | G Loss: 3.451636 | C Loss: -0.944502\n",
      "06/30/2022 13:37:22 - INFO - __main__ -   Text: ['The hippie is described as \"the ultimate pickpocket\".']\n",
      "06/30/2022 13:37:23 - INFO - __main__ -   Epoch: 40 | Batch: 4000/10001 (40%) | G Loss: 2.999702 | C Loss: -0.947240\n",
      "06/30/2022 13:37:23 - INFO - __main__ -   Text: ['Finally, it actually involves science writing!']\n",
      "06/30/2022 13:37:25 - INFO - __main__ -   Epoch: 40 | Batch: 4500/10001 (45%) | G Loss: 3.057016 | C Loss: -1.036634\n",
      "06/30/2022 13:37:25 - INFO - __main__ -   Text: ['Gau instead questions that of mono.']\n",
      "06/30/2022 13:37:26 - INFO - __main__ -   Epoch: 40 | Batch: 5000/10001 (50%) | G Loss: 3.146682 | C Loss: -1.095090\n",
      "06/30/2022 13:37:26 - INFO - __main__ -   Text: [\"There's a historical thing going on here.''\"]\n",
      "06/30/2022 13:37:28 - INFO - __main__ -   Epoch: 40 | Batch: 5500/10001 (55%) | G Loss: 2.794891 | C Loss: -1.203336\n",
      "06/30/2022 13:37:28 - INFO - __main__ -   Text: ['You can\\'t gainge\\'s level very much once one is eaten.\"']\n",
      "06/30/2022 13:37:29 - INFO - __main__ -   Epoch: 40 | Batch: 6000/10001 (60%) | G Loss: 2.763821 | C Loss: -0.965328\n",
      "06/30/2022 13:37:29 - INFO - __main__ -   Text: ['Sticks teeth to change the subject.']\n",
      "06/30/2022 13:37:31 - INFO - __main__ -   Epoch: 40 | Batch: 6500/10001 (65%) | G Loss: 2.981546 | C Loss: -1.804840\n",
      "06/30/2022 13:37:31 - INFO - __main__ -   Text: [\"There's no lens of truth here.\"]\n",
      "06/30/2022 13:37:32 - INFO - __main__ -   Epoch: 40 | Batch: 7000/10001 (70%) | G Loss: 2.928471 | C Loss: -1.130645\n",
      "06/30/2022 13:37:33 - INFO - __main__ -   Text: ['Yes, worse...aurrow viscid\")']\n",
      "06/30/2022 13:37:34 - INFO - __main__ -   Epoch: 40 | Batch: 7500/10001 (75%) | G Loss: 2.789683 | C Loss: -1.446334\n",
      "06/30/2022 13:37:34 - INFO - __main__ -   Text: [\"The gameplay is felt accurately by the It's Nothing!\"]\n",
      "06/30/2022 13:37:35 - INFO - __main__ -   Epoch: 40 | Batch: 8000/10001 (80%) | G Loss: 2.636698 | C Loss: -1.404483\n",
      "06/30/2022 13:37:36 - INFO - __main__ -   Text: ['Hence all engineering types have identical \"Hickey arrow\".']\n",
      "06/30/2022 13:37:37 - INFO - __main__ -   Epoch: 40 | Batch: 8500/10001 (85%) | G Loss: 2.404262 | C Loss: -1.608779\n",
      "06/30/2022 13:37:37 - INFO - __main__ -   Text: ['They automatically attack anyone who shouts \"Urdu\".']\n",
      "06/30/2022 13:37:38 - INFO - __main__ -   Epoch: 40 | Batch: 9000/10001 (90%) | G Loss: 2.240269 | C Loss: -2.119392\n",
      "06/30/2022 13:37:39 - INFO - __main__ -   Text: ['The name itself \"Weird Explorers\".']\n",
      "06/30/2022 13:37:40 - INFO - __main__ -   Epoch: 40 | Batch: 9500/10001 (95%) | G Loss: 2.578728 | C Loss: -1.807220\n",
      "06/30/2022 13:37:40 - INFO - __main__ -   Text: ['The legal definition is \"legalism way to describe me\".']\n",
      "06/30/2022 13:37:41 - INFO - __main__ -   * (Train) Epoch: 40 | G Loss: 2.6900 | C Loss: -1.1138 | Updates G: 56 | Updates C: 944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:37:48 - INFO - __main__ -   Bleu-2:0.205 | B-Bleu-2:0.226\n",
      "06/30/2022 13:37:48 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_20.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4315221251058279\n",
      "Train file used is number 20\n",
      "../../yahoo/subdivided_large/train_20.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 41 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:09.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:17.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:26.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:35.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:44.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:53.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:02.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:11.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:19.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:28.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:37.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:46.\n",
      "  Batch   130  of    172.    Elapsed: 0:01:55.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:03.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:12.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:21.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:30.\n",
      "\n",
      "  Average training loss discriminator: 0.018\n",
      "  Training epcoh took: 0:02:31\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:40:20 - INFO - __main__ -   Epoch: 41 | Batch: 0/10001 (0%) | G Loss: 2.427918 | C Loss: -1.970537\n",
      "06/30/2022 13:40:20 - INFO - __main__ -   Text: ['This brings you a totally different type of guru.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.448\n",
      "  Test Loss: 3.037\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:40:22 - INFO - __main__ -   Epoch: 41 | Batch: 500/10001 (5%) | G Loss: 2.513637 | C Loss: -1.421632\n",
      "06/30/2022 13:40:22 - INFO - __main__ -   Text: ['Tsantino has some very good advice for how to do comedy.\"']\n",
      "06/30/2022 13:40:23 - INFO - __main__ -   Epoch: 41 | Batch: 1000/10001 (10%) | G Loss: 2.494159 | C Loss: -2.061515\n",
      "06/30/2022 13:40:23 - INFO - __main__ -   Text: ['Such as Huazee status or piracy.']\n",
      "06/30/2022 13:40:25 - INFO - __main__ -   Epoch: 41 | Batch: 1500/10001 (15%) | G Loss: 2.251801 | C Loss: -1.720741\n",
      "06/30/2022 13:40:25 - INFO - __main__ -   Text: ['Retailers who want to sell physical goods have to obtain an audit.']\n",
      "06/30/2022 13:40:26 - INFO - __main__ -   Epoch: 41 | Batch: 2000/10001 (20%) | G Loss: 2.257002 | C Loss: -1.352914\n",
      "06/30/2022 13:40:26 - INFO - __main__ -   Text: ['So gets \"walking dogs\".']\n",
      "06/30/2022 13:40:28 - INFO - __main__ -   Epoch: 41 | Batch: 2500/10001 (25%) | G Loss: 2.164262 | C Loss: -1.587278\n",
      "06/30/2022 13:40:28 - INFO - __main__ -   Text: ['The show is about loving humans and playing nice with the sapient.']\n",
      "06/30/2022 13:40:29 - INFO - __main__ -   Epoch: 41 | Batch: 3000/10001 (30%) | G Loss: 2.131646 | C Loss: -1.545433\n",
      "06/30/2022 13:40:29 - INFO - __main__ -   Text: ['Any positive comment is advice on how to get good grades.']\n",
      "06/30/2022 13:40:31 - INFO - __main__ -   Epoch: 41 | Batch: 3500/10001 (35%) | G Loss: 2.337349 | C Loss: -1.642061\n",
      "06/30/2022 13:40:31 - INFO - __main__ -   Text: ['The problem is that people are not educated about comedy in real life.']\n",
      "06/30/2022 13:40:32 - INFO - __main__ -   Epoch: 41 | Batch: 4000/10001 (40%) | G Loss: 2.329679 | C Loss: -1.933563\n",
      "06/30/2022 13:40:32 - INFO - __main__ -   Text: ['The most amazing kind is the sex-racial instinct.']\n",
      "06/30/2022 13:40:34 - INFO - __main__ -   Epoch: 41 | Batch: 4500/10001 (45%) | G Loss: 2.036341 | C Loss: -1.991812\n",
      "06/30/2022 13:40:34 - INFO - __main__ -   Text: ['That lot like \"Birds of Prey\".']\n",
      "06/30/2022 13:40:35 - INFO - __main__ -   Epoch: 41 | Batch: 5000/10001 (50%) | G Loss: 1.765494 | C Loss: -2.150714\n",
      "06/30/2022 13:40:35 - INFO - __main__ -   Text: ['The player is also rewarded handsomely for making someone laugh.']\n",
      "06/30/2022 13:40:37 - INFO - __main__ -   Epoch: 41 | Batch: 5500/10001 (55%) | G Loss: 2.121933 | C Loss: -1.619998\n",
      "06/30/2022 13:40:37 - INFO - __main__ -   Text: ['Switching people to a certain cyberpunk is illegal.']\n",
      "06/30/2022 13:40:38 - INFO - __main__ -   Epoch: 41 | Batch: 6000/10001 (60%) | G Loss: 1.778793 | C Loss: -1.834068\n",
      "06/30/2022 13:40:38 - INFO - __main__ -   Text: ['He states that the snowboard is \"\"screwed up by the stupid love game.\"']\n",
      "06/30/2022 13:40:40 - INFO - __main__ -   Epoch: 41 | Batch: 6500/10001 (65%) | G Loss: 1.955278 | C Loss: -2.121893\n",
      "06/30/2022 13:40:40 - INFO - __main__ -   Text: ['In sum, be mindful.']\n",
      "06/30/2022 13:40:41 - INFO - __main__ -   Epoch: 41 | Batch: 7000/10001 (70%) | G Loss: 1.999644 | C Loss: -2.105973\n",
      "06/30/2022 13:40:41 - INFO - __main__ -   Text: ['\"Biozine Cure\" is often used.']\n",
      "06/30/2022 13:40:43 - INFO - __main__ -   Epoch: 41 | Batch: 7500/10001 (75%) | G Loss: 1.740645 | C Loss: -1.753098\n",
      "06/30/2022 13:40:43 - INFO - __main__ -   Text: ['Lord of the Rings is quite different from spellbinding.']\n",
      "06/30/2022 13:40:44 - INFO - __main__ -   Epoch: 41 | Batch: 8000/10001 (80%) | G Loss: 1.843991 | C Loss: -2.140498\n",
      "06/30/2022 13:40:44 - INFO - __main__ -   Text: ['\", if things go wrong like our newspaper enables.']\n",
      "06/30/2022 13:40:46 - INFO - __main__ -   Epoch: 41 | Batch: 8500/10001 (85%) | G Loss: 1.889146 | C Loss: -1.745986\n",
      "06/30/2022 13:40:46 - INFO - __main__ -   Text: [\"Goose's attitude is outrageous because it should make someone dumb.\"]\n",
      "06/30/2022 13:40:47 - INFO - __main__ -   Epoch: 41 | Batch: 9000/10001 (90%) | G Loss: 1.813547 | C Loss: -1.871085\n",
      "06/30/2022 13:40:47 - INFO - __main__ -   Text: [\"One of Pablo's tricks is to burst a ice boot in.\"]\n",
      "06/30/2022 13:40:48 - INFO - __main__ -   Epoch: 41 | Batch: 9500/10001 (95%) | G Loss: 1.951237 | C Loss: -1.798417\n",
      "06/30/2022 13:40:49 - INFO - __main__ -   Text: ['It is perhaps very useful for deciding the name for your drug of choice.']\n",
      "06/30/2022 13:40:50 - INFO - __main__ -   * (Train) Epoch: 41 | G Loss: 2.0081 | C Loss: -1.8780 | Updates G: 113 | Updates C: 887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:40:58 - INFO - __main__ -   Bleu-2:0.224 | B-Bleu-2:0.295\n",
      "06/30/2022 13:40:58 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_1.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.518720332805208\n",
      "Train file used is number 1\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 42 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:30.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:40.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:49.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:59.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:09.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:19.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:30.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:39.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:49.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:59.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:08.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:18.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:28.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:38.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:48.\n",
      "\n",
      "  Average training loss discriminator: 0.016\n",
      "  Training epcoh took: 0:02:49\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:43:48 - INFO - __main__ -   Epoch: 42 | Batch: 0/10000 (0%) | G Loss: 2.026318 | C Loss: -1.843926\n",
      "06/30/2022 13:43:48 - INFO - __main__ -   Text: ['On top of that stupid Steve calls it \"Maharbus\".']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.427\n",
      "  Test Loss: 3.135\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:43:49 - INFO - __main__ -   Epoch: 42 | Batch: 500/10000 (5%) | G Loss: 1.888256 | C Loss: -1.672243\n",
      "06/30/2022 13:43:50 - INFO - __main__ -   Text: ['\"If you want to wear anything, go to this website\".']\n",
      "06/30/2022 13:43:51 - INFO - __main__ -   Epoch: 42 | Batch: 1000/10000 (10%) | G Loss: 1.655943 | C Loss: -1.795548\n",
      "06/30/2022 13:43:51 - INFO - __main__ -   Text: ['People usually say that cheating is the cleanest method of life.']\n",
      "06/30/2022 13:43:53 - INFO - __main__ -   Epoch: 42 | Batch: 1500/10000 (15%) | G Loss: 1.932884 | C Loss: -1.951597\n",
      "06/30/2022 13:43:53 - INFO - __main__ -   Text: ['Aside from being sentimental, I wish the name felt relaxed.\"']\n",
      "06/30/2022 13:43:54 - INFO - __main__ -   Epoch: 42 | Batch: 2000/10000 (20%) | G Loss: 1.697640 | C Loss: -2.410464\n",
      "06/30/2022 13:43:54 - INFO - __main__ -   Text: ['This keeps the characters warm and hot.']\n",
      "06/30/2022 13:43:55 - INFO - __main__ -   Epoch: 42 | Batch: 2500/10000 (25%) | G Loss: 1.672974 | C Loss: -1.860689\n",
      "06/30/2022 13:43:56 - INFO - __main__ -   Text: ['It\\'s called retardating the mind.\"']\n",
      "06/30/2022 13:43:57 - INFO - __main__ -   Epoch: 42 | Batch: 3000/10000 (30%) | G Loss: 1.856822 | C Loss: -2.564406\n",
      "06/30/2022 13:43:57 - INFO - __main__ -   Text: [\"Hanson's research is theoretically linked to Roddenberry's scale.\"]\n",
      "06/30/2022 13:43:58 - INFO - __main__ -   Epoch: 42 | Batch: 3500/10000 (35%) | G Loss: 2.146051 | C Loss: -2.194504\n",
      "06/30/2022 13:43:59 - INFO - __main__ -   Text: ['There is also the name \"High risk orophies\".']\n",
      "06/30/2022 13:44:00 - INFO - __main__ -   Epoch: 42 | Batch: 4000/10000 (40%) | G Loss: 1.778985 | C Loss: -1.756418\n",
      "06/30/2022 13:44:00 - INFO - __main__ -   Text: ['Longer words is driving the trend... Code for sidesteps\".']\n",
      "06/30/2022 13:44:01 - INFO - __main__ -   Epoch: 42 | Batch: 4500/10000 (45%) | G Loss: 1.639504 | C Loss: -1.688066\n",
      "06/30/2022 13:44:02 - INFO - __main__ -   Text: ['Can you guess the attraction of the vampire from tea?']\n",
      "06/30/2022 13:44:03 - INFO - __main__ -   Epoch: 42 | Batch: 5000/10000 (50%) | G Loss: 1.745285 | C Loss: -2.111161\n",
      "06/30/2022 13:44:03 - INFO - __main__ -   Text: ['It is not possible to have a clear or topical vocabulary.']\n",
      "06/30/2022 13:44:04 - INFO - __main__ -   Epoch: 42 | Batch: 5500/10000 (55%) | G Loss: 1.799091 | C Loss: -1.886148\n",
      "06/30/2022 13:44:04 - INFO - __main__ -   Text: ['Academic honesty is merely to airline this book.\"']\n",
      "06/30/2022 13:44:06 - INFO - __main__ -   Epoch: 42 | Batch: 6000/10000 (60%) | G Loss: 1.641696 | C Loss: -1.568487\n",
      "06/30/2022 13:44:06 - INFO - __main__ -   Text: ['But, what do you think about 140 metres?']\n",
      "06/30/2022 13:44:07 - INFO - __main__ -   Epoch: 42 | Batch: 6500/10000 (65%) | G Loss: 1.573515 | C Loss: -1.895756\n",
      "06/30/2022 13:44:07 - INFO - __main__ -   Text: ['This is also why Guru Nan argues that JavaScript is the magical beast.']\n",
      "06/30/2022 13:44:09 - INFO - __main__ -   Epoch: 42 | Batch: 7000/10000 (70%) | G Loss: 1.429378 | C Loss: -2.110997\n",
      "06/30/2022 13:44:09 - INFO - __main__ -   Text: ['Anthony tells you that it comes from \"diyashki\".']\n",
      "06/30/2022 13:44:10 - INFO - __main__ -   Epoch: 42 | Batch: 7500/10000 (75%) | G Loss: 1.228369 | C Loss: -1.811498\n",
      "06/30/2022 13:44:10 - INFO - __main__ -   Text: ['With so many skeptics the answer is not \"world\".']\n",
      "06/30/2022 13:44:12 - INFO - __main__ -   Epoch: 42 | Batch: 8000/10000 (80%) | G Loss: 1.567046 | C Loss: -1.859519\n",
      "06/30/2022 13:44:12 - INFO - __main__ -   Text: ['Demonology for words refers to the numbers literally.']\n",
      "06/30/2022 13:44:13 - INFO - __main__ -   Epoch: 42 | Batch: 8500/10000 (85%) | G Loss: 1.692222 | C Loss: -1.857917\n",
      "06/30/2022 13:44:13 - INFO - __main__ -   Text: ['Technological problems are called geology and this is the first time we have ever seen so.']\n",
      "06/30/2022 13:44:15 - INFO - __main__ -   Epoch: 42 | Batch: 9000/10000 (90%) | G Loss: 1.762843 | C Loss: -2.172100\n",
      "06/30/2022 13:44:15 - INFO - __main__ -   Text: ['Handbook Book 2 also contains a manual for coping with fluctuating currents.']\n",
      "06/30/2022 13:44:16 - INFO - __main__ -   Epoch: 42 | Batch: 9500/10000 (95%) | G Loss: 1.648112 | C Loss: -2.298653\n",
      "06/30/2022 13:44:16 - INFO - __main__ -   Text: ['Remote remote sensing is the name that you give to strangers.']\n",
      "06/30/2022 13:44:18 - INFO - __main__ -   * (Train) Epoch: 42 | G Loss: 1.6331 | C Loss: -1.9991 | Updates G: 124 | Updates C: 876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:44:24 - INFO - __main__ -   Bleu-2:0.219 | B-Bleu-2:0.261\n",
      "06/30/2022 13:44:24 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_1.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4804454406767598\n",
      "Train file used is number 1\n",
      "../../yahoo/subdivided_large/train_1.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 43 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:08.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:16.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:24.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:32.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:40.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:48.\n",
      "  Batch    70  of    172.    Elapsed: 0:00:56.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:04.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:12.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:20.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:29.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:36.\n",
      "  Batch   130  of    172.    Elapsed: 0:01:44.\n",
      "  Batch   140  of    172.    Elapsed: 0:01:52.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:00.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:08.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:16.\n",
      "\n",
      "  Average training loss discriminator: 0.014\n",
      "  Training epcoh took: 0:02:17\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:46:42 - INFO - __main__ -   Epoch: 43 | Batch: 0/10000 (0%) | G Loss: 1.095367 | C Loss: -1.686700\n",
      "06/30/2022 13:46:42 - INFO - __main__ -   Text: ['Good foods are very powerful poison herbs.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.440\n",
      "  Test Loss: 3.045\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:46:43 - INFO - __main__ -   Epoch: 43 | Batch: 500/10000 (5%) | G Loss: 1.353992 | C Loss: -1.623532\n",
      "06/30/2022 13:46:43 - INFO - __main__ -   Text: ['The more you mumble and the more you are a relative, the better your score.']\n",
      "06/30/2022 13:46:45 - INFO - __main__ -   Epoch: 43 | Batch: 1000/10000 (10%) | G Loss: 1.709718 | C Loss: -1.720174\n",
      "06/30/2022 13:46:45 - INFO - __main__ -   Text: ['Using this method will help merge the Zodiac into your brain.']\n",
      "06/30/2022 13:46:46 - INFO - __main__ -   Epoch: 43 | Batch: 1500/10000 (15%) | G Loss: 2.519213 | C Loss: -2.890452\n",
      "06/30/2022 13:46:46 - INFO - __main__ -   Text: ['It gives almost any Dwarf a swampy interpretation.']\n",
      "06/30/2022 13:46:48 - INFO - __main__ -   Epoch: 43 | Batch: 2000/10000 (20%) | G Loss: 1.527811 | C Loss: -1.712034\n",
      "06/30/2022 13:46:48 - INFO - __main__ -   Text: ['Following science, the olding is placing a plate on its belly.']\n",
      "06/30/2022 13:46:49 - INFO - __main__ -   Epoch: 43 | Batch: 2500/10000 (25%) | G Loss: 1.197436 | C Loss: -1.945088\n",
      "06/30/2022 13:46:49 - INFO - __main__ -   Text: ['There is Decadence UP as well as The Punisher who are amongst the best ones.']\n",
      "06/30/2022 13:46:50 - INFO - __main__ -   Epoch: 43 | Batch: 3000/10000 (30%) | G Loss: 1.377964 | C Loss: -1.891832\n",
      "06/30/2022 13:46:51 - INFO - __main__ -   Text: ['\"It\\'s actually theeness of life.\"']\n",
      "06/30/2022 13:46:52 - INFO - __main__ -   Epoch: 43 | Batch: 3500/10000 (35%) | G Loss: 1.367077 | C Loss: -1.528543\n",
      "06/30/2022 13:46:52 - INFO - __main__ -   Text: ['While reading Persssigg\\'s poem, he begins \"Imb .\"']\n",
      "06/30/2022 13:46:53 - INFO - __main__ -   Epoch: 43 | Batch: 4000/10000 (40%) | G Loss: 1.839805 | C Loss: -1.867904\n",
      "06/30/2022 13:46:53 - INFO - __main__ -   Text: ['God Often Kindly Will.']\n",
      "06/30/2022 13:46:55 - INFO - __main__ -   Epoch: 43 | Batch: 4500/10000 (45%) | G Loss: 1.637834 | C Loss: -1.828755\n",
      "06/30/2022 13:46:55 - INFO - __main__ -   Text: ['Maybe it is false algebra by which we measured time.']\n",
      "06/30/2022 13:46:56 - INFO - __main__ -   Epoch: 43 | Batch: 5000/10000 (50%) | G Loss: 0.836023 | C Loss: -1.553998\n",
      "06/30/2022 13:46:56 - INFO - __main__ -   Text: ['An atheist, however, will not have faith.']\n",
      "06/30/2022 13:46:57 - INFO - __main__ -   Epoch: 43 | Batch: 5500/10000 (55%) | G Loss: 3.205086 | C Loss: -3.630013\n",
      "06/30/2022 13:46:57 - INFO - __main__ -   Text: [\"This little me isn't enough; I have to get bandied about with my men.\"]\n",
      "06/30/2022 13:46:59 - INFO - __main__ -   Epoch: 43 | Batch: 6000/10000 (60%) | G Loss: 6.438175 | C Loss: -1.272192\n",
      "06/30/2022 13:46:59 - INFO - __main__ -   Text: ['Screenstops and bookkeeping algorithms of Emacs.']\n",
      "06/30/2022 13:47:00 - INFO - __main__ -   Epoch: 43 | Batch: 6500/10000 (65%) | G Loss: -0.609471 | C Loss: -1.119686\n",
      "06/30/2022 13:47:00 - INFO - __main__ -   Text: ['This girl thinks she can sing and rap right now!']\n",
      "06/30/2022 13:47:01 - INFO - __main__ -   Epoch: 43 | Batch: 7000/10000 (70%) | G Loss: 2.654923 | C Loss: -1.841382\n",
      "06/30/2022 13:47:01 - INFO - __main__ -   Text: ['Noddle and shove from pulling.']\n",
      "06/30/2022 13:47:03 - INFO - __main__ -   Epoch: 43 | Batch: 7500/10000 (75%) | G Loss: 1.307760 | C Loss: -1.443030\n",
      "06/30/2022 13:47:03 - INFO - __main__ -   Text: ['BreathArran can also detect speech.']\n",
      "06/30/2022 13:47:04 - INFO - __main__ -   Epoch: 43 | Batch: 8000/10000 (80%) | G Loss: -0.127837 | C Loss: -0.858726\n",
      "06/30/2022 13:47:04 - INFO - __main__ -   Text: ['There is a link between him hurling and me!']\n",
      "06/30/2022 13:47:05 - INFO - __main__ -   Epoch: 43 | Batch: 8500/10000 (85%) | G Loss: 0.042449 | C Loss: -0.894157\n",
      "06/30/2022 13:47:05 - INFO - __main__ -   Text: ['This is where his sense of community comes from.']\n",
      "06/30/2022 13:47:06 - INFO - __main__ -   Epoch: 43 | Batch: 9000/10000 (90%) | G Loss: -2.474772 | C Loss: 6.096225\n",
      "06/30/2022 13:47:07 - INFO - __main__ -   Text: ['He He Chuck He funding team He Scott Chinese says Hand Championship Premier H working Trust Terry Corporate Baker B 1986..']\n",
      "06/30/2022 13:47:08 - INFO - __main__ -   Epoch: 43 | Batch: 9500/10000 (95%) | G Loss: -0.439862 | C Loss: -2.007766\n",
      "06/30/2022 13:47:08 - INFO - __main__ -   Text: [' Save the Earth, Navigate the oceans.']\n",
      "06/30/2022 13:47:09 - INFO - __main__ -   * (Train) Epoch: 43 | G Loss: 1.4691 | C Loss: -2.0623 | Updates G: 341 | Updates C: 659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:47:18 - INFO - __main__ -   Bleu-2:0.188 | B-Bleu-2:0.242\n",
      "06/30/2022 13:47:18 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_2.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43088980478833416\n",
      "Train file used is number 2\n",
      "../../yahoo/subdivided_large/train_2.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 44 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:09.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:19.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:28.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:37.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:46.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:56.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:05.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:15.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:24.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:32.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:42.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:51.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:00.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:10.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:19.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:28.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:37.\n",
      "\n",
      "  Average training loss discriminator: 0.021\n",
      "  Training epcoh took: 0:02:39\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:49:57 - INFO - __main__ -   Epoch: 44 | Batch: 0/10001 (0%) | G Loss: 2.794986 | C Loss: -0.877853\n",
      "06/30/2022 13:49:57 - INFO - __main__ -   Text: ['Medium Predictive plays mind.\" <PAD> Predictive is very low :)']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.427\n",
      "  Test Loss: 2.986\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:49:58 - INFO - __main__ -   Epoch: 44 | Batch: 500/10001 (5%) | G Loss: 4.101933 | C Loss: -2.033723\n",
      "06/30/2022 13:49:58 - INFO - __main__ -   Text: ['Many versions of \"Moneyball\" exist.']\n",
      "06/30/2022 13:49:59 - INFO - __main__ -   Epoch: 44 | Batch: 1000/10001 (10%) | G Loss: 0.777700 | C Loss: -0.394079\n",
      "06/30/2022 13:49:59 - INFO - __main__ -   Text: ['Siri sings this all the time, about unintelligence.']\n",
      "06/30/2022 13:49:59 - INFO - __main__ -   Epoch: 44 | Batch: 1500/10001 (15%) | G Loss: -1.759296 | C Loss: 1.360982\n",
      "06/30/2022 13:49:59 - INFO - __main__ -   Text: ['Expert name:']\n",
      "06/30/2022 13:50:00 - INFO - __main__ -   Epoch: 44 | Batch: 2000/10001 (20%) | G Loss: 9.154856 | C Loss: -4.825786\n",
      "06/30/2022 13:50:00 - INFO - __main__ -   Text: ['Medpacita service.']\n",
      "06/30/2022 13:50:01 - INFO - __main__ -   Epoch: 44 | Batch: 2500/10001 (25%) | G Loss: -0.292320 | C Loss: -0.980196\n",
      "06/30/2022 13:50:01 - INFO - __main__ -   Text: ['Matriscious words are not as funny when using them.']\n",
      "06/30/2022 13:50:02 - INFO - __main__ -   Epoch: 44 | Batch: 3000/10001 (30%) | G Loss: 2.766445 | C Loss: -0.153158\n",
      "06/30/2022 13:50:02 - INFO - __main__ -   Text: ['The Higher Ones can absorb probabilities equally well.']\n",
      "06/30/2022 13:50:02 - INFO - __main__ -   Epoch: 44 | Batch: 3500/10001 (35%) | G Loss: 1.521020 | C Loss: -0.485615\n",
      "06/30/2022 13:50:03 - INFO - __main__ -   Text: ['In that case the PCB should be buttressed with beta Kappa.\"']\n",
      "06/30/2022 13:50:03 - INFO - __main__ -   Epoch: 44 | Batch: 4000/10001 (40%) | G Loss: -0.564402 | C Loss: 0.264735\n",
      "06/30/2022 13:50:03 - INFO - __main__ -   Text: ['Somers is beyond simple?']\n",
      "06/30/2022 13:50:04 - INFO - __main__ -   Epoch: 44 | Batch: 4500/10001 (45%) | G Loss: 1.808372 | C Loss: -1.756829\n",
      "06/30/2022 13:50:04 - INFO - __main__ -   Text: [\"She's working at it.\"]\n",
      "06/30/2022 13:50:05 - INFO - __main__ -   Epoch: 44 | Batch: 5000/10001 (50%) | G Loss: 3.001803 | C Loss: -0.912399\n",
      "06/30/2022 13:50:05 - INFO - __main__ -   Text: [\"The comedian isigh=uangund, who says he'll strive to be in celebrity status for the first time\"]\n",
      "06/30/2022 13:50:06 - INFO - __main__ -   Epoch: 44 | Batch: 5500/10001 (55%) | G Loss: 1.920119 | C Loss: -1.907572\n",
      "06/30/2022 13:50:06 - INFO - __main__ -   Text: ['As best as I can tell, Jaap just doesn\\'t care about making albums!\"']\n",
      "06/30/2022 13:50:07 - INFO - __main__ -   Epoch: 44 | Batch: 6000/10001 (60%) | G Loss: 2.130548 | C Loss: -0.924329\n",
      "06/30/2022 13:50:07 - INFO - __main__ -   Text: ['The Internet is a science science.']\n",
      "06/30/2022 13:50:07 - INFO - __main__ -   Epoch: 44 | Batch: 6500/10001 (65%) | G Loss: 0.884793 | C Loss: -0.807921\n",
      "06/30/2022 13:50:07 - INFO - __main__ -   Text: ['Ones suffer.']\n",
      "06/30/2022 13:50:08 - INFO - __main__ -   Epoch: 44 | Batch: 7000/10001 (70%) | G Loss: -0.604279 | C Loss: -0.387973\n",
      "06/30/2022 13:50:08 - INFO - __main__ -   Text: ['On other teams, the \"\" have more speed!\" I lump things like gankers in with first guess.']\n",
      "06/30/2022 13:50:09 - INFO - __main__ -   Epoch: 44 | Batch: 7500/10001 (75%) | G Loss: 0.337728 | C Loss: -1.042658\n",
      "06/30/2022 13:50:09 - INFO - __main__ -   Text: ['Some see it as \"luxurious graphics\", compared to Books software\"\".']\n",
      "06/30/2022 13:50:10 - INFO - __main__ -   Epoch: 44 | Batch: 8000/10001 (80%) | G Loss: 2.844694 | C Loss: -1.539728\n",
      "06/30/2022 13:50:10 - INFO - __main__ -   Text: [\"It's a clever nickname for polysyllabic friends!\"]\n",
      "06/30/2022 13:50:11 - INFO - __main__ -   Epoch: 44 | Batch: 8500/10001 (85%) | G Loss: 0.167843 | C Loss: 0.101668\n",
      "06/30/2022 13:50:11 - INFO - __main__ -   Text: ['Due to his baseball background, Smart mentions this as being one of his major goals for training.']\n",
      "06/30/2022 13:50:12 - INFO - __main__ -   Epoch: 44 | Batch: 9000/10001 (90%) | G Loss: 1.643635 | C Loss: -2.238609\n",
      "06/30/2022 13:50:12 - INFO - __main__ -   Text: ['Other round card judges say that Because series finisher Eddie Murray is not from a big shopping town, he works as']\n",
      "06/30/2022 13:50:12 - INFO - __main__ -   Epoch: 44 | Batch: 9500/10001 (95%) | G Loss: 0.627453 | C Loss: -0.735403\n",
      "06/30/2022 13:50:12 - INFO - __main__ -   Text: ['However, if Luke Deuteronomy says, doing.\"']\n",
      "06/30/2022 13:50:13 - INFO - __main__ -   * (Train) Epoch: 44 | G Loss: 1.1180 | C Loss: -1.4945 | Updates G: 323 | Updates C: 677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:50:23 - INFO - __main__ -   Bleu-2:0.214 | B-Bleu-2:0.296\n",
      "06/30/2022 13:50:23 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_3.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5097088570365559\n",
      "Train file used is number 3\n",
      "../../yahoo/subdivided_large/train_3.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 45 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:12.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:24.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:36.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:48.\n",
      "  Batch    50  of    172.    Elapsed: 0:01:00.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:12.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:24.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:36.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:48.\n",
      "  Batch   100  of    172.    Elapsed: 0:02:00.\n",
      "  Batch   110  of    172.    Elapsed: 0:02:12.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:24.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:36.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:48.\n",
      "  Batch   150  of    172.    Elapsed: 0:03:00.\n",
      "  Batch   160  of    172.    Elapsed: 0:03:12.\n",
      "  Batch   170  of    172.    Elapsed: 0:03:24.\n",
      "\n",
      "  Average training loss discriminator: 0.016\n",
      "  Training epcoh took: 0:03:26\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:53:49 - INFO - __main__ -   Epoch: 45 | Batch: 0/10001 (0%) | G Loss: 0.575364 | C Loss: -1.010845\n",
      "06/30/2022 13:53:49 - INFO - __main__ -   Text: ['Aspects of consciousness.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.432\n",
      "  Test Loss: 3.027\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:53:50 - INFO - __main__ -   Epoch: 45 | Batch: 500/10001 (5%) | G Loss: 2.186878 | C Loss: -0.982432\n",
      "06/30/2022 13:53:51 - INFO - __main__ -   Text: ['He occasionally jokes about why Pennants Starleague is \"fantastic\".']\n",
      "06/30/2022 13:53:52 - INFO - __main__ -   Epoch: 45 | Batch: 1000/10001 (10%) | G Loss: 1.141465 | C Loss: -0.854059\n",
      "06/30/2022 13:53:52 - INFO - __main__ -   Text: ['']\n",
      "06/30/2022 13:53:53 - INFO - __main__ -   Epoch: 45 | Batch: 1500/10001 (15%) | G Loss: 2.853044 | C Loss: -0.942704\n",
      "06/30/2022 13:53:53 - INFO - __main__ -   Text: ['The characterism.']\n",
      "06/30/2022 13:53:54 - INFO - __main__ -   Epoch: 45 | Batch: 2000/10001 (20%) | G Loss: -0.238620 | C Loss: -0.787138\n",
      "06/30/2022 13:53:54 - INFO - __main__ -   Text: [\"Baker hubbs and pazster Jim comes up with 'Gaga x'.\"]\n",
      "06/30/2022 13:53:55 - INFO - __main__ -   Epoch: 45 | Batch: 2500/10001 (25%) | G Loss: 2.578088 | C Loss: -2.156950\n",
      "06/30/2022 13:53:55 - INFO - __main__ -   Text: ['Such is God\\'s fear, that is more empty than a pearl in the lighthouse\".']\n",
      "06/30/2022 13:53:56 - INFO - __main__ -   Epoch: 45 | Batch: 3000/10001 (30%) | G Loss: 0.072289 | C Loss: -0.574228\n",
      "06/30/2022 13:53:56 - INFO - __main__ -   Text: ['It is eternal.']\n",
      "06/30/2022 13:53:57 - INFO - __main__ -   Epoch: 45 | Batch: 3500/10001 (35%) | G Loss: 1.739486 | C Loss: -1.476555\n",
      "06/30/2022 13:53:57 - INFO - __main__ -   Text: ['In this hellish world, will the Pope withstand the holy act?']\n",
      "06/30/2022 13:53:59 - INFO - __main__ -   Epoch: 45 | Batch: 4000/10001 (40%) | G Loss: 3.948564 | C Loss: -2.474278\n",
      "06/30/2022 13:53:59 - INFO - __main__ -   Text: ['Print nav() lines.']\n",
      "06/30/2022 13:54:00 - INFO - __main__ -   Epoch: 45 | Batch: 4500/10001 (45%) | G Loss: -0.218485 | C Loss: -0.054847\n",
      "06/30/2022 13:54:00 - INFO - __main__ -   Text: ['She resembles a lot of things.\"']\n",
      "06/30/2022 13:54:01 - INFO - __main__ -   Epoch: 45 | Batch: 5000/10001 (50%) | G Loss: 3.514659 | C Loss: -2.070511\n",
      "06/30/2022 13:54:01 - INFO - __main__ -   Text: ['Change recognition.']\n",
      "06/30/2022 13:54:02 - INFO - __main__ -   Epoch: 45 | Batch: 5500/10001 (55%) | G Loss: 1.888360 | C Loss: -0.720925\n",
      "06/30/2022 13:54:02 - INFO - __main__ -   Text: ['Also references to \"Gravity\".']\n",
      "06/30/2022 13:54:03 - INFO - __main__ -   Epoch: 45 | Batch: 6000/10001 (60%) | G Loss: 0.635762 | C Loss: -0.894734\n",
      "06/30/2022 13:54:03 - INFO - __main__ -   Text: ['Moreover everything belongs to genus .']\n",
      "06/30/2022 13:54:04 - INFO - __main__ -   Epoch: 45 | Batch: 6500/10001 (65%) | G Loss: 2.559274 | C Loss: -1.040871\n",
      "06/30/2022 13:54:04 - INFO - __main__ -   Text: ['The result is oftenitis recall.']\n",
      "06/30/2022 13:54:05 - INFO - __main__ -   Epoch: 45 | Batch: 7000/10001 (70%) | G Loss: 1.183044 | C Loss: -1.762643\n",
      "06/30/2022 13:54:06 - INFO - __main__ -   Text: [\"Horrible's what I do!\"]\n",
      "06/30/2022 13:54:07 - INFO - __main__ -   Epoch: 45 | Batch: 7500/10001 (75%) | G Loss: 2.107011 | C Loss: -1.410492\n",
      "06/30/2022 13:54:07 - INFO - __main__ -   Text: ['She speaks much more open than words.']\n",
      "06/30/2022 13:54:08 - INFO - __main__ -   Epoch: 45 | Batch: 8000/10001 (80%) | G Loss: 1.802717 | C Loss: -1.377183\n",
      "06/30/2022 13:54:08 - INFO - __main__ -   Text: ['The word \"graphic\" or \"M) impulse makes its way into the vocabulary.\"']\n",
      "06/30/2022 13:54:09 - INFO - __main__ -   Epoch: 45 | Batch: 8500/10001 (85%) | G Loss: 1.607906 | C Loss: -1.234568\n",
      "06/30/2022 13:54:09 - INFO - __main__ -   Text: ['Accelerance texts Resourceness.']\n",
      "06/30/2022 13:54:10 - INFO - __main__ -   Epoch: 45 | Batch: 9000/10001 (90%) | G Loss: 1.399128 | C Loss: -2.096272\n",
      "06/30/2022 13:54:10 - INFO - __main__ -   Text: ['At the same time, man is the master of the world.']\n",
      "06/30/2022 13:54:11 - INFO - __main__ -   Epoch: 45 | Batch: 9500/10001 (95%) | G Loss: -0.652309 | C Loss: -0.407963\n",
      "06/30/2022 13:54:11 - INFO - __main__ -   Text: ['These blogs are known as traffic optimization blogs.']\n",
      "06/30/2022 13:54:12 - INFO - __main__ -   * (Train) Epoch: 45 | G Loss: 0.9094 | C Loss: -1.1727 | Updates G: 335 | Updates C: 665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:54:21 - INFO - __main__ -   Bleu-2:0.204 | B-Bleu-2:0.284\n",
      "06/30/2022 13:54:21 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_4.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4881452214312926\n",
      "Train file used is number 4\n",
      "../../yahoo/subdivided_large/train_4.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 46 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:31.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:41.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:51.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:01.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:12.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:22.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:32.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:42.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:52.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:03.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:14.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:24.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:34.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:45.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss discriminator: 0.019\n",
      "  Training epcoh took: 0:02:57\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:57:19 - INFO - __main__ -   Epoch: 46 | Batch: 0/10001 (0%) | G Loss: 3.412486 | C Loss: -1.460574\n",
      "06/30/2022 13:57:19 - INFO - __main__ -   Text: ['Examples include neurosomatic electrical stimulation.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.425\n",
      "  Test Loss: 3.044\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:57:20 - INFO - __main__ -   Epoch: 46 | Batch: 500/10001 (5%) | G Loss: -0.522325 | C Loss: -0.645296\n",
      "06/30/2022 13:57:20 - INFO - __main__ -   Text: ['The critic can easily pull off a joke one time and say \"I can\\'t happen\".']\n",
      "06/30/2022 13:57:22 - INFO - __main__ -   Epoch: 46 | Batch: 1000/10001 (10%) | G Loss: 3.762346 | C Loss: -1.746538\n",
      "06/30/2022 13:57:22 - INFO - __main__ -   Text: ['The ICCPR has fairly high self reports.']\n",
      "06/30/2022 13:57:23 - INFO - __main__ -   Epoch: 46 | Batch: 1500/10001 (15%) | G Loss: 0.024527 | C Loss: -0.982569\n",
      "06/30/2022 13:57:23 - INFO - __main__ -   Text: ['Instead of writing hypnotic spells, granted, you write a literally infinite number of these.\"']\n",
      "06/30/2022 13:57:24 - INFO - __main__ -   Epoch: 46 | Batch: 2000/10001 (20%) | G Loss: -0.670247 | C Loss: -0.258343\n",
      "06/30/2022 13:57:25 - INFO - __main__ -   Text: ['\"Kept here\").']\n",
      "06/30/2022 13:57:26 - INFO - __main__ -   Epoch: 46 | Batch: 2500/10001 (25%) | G Loss: 2.878252 | C Loss: -1.379530\n",
      "06/30/2022 13:57:26 - INFO - __main__ -   Text: ['As usually, I would cheer it up with some ass laugh.\"']\n",
      "06/30/2022 13:57:27 - INFO - __main__ -   Epoch: 46 | Batch: 3000/10001 (30%) | G Loss: -0.914896 | C Loss: -1.372240\n",
      "06/30/2022 13:57:27 - INFO - __main__ -   Text: ['There are three fundamental aspects.']\n",
      "06/30/2022 13:57:29 - INFO - __main__ -   Epoch: 46 | Batch: 3500/10001 (35%) | G Loss: 2.720125 | C Loss: -1.003029\n",
      "06/30/2022 13:57:29 - INFO - __main__ -   Text: ['Species explanations are gained by languages.']\n",
      "06/30/2022 13:57:30 - INFO - __main__ -   Epoch: 46 | Batch: 4000/10001 (40%) | G Loss: 0.160036 | C Loss: -0.640051\n",
      "06/30/2022 13:57:30 - INFO - __main__ -   Text: ['Steck Burger isn\\'t vegan, grins creamy.\"']\n",
      "06/30/2022 13:57:31 - INFO - __main__ -   Epoch: 46 | Batch: 4500/10001 (45%) | G Loss: 2.220578 | C Loss: -0.752576\n",
      "06/30/2022 13:57:31 - INFO - __main__ -   Text: ['They are very successful in using force to subjugate them.']\n",
      "06/30/2022 13:57:33 - INFO - __main__ -   Epoch: 46 | Batch: 5000/10001 (50%) | G Loss: -1.381907 | C Loss: -0.653904\n",
      "06/30/2022 13:57:33 - INFO - __main__ -   Text: ['Meat is sexy in my opinion.\"']\n",
      "06/30/2022 13:57:34 - INFO - __main__ -   Epoch: 46 | Batch: 5500/10001 (55%) | G Loss: 4.082286 | C Loss: -1.875680\n",
      "06/30/2022 13:57:34 - INFO - __main__ -   Text: ['More important, these conditions actually relate to mold.']\n",
      "06/30/2022 13:57:35 - INFO - __main__ -   Epoch: 46 | Batch: 6000/10001 (60%) | G Loss: -0.059850 | C Loss: -1.811362\n",
      "06/30/2022 13:57:35 - INFO - __main__ -   Text: ['This term describes an American political establishment.']\n",
      "06/30/2022 13:57:37 - INFO - __main__ -   Epoch: 46 | Batch: 6500/10001 (65%) | G Loss: 2.879460 | C Loss: -1.494044\n",
      "06/30/2022 13:57:37 - INFO - __main__ -   Text: ['Its true assertion is very close to the hood as opposed to your target.\"']\n",
      "06/30/2022 13:57:38 - INFO - __main__ -   Epoch: 46 | Batch: 7000/10001 (70%) | G Loss: 4.029371 | C Loss: -1.299689\n",
      "06/30/2022 13:57:38 - INFO - __main__ -   Text: ['His bad attitude really comes in handy if the woman he is partying with pulls on her hair.']\n",
      "06/30/2022 13:57:40 - INFO - __main__ -   Epoch: 46 | Batch: 7500/10001 (75%) | G Loss: -1.660416 | C Loss: -1.300835\n",
      "06/30/2022 13:57:40 - INFO - __main__ -   Text: ['Most people store their emissions in a smoke-filled-out lab.']\n",
      "06/30/2022 13:57:41 - INFO - __main__ -   Epoch: 46 | Batch: 8000/10001 (80%) | G Loss: 4.815932 | C Loss: -1.828846\n",
      "06/30/2022 13:57:41 - INFO - __main__ -   Text: ['Write the protein.']\n",
      "06/30/2022 13:57:42 - INFO - __main__ -   Epoch: 46 | Batch: 8500/10001 (85%) | G Loss: 0.994085 | C Loss: -1.043342\n",
      "06/30/2022 13:57:42 - INFO - __main__ -   Text: ['\"\\'Fo\".']\n",
      "06/30/2022 13:57:43 - INFO - __main__ -   Epoch: 46 | Batch: 9000/10001 (90%) | G Loss: 2.746837 | C Loss: -1.517176\n",
      "06/30/2022 13:57:43 - INFO - __main__ -   Text: ['']\n",
      "06/30/2022 13:57:45 - INFO - __main__ -   Epoch: 46 | Batch: 9500/10001 (95%) | G Loss: 0.223763 | C Loss: -0.695968\n",
      "06/30/2022 13:57:45 - INFO - __main__ -   Text: ['I will choose chores over things.\"']\n",
      "06/30/2022 13:57:46 - INFO - __main__ -   * (Train) Epoch: 46 | G Loss: 0.7132 | C Loss: -1.1788 | Updates G: 326 | Updates C: 674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 13:57:55 - INFO - __main__ -   Bleu-2:0.205 | B-Bleu-2:0.267\n",
      "06/30/2022 13:57:55 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4719568415010654\n",
      "Train file used is number 5\n",
      "../../yahoo/subdivided_large/train_5.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 47 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:09.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:19.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:29.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:38.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:47.\n",
      "  Batch    60  of    172.    Elapsed: 0:00:56.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:06.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:16.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:25.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:35.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:45.\n",
      "  Batch   120  of    172.    Elapsed: 0:01:56.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:05.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:15.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:24.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:35.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:45.\n",
      "\n",
      "  Average training loss discriminator: 0.021\n",
      "  Training epcoh took: 0:02:47\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:00:42 - INFO - __main__ -   Epoch: 47 | Batch: 0/10001 (0%) | G Loss: -0.452261 | C Loss: -0.556321\n",
      "06/30/2022 14:00:42 - INFO - __main__ -   Text: [\"Allmusic,452 lolo '.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.443\n",
      "  Test Loss: 3.013\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:00:44 - INFO - __main__ -   Epoch: 47 | Batch: 500/10001 (5%) | G Loss: -0.009751 | C Loss: -1.000831\n",
      "06/30/2022 14:00:44 - INFO - __main__ -   Text: ['Investigation of Computational Cultures.']\n",
      "06/30/2022 14:00:45 - INFO - __main__ -   Epoch: 47 | Batch: 1000/10001 (10%) | G Loss: 1.149846 | C Loss: -1.296208\n",
      "06/30/2022 14:00:45 - INFO - __main__ -   Text: ['The term Closer is applied to this.']\n",
      "06/30/2022 14:00:46 - INFO - __main__ -   Epoch: 47 | Batch: 1500/10001 (15%) | G Loss: 2.678196 | C Loss: -1.160315\n",
      "06/30/2022 14:00:46 - INFO - __main__ -   Text: [':\".']\n",
      "06/30/2022 14:00:48 - INFO - __main__ -   Epoch: 47 | Batch: 2000/10001 (20%) | G Loss: -2.190183 | C Loss: -0.323834\n",
      "06/30/2022 14:00:48 - INFO - __main__ -   Text: ['Being homosexual does not mean that homosexuals should be punished.']\n",
      "06/30/2022 14:00:49 - INFO - __main__ -   Epoch: 47 | Batch: 2500/10001 (25%) | G Loss: 3.677016 | C Loss: -1.087197\n",
      "06/30/2022 14:00:49 - INFO - __main__ -   Text: ['\"Goddamn it!\"']\n",
      "06/30/2022 14:00:50 - INFO - __main__ -   Epoch: 47 | Batch: 3000/10001 (30%) | G Loss: -0.150369 | C Loss: -0.660876\n",
      "06/30/2022 14:00:50 - INFO - __main__ -   Text: ['Mobile rogesare\".']\n",
      "06/30/2022 14:00:52 - INFO - __main__ -   Epoch: 47 | Batch: 3500/10001 (35%) | G Loss: 3.177214 | C Loss: -0.428514\n",
      "06/30/2022 14:00:52 - INFO - __main__ -   Text: ['Upper oxygen sources deplete brain activities.']\n",
      "06/30/2022 14:00:53 - INFO - __main__ -   Epoch: 47 | Batch: 4000/10001 (40%) | G Loss: 4.756433 | C Loss: -2.079357\n",
      "06/30/2022 14:00:53 - INFO - __main__ -   Text: ['It is like that.']\n",
      "06/30/2022 14:00:54 - INFO - __main__ -   Epoch: 47 | Batch: 4500/10001 (45%) | G Loss: -1.570185 | C Loss: -0.809809\n",
      "06/30/2022 14:00:55 - INFO - __main__ -   Text: ['knight is how do I get guys to respond 9 times per week instead?']\n",
      "06/30/2022 14:00:56 - INFO - __main__ -   Epoch: 47 | Batch: 5000/10001 (50%) | G Loss: 0.133791 | C Loss: -0.683562\n",
      "06/30/2022 14:00:56 - INFO - __main__ -   Text: ['. \"Munch!\"']\n",
      "06/30/2022 14:00:57 - INFO - __main__ -   Epoch: 47 | Batch: 5500/10001 (55%) | G Loss: 4.015441 | C Loss: -1.197875\n",
      "06/30/2022 14:00:57 - INFO - __main__ -   Text: ['The first interpreted error perhaps concerns \"frog culture\".']\n",
      "06/30/2022 14:00:59 - INFO - __main__ -   Epoch: 47 | Batch: 6000/10001 (60%) | G Loss: -0.158005 | C Loss: -0.711298\n",
      "06/30/2022 14:00:59 - INFO - __main__ -   Text: ['However, Office 365 is a networking consultancy company.']\n",
      "06/30/2022 14:01:00 - INFO - __main__ -   Epoch: 47 | Batch: 6500/10001 (65%) | G Loss: 1.236832 | C Loss: -1.418418\n",
      "06/30/2022 14:01:00 - INFO - __main__ -   Text: ['Brown carries out such ritualistic thinking that the \"Fusion\" phenomenon is known.']\n",
      "06/30/2022 14:01:01 - INFO - __main__ -   Epoch: 47 | Batch: 7000/10001 (70%) | G Loss: -0.438158 | C Loss: -0.724098\n",
      "06/30/2022 14:01:02 - INFO - __main__ -   Text: ['When I ask how close I am to them, they say no.']\n",
      "06/30/2022 14:01:03 - INFO - __main__ -   Epoch: 47 | Batch: 7500/10001 (75%) | G Loss: 2.345888 | C Loss: -1.224283\n",
      "06/30/2022 14:01:03 - INFO - __main__ -   Text: ['This technology can be used in disease.']\n",
      "06/30/2022 14:01:04 - INFO - __main__ -   Epoch: 47 | Batch: 8000/10001 (80%) | G Loss: -0.967188 | C Loss: -0.939972\n",
      "06/30/2022 14:01:04 - INFO - __main__ -   Text: ['Is there just another adult porn addict?']\n",
      "06/30/2022 14:01:05 - INFO - __main__ -   Epoch: 47 | Batch: 8500/10001 (85%) | G Loss: 0.189142 | C Loss: -0.649744\n",
      "06/30/2022 14:01:05 - INFO - __main__ -   Text: ['e.g.']\n",
      "06/30/2022 14:01:07 - INFO - __main__ -   Epoch: 47 | Batch: 9000/10001 (90%) | G Loss: 2.041151 | C Loss: -1.312489\n",
      "06/30/2022 14:01:07 - INFO - __main__ -   Text: ['Secondary Electronics and Roller Mirror are brands.']\n",
      "06/30/2022 14:01:08 - INFO - __main__ -   Epoch: 47 | Batch: 9500/10001 (95%) | G Loss: -0.064744 | C Loss: -0.215468\n",
      "06/30/2022 14:01:08 - INFO - __main__ -   Text: ['Sidebars offer users a proprietary cheat sheet format.']\n",
      "06/30/2022 14:01:09 - INFO - __main__ -   * (Train) Epoch: 47 | G Loss: 0.8031 | C Loss: -0.9471 | Updates G: 327 | Updates C: 673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:01:19 - INFO - __main__ -   Bleu-2:0.187 | B-Bleu-2:0.244\n",
      "06/30/2022 14:01:19 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_6.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43023533354766086\n",
      "Train file used is number 6\n",
      "../../yahoo/subdivided_large/train_6.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 48 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:12.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:24.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:36.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:48.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:59.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:11.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:23.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:35.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:47.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:59.\n",
      "  Batch   110  of    172.    Elapsed: 0:02:11.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:23.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:35.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:47.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:58.\n",
      "  Batch   160  of    172.    Elapsed: 0:03:10.\n",
      "  Batch   170  of    172.    Elapsed: 0:03:22.\n",
      "\n",
      "  Average training loss discriminator: 0.008\n",
      "  Training epcoh took: 0:03:24\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:04:43 - INFO - __main__ -   Epoch: 48 | Batch: 0/10001 (0%) | G Loss: 2.727123 | C Loss: -1.758859\n",
      "06/30/2022 14:04:43 - INFO - __main__ -   Text: ['Snel may describe the more efficient metabolism to ligand signaling genes.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.422\n",
      "  Test Loss: 3.036\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:04:44 - INFO - __main__ -   Epoch: 48 | Batch: 500/10001 (5%) | G Loss: -1.514205 | C Loss: -0.448179\n",
      "06/30/2022 14:04:44 - INFO - __main__ -   Text: ['The Force\".']\n",
      "06/30/2022 14:04:46 - INFO - __main__ -   Epoch: 48 | Batch: 1000/10001 (10%) | G Loss: 5.051387 | C Loss: -1.079697\n",
      "06/30/2022 14:04:46 - INFO - __main__ -   Text: ['L-form doesn\\'t really feed another program.\"']\n",
      "06/30/2022 14:04:47 - INFO - __main__ -   Epoch: 48 | Batch: 1500/10001 (15%) | G Loss: 0.850474 | C Loss: -0.698595\n",
      "06/30/2022 14:04:47 - INFO - __main__ -   Text: ['He\\'s not saying something, but knows this.\"']\n",
      "06/30/2022 14:04:49 - INFO - __main__ -   Epoch: 48 | Batch: 2000/10001 (20%) | G Loss: -1.008917 | C Loss: -0.689528\n",
      "06/30/2022 14:04:49 - INFO - __main__ -   Text: ['Ali Haromsky declares \"Heteroplasm is always moving fast\".']\n",
      "06/30/2022 14:04:50 - INFO - __main__ -   Epoch: 48 | Batch: 2500/10001 (25%) | G Loss: 4.631743 | C Loss: -1.566937\n",
      "06/30/2022 14:04:50 - INFO - __main__ -   Text: ['Learning Business Security (LESP) is a choice first established by the Center.']\n",
      "06/30/2022 14:04:51 - INFO - __main__ -   Epoch: 48 | Batch: 3000/10001 (30%) | G Loss: 1.358339 | C Loss: -0.833102\n",
      "06/30/2022 14:04:52 - INFO - __main__ -   Text: [\"You worry it out, so you worry it out, sorry for him that he doesn't want to know.\"]\n",
      "06/30/2022 14:04:53 - INFO - __main__ -   Epoch: 48 | Batch: 3500/10001 (35%) | G Loss: -1.150927 | C Loss: -0.737319\n",
      "06/30/2022 14:04:53 - INFO - __main__ -   Text: ['They can be talk to me.']\n",
      "06/30/2022 14:04:54 - INFO - __main__ -   Epoch: 48 | Batch: 4000/10001 (40%) | G Loss: 4.484361 | C Loss: -1.283720\n",
      "06/30/2022 14:04:54 - INFO - __main__ -   Text: ['It is faster than !']\n",
      "06/30/2022 14:04:56 - INFO - __main__ -   Epoch: 48 | Batch: 4500/10001 (45%) | G Loss: -1.514592 | C Loss: -0.603480\n",
      "06/30/2022 14:04:56 - INFO - __main__ -   Text: ['Time and time again I speak for him Robbins.']\n",
      "06/30/2022 14:04:57 - INFO - __main__ -   Epoch: 48 | Batch: 5000/10001 (50%) | G Loss: 0.405849 | C Loss: -0.107980\n",
      "06/30/2022 14:04:57 - INFO - __main__ -   Text: ['This type of relationship produces student issues along gender roles.']\n",
      "06/30/2022 14:04:58 - INFO - __main__ -   Epoch: 48 | Batch: 5500/10001 (55%) | G Loss: 0.682807 | C Loss: -0.366636\n",
      "06/30/2022 14:04:58 - INFO - __main__ -   Text: ['It is a terabyte service which provides data aggregation and communication.']\n",
      "06/30/2022 14:05:00 - INFO - __main__ -   Epoch: 48 | Batch: 6000/10001 (60%) | G Loss: -0.051876 | C Loss: -0.210725\n",
      "06/30/2022 14:05:00 - INFO - __main__ -   Text: ['12 has interpreted the \"-4\" word in Northern Spanish.']\n",
      "06/30/2022 14:05:01 - INFO - __main__ -   Epoch: 48 | Batch: 6500/10001 (65%) | G Loss: 4.260510 | C Loss: -1.773386\n",
      "06/30/2022 14:05:01 - INFO - __main__ -   Text: ['They view God as the sick.']\n",
      "06/30/2022 14:05:02 - INFO - __main__ -   Epoch: 48 | Batch: 7000/10001 (70%) | G Loss: -0.523230 | C Loss: 0.733717\n",
      "06/30/2022 14:05:02 - INFO - __main__ -   Text: ['The fear that exists and the untamed nature of human nature.']\n",
      "06/30/2022 14:05:04 - INFO - __main__ -   Epoch: 48 | Batch: 7500/10001 (75%) | G Loss: 6.614458 | C Loss: -3.068793\n",
      "06/30/2022 14:05:04 - INFO - __main__ -   Text: ['switching through a queue of items with a diff joke!\"']\n",
      "06/30/2022 14:05:05 - INFO - __main__ -   Epoch: 48 | Batch: 8000/10001 (80%) | G Loss: -1.311140 | C Loss: -0.368669\n",
      "06/30/2022 14:05:05 - INFO - __main__ -   Text: ['You can write a data model that defines all vectors and charges.']\n",
      "06/30/2022 14:05:06 - INFO - __main__ -   Epoch: 48 | Batch: 8500/10001 (85%) | G Loss: 1.534998 | C Loss: 0.022832\n",
      "06/30/2022 14:05:06 - INFO - __main__ -   Text: ['One example is homeless recovery from Noodle Rescue.']\n",
      "06/30/2022 14:05:08 - INFO - __main__ -   Epoch: 48 | Batch: 9000/10001 (90%) | G Loss: 0.767146 | C Loss: -0.838495\n",
      "06/30/2022 14:05:08 - INFO - __main__ -   Text: ['<ellipsis> Write Postscript']\n",
      "06/30/2022 14:05:09 - INFO - __main__ -   Epoch: 48 | Batch: 9500/10001 (95%) | G Loss: -0.161365 | C Loss: -0.069093\n",
      "06/30/2022 14:05:09 - INFO - __main__ -   Text: ['I repeat, his thought is cloud.']\n",
      "06/30/2022 14:05:11 - INFO - __main__ -   * (Train) Epoch: 48 | G Loss: 0.7650 | C Loss: -0.9726 | Updates G: 274 | Updates C: 726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:05:19 - INFO - __main__ -   Bleu-2:0.217 | B-Bleu-2:0.261\n",
      "06/30/2022 14:05:19 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_7.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47763188811699653\n",
      "Train file used is number 7\n",
      "../../yahoo/subdivided_large/train_7.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 49 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:30.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:41.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:51.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:02.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:14.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:24.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:34.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:44.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:55.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:06.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:16.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:27.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:37.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:48.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:59.\n",
      "\n",
      "  Average training loss discriminator: 0.021\n",
      "  Training epcoh took: 0:03:01\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:08:20 - INFO - __main__ -   Epoch: 49 | Batch: 0/10001 (0%) | G Loss: 3.316282 | C Loss: -1.047876\n",
      "06/30/2022 14:08:21 - INFO - __main__ -   Text: ['The Witness is a hero!\"']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.430\n",
      "  Test Loss: 3.040\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:08:22 - INFO - __main__ -   Epoch: 49 | Batch: 500/10001 (5%) | G Loss: 3.624287 | C Loss: -1.305548\n",
      "06/30/2022 14:08:22 - INFO - __main__ -   Text: ['']\n",
      "06/30/2022 14:08:23 - INFO - __main__ -   Epoch: 49 | Batch: 1000/10001 (10%) | G Loss: -1.941831 | C Loss: -0.789700\n",
      "06/30/2022 14:08:23 - INFO - __main__ -   Text: ['A track parody is called \"The Hollypool Roundtable\".']\n",
      "06/30/2022 14:08:24 - INFO - __main__ -   Epoch: 49 | Batch: 1500/10001 (15%) | G Loss: 0.983624 | C Loss: -0.098553\n",
      "06/30/2022 14:08:25 - INFO - __main__ -   Text: ['So much magic tricks Lee bastard nonsense.']\n",
      "06/30/2022 14:08:26 - INFO - __main__ -   Epoch: 49 | Batch: 2000/10001 (20%) | G Loss: 2.108736 | C Loss: -1.026755\n",
      "06/30/2022 14:08:26 - INFO - __main__ -   Text: ['The song describes a sarcastic mood or dramatic oracles.']\n",
      "06/30/2022 14:08:27 - INFO - __main__ -   Epoch: 49 | Batch: 2500/10001 (25%) | G Loss: 1.846084 | C Loss: -1.222646\n",
      "06/30/2022 14:08:27 - INFO - __main__ -   Text: ['She is sometimes referred to as the \"evil twin\".']\n",
      "06/30/2022 14:08:29 - INFO - __main__ -   Epoch: 49 | Batch: 3000/10001 (30%) | G Loss: 0.121183 | C Loss: -0.190595\n",
      "06/30/2022 14:08:29 - INFO - __main__ -   Text: [\"Based on their definition of 'Sunday Bid' football is fun!\"]\n",
      "06/30/2022 14:08:30 - INFO - __main__ -   Epoch: 49 | Batch: 3500/10001 (35%) | G Loss: 1.676669 | C Loss: -1.571002\n",
      "06/30/2022 14:08:30 - INFO - __main__ -   Text: ['Obesity in the brain can make us act like a fat person.']\n",
      "06/30/2022 14:08:32 - INFO - __main__ -   Epoch: 49 | Batch: 4000/10001 (40%) | G Loss: -0.161741 | C Loss: -0.288182\n",
      "06/30/2022 14:08:32 - INFO - __main__ -   Text: ['According to him, a relationship lacks any attraction.']\n",
      "06/30/2022 14:08:33 - INFO - __main__ -   Epoch: 49 | Batch: 4500/10001 (45%) | G Loss: 3.230093 | C Loss: -1.776167\n",
      "06/30/2022 14:08:33 - INFO - __main__ -   Text: ['\"What it says is:- \\'An excellent job exciting science.']\n",
      "06/30/2022 14:08:34 - INFO - __main__ -   Epoch: 49 | Batch: 5000/10001 (50%) | G Loss: -0.361787 | C Loss: -0.039260\n",
      "06/30/2022 14:08:34 - INFO - __main__ -   Text: ['https://support.php. This is easy to understand\".']\n",
      "06/30/2022 14:08:35 - INFO - __main__ -   Epoch: 49 | Batch: 5500/10001 (55%) | G Loss: 3.909868 | C Loss: -1.338967\n",
      "06/30/2022 14:08:36 - INFO - __main__ -   Text: ['She has assisted!.']\n",
      "06/30/2022 14:08:37 - INFO - __main__ -   Epoch: 49 | Batch: 6000/10001 (60%) | G Loss: 0.326397 | C Loss: -0.806349\n",
      "06/30/2022 14:08:37 - INFO - __main__ -   Text: [\"It's Day 1.\"]\n",
      "06/30/2022 14:08:38 - INFO - __main__ -   Epoch: 49 | Batch: 6500/10001 (65%) | G Loss: 0.632583 | C Loss: -0.968199\n",
      "06/30/2022 14:08:38 - INFO - __main__ -   Text: ['In other words, the Indianapolis 500 is a good long-term strategy.']\n",
      "06/30/2022 14:08:40 - INFO - __main__ -   Epoch: 49 | Batch: 7000/10001 (70%) | G Loss: 0.762253 | C Loss: -0.524543\n",
      "06/30/2022 14:08:40 - INFO - __main__ -   Text: ['The earth needs to love the comedian Kathy Burchill because of her new girlfriend.']\n",
      "06/30/2022 14:08:41 - INFO - __main__ -   Epoch: 49 | Batch: 7500/10001 (75%) | G Loss: 2.185729 | C Loss: -1.404682\n",
      "06/30/2022 14:08:41 - INFO - __main__ -   Text: ['It may also include tripe n YouTube magazine headlines.']\n",
      "06/30/2022 14:08:42 - INFO - __main__ -   Epoch: 49 | Batch: 8000/10001 (80%) | G Loss: -0.150324 | C Loss: -1.711353\n",
      "06/30/2022 14:08:42 - INFO - __main__ -   Text: ['Whittle produces precision-oriented welding techniques on a stream flyby.']\n",
      "06/30/2022 14:08:44 - INFO - __main__ -   Epoch: 49 | Batch: 8500/10001 (85%) | G Loss: 3.189991 | C Loss: -1.676984\n",
      "06/30/2022 14:08:44 - INFO - __main__ -   Text: [\"The name 'hya' is bad.\"]\n",
      "06/30/2022 14:08:45 - INFO - __main__ -   Epoch: 49 | Batch: 9000/10001 (90%) | G Loss: -0.106240 | C Loss: 0.588387\n",
      "06/30/2022 14:08:45 - INFO - __main__ -   Text: ['Hate isn\\'t something you\\'re ashamed of.\"']\n",
      "06/30/2022 14:08:46 - INFO - __main__ -   Epoch: 49 | Batch: 9500/10001 (95%) | G Loss: 11.235813 | C Loss: -5.844588\n",
      "06/30/2022 14:08:46 - INFO - __main__ -   Text: ['The mothers name is \"Moe.\"']\n",
      "06/30/2022 14:08:48 - INFO - __main__ -   * (Train) Epoch: 49 | G Loss: 1.0569 | C Loss: -1.0523 | Updates G: 304 | Updates C: 696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:08:57 - INFO - __main__ -   Bleu-2:0.180 | B-Bleu-2:0.229\n",
      "06/30/2022 14:08:57 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_8.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4097141639427184\n",
      "Train file used is number 8\n",
      "../../yahoo/subdivided_large/train_8.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 50 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:12.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:23.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:35.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:46.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:58.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:09.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:21.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:33.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:44.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:56.\n",
      "  Batch   110  of    172.    Elapsed: 0:02:08.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:20.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:31.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:43.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:54.\n",
      "  Batch   160  of    172.    Elapsed: 0:03:06.\n",
      "  Batch   170  of    172.    Elapsed: 0:03:18.\n",
      "\n",
      "  Average training loss discriminator: 0.014\n",
      "  Training epcoh took: 0:03:20\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:12:17 - INFO - __main__ -   Epoch: 50 | Batch: 0/10001 (0%) | G Loss: -0.311475 | C Loss: -1.185035\n",
      "06/30/2022 14:12:17 - INFO - __main__ -   Text: ['Accelerations towards knowledge of [].']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.403\n",
      "  Test Loss: 3.191\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:12:18 - INFO - __main__ -   Epoch: 50 | Batch: 500/10001 (5%) | G Loss: -0.283913 | C Loss: 0.804672\n",
      "06/30/2022 14:12:18 - INFO - __main__ -   Text: ['The Josh are version of goldfish out of annoyance.']\n",
      "06/30/2022 14:12:20 - INFO - __main__ -   Epoch: 50 | Batch: 1000/10001 (10%) | G Loss: 4.909886 | C Loss: -2.256611\n",
      "06/30/2022 14:12:20 - INFO - __main__ -   Text: [\"Now on a rainy day, 'Teen Titans' don't flash the stage and they don't really care.\"]\n",
      "06/30/2022 14:12:21 - INFO - __main__ -   Epoch: 50 | Batch: 1500/10001 (15%) | G Loss: -1.739184 | C Loss: -0.464555\n",
      "06/30/2022 14:12:21 - INFO - __main__ -   Text: ['Deheit!']\n",
      "06/30/2022 14:12:22 - INFO - __main__ -   Epoch: 50 | Batch: 2000/10001 (20%) | G Loss: 4.105190 | C Loss: -0.466938\n",
      "06/30/2022 14:12:23 - INFO - __main__ -   Text: ['A better solution in the problem is in the DSM-III.']\n",
      "06/30/2022 14:12:24 - INFO - __main__ -   Epoch: 50 | Batch: 2500/10001 (25%) | G Loss: 2.041538 | C Loss: -1.188854\n",
      "06/30/2022 14:12:24 - INFO - __main__ -   Text: ['Ditto on the last post (no book this time).\"']\n",
      "06/30/2022 14:12:25 - INFO - __main__ -   Epoch: 50 | Batch: 3000/10001 (30%) | G Loss: -0.721112 | C Loss: -0.116706\n",
      "06/30/2022 14:12:25 - INFO - __main__ -   Text: ['The sorcerers user)']\n",
      "06/30/2022 14:12:27 - INFO - __main__ -   Epoch: 50 | Batch: 3500/10001 (35%) | G Loss: 4.535263 | C Loss: -1.103282\n",
      "06/30/2022 14:12:27 - INFO - __main__ -   Text: ['Love is more important than fear of liability.']\n",
      "06/30/2022 14:12:28 - INFO - __main__ -   Epoch: 50 | Batch: 4000/10001 (40%) | G Loss: 0.310671 | C Loss: -0.372787\n",
      "06/30/2022 14:12:28 - INFO - __main__ -   Text: ['Doctor-based provision.']\n",
      "06/30/2022 14:12:30 - INFO - __main__ -   Epoch: 50 | Batch: 4500/10001 (45%) | G Loss: 4.460863 | C Loss: -0.671692\n",
      "06/30/2022 14:12:30 - INFO - __main__ -   Text: ['There is yet an un MMTP begrudgingly offered.']\n",
      "06/30/2022 14:12:31 - INFO - __main__ -   Epoch: 50 | Batch: 5000/10001 (50%) | G Loss: 0.214682 | C Loss: -0.205308\n",
      "06/30/2022 14:12:31 - INFO - __main__ -   Text: ['His schedule allows him to listen to texts written simultaneously.']\n",
      "06/30/2022 14:12:32 - INFO - __main__ -   Epoch: 50 | Batch: 5500/10001 (55%) | G Loss: 3.526461 | C Loss: -1.398941\n",
      "06/30/2022 14:12:32 - INFO - __main__ -   Text: ['It seems that <elo> not too much.']\n",
      "06/30/2022 14:12:34 - INFO - __main__ -   Epoch: 50 | Batch: 6000/10001 (60%) | G Loss: 1.234635 | C Loss: -0.596036\n",
      "06/30/2022 14:12:34 - INFO - __main__ -   Text: ['This echoes the other championships).']\n",
      "06/30/2022 14:12:35 - INFO - __main__ -   Epoch: 50 | Batch: 6500/10001 (65%) | G Loss: 1.540697 | C Loss: -0.735011\n",
      "06/30/2022 14:12:35 - INFO - __main__ -   Text: ['The helper module should be able to check the following factors: you must change one of the pharmacy parts.']\n",
      "06/30/2022 14:12:37 - INFO - __main__ -   Epoch: 50 | Batch: 7000/10001 (70%) | G Loss: 0.835533 | C Loss: -1.395407\n",
      "06/30/2022 14:12:37 - INFO - __main__ -   Text: ['The first thing you want to do here is to guide it from \"normal\".']\n",
      "06/30/2022 14:12:38 - INFO - __main__ -   Epoch: 50 | Batch: 7500/10001 (75%) | G Loss: 2.461713 | C Loss: -0.652722\n",
      "06/30/2022 14:12:38 - INFO - __main__ -   Text: ['Other schools use (or are makes mythical).']\n",
      "06/30/2022 14:12:40 - INFO - __main__ -   Epoch: 50 | Batch: 8000/10001 (80%) | G Loss: 2.076089 | C Loss: -1.684097\n",
      "06/30/2022 14:12:40 - INFO - __main__ -   Text: ['The process is described in terms of magic explosions..']\n",
      "06/30/2022 14:12:41 - INFO - __main__ -   Epoch: 50 | Batch: 8500/10001 (85%) | G Loss: 2.569605 | C Loss: -0.791982\n",
      "06/30/2022 14:12:41 - INFO - __main__ -   Text: ['The CIA does not commence computational tasks by using a PC.']\n",
      "06/30/2022 14:12:42 - INFO - __main__ -   Epoch: 50 | Batch: 9000/10001 (90%) | G Loss: 0.654474 | C Loss: -0.856319\n",
      "06/30/2022 14:12:42 - INFO - __main__ -   Text: ['from this extensive memory.']\n",
      "06/30/2022 14:12:44 - INFO - __main__ -   Epoch: 50 | Batch: 9500/10001 (95%) | G Loss: 3.189746 | C Loss: -1.173578\n",
      "06/30/2022 14:12:44 - INFO - __main__ -   Text: ['Stop comparing us to electricity.']\n",
      "06/30/2022 14:12:45 - INFO - __main__ -   * (Train) Epoch: 50 | G Loss: 1.0953 | C Loss: -0.7765 | Updates G: 238 | Updates C: 762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:12:54 - INFO - __main__ -   Bleu-2:0.201 | B-Bleu-2:0.274\n",
      "06/30/2022 14:12:54 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_9.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4743346170681084\n",
      "Train file used is number 9\n",
      "../../yahoo/subdivided_large/train_9.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 51 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:21.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:31.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:42.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:53.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:04.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:14.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:25.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:36.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:47.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:59.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:10.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:21.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:32.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:43.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:54.\n",
      "  Batch   170  of    172.    Elapsed: 0:03:05.\n",
      "\n",
      "  Average training loss discriminator: 0.016\n",
      "  Training epcoh took: 0:03:07\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:16:02 - INFO - __main__ -   Epoch: 51 | Batch: 0/10001 (0%) | G Loss: 0.865032 | C Loss: -0.490336\n",
      "06/30/2022 14:16:02 - INFO - __main__ -   Text: ['These government lives, but no poet knows what is beautiful about Chicago.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.422\n",
      "  Test Loss: 3.060\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:16:03 - INFO - __main__ -   Epoch: 51 | Batch: 500/10001 (5%) | G Loss: 2.941051 | C Loss: -0.786153\n",
      "06/30/2022 14:16:04 - INFO - __main__ -   Text: ['It should have 1 user running a rotatori system.']\n",
      "06/30/2022 14:16:05 - INFO - __main__ -   Epoch: 51 | Batch: 1000/10001 (10%) | G Loss: 1.684849 | C Loss: -0.566436\n",
      "06/30/2022 14:16:05 - INFO - __main__ -   Text: ['']\n",
      "06/30/2022 14:16:06 - INFO - __main__ -   Epoch: 51 | Batch: 1500/10001 (15%) | G Loss: 3.912945 | C Loss: -1.283503\n",
      "06/30/2022 14:16:06 - INFO - __main__ -   Text: ['The main problem with classical synthetic energy is the production of energy.']\n",
      "06/30/2022 14:16:08 - INFO - __main__ -   Epoch: 51 | Batch: 2000/10001 (20%) | G Loss: 1.710790 | C Loss: -0.896278\n",
      "06/30/2022 14:16:08 - INFO - __main__ -   Text: ['\"\" clowns.\"']\n",
      "06/30/2022 14:16:09 - INFO - __main__ -   Epoch: 51 | Batch: 2500/10001 (25%) | G Loss: 0.843175 | C Loss: -0.781331\n",
      "06/30/2022 14:16:09 - INFO - __main__ -   Text: ['CareerInfo has specialized-edge scientific information.']\n",
      "06/30/2022 14:16:10 - INFO - __main__ -   Epoch: 51 | Batch: 3000/10001 (30%) | G Loss: 2.985369 | C Loss: -1.373333\n",
      "06/30/2022 14:16:10 - INFO - __main__ -   Text: [\"This is just disrespectful to them, isn't it?\"]\n",
      "06/30/2022 14:16:12 - INFO - __main__ -   Epoch: 51 | Batch: 3500/10001 (35%) | G Loss: 2.666875 | C Loss: -1.543836\n",
      "06/30/2022 14:16:12 - INFO - __main__ -   Text: ['This class is established to help people find work that implements an online dating app.']\n",
      "06/30/2022 14:16:13 - INFO - __main__ -   Epoch: 51 | Batch: 4000/10001 (40%) | G Loss: -0.033348 | C Loss: -0.597611\n",
      "06/30/2022 14:16:13 - INFO - __main__ -   Text: ['There is nothing relating to law graduate.']\n",
      "06/30/2022 14:16:15 - INFO - __main__ -   Epoch: 51 | Batch: 4500/10001 (45%) | G Loss: 5.183771 | C Loss: -1.361912\n",
      "06/30/2022 14:16:15 - INFO - __main__ -   Text: ['Backlash starts bloody bloody but #a is really cute.']\n",
      "06/30/2022 14:16:16 - INFO - __main__ -   Epoch: 51 | Batch: 5000/10001 (50%) | G Loss: 2.305652 | C Loss: -1.721943\n",
      "06/30/2022 14:16:16 - INFO - __main__ -   Text: ['Sex Workers: Brands.']\n",
      "06/30/2022 14:16:17 - INFO - __main__ -   Epoch: 51 | Batch: 5500/10001 (55%) | G Loss: 0.098440 | C Loss: -0.370795\n",
      "06/30/2022 14:16:17 - INFO - __main__ -   Text: ['The same person is saying \"That same country\".']\n",
      "06/30/2022 14:16:19 - INFO - __main__ -   Epoch: 51 | Batch: 6000/10001 (60%) | G Loss: 3.538019 | C Loss: -1.128056\n",
      "06/30/2022 14:16:19 - INFO - __main__ -   Text: ['The term \"Chinese Book Transfer\" is widely considered a good way to learn to Translate.']\n",
      "06/30/2022 14:16:20 - INFO - __main__ -   Epoch: 51 | Batch: 6500/10001 (65%) | G Loss: 1.733835 | C Loss: -1.060258\n",
      "06/30/2022 14:16:20 - INFO - __main__ -   Text: [\"This isn't a civilization.\"]\n",
      "06/30/2022 14:16:22 - INFO - __main__ -   Epoch: 51 | Batch: 7000/10001 (70%) | G Loss: 0.998357 | C Loss: -0.850794\n",
      "06/30/2022 14:16:22 - INFO - __main__ -   Text: ['This is an act intended to persuade you that you \"is\".']\n",
      "06/30/2022 14:16:23 - INFO - __main__ -   Epoch: 51 | Batch: 7500/10001 (75%) | G Loss: 2.170703 | C Loss: -1.040839\n",
      "06/30/2022 14:16:23 - INFO - __main__ -   Text: ['The Logos will let go of whatever material this world throws at it.']\n",
      "06/30/2022 14:16:24 - INFO - __main__ -   Epoch: 51 | Batch: 8000/10001 (80%) | G Loss: 1.276353 | C Loss: -0.623902\n",
      "06/30/2022 14:16:24 - INFO - __main__ -   Text: ['BCAA offers draft cards.']\n",
      "06/30/2022 14:16:26 - INFO - __main__ -   Epoch: 51 | Batch: 8500/10001 (85%) | G Loss: 3.674707 | C Loss: -1.508452\n",
      "06/30/2022 14:16:26 - INFO - __main__ -   Text: ['Muq is options 252 and 253.']\n",
      "06/30/2022 14:16:27 - INFO - __main__ -   Epoch: 51 | Batch: 9000/10001 (90%) | G Loss: 0.735091 | C Loss: -0.773589\n",
      "06/30/2022 14:16:27 - INFO - __main__ -   Text: ['Being \"labyrinth\" free of stress, all sets of thoughts will have evolved.']\n",
      "06/30/2022 14:16:29 - INFO - __main__ -   Epoch: 51 | Batch: 9500/10001 (95%) | G Loss: 1.890859 | C Loss: -0.562778\n",
      "06/30/2022 14:16:29 - INFO - __main__ -   Text: ['Meoneou diabetes\" (\"Eenko\").']\n",
      "06/30/2022 14:16:30 - INFO - __main__ -   * (Train) Epoch: 51 | G Loss: 1.3317 | C Loss: -0.9278 | Updates G: 213 | Updates C: 787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:16:39 - INFO - __main__ -   Bleu-2:0.208 | B-Bleu-2:0.261\n",
      "06/30/2022 14:16:39 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_10.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46851830390927623\n",
      "Train file used is number 10\n",
      "../../yahoo/subdivided_large/train_10.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 52 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:22.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:32.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:44.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:54.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:05.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:17.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:27.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:38.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:49.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:59.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:11.\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:21.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:32.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:43.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:53.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:04.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:14.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:25.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:36.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:46.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:57.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:07.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:18.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:29.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:40.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:50.\n",
      "  Batch   170  of    172.    Elapsed: 0:03:01.\n",
      "\n",
      "  Average training loss discriminator: 0.015\n",
      "  Training epcoh took: 0:03:03\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:23:28 - INFO - __main__ -   Epoch: 53 | Batch: 0/10001 (0%) | G Loss: 2.082264 | C Loss: -0.836157\n",
      "06/30/2022 14:23:28 - INFO - __main__ -   Text: ['The cult of evil is a deeply ideological sort of heresy.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.432\n",
      "  Test Loss: 3.128\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:23:29 - INFO - __main__ -   Epoch: 53 | Batch: 500/10001 (5%) | G Loss: 0.721507 | C Loss: -1.076896\n",
      "06/30/2022 14:23:29 - INFO - __main__ -   Text: ['\"For chiram\".']\n",
      "06/30/2022 14:23:30 - INFO - __main__ -   Epoch: 53 | Batch: 1000/10001 (10%) | G Loss: 3.499335 | C Loss: -1.148708\n",
      "06/30/2022 14:23:30 - INFO - __main__ -   Text: ['Despite these circumstances, the most powerful man is also the best.']\n",
      "06/30/2022 14:23:32 - INFO - __main__ -   Epoch: 53 | Batch: 1500/10001 (15%) | G Loss: 1.263291 | C Loss: -1.310088\n",
      "06/30/2022 14:23:32 - INFO - __main__ -   Text: ['bonus players do not get access to this port.']\n",
      "06/30/2022 14:23:33 - INFO - __main__ -   Epoch: 53 | Batch: 2000/10001 (20%) | G Loss: 1.097177 | C Loss: -1.134891\n",
      "06/30/2022 14:23:33 - INFO - __main__ -   Text: [\"They bet me on it but they don't wish to hear.\"]\n",
      "06/30/2022 14:23:34 - INFO - __main__ -   Epoch: 53 | Batch: 2500/10001 (25%) | G Loss: 3.270196 | C Loss: -1.541493\n",
      "06/30/2022 14:23:35 - INFO - __main__ -   Text: ['He decides to be gay.']\n",
      "06/30/2022 14:23:36 - INFO - __main__ -   Epoch: 53 | Batch: 3000/10001 (30%) | G Loss: 3.497984 | C Loss: -1.594471\n",
      "06/30/2022 14:23:36 - INFO - __main__ -   Text: [\"This scarecrow guy doesn't understand how most people think.\"]\n",
      "06/30/2022 14:23:37 - INFO - __main__ -   Epoch: 53 | Batch: 3500/10001 (35%) | G Loss: 0.401994 | C Loss: -0.403306\n",
      "06/30/2022 14:23:37 - INFO - __main__ -   Text: ['A computer can be forensics expert or adventure book author.']\n",
      "06/30/2022 14:23:39 - INFO - __main__ -   Epoch: 53 | Batch: 4000/10001 (40%) | G Loss: 6.622037 | C Loss: -2.641442\n",
      "06/30/2022 14:23:39 - INFO - __main__ -   Text: ['Bubfreyredor is soon becoming one of my favourite favourites.']\n",
      "06/30/2022 14:23:40 - INFO - __main__ -   Epoch: 53 | Batch: 4500/10001 (45%) | G Loss: -0.676817 | C Loss: 0.097976\n",
      "06/30/2022 14:23:40 - INFO - __main__ -   Text: ['Future market research is sure to find a way to lower the power consumption of an industry.']\n",
      "06/30/2022 14:23:42 - INFO - __main__ -   Epoch: 53 | Batch: 5000/10001 (50%) | G Loss: 1.902036 | C Loss: -0.947504\n",
      "06/30/2022 14:23:42 - INFO - __main__ -   Text: ['A word belongs to someone ment uses.']\n",
      "06/30/2022 14:23:43 - INFO - __main__ -   Epoch: 53 | Batch: 5500/10001 (55%) | G Loss: 2.933946 | C Loss: -1.187749\n",
      "06/30/2022 14:23:43 - INFO - __main__ -   Text: ['According to Wordfable, the single is \"Cool Kids News.\"']\n",
      "06/30/2022 14:23:44 - INFO - __main__ -   Epoch: 53 | Batch: 6000/10001 (60%) | G Loss: 1.287182 | C Loss: -1.055732\n",
      "06/30/2022 14:23:45 - INFO - __main__ -   Text: ['The Listener to Memory Archive!']\n",
      "06/30/2022 14:23:46 - INFO - __main__ -   Epoch: 53 | Batch: 6500/10001 (65%) | G Loss: 1.643826 | C Loss: -0.942285\n",
      "06/30/2022 14:23:46 - INFO - __main__ -   Text: [\"Spectrum is one of America's most profitable agency.\"]\n",
      "06/30/2022 14:23:47 - INFO - __main__ -   Epoch: 53 | Batch: 7000/10001 (70%) | G Loss: 1.654593 | C Loss: -1.006934\n",
      "06/30/2022 14:23:47 - INFO - __main__ -   Text: [\"That's it!\"]\n",
      "06/30/2022 14:23:48 - INFO - __main__ -   Epoch: 53 | Batch: 7500/10001 (75%) | G Loss: 3.191010 | C Loss: -1.914886\n",
      "06/30/2022 14:23:48 - INFO - __main__ -   Text: ['There is a process going on inside our body.']\n",
      "06/30/2022 14:23:50 - INFO - __main__ -   Epoch: 53 | Batch: 8000/10001 (80%) | G Loss: 2.936457 | C Loss: -0.924651\n",
      "06/30/2022 14:23:50 - INFO - __main__ -   Text: ['Such algorithms often use mathematics.']\n",
      "06/30/2022 14:23:51 - INFO - __main__ -   Epoch: 53 | Batch: 8500/10001 (85%) | G Loss: 0.333358 | C Loss: -0.805909\n",
      "06/30/2022 14:23:51 - INFO - __main__ -   Text: ['These yarns can be harmful to customers\".']\n",
      "06/30/2022 14:23:52 - INFO - __main__ -   Epoch: 53 | Batch: 9000/10001 (90%) | G Loss: 5.258788 | C Loss: -2.127576\n",
      "06/30/2022 14:23:53 - INFO - __main__ -   Text: ['it is quite funny.']\n",
      "06/30/2022 14:23:54 - INFO - __main__ -   Epoch: 53 | Batch: 9500/10001 (95%) | G Loss: 2.288489 | C Loss: -1.264496\n",
      "06/30/2022 14:23:54 - INFO - __main__ -   Text: ['Resistance is mainly intoxicating since many people target horses.']\n",
      "06/30/2022 14:23:55 - INFO - __main__ -   * (Train) Epoch: 53 | G Loss: 1.3725 | C Loss: -1.1980 | Updates G: 264 | Updates C: 736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:24:04 - INFO - __main__ -   Bleu-2:0.228 | B-Bleu-2:0.245\n",
      "06/30/2022 14:24:04 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_12.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4724280661374003\n",
      "Train file used is number 12\n",
      "../../yahoo/subdivided_large/train_12.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 54 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:22.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:33.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:44.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:55.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:07.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:18.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:29.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:40.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:51.\n",
      "  Batch   110  of    172.    Elapsed: 0:02:02.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:14.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:25.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:36.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:47.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:58.\n",
      "  Batch   170  of    172.    Elapsed: 0:03:08.\n",
      "\n",
      "  Average training loss discriminator: 0.018\n",
      "  Training epcoh took: 0:03:11\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:27:15 - INFO - __main__ -   Epoch: 54 | Batch: 0/10001 (0%) | G Loss: -0.216918 | C Loss: -0.542599\n",
      "06/30/2022 14:27:15 - INFO - __main__ -   Text: [\"Mania suggests that we're asleep in a dead universe that appears to lie somewhere on our planet.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.438\n",
      "  Test Loss: 2.932\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:27:17 - INFO - __main__ -   Epoch: 54 | Batch: 500/10001 (5%) | G Loss: 4.877992 | C Loss: -2.637726\n",
      "06/30/2022 14:27:17 - INFO - __main__ -   Text: ['a lot of people try to force me to do this.']\n",
      "06/30/2022 14:27:18 - INFO - __main__ -   Epoch: 54 | Batch: 1000/10001 (10%) | G Loss: 1.228851 | C Loss: -0.835265\n",
      "06/30/2022 14:27:18 - INFO - __main__ -   Text: ['This is an important distinction since Christopher Columbus said that this planet emits a cloud.']\n",
      "06/30/2022 14:27:19 - INFO - __main__ -   Epoch: 54 | Batch: 1500/10001 (15%) | G Loss: 1.393577 | C Loss: -0.902466\n",
      "06/30/2022 14:27:20 - INFO - __main__ -   Text: ['References book may be in English.']\n",
      "06/30/2022 14:27:21 - INFO - __main__ -   Epoch: 54 | Batch: 2000/10001 (20%) | G Loss: 0.947794 | C Loss: -1.145992\n",
      "06/30/2022 14:27:21 - INFO - __main__ -   Text: ['It\\'s like I\\'ll ask you questions and if you don\\'t say anything.\"']\n",
      "06/30/2022 14:27:22 - INFO - __main__ -   Epoch: 54 | Batch: 2500/10001 (25%) | G Loss: 1.275711 | C Loss: -0.857716\n",
      "06/30/2022 14:27:22 - INFO - __main__ -   Text: ['I can\\'t take that one wrong.\"']\n",
      "06/30/2022 14:27:23 - INFO - __main__ -   Epoch: 54 | Batch: 3000/10001 (30%) | G Loss: 1.305916 | C Loss: -1.184075\n",
      "06/30/2022 14:27:23 - INFO - __main__ -   Text: ['Potentially there is a Mac language that is simple but elusive.']\n",
      "06/30/2022 14:27:25 - INFO - __main__ -   Epoch: 54 | Batch: 3500/10001 (35%) | G Loss: 1.277619 | C Loss: -1.089701\n",
      "06/30/2022 14:27:25 - INFO - __main__ -   Text: ['With this arrangement, it is responsible for charging consumers more taxes than an insurance company.']\n",
      "06/30/2022 14:27:26 - INFO - __main__ -   Epoch: 54 | Batch: 4000/10001 (40%) | G Loss: 1.136652 | C Loss: -0.913458\n",
      "06/30/2022 14:27:26 - INFO - __main__ -   Text: [\"Mago's puzzles are quite close, and after reading one version\"]\n",
      "06/30/2022 14:27:28 - INFO - __main__ -   Epoch: 54 | Batch: 4500/10001 (45%) | G Loss: 1.426112 | C Loss: -1.250517\n",
      "06/30/2022 14:27:28 - INFO - __main__ -   Text: ['She is also opposed to militant eco-tricks.']\n",
      "06/30/2022 14:27:29 - INFO - __main__ -   Epoch: 54 | Batch: 5000/10001 (50%) | G Loss: 1.119676 | C Loss: -0.875453\n",
      "06/30/2022 14:27:29 - INFO - __main__ -   Text: [\"The threat of suicide doesn't come with the lock-up on his fifth note and everyone wonders why he isn't\"]\n",
      "06/30/2022 14:27:31 - INFO - __main__ -   Epoch: 54 | Batch: 5500/10001 (55%) | G Loss: 1.158220 | C Loss: -0.760040\n",
      "06/30/2022 14:27:31 - INFO - __main__ -   Text: ['Typical person\\'s supplies are Internet \"\"\".']\n",
      "06/30/2022 14:27:32 - INFO - __main__ -   Epoch: 54 | Batch: 6000/10001 (60%) | G Loss: 1.305079 | C Loss: -0.983033\n",
      "06/30/2022 14:27:32 - INFO - __main__ -   Text: ['Cesar Epsteins 10 worst comedians: Eminem.']\n",
      "06/30/2022 14:27:33 - INFO - __main__ -   Epoch: 54 | Batch: 6500/10001 (65%) | G Loss: 1.891971 | C Loss: -1.244809\n",
      "06/30/2022 14:27:34 - INFO - __main__ -   Text: ['To give an example, Nogis would only use English.']\n",
      "06/30/2022 14:27:35 - INFO - __main__ -   Epoch: 54 | Batch: 7000/10001 (70%) | G Loss: 1.020218 | C Loss: -1.013532\n",
      "06/30/2022 14:27:35 - INFO - __main__ -   Text: ['It takes 8 minutes to spit it, and then whoop.']\n",
      "06/30/2022 14:27:36 - INFO - __main__ -   Epoch: 54 | Batch: 7500/10001 (75%) | G Loss: 0.944358 | C Loss: -1.073660\n",
      "06/30/2022 14:27:37 - INFO - __main__ -   Text: ['Good value for 5 an iritue.\"']\n",
      "06/30/2022 14:27:37 - INFO - __main__ -   Epoch: 54 | Batch: 8000/10001 (80%) | G Loss: 1.381933 | C Loss: -1.022201\n",
      "06/30/2022 14:27:37 - INFO - __main__ -   Text: ['Goth, and Dan that\\'s fine.\"']\n",
      "06/30/2022 14:27:38 - INFO - __main__ -   Epoch: 54 | Batch: 8500/10001 (85%) | G Loss: 1.528299 | C Loss: -1.162020\n",
      "06/30/2022 14:27:38 - INFO - __main__ -   Text: ['Such reasoning is what Fare put to study.']\n",
      "06/30/2022 14:27:39 - INFO - __main__ -   Epoch: 54 | Batch: 9000/10001 (90%) | G Loss: 1.159096 | C Loss: -1.241608\n",
      "06/30/2022 14:27:39 - INFO - __main__ -   Text: ['Besides being able to speak many other languages, Persephone is of Middle Eastern origin.']\n",
      "06/30/2022 14:27:40 - INFO - __main__ -   Epoch: 54 | Batch: 9500/10001 (95%) | G Loss: 1.579890 | C Loss: -1.103013\n",
      "06/30/2022 14:27:40 - INFO - __main__ -   Text: ['In the above chart, you must have Salang National.']\n",
      "06/30/2022 14:27:41 - INFO - __main__ -   * (Train) Epoch: 54 | G Loss: 1.3805 | C Loss: -1.0924 | Updates G: 200 | Updates C: 800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:27:50 - INFO - __main__ -   Bleu-2:0.212 | B-Bleu-2:0.274\n",
      "06/30/2022 14:27:50 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_13.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4861538254297571\n",
      "Train file used is number 13\n",
      "../../yahoo/subdivided_large/train_13.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 55 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:23.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:34.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:46.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:57.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:09.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:20.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:32.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:43.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:55.\n",
      "  Batch   110  of    172.    Elapsed: 0:02:07.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:18.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:30.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:41.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:53.\n",
      "  Batch   160  of    172.    Elapsed: 0:03:05.\n",
      "  Batch   170  of    172.    Elapsed: 0:03:17.\n",
      "\n",
      "  Average training loss discriminator: 0.011\n",
      "  Training epcoh took: 0:03:19\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:31:09 - INFO - __main__ -   Epoch: 55 | Batch: 0/10001 (0%) | G Loss: 1.230238 | C Loss: -1.165948\n",
      "06/30/2022 14:31:09 - INFO - __main__ -   Text: ['Alternate letters are the extremes of truth.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.425\n",
      "  Test Loss: 2.977\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:31:10 - INFO - __main__ -   Epoch: 55 | Batch: 500/10001 (5%) | G Loss: 1.578536 | C Loss: -1.299091\n",
      "06/30/2022 14:31:10 - INFO - __main__ -   Text: ['This account in fact is exactly the reverse of the babbling Mongolian.']\n",
      "06/30/2022 14:31:11 - INFO - __main__ -   Epoch: 55 | Batch: 1000/10001 (10%) | G Loss: 1.484669 | C Loss: -0.814960\n",
      "06/30/2022 14:31:11 - INFO - __main__ -   Text: ['Travelers interface quantum conditioning through Somom.']\n",
      "06/30/2022 14:31:13 - INFO - __main__ -   Epoch: 55 | Batch: 1500/10001 (15%) | G Loss: 1.800642 | C Loss: -0.844439\n",
      "06/30/2022 14:31:13 - INFO - __main__ -   Text: ['In mathematics, \"Big -a\" is the commonly used choice.']\n",
      "06/30/2022 14:31:14 - INFO - __main__ -   Epoch: 55 | Batch: 2000/10001 (20%) | G Loss: 1.139413 | C Loss: -0.932689\n",
      "06/30/2022 14:31:14 - INFO - __main__ -   Text: ['\"Messenger Knot\" number 38.\"']\n",
      "06/30/2022 14:31:15 - INFO - __main__ -   Epoch: 55 | Batch: 2500/10001 (25%) | G Loss: 1.572074 | C Loss: -1.225863\n",
      "06/30/2022 14:31:15 - INFO - __main__ -   Text: ['\"F\" for two places.']\n",
      "06/30/2022 14:31:16 - INFO - __main__ -   Epoch: 55 | Batch: 3000/10001 (30%) | G Loss: 1.996437 | C Loss: -1.022094\n",
      "06/30/2022 14:31:16 - INFO - __main__ -   Text: ['Trolley split!']\n",
      "06/30/2022 14:31:17 - INFO - __main__ -   Epoch: 55 | Batch: 3500/10001 (35%) | G Loss: 1.563746 | C Loss: -1.331450\n",
      "06/30/2022 14:31:18 - INFO - __main__ -   Text: ['This fax modem utilizes a protector which lets it read e.g.']\n",
      "06/30/2022 14:31:19 - INFO - __main__ -   Epoch: 55 | Batch: 4000/10001 (40%) | G Loss: 1.034049 | C Loss: -1.335685\n",
      "06/30/2022 14:31:19 - INFO - __main__ -   Text: ['The most dangerous discovery of only a fascist threat is the world rather than the United States.']\n",
      "06/30/2022 14:31:20 - INFO - __main__ -   Epoch: 55 | Batch: 4500/10001 (45%) | G Loss: 1.600250 | C Loss: -1.155872\n",
      "06/30/2022 14:31:20 - INFO - __main__ -   Text: ['When reading in French, it is almost impossible to distinguish one literate from the other.']\n",
      "06/30/2022 14:31:21 - INFO - __main__ -   Epoch: 55 | Batch: 5000/10001 (50%) | G Loss: 1.828087 | C Loss: -1.519979\n",
      "06/30/2022 14:31:21 - INFO - __main__ -   Text: [\"The original name makes this Douglas the worst 'investigator' in Star Wars.\"]\n",
      "06/30/2022 14:31:23 - INFO - __main__ -   Epoch: 55 | Batch: 5500/10001 (55%) | G Loss: 1.104056 | C Loss: -1.077420\n",
      "06/30/2022 14:31:23 - INFO - __main__ -   Text: ['This is, essentially, a wish list of traditions, not states.']\n",
      "06/30/2022 14:31:24 - INFO - __main__ -   Epoch: 55 | Batch: 6000/10001 (60%) | G Loss: 1.336796 | C Loss: -0.921868\n",
      "06/30/2022 14:31:24 - INFO - __main__ -   Text: ['The show averages every week by 0 points in Low Score positions boost.']\n",
      "06/30/2022 14:31:25 - INFO - __main__ -   Epoch: 55 | Batch: 6500/10001 (65%) | G Loss: 1.459166 | C Loss: -1.154197\n",
      "06/30/2022 14:31:25 - INFO - __main__ -   Text: ['Gospel theology offers a reasoned approach to faith.']\n",
      "06/30/2022 14:31:26 - INFO - __main__ -   Epoch: 55 | Batch: 7000/10001 (70%) | G Loss: 1.687259 | C Loss: -1.011913\n",
      "06/30/2022 14:31:26 - INFO - __main__ -   Text: ['This leads him to know that sometimes he will like the crush more than once.']\n",
      "06/30/2022 14:31:27 - INFO - __main__ -   Epoch: 55 | Batch: 7500/10001 (75%) | G Loss: 1.704012 | C Loss: -0.909522\n",
      "06/30/2022 14:31:28 - INFO - __main__ -   Text: ['The information he knows, through Google, must be patented to an alien.']\n",
      "06/30/2022 14:31:29 - INFO - __main__ -   Epoch: 55 | Batch: 8000/10001 (80%) | G Loss: 1.706107 | C Loss: -1.163218\n",
      "06/30/2022 14:31:29 - INFO - __main__ -   Text: ['It\\'s like psychology\".']\n",
      "06/30/2022 14:31:30 - INFO - __main__ -   Epoch: 55 | Batch: 8500/10001 (85%) | G Loss: 1.013230 | C Loss: -0.904620\n",
      "06/30/2022 14:31:30 - INFO - __main__ -   Text: ['socio-cultural music is not sexy.\"']\n",
      "06/30/2022 14:31:31 - INFO - __main__ -   Epoch: 55 | Batch: 9000/10001 (90%) | G Loss: 1.795028 | C Loss: -1.197955\n",
      "06/30/2022 14:31:31 - INFO - __main__ -   Text: ['Allah will help you to write.\"']\n",
      "06/30/2022 14:31:32 - INFO - __main__ -   Epoch: 55 | Batch: 9500/10001 (95%) | G Loss: 1.710501 | C Loss: -1.135961\n",
      "06/30/2022 14:31:32 - INFO - __main__ -   Text: ['It\\'s like lifting weights quickly and taking the many steps.\"']\n",
      "06/30/2022 14:31:33 - INFO - __main__ -   * (Train) Epoch: 55 | G Loss: 1.5380 | C Loss: -1.0980 | Updates G: 196 | Updates C: 804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:31:43 - INFO - __main__ -   Bleu-2:0.215 | B-Bleu-2:0.249\n",
      "06/30/2022 14:31:43 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_14.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4634104558741452\n",
      "Train file used is number 14\n",
      "../../yahoo/subdivided_large/train_14.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 56 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:23.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:34.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:46.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:57.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:08.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:20.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:31.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:43.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:55.\n",
      "  Batch   110  of    172.    Elapsed: 0:02:06.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:18.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:29.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:41.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:53.\n",
      "  Batch   160  of    172.    Elapsed: 0:03:04.\n",
      "  Batch   170  of    172.    Elapsed: 0:03:15.\n",
      "\n",
      "  Average training loss discriminator: 0.013\n",
      "  Training epcoh took: 0:03:17\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:35:01 - INFO - __main__ -   Epoch: 56 | Batch: 0/10001 (0%) | G Loss: 1.781810 | C Loss: -1.101037\n",
      "06/30/2022 14:35:01 - INFO - __main__ -   Text: ['The best of both worlds is to repeat.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.445\n",
      "  Test Loss: 2.940\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:35:02 - INFO - __main__ -   Epoch: 56 | Batch: 500/10001 (5%) | G Loss: 1.870254 | C Loss: -0.944726\n",
      "06/30/2022 14:35:02 - INFO - __main__ -   Text: ['The Mayhem spirit website exists ist spam.']\n",
      "06/30/2022 14:35:03 - INFO - __main__ -   Epoch: 56 | Batch: 1000/10001 (10%) | G Loss: 1.627418 | C Loss: -0.938807\n",
      "06/30/2022 14:35:03 - INFO - __main__ -   Text: ['He likes it when people tell him they love him.']\n",
      "06/30/2022 14:35:05 - INFO - __main__ -   Epoch: 56 | Batch: 1500/10001 (15%) | G Loss: 2.056051 | C Loss: -1.152291\n",
      "06/30/2022 14:35:05 - INFO - __main__ -   Text: ['In this blog, I have been discussing treatments for autism.']\n",
      "06/30/2022 14:35:06 - INFO - __main__ -   Epoch: 56 | Batch: 2000/10001 (20%) | G Loss: 1.812089 | C Loss: -0.885378\n",
      "06/30/2022 14:35:06 - INFO - __main__ -   Text: ['\"Although wings can dig in, they are not effective as filter tracks.\"']\n",
      "06/30/2022 14:35:08 - INFO - __main__ -   Epoch: 56 | Batch: 2500/10001 (25%) | G Loss: 1.833609 | C Loss: -1.043090\n",
      "06/30/2022 14:35:08 - INFO - __main__ -   Text: ['This is our delving.']\n",
      "06/30/2022 14:35:09 - INFO - __main__ -   Epoch: 56 | Batch: 3000/10001 (30%) | G Loss: 1.206369 | C Loss: -0.905751\n",
      "06/30/2022 14:35:09 - INFO - __main__ -   Text: ['Much of what is mentioned is sometimes inadequate.']\n",
      "06/30/2022 14:35:10 - INFO - __main__ -   Epoch: 56 | Batch: 3500/10001 (35%) | G Loss: 2.763013 | C Loss: -1.032304\n",
      "06/30/2022 14:35:10 - INFO - __main__ -   Text: [\"Internet blogs have always been my husband's go-to book reading regimen.\"]\n",
      "06/30/2022 14:35:11 - INFO - __main__ -   Epoch: 56 | Batch: 4000/10001 (40%) | G Loss: 1.332643 | C Loss: -0.889556\n",
      "06/30/2022 14:35:11 - INFO - __main__ -   Text: ['It is easy to share political views.\"']\n",
      "06/30/2022 14:35:12 - INFO - __main__ -   Epoch: 56 | Batch: 4500/10001 (45%) | G Loss: 1.984646 | C Loss: -0.997519\n",
      "06/30/2022 14:35:12 - INFO - __main__ -   Text: ['The necessary quantum engineering instructs us to assume that potable water is abundant.']\n",
      "06/30/2022 14:35:13 - INFO - __main__ -   Epoch: 56 | Batch: 5000/10001 (50%) | G Loss: 1.460896 | C Loss: -1.018425\n",
      "06/30/2022 14:35:13 - INFO - __main__ -   Text: ['Think about it.']\n",
      "06/30/2022 14:35:15 - INFO - __main__ -   Epoch: 56 | Batch: 5500/10001 (55%) | G Loss: 1.686683 | C Loss: -1.106435\n",
      "06/30/2022 14:35:15 - INFO - __main__ -   Text: ['He knows that this is an extreme step, and plans to study flight.']\n",
      "06/30/2022 14:35:16 - INFO - __main__ -   Epoch: 56 | Batch: 6000/10001 (60%) | G Loss: 1.194584 | C Loss: -0.960355\n",
      "06/30/2022 14:35:16 - INFO - __main__ -   Text: ['\"You\\'re sugar-flashing.\"']\n",
      "06/30/2022 14:35:17 - INFO - __main__ -   Epoch: 56 | Batch: 6500/10001 (65%) | G Loss: 2.489440 | C Loss: -1.229628\n",
      "06/30/2022 14:35:18 - INFO - __main__ -   Text: ['But my love is not strength but a hand.']\n",
      "06/30/2022 14:35:19 - INFO - __main__ -   Epoch: 56 | Batch: 7000/10001 (70%) | G Loss: 1.040214 | C Loss: -0.784069\n",
      "06/30/2022 14:35:19 - INFO - __main__ -   Text: ['These are not limited to life sciences theories.']\n",
      "06/30/2022 14:35:20 - INFO - __main__ -   Epoch: 56 | Batch: 7500/10001 (75%) | G Loss: 2.439773 | C Loss: -1.252142\n",
      "06/30/2022 14:35:20 - INFO - __main__ -   Text: ['that makes it a good candidate to include mathematical topics in your company.']\n",
      "06/30/2022 14:35:22 - INFO - __main__ -   Epoch: 56 | Batch: 8000/10001 (80%) | G Loss: 1.935560 | C Loss: -1.015435\n",
      "06/30/2022 14:35:22 - INFO - __main__ -   Text: [\"W $'. <PAD> !!\"]\n",
      "06/30/2022 14:35:23 - INFO - __main__ -   Epoch: 56 | Batch: 8500/10001 (85%) | G Loss: 1.620825 | C Loss: -0.862760\n",
      "06/30/2022 14:35:23 - INFO - __main__ -   Text: ['It is possible to play the same game the two of us have did, when we are too old.']\n",
      "06/30/2022 14:35:25 - INFO - __main__ -   Epoch: 56 | Batch: 9000/10001 (90%) | G Loss: 1.780914 | C Loss: -0.766306\n",
      "06/30/2022 14:35:25 - INFO - __main__ -   Text: ['It is called \"Motor Songs Wil Mudhead.\"']\n",
      "06/30/2022 14:35:26 - INFO - __main__ -   Epoch: 56 | Batch: 9500/10001 (95%) | G Loss: 0.977832 | C Loss: -0.914088\n",
      "06/30/2022 14:35:26 - INFO - __main__ -   Text: ['These drugs bind to animals to cause a sensation that they may never again experience.']\n",
      "06/30/2022 14:35:28 - INFO - __main__ -   * (Train) Epoch: 56 | G Loss: 1.6627 | C Loss: -1.0474 | Updates G: 181 | Updates C: 819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:35:37 - INFO - __main__ -   Bleu-2:0.193 | B-Bleu-2:0.252\n",
      "06/30/2022 14:35:37 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_15.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4456097141042573\n",
      "Train file used is number 15\n",
      "../../yahoo/subdivided_large/train_15.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 57 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:23.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:35.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:46.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:58.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:10.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:21.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:32.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:44.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:56.\n",
      "  Batch   110  of    172.    Elapsed: 0:02:07.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:19.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:30.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:42.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:54.\n",
      "  Batch   160  of    172.    Elapsed: 0:03:05.\n",
      "  Batch   170  of    172.    Elapsed: 0:03:17.\n",
      "\n",
      "  Average training loss discriminator: 0.012\n",
      "  Training epcoh took: 0:03:19\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:38:57 - INFO - __main__ -   Epoch: 57 | Batch: 0/10001 (0%) | G Loss: 1.642386 | C Loss: -0.847281\n",
      "06/30/2022 14:38:57 - INFO - __main__ -   Text: [\"I'm quite familiar with Molda bird.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.427\n",
      "  Test Loss: 3.065\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:38:58 - INFO - __main__ -   Epoch: 57 | Batch: 500/10001 (5%) | G Loss: 1.672674 | C Loss: -0.957829\n",
      "06/30/2022 14:38:58 - INFO - __main__ -   Text: ['He is unsure if this is a hoax.']\n",
      "06/30/2022 14:38:59 - INFO - __main__ -   Epoch: 57 | Batch: 1000/10001 (10%) | G Loss: 1.565947 | C Loss: -1.190812\n",
      "06/30/2022 14:39:00 - INFO - __main__ -   Text: ['This is realistic, and emotional.']\n",
      "06/30/2022 14:39:01 - INFO - __main__ -   Epoch: 57 | Batch: 1500/10001 (15%) | G Loss: 1.784477 | C Loss: -1.152454\n",
      "06/30/2022 14:39:01 - INFO - __main__ -   Text: ['They are creatures that frighten those who try to understand them.']\n",
      "06/30/2022 14:39:02 - INFO - __main__ -   Epoch: 57 | Batch: 2000/10001 (20%) | G Loss: 1.733031 | C Loss: -1.086607\n",
      "06/30/2022 14:39:02 - INFO - __main__ -   Text: ['Anything can come to me.']\n",
      "06/30/2022 14:39:04 - INFO - __main__ -   Epoch: 57 | Batch: 2500/10001 (25%) | G Loss: 1.529386 | C Loss: -1.023414\n",
      "06/30/2022 14:39:04 - INFO - __main__ -   Text: ['It is by far the best cornbread soured with a larger grain.']\n",
      "06/30/2022 14:39:05 - INFO - __main__ -   Epoch: 57 | Batch: 3000/10001 (30%) | G Loss: 2.019916 | C Loss: -1.082848\n",
      "06/30/2022 14:39:05 - INFO - __main__ -   Text: ['Ornithology terms are: Mountain Riders, Yorkshire Mountain Riders, Mannies Ladies.']\n",
      "06/30/2022 14:39:07 - INFO - __main__ -   Epoch: 57 | Batch: 3500/10001 (35%) | G Loss: 1.814101 | C Loss: -1.361822\n",
      "06/30/2022 14:39:07 - INFO - __main__ -   Text: ['He then explains why he refuses to wear a white & black T-shirt.']\n",
      "06/30/2022 14:39:08 - INFO - __main__ -   Epoch: 57 | Batch: 4000/10001 (40%) | G Loss: 1.862170 | C Loss: -0.811990\n",
      "06/30/2022 14:39:08 - INFO - __main__ -   Text: ['\"He is prolific and proud prose writer\".']\n",
      "06/30/2022 14:39:09 - INFO - __main__ -   Epoch: 57 | Batch: 4500/10001 (45%) | G Loss: 2.138203 | C Loss: -1.003099\n",
      "06/30/2022 14:39:09 - INFO - __main__ -   Text: ['He may have even unwittingly taken a path into astrology.']\n",
      "06/30/2022 14:39:11 - INFO - __main__ -   Epoch: 57 | Batch: 5000/10001 (50%) | G Loss: 1.418899 | C Loss: -0.903908\n",
      "06/30/2022 14:39:11 - INFO - __main__ -   Text: ['McDonald has found YouTube.']\n",
      "06/30/2022 14:39:12 - INFO - __main__ -   Epoch: 57 | Batch: 5500/10001 (55%) | G Loss: 1.926413 | C Loss: -0.991108\n",
      "06/30/2022 14:39:12 - INFO - __main__ -   Text: [\"He's socially ambiguous in the end... he would like to beat you.\"]\n",
      "06/30/2022 14:39:13 - INFO - __main__ -   Epoch: 57 | Batch: 6000/10001 (60%) | G Loss: 2.152119 | C Loss: -0.987994\n",
      "06/30/2022 14:39:14 - INFO - __main__ -   Text: ['\"Thieves\".']\n",
      "06/30/2022 14:39:15 - INFO - __main__ -   Epoch: 57 | Batch: 6500/10001 (65%) | G Loss: 1.461832 | C Loss: -0.711968\n",
      "06/30/2022 14:39:15 - INFO - __main__ -   Text: ['I am not a businessman.']\n",
      "06/30/2022 14:39:16 - INFO - __main__ -   Epoch: 57 | Batch: 7000/10001 (70%) | G Loss: 1.638920 | C Loss: -0.873519\n",
      "06/30/2022 14:39:16 - INFO - __main__ -   Text: ['Fatae assumes that paralyzed individuals are incorruptible.']\n",
      "06/30/2022 14:39:18 - INFO - __main__ -   Epoch: 57 | Batch: 7500/10001 (75%) | G Loss: 1.558659 | C Loss: -0.842936\n",
      "06/30/2022 14:39:18 - INFO - __main__ -   Text: ['Done is wrong; no evidence of guilt is proof.']\n",
      "06/30/2022 14:39:19 - INFO - __main__ -   Epoch: 57 | Batch: 8000/10001 (80%) | G Loss: 1.311411 | C Loss: -0.944311\n",
      "06/30/2022 14:39:19 - INFO - __main__ -   Text: ['A word patlariks.']\n",
      "06/30/2022 14:39:21 - INFO - __main__ -   Epoch: 57 | Batch: 8500/10001 (85%) | G Loss: 2.517791 | C Loss: -1.234840\n",
      "06/30/2022 14:39:21 - INFO - __main__ -   Text: [\"The ability to know is extremely useful for anonymous peer-to-peer peer-to-peer.''.\"]\n",
      "06/30/2022 14:39:22 - INFO - __main__ -   Epoch: 57 | Batch: 9000/10001 (90%) | G Loss: 3.121258 | C Loss: -0.898901\n",
      "06/30/2022 14:39:22 - INFO - __main__ -   Text: ['The Barbarian Rock String Job.']\n",
      "06/30/2022 14:39:23 - INFO - __main__ -   Epoch: 57 | Batch: 9500/10001 (95%) | G Loss: 1.088503 | C Loss: -1.058181\n",
      "06/30/2022 14:39:24 - INFO - __main__ -   Text: ['Nini Shankar Yeh!']\n",
      "06/30/2022 14:39:25 - INFO - __main__ -   * (Train) Epoch: 57 | G Loss: 1.7139 | C Loss: -0.9850 | Updates G: 152 | Updates C: 848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:39:33 - INFO - __main__ -   Bleu-2:0.185 | B-Bleu-2:0.256\n",
      "06/30/2022 14:39:33 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_16.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44105113673481483\n",
      "Train file used is number 16\n",
      "../../yahoo/subdivided_large/train_16.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 58 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:21.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:31.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:42.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:53.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:04.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:15.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:26.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:36.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:47.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:58.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:09.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:21.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:32.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:43.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:53.\n",
      "  Batch   170  of    172.    Elapsed: 0:03:03.\n",
      "\n",
      "  Average training loss discriminator: 0.014\n",
      "  Training epcoh took: 0:03:05\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:42:39 - INFO - __main__ -   Epoch: 58 | Batch: 0/10001 (0%) | G Loss: 2.626850 | C Loss: -1.016377\n",
      "06/30/2022 14:42:39 - INFO - __main__ -   Text: ['But yet, \"I or ... is not true\".']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.440\n",
      "  Test Loss: 2.969\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:42:40 - INFO - __main__ -   Epoch: 58 | Batch: 500/10001 (5%) | G Loss: 1.426775 | C Loss: -0.900959\n",
      "06/30/2022 14:42:40 - INFO - __main__ -   Text: ['in the most sports leagues.']\n",
      "06/30/2022 14:42:41 - INFO - __main__ -   Epoch: 58 | Batch: 1000/10001 (10%) | G Loss: 3.794503 | C Loss: -1.492622\n",
      "06/30/2022 14:42:42 - INFO - __main__ -   Text: [\"Thief's eye is black.\"]\n",
      "06/30/2022 14:42:43 - INFO - __main__ -   Epoch: 58 | Batch: 1500/10001 (15%) | G Loss: 1.635378 | C Loss: -0.901252\n",
      "06/30/2022 14:42:43 - INFO - __main__ -   Text: ['From here, it is Roader\".']\n",
      "06/30/2022 14:42:44 - INFO - __main__ -   Epoch: 58 | Batch: 2000/10001 (20%) | G Loss: 0.797086 | C Loss: -0.887131\n",
      "06/30/2022 14:42:44 - INFO - __main__ -   Text: ['to another MSM.']\n",
      "06/30/2022 14:42:46 - INFO - __main__ -   Epoch: 58 | Batch: 2500/10001 (25%) | G Loss: 1.864665 | C Loss: -0.840478\n",
      "06/30/2022 14:42:46 - INFO - __main__ -   Text: ['The process_0 formula is the formula_1 has\".']\n",
      "06/30/2022 14:42:47 - INFO - __main__ -   Epoch: 58 | Batch: 3000/10001 (30%) | G Loss: 1.145128 | C Loss: -1.011167\n",
      "06/30/2022 14:42:47 - INFO - __main__ -   Text: ['Arabian Ghah!']\n",
      "06/30/2022 14:42:48 - INFO - __main__ -   Epoch: 58 | Batch: 3500/10001 (35%) | G Loss: 1.509650 | C Loss: -0.927012\n",
      "06/30/2022 14:42:48 - INFO - __main__ -   Text: ['The night lab is to make clever experiments.']\n",
      "06/30/2022 14:42:50 - INFO - __main__ -   Epoch: 58 | Batch: 4000/10001 (40%) | G Loss: 1.844340 | C Loss: -0.878646\n",
      "06/30/2022 14:42:50 - INFO - __main__ -   Text: ['Non-smoking foods can be consumed through smoking.']\n",
      "06/30/2022 14:42:51 - INFO - __main__ -   Epoch: 58 | Batch: 4500/10001 (45%) | G Loss: 1.735366 | C Loss: -0.938586\n",
      "06/30/2022 14:42:51 - INFO - __main__ -   Text: ['Not only can the fear of group be broken, but it will also send a telepathic message to cascade.']\n",
      "06/30/2022 14:42:52 - INFO - __main__ -   Epoch: 58 | Batch: 5000/10001 (50%) | G Loss: 1.626934 | C Loss: -0.945399\n",
      "06/30/2022 14:42:52 - INFO - __main__ -   Text: ['Vanillia, it\\'s not that she wants to \"take you in a kissing piece\".']\n",
      "06/30/2022 14:42:53 - INFO - __main__ -   Epoch: 58 | Batch: 5500/10001 (55%) | G Loss: 2.082214 | C Loss: -0.786364\n",
      "06/30/2022 14:42:54 - INFO - __main__ -   Text: ['It is called \"Shindig Message\" a special online newsletter so with it -~~Give e-mail a']\n",
      "06/30/2022 14:42:55 - INFO - __main__ -   Epoch: 58 | Batch: 6000/10001 (60%) | G Loss: 1.884194 | C Loss: -1.099653\n",
      "06/30/2022 14:42:55 - INFO - __main__ -   Text: ['Fans of Deadwood experience this happening since its release on April 16, 2003 and has become horrid.']\n",
      "06/30/2022 14:42:57 - INFO - __main__ -   Epoch: 58 | Batch: 6500/10001 (65%) | G Loss: 1.958110 | C Loss: -0.821847\n",
      "06/30/2022 14:42:57 - INFO - __main__ -   Text: ['They refer to these two places: \"Axel Ostermeister\" and \"Ekaar\".']\n",
      "06/30/2022 14:42:58 - INFO - __main__ -   Epoch: 58 | Batch: 7000/10001 (70%) | G Loss: 2.319530 | C Loss: -0.938658\n",
      "06/30/2022 14:42:58 - INFO - __main__ -   Text: ['If it can, then it can be an algorithm.']\n",
      "06/30/2022 14:43:00 - INFO - __main__ -   Epoch: 58 | Batch: 7500/10001 (75%) | G Loss: 2.297223 | C Loss: -0.830608\n",
      "06/30/2022 14:43:00 - INFO - __main__ -   Text: ['Then, it is also becoming increasingly popular for LGBT athletes.']\n",
      "06/30/2022 14:43:01 - INFO - __main__ -   Epoch: 58 | Batch: 8000/10001 (80%) | G Loss: 1.765400 | C Loss: -0.840782\n",
      "06/30/2022 14:43:01 - INFO - __main__ -   Text: ['Big Brother is the runner-up in the 2018 Half-Life Prize.']\n",
      "06/30/2022 14:43:03 - INFO - __main__ -   Epoch: 58 | Batch: 8500/10001 (85%) | G Loss: 1.297164 | C Loss: -0.953007\n",
      "06/30/2022 14:43:03 - INFO - __main__ -   Text: ['If you don\\'t, you\\'ll get caught.\"']\n",
      "06/30/2022 14:43:04 - INFO - __main__ -   Epoch: 58 | Batch: 9000/10001 (90%) | G Loss: 2.806286 | C Loss: -1.047560\n",
      "06/30/2022 14:43:04 - INFO - __main__ -   Text: [\"Their eyes are very shiny and they're very handsome.\"]\n",
      "06/30/2022 14:43:05 - INFO - __main__ -   Epoch: 58 | Batch: 9500/10001 (95%) | G Loss: 1.418420 | C Loss: -0.948315\n",
      "06/30/2022 14:43:06 - INFO - __main__ -   Text: ['The proof is that \"or really\" is the simplest method.\"']\n",
      "06/30/2022 14:43:07 - INFO - __main__ -   * (Train) Epoch: 58 | G Loss: 1.8238 | C Loss: -0.9239 | Updates G: 151 | Updates C: 849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:43:16 - INFO - __main__ -   Bleu-2:0.201 | B-Bleu-2:0.253\n",
      "06/30/2022 14:43:16 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_17.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4533864243036166\n",
      "Train file used is number 17\n",
      "../../yahoo/subdivided_large/train_17.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 59 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:12.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:23.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:35.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:47.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:58.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:10.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:21.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:32.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:44.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:56.\n",
      "  Batch   110  of    172.    Elapsed: 0:02:07.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:19.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:30.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:42.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:53.\n",
      "  Batch   160  of    172.    Elapsed: 0:03:05.\n",
      "  Batch   170  of    172.    Elapsed: 0:03:17.\n",
      "\n",
      "  Average training loss discriminator: 0.009\n",
      "  Training epcoh took: 0:03:19\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:46:36 - INFO - __main__ -   Epoch: 59 | Batch: 0/10001 (0%) | G Loss: 1.216166 | C Loss: -1.172797\n",
      "06/30/2022 14:46:36 - INFO - __main__ -   Text: ['A search of a lifetime causes a man to appreciate a great book in its proper place.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.450\n",
      "  Test Loss: 3.001\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:46:37 - INFO - __main__ -   Epoch: 59 | Batch: 500/10001 (5%) | G Loss: 2.266073 | C Loss: -0.865809\n",
      "06/30/2022 14:46:37 - INFO - __main__ -   Text: ['\"Those people fall with the Turks and only with the Greeks\".']\n",
      "06/30/2022 14:46:39 - INFO - __main__ -   Epoch: 59 | Batch: 1000/10001 (10%) | G Loss: 1.627509 | C Loss: -0.950980\n",
      "06/30/2022 14:46:39 - INFO - __main__ -   Text: ['603 is not as nave as she claims.']\n",
      "06/30/2022 14:46:40 - INFO - __main__ -   Epoch: 59 | Batch: 1500/10001 (15%) | G Loss: 1.917997 | C Loss: -0.941350\n",
      "06/30/2022 14:46:40 - INFO - __main__ -   Text: ['Some people have suggested that the narrator is jealous; friends of tim walked in and said that.']\n",
      "06/30/2022 14:46:42 - INFO - __main__ -   Epoch: 59 | Batch: 2000/10001 (20%) | G Loss: 1.668528 | C Loss: -0.893900\n",
      "06/30/2022 14:46:42 - INFO - __main__ -   Text: ['In support of LGBT Support.']\n",
      "06/30/2022 14:46:43 - INFO - __main__ -   Epoch: 59 | Batch: 2500/10001 (25%) | G Loss: 2.821100 | C Loss: -0.933051\n",
      "06/30/2022 14:46:43 - INFO - __main__ -   Text: ['Sometimes it can help the fever go down.\"']\n",
      "06/30/2022 14:46:45 - INFO - __main__ -   Epoch: 59 | Batch: 3000/10001 (30%) | G Loss: 2.054810 | C Loss: -0.749661\n",
      "06/30/2022 14:46:45 - INFO - __main__ -   Text: [\"The traditional term is 'ilfunni'.\"]\n",
      "06/30/2022 14:46:46 - INFO - __main__ -   Epoch: 59 | Batch: 3500/10001 (35%) | G Loss: 1.490953 | C Loss: -1.153004\n",
      "06/30/2022 14:46:46 - INFO - __main__ -   Text: ['Visitors can claim confiscation of those pounds.']\n",
      "06/30/2022 14:46:48 - INFO - __main__ -   Epoch: 59 | Batch: 4000/10001 (40%) | G Loss: 2.397460 | C Loss: -0.763155\n",
      "06/30/2022 14:46:48 - INFO - __main__ -   Text: ['Jader!\"']\n",
      "06/30/2022 14:46:49 - INFO - __main__ -   Epoch: 59 | Batch: 4500/10001 (45%) | G Loss: 1.961946 | C Loss: -0.947188\n",
      "06/30/2022 14:46:49 - INFO - __main__ -   Text: ['It tests what you eat and how you will become a better farmer.']\n",
      "06/30/2022 14:46:50 - INFO - __main__ -   Epoch: 59 | Batch: 5000/10001 (50%) | G Loss: 2.270880 | C Loss: -0.896670\n",
      "06/30/2022 14:46:51 - INFO - __main__ -   Text: ['It\\'s not a that\\'s important - it\\'s a question.\"']\n",
      "06/30/2022 14:46:52 - INFO - __main__ -   Epoch: 59 | Batch: 5500/10001 (55%) | G Loss: 1.988024 | C Loss: -0.753811\n",
      "06/30/2022 14:46:52 - INFO - __main__ -   Text: ['The narrator says: And if we believe in Gods fulfilling uss dream.']\n",
      "06/30/2022 14:46:54 - INFO - __main__ -   Epoch: 59 | Batch: 6000/10001 (60%) | G Loss: 1.798005 | C Loss: -0.931393\n",
      "06/30/2022 14:46:54 - INFO - __main__ -   Text: ['They don\\'t know!\".']\n",
      "06/30/2022 14:46:55 - INFO - __main__ -   Epoch: 59 | Batch: 6500/10001 (65%) | G Loss: 1.553793 | C Loss: -1.102182\n",
      "06/30/2022 14:46:55 - INFO - __main__ -   Text: ['\", has many links to other suggestions.']\n",
      "06/30/2022 14:46:56 - INFO - __main__ -   Epoch: 59 | Batch: 7000/10001 (70%) | G Loss: 1.753563 | C Loss: -0.967156\n",
      "06/30/2022 14:46:56 - INFO - __main__ -   Text: ['Palm shells are burlesque in torrents of sweat.']\n",
      "06/30/2022 14:46:58 - INFO - __main__ -   Epoch: 59 | Batch: 7500/10001 (75%) | G Loss: 2.636628 | C Loss: -1.105136\n",
      "06/30/2022 14:46:58 - INFO - __main__ -   Text: ['Is it not deviant that the brain has no spiders?']\n",
      "06/30/2022 14:46:59 - INFO - __main__ -   Epoch: 59 | Batch: 8000/10001 (80%) | G Loss: 1.917328 | C Loss: -0.960385\n",
      "06/30/2022 14:46:59 - INFO - __main__ -   Text: ['The current horse riding state is (IBNCom).']\n",
      "06/30/2022 14:47:01 - INFO - __main__ -   Epoch: 59 | Batch: 8500/10001 (85%) | G Loss: 1.524583 | C Loss: -0.797502\n",
      "06/30/2022 14:47:01 - INFO - __main__ -   Text: ['Legal arbitration also exists.']\n",
      "06/30/2022 14:47:02 - INFO - __main__ -   Epoch: 59 | Batch: 9000/10001 (90%) | G Loss: 1.921276 | C Loss: -0.803683\n",
      "06/30/2022 14:47:02 - INFO - __main__ -   Text: ['The pirate guy in this case is a computer programmer.']\n",
      "06/30/2022 14:47:03 - INFO - __main__ -   Epoch: 59 | Batch: 9500/10001 (95%) | G Loss: 1.992253 | C Loss: -1.052907\n",
      "06/30/2022 14:47:04 - INFO - __main__ -   Text: ['This means that this fellow has already converted A. B.']\n",
      "06/30/2022 14:47:05 - INFO - __main__ -   * (Train) Epoch: 59 | G Loss: 1.8675 | C Loss: -0.9154 | Updates G: 143 | Updates C: 857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:47:14 - INFO - __main__ -   Bleu-2:0.194 | B-Bleu-2:0.256\n",
      "06/30/2022 14:47:14 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_18.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4494582937364684\n",
      "Train file used is number 18\n",
      "../../yahoo/subdivided_large/train_18.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 60 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:23.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:34.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:45.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:56.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:07.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:19.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:29.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:40.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:52.\n",
      "  Batch   110  of    172.    Elapsed: 0:02:03.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:14.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:25.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:36.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:47.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:58.\n",
      "  Batch   170  of    172.    Elapsed: 0:03:09.\n",
      "\n",
      "  Average training loss discriminator: 0.013\n",
      "  Training epcoh took: 0:03:12\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:50:26 - INFO - __main__ -   Epoch: 60 | Batch: 0/10001 (0%) | G Loss: 1.472239 | C Loss: -0.895491\n",
      "06/30/2022 14:50:26 - INFO - __main__ -   Text: [\"There are two ways of representing the people: 'simplified'.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.432\n",
      "  Test Loss: 3.052\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:50:28 - INFO - __main__ -   Epoch: 60 | Batch: 500/10001 (5%) | G Loss: 1.555063 | C Loss: -1.011297\n",
      "06/30/2022 14:50:28 - INFO - __main__ -   Text: ['Only 2 more of them start naming themselves: Don Ferris is the Mets living in Newport Beach, as well.']\n",
      "06/30/2022 14:50:29 - INFO - __main__ -   Epoch: 60 | Batch: 1000/10001 (10%) | G Loss: 1.881593 | C Loss: -0.944279\n",
      "06/30/2022 14:50:29 - INFO - __main__ -   Text: ['The poem is about the realities of human life.']\n",
      "06/30/2022 14:50:31 - INFO - __main__ -   Epoch: 60 | Batch: 1500/10001 (15%) | G Loss: 2.292072 | C Loss: -1.049119\n",
      "06/30/2022 14:50:31 - INFO - __main__ -   Text: ['After android searching find her the BLOG on Facebook.']\n",
      "06/30/2022 14:50:32 - INFO - __main__ -   Epoch: 60 | Batch: 2000/10001 (20%) | G Loss: 2.231642 | C Loss: -0.748196\n",
      "06/30/2022 14:50:32 - INFO - __main__ -   Text: ['\"Whang over it.\"']\n",
      "06/30/2022 14:50:33 - INFO - __main__ -   Epoch: 60 | Batch: 2500/10001 (25%) | G Loss: 1.445118 | C Loss: -0.855634\n",
      "06/30/2022 14:50:33 - INFO - __main__ -   Text: ['But Samuels may have been taking rabbits.']\n",
      "06/30/2022 14:50:35 - INFO - __main__ -   Epoch: 60 | Batch: 3000/10001 (30%) | G Loss: 2.506654 | C Loss: -1.061745\n",
      "06/30/2022 14:50:35 - INFO - __main__ -   Text: ['she appears to not want to talk.']\n",
      "06/30/2022 14:50:36 - INFO - __main__ -   Epoch: 60 | Batch: 3500/10001 (35%) | G Loss: 2.801665 | C Loss: -0.808516\n",
      "06/30/2022 14:50:36 - INFO - __main__ -   Text: ['68 have ADHD.']\n",
      "06/30/2022 14:50:38 - INFO - __main__ -   Epoch: 60 | Batch: 4000/10001 (40%) | G Loss: 1.954721 | C Loss: -0.827115\n",
      "06/30/2022 14:50:38 - INFO - __main__ -   Text: ['In Hell, Schmitt.']\n",
      "06/30/2022 14:50:39 - INFO - __main__ -   Epoch: 60 | Batch: 4500/10001 (45%) | G Loss: 1.691937 | C Loss: -0.902067\n",
      "06/30/2022 14:50:39 - INFO - __main__ -   Text: ['\" Mordos only \"\".']\n",
      "06/30/2022 14:50:41 - INFO - __main__ -   Epoch: 60 | Batch: 5000/10001 (50%) | G Loss: 2.351377 | C Loss: -0.960027\n",
      "06/30/2022 14:50:41 - INFO - __main__ -   Text: ['Jarzarkits is an acronym for a fictitious technology.']\n",
      "06/30/2022 14:50:42 - INFO - __main__ -   Epoch: 60 | Batch: 5500/10001 (55%) | G Loss: 1.348326 | C Loss: -0.893765\n",
      "06/30/2022 14:50:42 - INFO - __main__ -   Text: ['A sea monster as powerful as his!']\n",
      "06/30/2022 14:50:43 - INFO - __main__ -   Epoch: 60 | Batch: 6000/10001 (60%) | G Loss: 2.075532 | C Loss: -0.882825\n",
      "06/30/2022 14:50:44 - INFO - __main__ -   Text: ['95% of it is just walking.']\n",
      "06/30/2022 14:50:45 - INFO - __main__ -   Epoch: 60 | Batch: 6500/10001 (65%) | G Loss: 2.445884 | C Loss: -0.662993\n",
      "06/30/2022 14:50:45 - INFO - __main__ -   Text: ['It also allows you to write:']\n",
      "06/30/2022 14:50:46 - INFO - __main__ -   Epoch: 60 | Batch: 7000/10001 (70%) | G Loss: 2.410250 | C Loss: -0.942409\n",
      "06/30/2022 14:50:47 - INFO - __main__ -   Text: ['They are described as: KotA visa - NZ visa GSB Franco institute visa (kanji printed']\n",
      "06/30/2022 14:50:48 - INFO - __main__ -   Epoch: 60 | Batch: 7500/10001 (75%) | G Loss: 2.241963 | C Loss: -0.991411\n",
      "06/30/2022 14:50:48 - INFO - __main__ -   Text: ['However, US power is always left to the nation.']\n",
      "06/30/2022 14:50:49 - INFO - __main__ -   Epoch: 60 | Batch: 8000/10001 (80%) | G Loss: 2.235641 | C Loss: -0.881726\n",
      "06/30/2022 14:50:50 - INFO - __main__ -   Text: ['Reporter Ken CrossEast has gone through all the school procedures he needs to become a news correspondent before raising his shots.']\n",
      "06/30/2022 14:50:51 - INFO - __main__ -   Epoch: 60 | Batch: 8500/10001 (85%) | G Loss: 1.838154 | C Loss: -0.682544\n",
      "06/30/2022 14:50:51 - INFO - __main__ -   Text: [\"Most likely this won't be dealt with for a while.\"]\n",
      "06/30/2022 14:50:52 - INFO - __main__ -   Epoch: 60 | Batch: 9000/10001 (90%) | G Loss: 1.498374 | C Loss: -1.100966\n",
      "06/30/2022 14:50:53 - INFO - __main__ -   Text: [\"They'd be a quite silly shot if a horse and rider are satisfied.\"]\n",
      "06/30/2022 14:50:54 - INFO - __main__ -   Epoch: 60 | Batch: 9500/10001 (95%) | G Loss: 2.396275 | C Loss: -0.788771\n",
      "06/30/2022 14:50:54 - INFO - __main__ -   Text: ['The program uses M-Turbo.']\n",
      "06/30/2022 14:50:55 - INFO - __main__ -   * (Train) Epoch: 60 | G Loss: 1.9597 | C Loss: -0.8892 | Updates G: 135 | Updates C: 865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:51:04 - INFO - __main__ -   Bleu-2:0.214 | B-Bleu-2:0.243\n",
      "06/30/2022 14:51:04 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_19.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45736516550316\n",
      "Train file used is number 19\n",
      "../../yahoo/subdivided_large/train_19.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 61 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:31.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:41.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:51.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:01.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:12.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:23.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:33.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:43.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:53.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:03.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:14.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:25.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:35.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:46.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss discriminator: 0.011\n",
      "  Training epcoh took: 0:02:58\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:54:03 - INFO - __main__ -   Epoch: 61 | Batch: 0/10001 (0%) | G Loss: 2.678788 | C Loss: -0.854179\n",
      "06/30/2022 14:54:03 - INFO - __main__ -   Text: ['With this, you must guess that Mike Jorgensen shoots His Wife']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.443\n",
      "  Test Loss: 3.051\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:54:04 - INFO - __main__ -   Epoch: 61 | Batch: 500/10001 (5%) | G Loss: 1.376296 | C Loss: -0.956346\n",
      "06/30/2022 14:54:04 - INFO - __main__ -   Text: ['Ray-candy!\"']\n",
      "06/30/2022 14:54:06 - INFO - __main__ -   Epoch: 61 | Batch: 1000/10001 (10%) | G Loss: 2.000556 | C Loss: -1.152859\n",
      "06/30/2022 14:54:06 - INFO - __main__ -   Text: ['The office then conducts a bounty investigation.']\n",
      "06/30/2022 14:54:07 - INFO - __main__ -   Epoch: 61 | Batch: 1500/10001 (15%) | G Loss: 2.754290 | C Loss: -0.711102\n",
      "06/30/2022 14:54:07 - INFO - __main__ -   Text: [\"It's not terrible though Twitter is not racist enough!\"]\n",
      "06/30/2022 14:54:09 - INFO - __main__ -   Epoch: 61 | Batch: 2000/10001 (20%) | G Loss: 1.837419 | C Loss: -0.936338\n",
      "06/30/2022 14:54:09 - INFO - __main__ -   Text: ['Xenon has a web interface that allows users to write databases.\"']\n",
      "06/30/2022 14:54:10 - INFO - __main__ -   Epoch: 61 | Batch: 2500/10001 (25%) | G Loss: 1.957515 | C Loss: -1.019876\n",
      "06/30/2022 14:54:10 - INFO - __main__ -   Text: ['Lachlan also comes to Metalheart.']\n",
      "06/30/2022 14:54:12 - INFO - __main__ -   Epoch: 61 | Batch: 3000/10001 (30%) | G Loss: 2.765831 | C Loss: -0.984137\n",
      "06/30/2022 14:54:12 - INFO - __main__ -   Text: [\"It's named after the 10-somethings.\"]\n",
      "06/30/2022 14:54:13 - INFO - __main__ -   Epoch: 61 | Batch: 3500/10001 (35%) | G Loss: 1.786883 | C Loss: -0.889928\n",
      "06/30/2022 14:54:13 - INFO - __main__ -   Text: ['Manufacturing Power Engineering is turned against him.']\n",
      "06/30/2022 14:54:15 - INFO - __main__ -   Epoch: 61 | Batch: 4000/10001 (40%) | G Loss: 1.485657 | C Loss: -0.730504\n",
      "06/30/2022 14:54:15 - INFO - __main__ -   Text: ['Of many of them \"Skinnydant\".']\n",
      "06/30/2022 14:54:16 - INFO - __main__ -   Epoch: 61 | Batch: 4500/10001 (45%) | G Loss: 2.608049 | C Loss: -1.003079\n",
      "06/30/2022 14:54:16 - INFO - __main__ -   Text: ['Getting into sex is really dangerous but is always desired.']\n",
      "06/30/2022 14:54:17 - INFO - __main__ -   Epoch: 61 | Batch: 5000/10001 (50%) | G Loss: 1.560270 | C Loss: -0.988455\n",
      "06/30/2022 14:54:18 - INFO - __main__ -   Text: [\"Haron. <BOS> Billionaire Ideas is the UK's biggest annual magazine.\"]\n",
      "06/30/2022 14:54:19 - INFO - __main__ -   Epoch: 61 | Batch: 5500/10001 (55%) | G Loss: 2.496299 | C Loss: -1.258355\n",
      "06/30/2022 14:54:19 - INFO - __main__ -   Text: ['An extraordinary documentary about the Harassment of Oregon commuters.']\n",
      "06/30/2022 14:54:20 - INFO - __main__ -   Epoch: 61 | Batch: 6000/10001 (60%) | G Loss: 1.203425 | C Loss: -0.833385\n",
      "06/30/2022 14:54:21 - INFO - __main__ -   Text: ['The podcast stories are also to be enlightening and informative.']\n",
      "06/30/2022 14:54:22 - INFO - __main__ -   Epoch: 61 | Batch: 6500/10001 (65%) | G Loss: 1.969920 | C Loss: -0.861191\n",
      "06/30/2022 14:54:22 - INFO - __main__ -   Text: [\"It's obviously flattering for the Canadian political establishment to run against Brooklyn and ask New York City voters to put a vote\"]\n",
      "06/30/2022 14:54:23 - INFO - __main__ -   Epoch: 61 | Batch: 7000/10001 (70%) | G Loss: 2.825007 | C Loss: -0.875994\n",
      "06/30/2022 14:54:24 - INFO - __main__ -   Text: ['The Old Arizona Mountains do occur in the fall.']\n",
      "06/30/2022 14:54:25 - INFO - __main__ -   Epoch: 61 | Batch: 7500/10001 (75%) | G Loss: 1.849911 | C Loss: -1.032718\n",
      "06/30/2022 14:54:25 - INFO - __main__ -   Text: ['This definition of Human Development is consistent with the view supported by Kaiser.']\n",
      "06/30/2022 14:54:26 - INFO - __main__ -   Epoch: 61 | Batch: 8000/10001 (80%) | G Loss: 2.013550 | C Loss: -0.917237\n",
      "06/30/2022 14:54:26 - INFO - __main__ -   Text: ['\"It\\'s a toxic organism.\"']\n",
      "06/30/2022 14:54:28 - INFO - __main__ -   Epoch: 61 | Batch: 8500/10001 (85%) | G Loss: 2.579977 | C Loss: -0.914763\n",
      "06/30/2022 14:54:28 - INFO - __main__ -   Text: ['He more often plays fireball: Dintou, GitHub, Sayia.']\n",
      "06/30/2022 14:54:29 - INFO - __main__ -   Epoch: 61 | Batch: 9000/10001 (90%) | G Loss: 2.165909 | C Loss: -0.838734\n",
      "06/30/2022 14:54:29 - INFO - __main__ -   Text: ['People try to rescue Japan from economic ruin.']\n",
      "06/30/2022 14:54:31 - INFO - __main__ -   Epoch: 61 | Batch: 9500/10001 (95%) | G Loss: 1.797322 | C Loss: -0.738127\n",
      "06/30/2022 14:54:31 - INFO - __main__ -   Text: ['It is good that DJ Hurley has heard before and God\\'s anger is awful.\"']\n",
      "06/30/2022 14:54:32 - INFO - __main__ -   * (Train) Epoch: 61 | G Loss: 1.9811 | C Loss: -0.9177 | Updates G: 129 | Updates C: 871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:54:41 - INFO - __main__ -   Bleu-2:0.201 | B-Bleu-2:0.234\n",
      "06/30/2022 14:54:41 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_20.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4357555225072539\n",
      "Train file used is number 20\n",
      "../../yahoo/subdivided_large/train_20.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 62 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:10.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:20.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:31.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:41.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:52.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:02.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:12.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:23.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:33.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:44.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:54.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:05.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:15.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:25.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:36.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:46.\n",
      "  Batch   170  of    172.    Elapsed: 0:02:57.\n",
      "\n",
      "  Average training loss discriminator: 0.009\n",
      "  Training epcoh took: 0:02:59\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:57:40 - INFO - __main__ -   Epoch: 62 | Batch: 0/10001 (0%) | G Loss: 2.189151 | C Loss: -1.077025\n",
      "06/30/2022 14:57:40 - INFO - __main__ -   Text: ['Picture #212!']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.435\n",
      "  Test Loss: 3.092\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:57:42 - INFO - __main__ -   Epoch: 62 | Batch: 500/10001 (5%) | G Loss: 2.443411 | C Loss: -1.125825\n",
      "06/30/2022 14:57:42 - INFO - __main__ -   Text: ['\"Stinky.\"']\n",
      "06/30/2022 14:57:43 - INFO - __main__ -   Epoch: 62 | Batch: 1000/10001 (10%) | G Loss: 1.613586 | C Loss: -1.109497\n",
      "06/30/2022 14:57:43 - INFO - __main__ -   Text: [\"Phraing can't help but suggest more people to get into karting.\"]\n",
      "06/30/2022 14:57:45 - INFO - __main__ -   Epoch: 62 | Batch: 1500/10001 (15%) | G Loss: 2.028054 | C Loss: -0.837728\n",
      "06/30/2022 14:57:45 - INFO - __main__ -   Text: ['Generally, interest is in Germany but knowing that the forums are forever will also drive me.']\n",
      "06/30/2022 14:57:46 - INFO - __main__ -   Epoch: 62 | Batch: 2000/10001 (20%) | G Loss: 1.934996 | C Loss: -0.854289\n",
      "06/30/2022 14:57:46 - INFO - __main__ -   Text: ['Having a mind to seek happiness, nassan works.']\n",
      "06/30/2022 14:57:48 - INFO - __main__ -   Epoch: 62 | Batch: 2500/10001 (25%) | G Loss: 2.638510 | C Loss: -0.887513\n",
      "06/30/2022 14:57:48 - INFO - __main__ -   Text: ['It is connected to Siam Spice Guide by which of our hippie <BOS>s.']\n",
      "06/30/2022 14:57:49 - INFO - __main__ -   Epoch: 62 | Batch: 3000/10001 (30%) | G Loss: 2.400875 | C Loss: -1.031886\n",
      "06/30/2022 14:57:49 - INFO - __main__ -   Text: ['Jack is interested in real example technology and get more integration with others.']\n",
      "06/30/2022 14:57:51 - INFO - __main__ -   Epoch: 62 | Batch: 3500/10001 (35%) | G Loss: 1.894585 | C Loss: -1.046563\n",
      "06/30/2022 14:57:51 - INFO - __main__ -   Text: [\"Like you're searching for something and it's a very quick way to find something.\"]\n",
      "06/30/2022 14:57:52 - INFO - __main__ -   Epoch: 62 | Batch: 4000/10001 (40%) | G Loss: 1.585581 | C Loss: -1.030101\n",
      "06/30/2022 14:57:52 - INFO - __main__ -   Text: ['It can also see teeth.']\n",
      "06/30/2022 14:57:54 - INFO - __main__ -   Epoch: 62 | Batch: 4500/10001 (45%) | G Loss: 2.364401 | C Loss: -0.834434\n",
      "06/30/2022 14:57:54 - INFO - __main__ -   Text: ['While strata are more easily avoided, you can also use the...']\n",
      "06/30/2022 14:57:55 - INFO - __main__ -   Epoch: 62 | Batch: 5000/10001 (50%) | G Loss: 2.349406 | C Loss: -1.029200\n",
      "06/30/2022 14:57:55 - INFO - __main__ -   Text: ['The Lizard Boyfriends of Newcastle.']\n",
      "06/30/2022 14:57:57 - INFO - __main__ -   Epoch: 62 | Batch: 5500/10001 (55%) | G Loss: 1.773143 | C Loss: -1.006748\n",
      "06/30/2022 14:57:57 - INFO - __main__ -   Text: ['Remix is nice too!']\n",
      "06/30/2022 14:57:58 - INFO - __main__ -   Epoch: 62 | Batch: 6000/10001 (60%) | G Loss: 1.854391 | C Loss: -0.800402\n",
      "06/30/2022 14:57:58 - INFO - __main__ -   Text: ['Try pictorial and similes.\"']\n",
      "06/30/2022 14:58:00 - INFO - __main__ -   Epoch: 62 | Batch: 6500/10001 (65%) | G Loss: 2.004165 | C Loss: -1.000589\n",
      "06/30/2022 14:58:00 - INFO - __main__ -   Text: ['Insidious will say either, here we shall be.\"']\n",
      "06/30/2022 14:58:01 - INFO - __main__ -   Epoch: 62 | Batch: 7000/10001 (70%) | G Loss: 1.982573 | C Loss: -0.919719\n",
      "06/30/2022 14:58:01 - INFO - __main__ -   Text: ['\"Memory\", again: Buy her an Irish Jew.']\n",
      "06/30/2022 14:58:02 - INFO - __main__ -   Epoch: 62 | Batch: 7500/10001 (75%) | G Loss: 2.286579 | C Loss: -1.051738\n",
      "06/30/2022 14:58:03 - INFO - __main__ -   Text: ['Who confesses finally?\"']\n",
      "06/30/2022 14:58:04 - INFO - __main__ -   Epoch: 62 | Batch: 8000/10001 (80%) | G Loss: 1.839500 | C Loss: -0.912109\n",
      "06/30/2022 14:58:04 - INFO - __main__ -   Text: ['Insight of a Cure for Cancer is a 2009 version.']\n",
      "06/30/2022 14:58:05 - INFO - __main__ -   Epoch: 62 | Batch: 8500/10001 (85%) | G Loss: 1.882647 | C Loss: -0.925727\n",
      "06/30/2022 14:58:05 - INFO - __main__ -   Text: ['Buck must travel widely across the nation.']\n",
      "06/30/2022 14:58:07 - INFO - __main__ -   Epoch: 62 | Batch: 9000/10001 (90%) | G Loss: 2.191785 | C Loss: -1.069052\n",
      "06/30/2022 14:58:07 - INFO - __main__ -   Text: ['You can understand that Ubere is a nice, music-oriented port.']\n",
      "06/30/2022 14:58:08 - INFO - __main__ -   Epoch: 62 | Batch: 9500/10001 (95%) | G Loss: 1.908902 | C Loss: -0.990288\n",
      "06/30/2022 14:58:08 - INFO - __main__ -   Text: ['This can be used if your mako has time to recharge.']\n",
      "06/30/2022 14:58:10 - INFO - __main__ -   * (Train) Epoch: 62 | G Loss: 1.9645 | C Loss: -0.9660 | Updates G: 105 | Updates C: 895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 14:58:19 - INFO - __main__ -   Bleu-2:0.215 | B-Bleu-2:0.238\n",
      "06/30/2022 14:58:19 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_1.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4524523289581186\n",
      "Train file used is number 1\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 63 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:22.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:32.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:42.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:53.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:03.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:14.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:25.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:36.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:46.\n",
      "  Batch   110  of    172.    Elapsed: 0:01:56.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:07.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:18.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:29.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:39.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:50.\n",
      "  Batch   170  of    172.    Elapsed: 0:03:01.\n",
      "\n",
      "  Average training loss discriminator: 0.013\n",
      "  Training epcoh took: 0:03:03\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 15:01:23 - INFO - __main__ -   Epoch: 63 | Batch: 0/10000 (0%) | G Loss: 1.950490 | C Loss: -0.907725\n",
      "06/30/2022 15:01:23 - INFO - __main__ -   Text: [\"It's called Levacka LoZac and I'm surprised that once growing before Ethiopia.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.435\n",
      "  Test Loss: 3.135\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 15:01:24 - INFO - __main__ -   Epoch: 63 | Batch: 500/10000 (5%) | G Loss: 1.836892 | C Loss: -0.915560\n",
      "06/30/2022 15:01:24 - INFO - __main__ -   Text: ['Automatic carry-orders are an application of these patents.']\n",
      "06/30/2022 15:01:26 - INFO - __main__ -   Epoch: 63 | Batch: 1000/10000 (10%) | G Loss: 1.976569 | C Loss: -0.912486\n",
      "06/30/2022 15:01:26 - INFO - __main__ -   Text: ['How boring!']\n",
      "06/30/2022 15:01:27 - INFO - __main__ -   Epoch: 63 | Batch: 1500/10000 (15%) | G Loss: 2.424869 | C Loss: -0.986887\n",
      "06/30/2022 15:01:27 - INFO - __main__ -   Text: ['The ratings-average sports game from the west.']\n",
      "06/30/2022 15:01:28 - INFO - __main__ -   Epoch: 63 | Batch: 2000/10000 (20%) | G Loss: 2.336243 | C Loss: -1.039268\n",
      "06/30/2022 15:01:28 - INFO - __main__ -   Text: ['He is broadly comfortable with this kind of thing.']\n",
      "06/30/2022 15:01:30 - INFO - __main__ -   Epoch: 63 | Batch: 2500/10000 (25%) | G Loss: 1.562611 | C Loss: -0.994047\n",
      "06/30/2022 15:01:30 - INFO - __main__ -   Text: ['The Irish Test is the best distance I\\'ve been in in a long time\".']\n",
      "06/30/2022 15:01:31 - INFO - __main__ -   Epoch: 63 | Batch: 3000/10000 (30%) | G Loss: 2.584646 | C Loss: -1.031248\n",
      "06/30/2022 15:01:31 - INFO - __main__ -   Text: ['bubblecable can build 100\".']\n",
      "06/30/2022 15:01:33 - INFO - __main__ -   Epoch: 63 | Batch: 3500/10000 (35%) | G Loss: 2.005632 | C Loss: -0.512529\n",
      "06/30/2022 15:01:33 - INFO - __main__ -   Text: ['Development is not building computers: that is an intellectual breakthrough.']\n",
      "06/30/2022 15:01:34 - INFO - __main__ -   Epoch: 63 | Batch: 4000/10000 (40%) | G Loss: 1.744234 | C Loss: -1.000973\n",
      "06/30/2022 15:01:34 - INFO - __main__ -   Text: [\"She is Villa's first successful publisher, as we find out later.\"]\n",
      "06/30/2022 15:01:36 - INFO - __main__ -   Epoch: 63 | Batch: 4500/10000 (45%) | G Loss: 1.793909 | C Loss: -0.875191\n",
      "06/30/2022 15:01:36 - INFO - __main__ -   Text: ['The players whistle a lot like \\'p\\' button\\' \"AKa!']\n",
      "06/30/2022 15:01:37 - INFO - __main__ -   Epoch: 63 | Batch: 5000/10000 (50%) | G Loss: 2.162357 | C Loss: -1.100035\n",
      "06/30/2022 15:01:37 - INFO - __main__ -   Text: ['Lucky then, IdUt 0 FT.']\n",
      "06/30/2022 15:01:39 - INFO - __main__ -   Epoch: 63 | Batch: 5500/10000 (55%) | G Loss: 2.115646 | C Loss: -1.011097\n",
      "06/30/2022 15:01:39 - INFO - __main__ -   Text: ['It is about love in the way \"Anna or Stefan.\"']\n",
      "06/30/2022 15:01:40 - INFO - __main__ -   Epoch: 63 | Batch: 6000/10000 (60%) | G Loss: 1.757645 | C Loss: -0.970641\n",
      "06/30/2022 15:01:40 - INFO - __main__ -   Text: ['He is an interstellar landworm who has adventures the next week.']\n",
      "06/30/2022 15:01:42 - INFO - __main__ -   Epoch: 63 | Batch: 6500/10000 (65%) | G Loss: 1.879444 | C Loss: -1.043104\n",
      "06/30/2022 15:01:42 - INFO - __main__ -   Text: ['In addition, animals can specialize in certain dangers.']\n",
      "06/30/2022 15:01:43 - INFO - __main__ -   Epoch: 63 | Batch: 7000/10000 (70%) | G Loss: 1.701738 | C Loss: -1.065841\n",
      "06/30/2022 15:01:43 - INFO - __main__ -   Text: ['Questions mean a lot about Parting with Me.\"']\n",
      "06/30/2022 15:01:45 - INFO - __main__ -   Epoch: 63 | Batch: 7500/10000 (75%) | G Loss: 1.840053 | C Loss: -1.140301\n",
      "06/30/2022 15:01:45 - INFO - __main__ -   Text: ['\"Teleology is the word of God.\"']\n",
      "06/30/2022 15:01:46 - INFO - __main__ -   Epoch: 63 | Batch: 8000/10000 (80%) | G Loss: 1.620924 | C Loss: -0.687565\n",
      "06/30/2022 15:01:46 - INFO - __main__ -   Text: ['These tests can potentially test whether Python is suitable for a particular application.']\n",
      "06/30/2022 15:01:48 - INFO - __main__ -   Epoch: 63 | Batch: 8500/10000 (85%) | G Loss: 2.070836 | C Loss: -0.964303\n",
      "06/30/2022 15:01:48 - INFO - __main__ -   Text: ['.']\n",
      "06/30/2022 15:01:49 - INFO - __main__ -   Epoch: 63 | Batch: 9000/10000 (90%) | G Loss: 2.154267 | C Loss: -1.053644\n",
      "06/30/2022 15:01:49 - INFO - __main__ -   Text: ['Sometime \"Concrete Balls\" are commemorated with orange goo over the eyes.']\n",
      "06/30/2022 15:01:51 - INFO - __main__ -   Epoch: 63 | Batch: 9500/10000 (95%) | G Loss: 1.940330 | C Loss: -1.008490\n",
      "06/30/2022 15:01:51 - INFO - __main__ -   Text: ['Boys Out!']\n",
      "06/30/2022 15:01:52 - INFO - __main__ -   * (Train) Epoch: 63 | G Loss: 1.8988 | C Loss: -0.9861 | Updates G: 117 | Updates C: 883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 15:02:01 - INFO - __main__ -   Bleu-2:0.207 | B-Bleu-2:0.238\n",
      "06/30/2022 15:02:01 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_1.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44549068827858745\n",
      "Train file used is number 1\n",
      "../../yahoo/subdivided_large/train_1.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 64 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:23.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:34.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:45.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:57.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:08.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:19.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:30.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:41.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:53.\n",
      "  Batch   110  of    172.    Elapsed: 0:02:04.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:15.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:27.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:38.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:34.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:45.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:56.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:07.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:18.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:29.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:41.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:52.\n",
      "  Batch   110  of    172.    Elapsed: 0:02:04.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:15.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:26.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:37.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:48.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:59.\n",
      "  Batch   170  of    172.    Elapsed: 0:03:10.\n",
      "\n",
      "  Average training loss discriminator: 0.012\n",
      "  Training epcoh took: 0:03:12\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 15:09:07 - INFO - __main__ -   Epoch: 65 | Batch: 0/10001 (0%) | G Loss: 1.822661 | C Loss: -1.095192\n",
      "06/30/2022 15:09:07 - INFO - __main__ -   Text: ['It is similar to Machiavelles Daydream - like Marathon\".']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.448\n",
      "  Test Loss: 3.085\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 15:09:08 - INFO - __main__ -   Epoch: 65 | Batch: 500/10001 (5%) | G Loss: 1.938980 | C Loss: -0.921193\n",
      "06/30/2022 15:09:08 - INFO - __main__ -   Text: ['It is the supreme court rule, but no lower court picture!']\n",
      "06/30/2022 15:09:10 - INFO - __main__ -   Epoch: 65 | Batch: 1000/10001 (10%) | G Loss: 1.643715 | C Loss: -0.825741\n",
      "06/30/2022 15:09:10 - INFO - __main__ -   Text: ['It is a simple proof of justification for wanting to use a telescope.']\n",
      "06/30/2022 15:09:11 - INFO - __main__ -   Epoch: 65 | Batch: 1500/10001 (15%) | G Loss: 2.490124 | C Loss: -0.934116\n",
      "06/30/2022 15:09:12 - INFO - __main__ -   Text: ['In Bangladesh, the name \"Ben Kano\" has a surprising effect on the debate.']\n",
      "06/30/2022 15:09:13 - INFO - __main__ -   Epoch: 65 | Batch: 2000/10001 (20%) | G Loss: 2.646502 | C Loss: -0.869500\n",
      "06/30/2022 15:09:13 - INFO - __main__ -   Text: ['When Nazi Marxism enters the world, happiness explodes.\"']\n",
      "06/30/2022 15:09:14 - INFO - __main__ -   Epoch: 65 | Batch: 2500/10001 (25%) | G Loss: 1.655713 | C Loss: -1.032634\n",
      "06/30/2022 15:09:14 - INFO - __main__ -   Text: ['The term is loosely connected to George Orwell.']\n",
      "06/30/2022 15:09:16 - INFO - __main__ -   Epoch: 65 | Batch: 3000/10001 (30%) | G Loss: 1.592639 | C Loss: -0.926593\n",
      "06/30/2022 15:09:16 - INFO - __main__ -   Text: ['The panelists include: underwriters: it is the science of writing.']\n",
      "06/30/2022 15:09:17 - INFO - __main__ -   Epoch: 65 | Batch: 3500/10001 (35%) | G Loss: 1.983531 | C Loss: -0.865731\n",
      "06/30/2022 15:09:17 - INFO - __main__ -   Text: ['I teach a wide range of topics for students answering questions.']\n",
      "06/30/2022 15:09:19 - INFO - __main__ -   Epoch: 65 | Batch: 4000/10001 (40%) | G Loss: 2.261174 | C Loss: -0.995432\n",
      "06/30/2022 15:09:19 - INFO - __main__ -   Text: ['If two or more questions are sent along to the program.']\n",
      "06/30/2022 15:09:20 - INFO - __main__ -   Epoch: 65 | Batch: 4500/10001 (45%) | G Loss: 2.230210 | C Loss: -1.064749\n",
      "06/30/2022 15:09:20 - INFO - __main__ -   Text: ['Saved!']\n",
      "06/30/2022 15:09:22 - INFO - __main__ -   Epoch: 65 | Batch: 5000/10001 (50%) | G Loss: 1.430456 | C Loss: -0.930520\n",
      "06/30/2022 15:09:22 - INFO - __main__ -   Text: ['These tasks can be set in play within the environment.']\n",
      "06/30/2022 15:09:23 - INFO - __main__ -   Epoch: 65 | Batch: 5500/10001 (55%) | G Loss: 1.972399 | C Loss: -0.897984\n",
      "06/30/2022 15:09:23 - INFO - __main__ -   Text: ['Thy skydiving education is a bitter pill.']\n",
      "06/30/2022 15:09:25 - INFO - __main__ -   Epoch: 65 | Batch: 6000/10001 (60%) | G Loss: 2.106570 | C Loss: -0.837533\n",
      "06/30/2022 15:09:25 - INFO - __main__ -   Text: ['According to Time Magazine, Maggie Seldon is Jewish.']\n",
      "06/30/2022 15:09:26 - INFO - __main__ -   Epoch: 65 | Batch: 6500/10001 (65%) | G Loss: 1.671928 | C Loss: -1.039217\n",
      "06/30/2022 15:09:26 - INFO - __main__ -   Text: ['She is just making a fun cheap joke on such silly things as science.\"']\n",
      "06/30/2022 15:09:28 - INFO - __main__ -   Epoch: 65 | Batch: 7000/10001 (70%) | G Loss: 2.357734 | C Loss: -0.892720\n",
      "06/30/2022 15:09:28 - INFO - __main__ -   Text: ['Ganja has some prior experience with animal rights.']\n",
      "06/30/2022 15:09:29 - INFO - __main__ -   Epoch: 65 | Batch: 7500/10001 (75%) | G Loss: 1.535759 | C Loss: -1.070792\n",
      "06/30/2022 15:09:29 - INFO - __main__ -   Text: ['The dog has also read \"PHONE\".']\n",
      "06/30/2022 15:09:31 - INFO - __main__ -   Epoch: 65 | Batch: 8000/10001 (80%) | G Loss: 2.040678 | C Loss: -0.803145\n",
      "06/30/2022 15:09:31 - INFO - __main__ -   Text: ['The transmission of feelings from mind to body is extremely strong.']\n",
      "06/30/2022 15:09:32 - INFO - __main__ -   Epoch: 65 | Batch: 8500/10001 (85%) | G Loss: 1.749913 | C Loss: -0.805971\n",
      "06/30/2022 15:09:32 - INFO - __main__ -   Text: ['On the other hand, the northern king is here engaged in petty disputes.']\n",
      "06/30/2022 15:09:34 - INFO - __main__ -   Epoch: 65 | Batch: 9000/10001 (90%) | G Loss: 2.537124 | C Loss: -0.999457\n",
      "06/30/2022 15:09:34 - INFO - __main__ -   Text: ['Upset about how she looks, Jessi had ridden her unicorn in winter.']\n",
      "06/30/2022 15:09:35 - INFO - __main__ -   Epoch: 65 | Batch: 9500/10001 (95%) | G Loss: 2.114910 | C Loss: -0.798601\n",
      "06/30/2022 15:09:35 - INFO - __main__ -   Text: ['Bees consume the liquid when hobbities grow.']\n",
      "06/30/2022 15:09:36 - INFO - __main__ -   * (Train) Epoch: 65 | G Loss: 1.9044 | C Loss: -0.9362 | Updates G: 123 | Updates C: 877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 15:09:46 - INFO - __main__ -   Bleu-2:0.189 | B-Bleu-2:0.259\n",
      "06/30/2022 15:09:46 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_3.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4483546745163429\n",
      "Train file used is number 3\n",
      "../../yahoo/subdivided_large/train_3.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 66 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:21.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:32.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:44.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:54.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:06.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:18.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:29.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:40.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:52.\n",
      "  Batch   110  of    172.    Elapsed: 0:02:03.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:14.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:26.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:37.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:49.\n",
      "  Batch   160  of    172.    Elapsed: 0:03:00.\n",
      "  Batch   170  of    172.    Elapsed: 0:03:11.\n",
      "\n",
      "  Average training loss discriminator: 0.011\n",
      "  Training epcoh took: 0:03:14\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 15:13:00 - INFO - __main__ -   Epoch: 66 | Batch: 0/10001 (0%) | G Loss: 1.139017 | C Loss: -0.620755\n",
      "06/30/2022 15:13:00 - INFO - __main__ -   Text: ['Tony MacSage\\'s \"Himself You\\'re Not Alone\" is top of the charts.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.415\n",
      "  Test Loss: 3.032\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 15:13:01 - INFO - __main__ -   Epoch: 66 | Batch: 500/10001 (5%) | G Loss: 1.831804 | C Loss: -0.831342\n",
      "06/30/2022 15:13:01 - INFO - __main__ -   Text: ['\"TUT\", Rawlin Cricket is now held internationally.']\n",
      "06/30/2022 15:13:03 - INFO - __main__ -   Epoch: 66 | Batch: 1000/10001 (10%) | G Loss: 2.465158 | C Loss: -0.943639\n",
      "06/30/2022 15:13:03 - INFO - __main__ -   Text: [\"MacDonald's argument sidemenacles all the divisions.\"]\n",
      "06/30/2022 15:13:04 - INFO - __main__ -   Epoch: 66 | Batch: 1500/10001 (15%) | G Loss: 2.314605 | C Loss: -0.839589\n",
      "06/30/2022 15:13:04 - INFO - __main__ -   Text: ['Everyone knows that there is a composition in the fruit tree that looks like this: Evil.']\n",
      "06/30/2022 15:13:06 - INFO - __main__ -   Epoch: 66 | Batch: 2000/10001 (20%) | G Loss: 1.667589 | C Loss: -0.787503\n",
      "06/30/2022 15:13:06 - INFO - __main__ -   Text: [\"He's stayed especially positive of Koungy.\"]\n",
      "06/30/2022 15:13:07 - INFO - __main__ -   Epoch: 66 | Batch: 2500/10001 (25%) | G Loss: 1.952778 | C Loss: -1.135596\n",
      "06/30/2022 15:13:07 - INFO - __main__ -   Text: ['These abilities are called nanowire.']\n",
      "06/30/2022 15:13:09 - INFO - __main__ -   Epoch: 66 | Batch: 3000/10001 (30%) | G Loss: 1.750487 | C Loss: -0.637403\n",
      "06/30/2022 15:13:09 - INFO - __main__ -   Text: ['The course needs to be written in English.']\n",
      "06/30/2022 15:13:10 - INFO - __main__ -   Epoch: 66 | Batch: 3500/10001 (35%) | G Loss: 2.153011 | C Loss: -1.050120\n",
      "06/30/2022 15:13:10 - INFO - __main__ -   Text: ['It\\'s the Death Scream\".']\n",
      "06/30/2022 15:13:11 - INFO - __main__ -   Epoch: 66 | Batch: 4000/10001 (40%) | G Loss: 2.164547 | C Loss: -0.993079\n",
      "06/30/2022 15:13:11 - INFO - __main__ -   Text: ['There is neither doubt or affirmation of the name for which the person is paranoid.']\n",
      "06/30/2022 15:13:13 - INFO - __main__ -   Epoch: 66 | Batch: 4500/10001 (45%) | G Loss: 1.707710 | C Loss: -0.966115\n",
      "06/30/2022 15:13:13 - INFO - __main__ -   Text: ['The function repeats a task on the hopping Emily battery, but it does not follow this function.']\n",
      "06/30/2022 15:13:14 - INFO - __main__ -   Epoch: 66 | Batch: 5000/10001 (50%) | G Loss: 2.051547 | C Loss: -0.963486\n",
      "06/30/2022 15:13:14 - INFO - __main__ -   Text: ['Abandoned lives.']\n",
      "06/30/2022 15:13:16 - INFO - __main__ -   Epoch: 66 | Batch: 5500/10001 (55%) | G Loss: 1.829340 | C Loss: -1.012333\n",
      "06/30/2022 15:13:16 - INFO - __main__ -   Text: ['By contrast, porphyridium can account for COP.']\n",
      "06/30/2022 15:13:17 - INFO - __main__ -   Epoch: 66 | Batch: 6000/10001 (60%) | G Loss: 1.621733 | C Loss: -0.794214\n",
      "06/30/2022 15:13:17 - INFO - __main__ -   Text: ['As of 2008, Phrahma is probably the first major discipline to feature questionnaires.']\n",
      "06/30/2022 15:13:19 - INFO - __main__ -   Epoch: 66 | Batch: 6500/10001 (65%) | G Loss: 2.386836 | C Loss: -1.095212\n",
      "06/30/2022 15:13:19 - INFO - __main__ -   Text: ['They do not allow themselves to be condemned.\"']\n",
      "06/30/2022 15:13:20 - INFO - __main__ -   Epoch: 66 | Batch: 7000/10001 (70%) | G Loss: 2.032671 | C Loss: -0.778290\n",
      "06/30/2022 15:13:20 - INFO - __main__ -   Text: ['Dawkin plans to ensure that \"[advantages are] published\".']\n",
      "06/30/2022 15:13:22 - INFO - __main__ -   Epoch: 66 | Batch: 7500/10001 (75%) | G Loss: 1.877855 | C Loss: -0.922983\n",
      "06/30/2022 15:13:22 - INFO - __main__ -   Text: ['The barter is sometimes described as an immoral citizenship cult by LaMonroe and Crane (1989).']\n",
      "06/30/2022 15:13:23 - INFO - __main__ -   Epoch: 66 | Batch: 8000/10001 (80%) | G Loss: 1.647161 | C Loss: -0.942095\n",
      "06/30/2022 15:13:23 - INFO - __main__ -   Text: ['Then the dark color of wheat is cornish.']\n",
      "06/30/2022 15:13:25 - INFO - __main__ -   Epoch: 66 | Batch: 8500/10001 (85%) | G Loss: 2.482662 | C Loss: -1.054134\n",
      "06/30/2022 15:13:25 - INFO - __main__ -   Text: ['Students learn how to listen to music and communicate with classmates.']\n",
      "06/30/2022 15:13:26 - INFO - __main__ -   Epoch: 66 | Batch: 9000/10001 (90%) | G Loss: 1.981670 | C Loss: -0.871021\n",
      "06/30/2022 15:13:26 - INFO - __main__ -   Text: ['TRUUUUU!']\n",
      "06/30/2022 15:13:28 - INFO - __main__ -   Epoch: 66 | Batch: 9500/10001 (95%) | G Loss: 1.526011 | C Loss: -0.837401\n",
      "06/30/2022 15:13:28 - INFO - __main__ -   Text: ['It will let you print, assemble and store elaborate rules!']\n",
      "06/30/2022 15:13:29 - INFO - __main__ -   * (Train) Epoch: 66 | G Loss: 1.9475 | C Loss: -0.8913 | Updates G: 127 | Updates C: 873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 15:13:38 - INFO - __main__ -   Bleu-2:0.204 | B-Bleu-2:0.246\n",
      "06/30/2022 15:13:38 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_4.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4498818125866847\n",
      "Train file used is number 4\n",
      "../../yahoo/subdivided_large/train_4.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 67 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:23.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:34.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:45.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:57.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:09.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:20.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:32.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:43.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:55.\n",
      "  Batch   110  of    172.    Elapsed: 0:02:07.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:19.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:30.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:41.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:53.\n",
      "  Batch   160  of    172.    Elapsed: 0:03:04.\n",
      "  Batch   170  of    172.    Elapsed: 0:03:15.\n",
      "\n",
      "  Average training loss discriminator: 0.011\n",
      "  Training epcoh took: 0:03:17\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 15:16:56 - INFO - __main__ -   Epoch: 67 | Batch: 0/10001 (0%) | G Loss: 2.167977 | C Loss: -0.865474\n",
      "06/30/2022 15:16:56 - INFO - __main__ -   Text: ['Its blog is called Pay Attention.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.443\n",
      "  Test Loss: 3.040\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 15:16:57 - INFO - __main__ -   Epoch: 67 | Batch: 500/10001 (5%) | G Loss: 2.304930 | C Loss: -0.680606\n",
      "06/30/2022 15:16:58 - INFO - __main__ -   Text: ['He suggests it is the asexual tourism of Bulgarians which \"didn\\'t die out like..']\n",
      "06/30/2022 15:16:59 - INFO - __main__ -   Epoch: 67 | Batch: 1000/10001 (10%) | G Loss: 1.412868 | C Loss: -0.826888\n",
      "06/30/2022 15:16:59 - INFO - __main__ -   Text: [\"There is a military significance in Shorach's speech.\"]\n",
      "06/30/2022 15:17:01 - INFO - __main__ -   Epoch: 67 | Batch: 1500/10001 (15%) | G Loss: 2.471698 | C Loss: -0.785138\n",
      "06/30/2022 15:17:01 - INFO - __main__ -   Text: ['Bleeding is spiritual.']\n",
      "06/30/2022 15:17:02 - INFO - __main__ -   Epoch: 67 | Batch: 2000/10001 (20%) | G Loss: 2.229400 | C Loss: -0.797832\n",
      "06/30/2022 15:17:02 - INFO - __main__ -   Text: ['His motto is \"Potemps\".']\n",
      "06/30/2022 15:17:04 - INFO - __main__ -   Epoch: 67 | Batch: 2500/10001 (25%) | G Loss: 1.734378 | C Loss: -1.122226\n",
      "06/30/2022 15:17:04 - INFO - __main__ -   Text: ['It has Lambda .']\n",
      "06/30/2022 15:17:05 - INFO - __main__ -   Epoch: 67 | Batch: 3000/10001 (30%) | G Loss: 2.815308 | C Loss: -0.868079\n",
      "06/30/2022 15:17:05 - INFO - __main__ -   Text: ['\"But, \"He is an astronomer.\"']\n",
      "06/30/2022 15:17:06 - INFO - __main__ -   Epoch: 67 | Batch: 3500/10001 (35%) | G Loss: 1.636402 | C Loss: -0.775595\n",
      "06/30/2022 15:17:07 - INFO - __main__ -   Text: ['Bred is said to have one mind.']\n",
      "06/30/2022 15:17:08 - INFO - __main__ -   Epoch: 67 | Batch: 4000/10001 (40%) | G Loss: 2.263988 | C Loss: -0.940831\n",
      "06/30/2022 15:17:08 - INFO - __main__ -   Text: ['The last word is \"antiquirtum\".']\n",
      "06/30/2022 15:17:09 - INFO - __main__ -   Epoch: 67 | Batch: 4500/10001 (45%) | G Loss: 2.161969 | C Loss: -1.014690\n",
      "06/30/2022 15:17:10 - INFO - __main__ -   Text: ['This is illustrated by the tails investigator to manage authorized papers.']\n",
      "06/30/2022 15:17:11 - INFO - __main__ -   Epoch: 67 | Batch: 5000/10001 (50%) | G Loss: 1.902244 | C Loss: -0.860557\n",
      "06/30/2022 15:17:11 - INFO - __main__ -   Text: ['Avalon makes him a pirate.']\n",
      "06/30/2022 15:17:12 - INFO - __main__ -   Epoch: 67 | Batch: 5500/10001 (55%) | G Loss: 2.823698 | C Loss: -0.766030\n",
      "06/30/2022 15:17:12 - INFO - __main__ -   Text: ['He explores the causal universe.']\n",
      "06/30/2022 15:17:14 - INFO - __main__ -   Epoch: 67 | Batch: 6000/10001 (60%) | G Loss: 2.354206 | C Loss: -0.953291\n",
      "06/30/2022 15:17:14 - INFO - __main__ -   Text: ['Earthfire!']\n",
      "06/30/2022 15:17:15 - INFO - __main__ -   Epoch: 67 | Batch: 6500/10001 (65%) | G Loss: 1.894763 | C Loss: -1.092712\n",
      "06/30/2022 15:17:15 - INFO - __main__ -   Text: [\"It is Pramvara's case that every system is Pramodellar Types.\"]\n",
      "06/30/2022 15:17:17 - INFO - __main__ -   Epoch: 67 | Batch: 7000/10001 (70%) | G Loss: 2.066059 | C Loss: -0.863066\n",
      "06/30/2022 15:17:17 - INFO - __main__ -   Text: [\"She then gives the task 'auteur'.\"]\n",
      "06/30/2022 15:17:18 - INFO - __main__ -   Epoch: 67 | Batch: 7500/10001 (75%) | G Loss: 2.939740 | C Loss: -0.919173\n",
      "06/30/2022 15:17:18 - INFO - __main__ -   Text: ['No charges have been filed against bookseller Ross for this.']\n",
      "06/30/2022 15:17:20 - INFO - __main__ -   Epoch: 67 | Batch: 8000/10001 (80%) | G Loss: 1.937183 | C Loss: -0.915614\n",
      "06/30/2022 15:17:20 - INFO - __main__ -   Text: ['His resume will not be the most important factor in thread choice.\"']\n",
      "06/30/2022 15:17:21 - INFO - __main__ -   Epoch: 67 | Batch: 8500/10001 (85%) | G Loss: 1.663331 | C Loss: -0.758736\n",
      "06/30/2022 15:17:21 - INFO - __main__ -   Text: ['Within the journal is the Jungian Sergioths race.']\n",
      "06/30/2022 15:17:23 - INFO - __main__ -   Epoch: 67 | Batch: 9000/10001 (90%) | G Loss: 3.949374 | C Loss: -1.443413\n",
      "06/30/2022 15:17:23 - INFO - __main__ -   Text: ['\"Bump Dunk Bump Bump\".']\n",
      "06/30/2022 15:17:24 - INFO - __main__ -   Epoch: 67 | Batch: 9500/10001 (95%) | G Loss: 2.106079 | C Loss: -0.955435\n",
      "06/30/2022 15:17:24 - INFO - __main__ -   Text: ['Occasionally Ratas \"tribe\" really has \\'health\\'.']\n",
      "06/30/2022 15:17:26 - INFO - __main__ -   * (Train) Epoch: 67 | G Loss: 2.0175 | C Loss: -0.8552 | Updates G: 134 | Updates C: 866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 15:17:35 - INFO - __main__ -   Bleu-2:0.185 | B-Bleu-2:0.255\n",
      "06/30/2022 15:17:35 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4401078309996874\n",
      "Train file used is number 5\n",
      "../../yahoo/subdivided_large/train_5.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 68 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:22.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:32.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:44.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:55.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:06.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:17.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:28.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:39.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:51.\n",
      "  Batch   110  of    172.    Elapsed: 0:02:02.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:13.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:24.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:35.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:47.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:58.\n",
      "  Batch   170  of    172.    Elapsed: 0:03:09.\n",
      "\n",
      "  Average training loss discriminator: 0.010\n",
      "  Training epcoh took: 0:03:11\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 15:20:46 - INFO - __main__ -   Epoch: 68 | Batch: 0/10001 (0%) | G Loss: 1.592397 | C Loss: -0.972226\n",
      "06/30/2022 15:20:46 - INFO - __main__ -   Text: [\"Most people think that God doesn't exist, and therefore we should examine it.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.435\n",
      "  Test Loss: 3.108\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 15:20:48 - INFO - __main__ -   Epoch: 68 | Batch: 500/10001 (5%) | G Loss: 2.908934 | C Loss: -0.839584\n",
      "06/30/2022 15:20:48 - INFO - __main__ -   Text: ['In order to unlock newbie earports, you need to fly.']\n",
      "06/30/2022 15:20:49 - INFO - __main__ -   Epoch: 68 | Batch: 1000/10001 (10%) | G Loss: 1.743130 | C Loss: -0.659924\n",
      "06/30/2022 15:20:49 - INFO - __main__ -   Text: ['They can send phone numbers to whales and see whales.']\n",
      "06/30/2022 15:20:51 - INFO - __main__ -   Epoch: 68 | Batch: 1500/10001 (15%) | G Loss: 1.585975 | C Loss: -0.699047\n",
      "06/30/2022 15:20:51 - INFO - __main__ -   Text: ['Theyize Lmaoiso.\"']\n",
      "06/30/2022 15:20:52 - INFO - __main__ -   Epoch: 68 | Batch: 2000/10001 (20%) | G Loss: 2.722100 | C Loss: -0.942267\n",
      "06/30/2022 15:20:52 - INFO - __main__ -   Text: ['\" Kelly & Gladys\".']\n",
      "06/30/2022 15:20:54 - INFO - __main__ -   Epoch: 68 | Batch: 2500/10001 (25%) | G Loss: 1.320479 | C Loss: -0.983039\n",
      "06/30/2022 15:20:54 - INFO - __main__ -   Text: ['This heedes says: <br> [The use of \"barbonia\"] used to be a liberal']\n",
      "06/30/2022 15:20:55 - INFO - __main__ -   Epoch: 68 | Batch: 3000/10001 (30%) | G Loss: 2.430959 | C Loss: -0.862701\n",
      "06/30/2022 15:20:55 - INFO - __main__ -   Text: ['A socially dominant opinion is through question.']\n",
      "06/30/2022 15:20:57 - INFO - __main__ -   Epoch: 68 | Batch: 3500/10001 (35%) | G Loss: 1.708706 | C Loss: -0.638959\n",
      "06/30/2022 15:20:57 - INFO - __main__ -   Text: ['Except, sunshine and love.\"']\n",
      "06/30/2022 15:20:58 - INFO - __main__ -   Epoch: 68 | Batch: 4000/10001 (40%) | G Loss: 2.029154 | C Loss: -0.993847\n",
      "06/30/2022 15:20:58 - INFO - __main__ -   Text: ['It\\'s a literary marvel that now exists.\"']\n",
      "06/30/2022 15:21:00 - INFO - __main__ -   Epoch: 68 | Batch: 4500/10001 (45%) | G Loss: 2.140603 | C Loss: -0.637627\n",
      "06/30/2022 15:21:00 - INFO - __main__ -   Text: ['Seduction from many people around the world.']\n",
      "06/30/2022 15:21:01 - INFO - __main__ -   Epoch: 68 | Batch: 5000/10001 (50%) | G Loss: 1.769297 | C Loss: -0.607478\n",
      "06/30/2022 15:21:01 - INFO - __main__ -   Text: ['Besides these, they have provided tips on using Photoshop.']\n",
      "06/30/2022 15:21:02 - INFO - __main__ -   Epoch: 68 | Batch: 5500/10001 (55%) | G Loss: 1.978222 | C Loss: -0.714955\n",
      "06/30/2022 15:21:03 - INFO - __main__ -   Text: ['She is considered a world-class student who excels at populating disciplines.']\n",
      "06/30/2022 15:21:04 - INFO - __main__ -   Epoch: 68 | Batch: 6000/10001 (60%) | G Loss: 3.875672 | C Loss: -0.990263\n",
      "06/30/2022 15:21:04 - INFO - __main__ -   Text: ['A rationalist definition of gender implies that is the \"stertium\".']\n",
      "06/30/2022 15:21:05 - INFO - __main__ -   Epoch: 68 | Batch: 6500/10001 (65%) | G Loss: 1.061693 | C Loss: -0.912047\n",
      "06/30/2022 15:21:06 - INFO - __main__ -   Text: ['The model K determines that number of nerves without equations in amortoids to a module.']\n",
      "06/30/2022 15:21:07 - INFO - __main__ -   Epoch: 68 | Batch: 7000/10001 (70%) | G Loss: 1.372965 | C Loss: -0.795435\n",
      "06/30/2022 15:21:07 - INFO - __main__ -   Text: ['In this respect I am going to close the notch to their asymmetric/% space compared to']\n",
      "06/30/2022 15:21:09 - INFO - __main__ -   Epoch: 68 | Batch: 7500/10001 (75%) | G Loss: 2.002888 | C Loss: -0.901687\n",
      "06/30/2022 15:21:09 - INFO - __main__ -   Text: ['These are now being discussed.']\n",
      "06/30/2022 15:21:10 - INFO - __main__ -   Epoch: 68 | Batch: 8000/10001 (80%) | G Loss: 2.099169 | C Loss: -0.608020\n",
      "06/30/2022 15:21:10 - INFO - __main__ -   Text: ['Mortlick said: Yeah Yes.\"']\n",
      "06/30/2022 15:21:12 - INFO - __main__ -   Epoch: 68 | Batch: 8500/10001 (85%) | G Loss: 2.565277 | C Loss: -0.902122\n",
      "06/30/2022 15:21:12 - INFO - __main__ -   Text: [\"It's basically very exotic technology, and you need to figure out how much water to produce.\"]\n",
      "06/30/2022 15:21:13 - INFO - __main__ -   Epoch: 68 | Batch: 9000/10001 (90%) | G Loss: 2.215234 | C Loss: -0.848288\n",
      "06/30/2022 15:21:13 - INFO - __main__ -   Text: ['The first is called \"physics\".']\n",
      "06/30/2022 15:21:15 - INFO - __main__ -   Epoch: 68 | Batch: 9500/10001 (95%) | G Loss: 1.610673 | C Loss: -0.644137\n",
      "06/30/2022 15:21:15 - INFO - __main__ -   Text: ['Both the students say that the story that they were asking me has nothing to do with them.']\n",
      "06/30/2022 15:21:16 - INFO - __main__ -   * (Train) Epoch: 68 | G Loss: 2.0509 | C Loss: -0.8230 | Updates G: 141 | Updates C: 859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 15:21:25 - INFO - __main__ -   Bleu-2:0.234 | B-Bleu-2:0.268\n",
      "06/30/2022 15:21:25 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_6.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5025686945185399\n",
      "Train file used is number 6\n",
      "../../yahoo/subdivided_large/train_6.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 69 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:22.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:33.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:44.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:56.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:07.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:18.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:29.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:40.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:51.\n",
      "  Batch   110  of    172.    Elapsed: 0:02:02.\n",
      "  Batch   120  of    172.    Elapsed: 0:02:13.\n",
      "  Batch   130  of    172.    Elapsed: 0:02:25.\n",
      "  Batch   140  of    172.    Elapsed: 0:02:36.\n",
      "  Batch   150  of    172.    Elapsed: 0:02:47.\n",
      "  Batch   160  of    172.    Elapsed: 0:02:58.\n",
      "  Batch   170  of    172.    Elapsed: 0:03:10.\n",
      "\n",
      "  Average training loss discriminator: 0.012\n",
      "  Training epcoh took: 0:03:12\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 15:24:38 - INFO - __main__ -   Epoch: 69 | Batch: 0/10001 (0%) | G Loss: 1.868827 | C Loss: -0.936477\n",
      "06/30/2022 15:24:38 - INFO - __main__ -   Text: [\"When a god is bellows somebody's bellows.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.455\n",
      "  Test Loss: 3.051\n",
      "  Test took: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 15:24:40 - INFO - __main__ -   Epoch: 69 | Batch: 500/10001 (5%) | G Loss: 2.631987 | C Loss: -0.725455\n",
      "06/30/2022 15:24:40 - INFO - __main__ -   Text: ['Lifepack doesn\\'t have to be named of the spirit...\"']\n",
      "06/30/2022 15:24:41 - INFO - __main__ -   Epoch: 69 | Batch: 1000/10001 (10%) | G Loss: 2.955954 | C Loss: -0.575738\n",
      "06/30/2022 15:24:41 - INFO - __main__ -   Text: ['The term \"Jewish Medicine\".']\n",
      "06/30/2022 15:24:43 - INFO - __main__ -   Epoch: 69 | Batch: 1500/10001 (15%) | G Loss: 1.620108 | C Loss: -0.730198\n",
      "06/30/2022 15:24:43 - INFO - __main__ -   Text: ['She\\'s eating stupid, stupid.\"']\n",
      "06/30/2022 15:24:44 - INFO - __main__ -   Epoch: 69 | Batch: 2000/10001 (20%) | G Loss: 2.405679 | C Loss: -0.737330\n",
      "06/30/2022 15:24:44 - INFO - __main__ -   Text: ['= a (1 of 2).']\n",
      "06/30/2022 15:24:46 - INFO - __main__ -   Epoch: 69 | Batch: 2500/10001 (25%) | G Loss: 2.329486 | C Loss: -0.750909\n",
      "06/30/2022 15:24:46 - INFO - __main__ -   Text: ['Many hedgehogs of the shade.']\n",
      "06/30/2022 15:24:47 - INFO - __main__ -   Epoch: 69 | Batch: 3000/10001 (30%) | G Loss: 1.925933 | C Loss: -0.727361\n",
      "06/30/2022 15:24:47 - INFO - __main__ -   Text: ['These are incredibly positive!\"']\n",
      "06/30/2022 15:24:49 - INFO - __main__ -   Epoch: 69 | Batch: 3500/10001 (35%) | G Loss: 1.878930 | C Loss: -0.746229\n",
      "06/30/2022 15:24:49 - INFO - __main__ -   Text: ['XML devices evaluate a value.']\n",
      "06/30/2022 15:24:50 - INFO - __main__ -   Epoch: 69 | Batch: 4000/10001 (40%) | G Loss: 1.898885 | C Loss: -0.701264\n",
      "06/30/2022 15:24:50 - INFO - __main__ -   Text: [\"However, real estate owner Tony's finances have to be threatened by the police.\"]\n",
      "06/30/2022 15:24:52 - INFO - __main__ -   Epoch: 69 | Batch: 4500/10001 (45%) | G Loss: 2.068415 | C Loss: -0.758103\n",
      "06/30/2022 15:24:52 - INFO - __main__ -   Text: ['A could also refer to  you.']\n",
      "06/30/2022 15:24:53 - INFO - __main__ -   Epoch: 69 | Batch: 5000/10001 (50%) | G Loss: 2.487128 | C Loss: -0.794396\n",
      "06/30/2022 15:24:53 - INFO - __main__ -   Text: ['Can he prove the existence of the \"Telecom\".']\n",
      "06/30/2022 15:24:55 - INFO - __main__ -   Epoch: 69 | Batch: 5500/10001 (55%) | G Loss: 3.359988 | C Loss: -1.176798\n",
      "06/30/2022 15:24:55 - INFO - __main__ -   Text: ['Risk has noticed that she may be interested in college journalism without actually being a reporter.']\n",
      "06/30/2022 15:24:57 - INFO - __main__ -   Epoch: 69 | Batch: 6000/10001 (60%) | G Loss: 1.937444 | C Loss: -0.805968\n",
      "06/30/2022 15:24:57 - INFO - __main__ -   Text: ['He is devoted to nothing but humanity.']\n",
      "06/30/2022 15:24:58 - INFO - __main__ -   Epoch: 69 | Batch: 6500/10001 (65%) | G Loss: 2.129063 | C Loss: -0.739484\n",
      "06/30/2022 15:24:58 - INFO - __main__ -   Text: ['They will eat fried olives imported from the French.']\n",
      "06/30/2022 15:25:00 - INFO - __main__ -   Epoch: 69 | Batch: 7000/10001 (70%) | G Loss: 2.462234 | C Loss: -0.806608\n",
      "06/30/2022 15:25:00 - INFO - __main__ -   Text: ['\"\"All Players\"(51Sec.']\n",
      "06/30/2022 15:25:01 - INFO - __main__ -   Epoch: 69 | Batch: 7500/10001 (75%) | G Loss: 2.472514 | C Loss: -0.719502\n",
      "06/30/2022 15:25:01 - INFO - __main__ -   Text: ['Applied development techniques include routestudy.']\n",
      "06/30/2022 15:25:03 - INFO - __main__ -   Epoch: 69 | Batch: 8000/10001 (80%) | G Loss: 2.177090 | C Loss: -0.803909\n",
      "06/30/2022 15:25:03 - INFO - __main__ -   Text: [\"The literature on convergence today is MIT's corpus.\"]\n",
      "06/30/2022 15:25:04 - INFO - __main__ -   Epoch: 69 | Batch: 8500/10001 (85%) | G Loss: 2.656329 | C Loss: -0.763814\n",
      "06/30/2022 15:25:04 - INFO - __main__ -   Text: ['The last person that can be blamed for all this is \"God\".']\n",
      "06/30/2022 15:25:06 - INFO - __main__ -   Epoch: 69 | Batch: 9000/10001 (90%) | G Loss: 2.046457 | C Loss: -0.826193\n",
      "06/30/2022 15:25:06 - INFO - __main__ -   Text: ['Let\\'s say \"123\".']\n",
      "06/30/2022 15:25:07 - INFO - __main__ -   Epoch: 69 | Batch: 9500/10001 (95%) | G Loss: 1.991457 | C Loss: -0.814326\n",
      "06/30/2022 15:25:07 - INFO - __main__ -   Text: ['They are not accountable.\"']\n",
      "06/30/2022 15:25:09 - INFO - __main__ -   * (Train) Epoch: 69 | G Loss: 2.2376 | C Loss: -0.7967 | Updates G: 92 | Updates C: 908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2022 15:25:18 - INFO - __main__ -   Bleu-2:0.177 | B-Bleu-2:0.247\n",
      "06/30/2022 15:25:18 - INFO - utils -   Loading features from cached file ../../yahoo/subdivided_large/cached_lm_gpt_bert_100_train_7.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42372102704328496\n",
      "Train file used is number 7\n",
      "../../yahoo/subdivided_large/train_7.txt\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 70 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    172.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    172.    Elapsed: 0:00:21.\n",
      "  Batch    30  of    172.    Elapsed: 0:00:33.\n",
      "  Batch    40  of    172.    Elapsed: 0:00:43.\n",
      "  Batch    50  of    172.    Elapsed: 0:00:54.\n",
      "  Batch    60  of    172.    Elapsed: 0:01:05.\n",
      "  Batch    70  of    172.    Elapsed: 0:01:16.\n",
      "  Batch    80  of    172.    Elapsed: 0:01:27.\n",
      "  Batch    90  of    172.    Elapsed: 0:01:38.\n",
      "  Batch   100  of    172.    Elapsed: 0:01:48.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--seed', type=int, default=0)\n",
    "    parser.add_argument('--epochs', type=int, default=15)\n",
    "    parser.add_argument('--lr', type=float, default=1e-4)\n",
    "    parser.add_argument('--gp_lambda', type=int, default=10)\n",
    "    parser.add_argument('--n_layers', type=int, default=20, help=\"Number of layers of generator and critic\")\n",
    "    parser.add_argument('--block_dim', type=int, default=100)\n",
    "    parser.add_argument('--interval', type=int, default=10, help=\"Steps before logging output\")\n",
    "    parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())\n",
    "    \n",
    "    # Optimus parameters\n",
    "    parser.add_argument(\"--train_data_file\", default=None, type=str, required=True,\n",
    "                        help=\"The input training data file (a text file).\")\n",
    "    parser.add_argument(\"--valid_data_file\", default=None, type=str, required=True,\n",
    "                        help=\"The input validation data file (a text file).\")\n",
    "    parser.add_argument(\"--checkpoint_dir\", default=None, type=str, required=True,\n",
    "                        help=\"The directory where checkpoints are saved.\")\n",
    "    parser.add_argument('--generator_dir', default=None, type=str, help=\"Directory where GAN models are saved\")\n",
    "    parser.add_argument(\"--output_dir\", default=None, type=str, required=True,\n",
    "                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
    "    parser.add_argument(\"--dataset\", default='Snli', type=str, help=\"The dataset.\")    \n",
    "    parser.add_argument(\"--latent_size\", default=32, type=int, help=\"Latent space dimension.\")\n",
    "    ## Encoder options\n",
    "    parser.add_argument(\"--encoder_model_type\", default=\"bert\", type=str,\n",
    "                        help=\"The encoder model architecture to be fine-tuned.\")\n",
    "    parser.add_argument(\"--encoder_model_name_or_path\", default=\"bert-base-cased\", type=str,\n",
    "                        help=\"The encoder model checkpoint for weights initialization.\")\n",
    "    parser.add_argument(\"--encoder_config_name\", default=\"\", type=str,\n",
    "                        help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
    "    parser.add_argument(\"--encoder_tokenizer_name\", default=\"\", type=str,\n",
    "                        help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
    "    ## Decoder options\n",
    "    parser.add_argument(\"--decoder_model_type\", default=\"gpt2\", type=str,\n",
    "                        help=\"The decoder model architecture to be fine-tuned.\")\n",
    "    parser.add_argument(\"--decoder_model_name_or_path\", default=\"bert-base-cased\", type=str,\n",
    "                        help=\"The decoder model checkpoint for weights initialization.\")\n",
    "    parser.add_argument(\"--decoder_config_name\", default=\"\", type=str,\n",
    "                        help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
    "    parser.add_argument(\"--decoder_tokenizer_name\", default=\"\", type=str,\n",
    "                        help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
    "    parser.add_argument(\"--per_gpu_train_batch_size\", default=1, type=int,\n",
    "                        help=\"Batch size per GPU/CPU for training.\")\n",
    "    parser.add_argument(\"--max_seq_length\", default=512, type=int,\n",
    "                        help=\"Optional input sequence length before tokenization. The sequence will be dropped if it is longer the max_seq_length\")\n",
    "\n",
    "    ## Variational auto-encoder(check this)\n",
    "    parser.add_argument(\"--prompt\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--padding_text\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--length\", type=int, default=20)\n",
    "    parser.add_argument(\"--block_size\", default=-1, type=int,\n",
    "                        help=\"Optional input sequence length after tokenization.\"\n",
    "                             \"The training dataset will be truncated in block of this size for training.\"\n",
    "                             \"Default to the model max input length for single sentence inputs (take into account special tokens).\")\n",
    "    parser.add_argument(\"--do_lower_case\", action='store_true',\n",
    "                        help=\"Set this flag if you are using an uncased model.\")\n",
    "    parser.add_argument(\"--use_philly\", action='store_true',\n",
    "                        help=\"Use Philly for computing.\")\n",
    "    parser.add_argument('--gloabl_step_eval', type=int, default=661,\n",
    "                        help=\"Evaluate the results at the given global step\")\n",
    "    # Reinforcement learning parameters\n",
    "    parser.add_argument('--finetune_decoder', type=bool, default=True)\n",
    "    parser.add_argument('--epochs_rl', type=int, default=1000)\n",
    "    parser.add_argument('--batch_size_rl', type=int, default=32)\n",
    "    parser.add_argument('--lr_rl', type=float, default=1e-6)\n",
    "\n",
    "\n",
    "    # Load a trained Encoder model and vocabulary that you have fine-tuned\n",
    "    args = parser.parse_args(\"--dataset EMNLP \\\n",
    "    --checkpoint_dir=checkpoint-508523-768-0 \\\n",
    "    --output_dir=checkpoint-508523-768-0 \\\n",
    "    --encoder_model_type=bert \\\n",
    "    --encoder_model_name_or_path=bert-base-cased \\\n",
    "    --decoder_model_type=gpt2 \\\n",
    "    --decoder_model_name_or_path=gpt2 \\\n",
    "    --train_data_file=../../yahoo/subdivided_large/train \\\n",
    "    --valid_data_file=../../yahoo/unlabelled_short/test.txt \\\n",
    "    --per_gpu_train_batch_size 10 \\\n",
    "    --block_size 100 \\\n",
    "    --max_seq_length 24 \\\n",
    "    --gloabl_step_eval 508523 \\\n",
    "    --latent_size 768 \\\n",
    "    --block_dim 100 \\\n",
    "    --n_layers 10 \\\n",
    "    --interval 50 \\\n",
    "    --epochs 200 \\\n",
    "    --finetune_decoder False \\\n",
    "    --lr_rl 1e-6 \\\n",
    "    --epochs_rl 100 \\\n",
    "    --batch_size_rl 32\".split())\n",
    "    \n",
    "    print(args)\n",
    "\n",
    "    global_step = args.gloabl_step_eval\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    #args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    #args.n_gpu = torch.cuda.device_count()\n",
    "    args.device = torch.device(\"cuda:0\")\n",
    "    args.n_gpu=1\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)       \n",
    "    \n",
    "    args.encoder_model_type = args.encoder_model_type.lower()\n",
    "    args.decoder_model_type = args.decoder_model_type.lower()\n",
    "\n",
    "    output_encoder_dir = os.path.join(args.checkpoint_dir, 'checkpoint-encoder-{}'.format(global_step))\n",
    "    output_decoder_dir = os.path.join(args.checkpoint_dir, 'checkpoint-decoder-{}'.format(global_step)) \n",
    "    checkpoints = [ [output_encoder_dir, output_decoder_dir] ]\n",
    "\n",
    "    # Load a trained Encoder model and vocabulary that you have fine-tuned\n",
    "    encoder_config_class, encoder_model_class, encoder_tokenizer_class = MODEL_CLASSES[args.encoder_model_type]\n",
    "    model_encoder = encoder_model_class.from_pretrained(output_encoder_dir, latent_size=args.latent_size)\n",
    "    tokenizer_encoder = encoder_tokenizer_class.from_pretrained(args.encoder_tokenizer_name if args.encoder_tokenizer_name else args.encoder_model_name_or_path, do_lower_case=args.do_lower_case)\n",
    "\n",
    "    model_encoder.to(args.device)\n",
    "    if args.block_size <= 0:\n",
    "        args.block_size = tokenizer_encoder.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
    "    args.block_size = min(args.block_size, tokenizer_encoder.max_len_single_sentence)\n",
    "\n",
    "    # Load a trained Decoder model and vocabulary that you have fine-tuned\n",
    "    decoder_config_class, decoder_model_class, decoder_tokenizer_class = MODEL_CLASSES[args.decoder_model_type]\n",
    "    model_decoder = decoder_model_class.from_pretrained(output_decoder_dir, latent_size=args.latent_size)\n",
    "    tokenizer_decoder = decoder_tokenizer_class.from_pretrained(args.decoder_tokenizer_name if args.decoder_tokenizer_name else args.decoder_model_name_or_path, do_lower_case=args.do_lower_case)\n",
    "    model_decoder.to(args.device)\n",
    "    if args.block_size <= 0:\n",
    "        args.block_size = tokenizer_decoder.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
    "    args.block_size = min(args.block_size, tokenizer_decoder.max_len_single_sentence)\n",
    "\n",
    "    # Chunyuan: Add Padding token to GPT2\n",
    "    special_tokens_dict = {'pad_token': '<PAD>', 'bos_token': '<BOS>', 'eos_token': '<EOS>'}\n",
    "    num_added_toks = tokenizer_decoder.add_special_tokens(special_tokens_dict)\n",
    "    logger.info('We have added {} tokens to GPT2'.format(num_added_toks))\n",
    "    model_decoder.resize_token_embeddings(len(tokenizer_decoder))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n",
    "    assert tokenizer_decoder.pad_token == '<PAD>'\n",
    "\n",
    "    #train_loader, num_txt = build_dataload_and_cache_examples(args, [tokenizer_encoder, tokenizer_decoder], num_txt) \n",
    "    generator = Generator(args.n_layers, args.block_dim,args.latent_size)\n",
    "    critic = Critic(args.n_layers, args.block_dim,args.latent_size)\n",
    "\n",
    "    if args.generator_dir!=None:\n",
    "        logger.info(\"Loading generator and critic\")\n",
    "        generator.load_state_dict(torch.load(args.generator_dir+'/generator_'+str(args.gloabl_step_eval)+'.th'))\n",
    "        critic.load_state_dict(torch.load(args.generator_dir+'/critic_'+str(args.gloabl_step_eval)+'.th'))\n",
    "\n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=args.lr, betas=(0.5, 0.999))\n",
    "    c_optimizer = optim.Adam(critic.parameters(), lr=args.lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    if args.cuda:\n",
    "        generator = generator.cuda(device)\n",
    "        critic = critic.cuda(device)\n",
    "    \n",
    "    logger.info('G Parameters:{}'.format(sum([p.numel() for p in generator.parameters() if \\\n",
    "                                p.requires_grad])))\n",
    "    logger.info('C Parameters:{}'.format(sum([p.numel() for p in critic.parameters() if \\\n",
    "                                p.requires_grad])))\n",
    "    \n",
    "    device = args.device\n",
    "    \n",
    "    best_bleu = 0\n",
    "    reference = list()\n",
    "    with(open(args.valid_data_file,\"r\")) as valid:\n",
    "        for sents in valid:\n",
    "            reference.append(sents.replace(\"\\n\", \"\"))\n",
    "            \n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        \n",
    "        #Insert GAN-BERT Code Here\n",
    "        \n",
    "        train_loader, num_txt = build_dataload_and_cache_examples(args, [tokenizer_encoder, tokenizer_decoder], num_txt) \n",
    "        \n",
    "        print(\"Train classification discriminator\")\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        # Perform one full pass over the training set.\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch, args.epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        tr_g_loss = 0\n",
    "        tr_d_loss = 0\n",
    "\n",
    "        # Put the model into training mode.\n",
    "        transformer.train() \n",
    "        #generator.train()\n",
    "        discriminator.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Progress update every print_each_n_step batches.\n",
    "            if step % print_each_n_step == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            b_label_mask = batch[3].to(device)\n",
    "\n",
    "            real_batch_size = b_input_ids.shape[0]\n",
    "\n",
    "            # Encode real data in the Transformer\n",
    "            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask, output_hidden_states=True)\n",
    "            token_embeddings = model_outputs.hidden_states\n",
    "            #token_embeddings = model_outputs.last_hidden_state\n",
    "            #pooled = torch.max((token_embeddings * b_input_mask.unsqueeze(-1)), axis=1)\n",
    "            #mean_pooled = token_embeddings.sum(axis=1) / b_input_mask.sum(axis=-1).unsqueeze(-1)\n",
    "            custom_pooled = torch.cat(tuple([token_embeddings[i] for i in [-4, -3, -2, -1]]), dim=-1)\n",
    "            hidden_states = custom_pooled [:,0,:]\n",
    "            #hidden_states = model_outputs.last_hidden_state[:,0,:] \n",
    "            #hidden_states = model_outputs[-1]\n",
    "            #print(\"  Number of real sentences (labelled and unlabelled): {}\".format(len(hidden_states)))\n",
    "            \n",
    "            # Generate fake data that should have the same distribution of the ones\n",
    "            # encoded by the transformer. \n",
    "            # First noisy input are used in input to the Generator\n",
    "            fixed_noise = torch.Tensor(np.random.normal(0, 1, (real_batch_size, args.latent_size))).to(args.device)\n",
    "            test_z_gb = generator(fixed_noise).data\n",
    "            fake_sentences = rollout_test(model_decoder, test_z_gb, tokenizer_decoder, args.max_seq_length, real_batch_size, 0, 1)\n",
    "            #print(\"  Number of generated sentences: {}\".format(len(fake_sentences)))\n",
    "\n",
    "            b_input_ids_fake, b_input_mask_fake = generate_data_fake(fake_sentences)\n",
    "            b_input_ids_fake = b_input_ids_fake.to(device)\n",
    "            b_input_mask_fake = b_input_mask_fake.to(device)\n",
    "            model_outputs_fake = transformer(b_input_ids_fake, attention_mask=b_input_mask_fake, output_hidden_states=True)\n",
    "            token_embeddings_fake = model_outputs_fake.hidden_states\n",
    "            #token_embeddings = model_outputs.last_hidden_state\n",
    "            #pooled = torch.max((token_embeddings * b_input_mask.unsqueeze(-1)), axis=1)\n",
    "            #mean_pooled = token_embeddings.sum(axis=1) / b_input_mask.sum(axis=-1).unsqueeze(-1)\n",
    "            custom_pooled_fake = torch.cat(tuple([token_embeddings_fake[i] for i in [-4, -3, -2, -1]]), dim=-1)\n",
    "            hidden_states_fake = custom_pooled_fake[:,0,:]\n",
    "            #hidden_states_fake = model_outputs_fake.last_hidden_state[:,0,:] \n",
    "            #hidden_states_fake = model_outputs_fake[-1]\n",
    "\n",
    "            #noise = torch.zeros(real_batch_size, noise_size, device=device).uniform_(0, 1)\n",
    "            # Gnerate Fake data\n",
    "            #gen_rep = generator(noise)\n",
    "            #print(\"Length of generator output {}\".format(len(gen_rep)))\n",
    "            #print(\"Length of single generator output {}\".format(len(gen_rep[0])))\n",
    "\n",
    "            # Generate the output of the Discriminator for real and fake data.\n",
    "            # First, we put together the output of the tranformer and the generator\n",
    "            disciminator_input = torch.cat([hidden_states, hidden_states_fake], dim=0)\n",
    "            # Then, we select the output of the disciminator\n",
    "            features, logits, probs = discriminator(disciminator_input)\n",
    "\n",
    "            # Finally, we separate the discriminator's output for the real and fake\n",
    "            # data\n",
    "            features_list = torch.split(features, real_batch_size)\n",
    "            D_real_features = features_list[0]\n",
    "            D_fake_features = features_list[1]\n",
    "\n",
    "            logits_list = torch.split(logits, real_batch_size)\n",
    "            D_real_logits = logits_list[0]\n",
    "            D_fake_logits = logits_list[1]\n",
    "\n",
    "            probs_list = torch.split(probs, real_batch_size)\n",
    "            D_real_probs = probs_list[0]\n",
    "            D_fake_probs = probs_list[1]\n",
    "\n",
    "            #---------------------------------\n",
    "            #  LOSS evaluation\n",
    "            #---------------------------------\n",
    "            # Generator's LOSS estimation\n",
    "            #g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + epsilon))\n",
    "            #g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n",
    "            #g_loss = g_loss_d + g_feat_reg\n",
    "\n",
    "            # Disciminator's LOSS estimation\n",
    "            logits = D_real_logits[:,0:-1]\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            # The discriminator provides an output for labeled and unlabeled real data\n",
    "            # so the loss evaluated for unlabeled data is ignored (masked)\n",
    "            label2one_hot = torch.nn.functional.one_hot(b_labels, len(label_list))\n",
    "            per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n",
    "            per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n",
    "            labeled_example_count = per_example_loss.type(torch.float32).numel()\n",
    "\n",
    "            # It may be the case that a batch does not contain labeled examples, \n",
    "            # so the \"supervised loss\" in this case is not evaluated\n",
    "            if labeled_example_count == 0:\n",
    "              D_L_Supervised = 0\n",
    "            else:\n",
    "              D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n",
    "\n",
    "            D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n",
    "            D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n",
    "            d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n",
    "\n",
    "            #---------------------------------\n",
    "            #  OPTIMIZATION\n",
    "            #---------------------------------\n",
    "            # Avoid gradient accumulation\n",
    "            #gen_optimizer.zero_grad()\n",
    "            dis_optimizer.zero_grad()\n",
    "\n",
    "            # Calculate weigth updates\n",
    "            # retain_graph=True is required since the underlying graph will be deleted after backward\n",
    "            #g_loss.backward(retain_graph=True)\n",
    "            d_loss.backward() \n",
    "\n",
    "            # Apply modifications\n",
    "            #gen_optimizer.step()\n",
    "            dis_optimizer.step()\n",
    "\n",
    "            # A detail log of the individual losses\n",
    "            #print(\"{0:.4f}\\t{1:.4f}\\t{2:.4f}\\t{3:.4f}\\t{4:.4f}\".\n",
    "            #      format(D_L_Supervised, D_L_unsupervised1U, D_L_unsupervised2U,\n",
    "            #             g_loss_d, g_feat_reg))\n",
    "\n",
    "            # Save the losses to print them later\n",
    "            #tr_g_loss += g_loss.item()\n",
    "            tr_d_loss += d_loss.item()\n",
    "\n",
    "            # Update the learning rate with the scheduler\n",
    "            if apply_scheduler:\n",
    "              scheduler_d.step()\n",
    "              #scheduler_g.step()\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        #avg_train_loss_g = tr_g_loss / len(train_dataloader)\n",
    "        avg_train_loss_d = tr_d_loss / len(train_dataloader)             \n",
    "\n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        #print(\"  Average training loss generetor: {0:.3f}\".format(avg_train_loss_g))\n",
    "        print(\"  Average training loss discriminator: {0:.3f}\".format(avg_train_loss_d))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "        # ========================================\n",
    "        #     TEST ON THE EVALUATION DATASET\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our test set.\n",
    "        print(\"\")\n",
    "        print(\"Running Test...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        transformer.eval() #maybe redundant\n",
    "        discriminator.eval()\n",
    "        #generator.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_test_accuracy = 0\n",
    "\n",
    "        total_test_loss = 0\n",
    "        nb_test_steps = 0\n",
    "\n",
    "        all_preds = []\n",
    "        all_labels_ids = []\n",
    "\n",
    "        #loss\n",
    "        nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in test_dataloader:\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "                model_outputs = transformer(b_input_ids, attention_mask=b_input_mask, output_hidden_states=True)\n",
    "                token_embeddings = model_outputs.hidden_states\n",
    "                #token_embeddings = model_outputs.last_hidden_state\n",
    "                #pooled = torch.max((token_embeddings * b_input_mask.unsqueeze(-1)), axis=1)\n",
    "                #mean_pooled = token_embeddings.sum(axis=1) / b_input_mask.sum(axis=-1).unsqueeze(-1)\n",
    "                custom_pooled = torch.cat(tuple([token_embeddings[i] for i in [-4, -3, -2, -1]]), dim=-1)\n",
    "                hidden_states = custom_pooled [:,0,:]\n",
    "                #hidden_states = model_outputs.last_hidden_state[:,0,:] \n",
    "                #hidden_states = model_outputs[-1]\n",
    "                _, logits, probs = discriminator(hidden_states)\n",
    "                ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n",
    "                filtered_logits = logits[:,0:-1]\n",
    "                # Accumulate the test loss.\n",
    "                total_test_loss += nll_loss(filtered_logits, b_labels)\n",
    "\n",
    "            # Accumulate the predictions and the input labels\n",
    "            _, preds = torch.max(filtered_logits, 1)\n",
    "            all_preds += preds.detach().cpu()\n",
    "            all_labels_ids += b_labels.detach().cpu()\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        all_preds = torch.stack(all_preds).numpy()\n",
    "        all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
    "        test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
    "        print(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "        avg_test_loss = avg_test_loss.item()\n",
    "\n",
    "        # Measure how long the validation run took.\n",
    "        test_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"  Test Loss: {0:.3f}\".format(avg_test_loss))\n",
    "        print(\"  Test took: {:}\".format(test_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch + 1,\n",
    "                #'Training Loss generator': avg_train_loss_g,\n",
    "                'Training Loss discriminator': avg_train_loss_d,\n",
    "                'Valid. Loss': avg_test_loss,\n",
    "                'Valid. Accur.': test_accuracy,\n",
    "                'Training Time': training_time,\n",
    "                'Test Time': test_time\n",
    "            }\n",
    "        )\n",
    "\n",
    "        accuracy_array.append(test_accuracy)\n",
    "        \n",
    "        #OPTAGAN Code\n",
    "        \n",
    "        g_loss, c_loss = train(epoch)\n",
    "\n",
    "        data_test = list()\n",
    "        for i in range(2):\n",
    "            test_noise = torch.Tensor(np.random.normal(0, 1, (250, args.latent_size))).to(args.device)\n",
    "            test_z = generator(test_noise).data\n",
    "            new_sent = rollout_test(model_decoder, test_z, tokenizer_decoder, args.max_seq_length, 250, 0, 1)\n",
    "            data_test.extend(new_sent)\n",
    "\n",
    "        p_reference = random.sample(reference, 500)\n",
    "        bleu = calc_blue_parallel_func(p_reference, data_test, 2, 500)\n",
    "        b_bleu = calc_blue_parallel_func(data_test, p_reference, 2, 500)\n",
    "        logger.info(\"Bleu-2:{:0.3f} | B-Bleu-2:{:0.3f}\".format(bleu, b_bleu))\n",
    "        \n",
    "        print(bleu+b_bleu)\n",
    "        if (bleu+b_bleu) > best_bleu:\n",
    "            best_bleu = bleu + b_bleu\n",
    "            logger.info('* Saving. Best Score:{:0.3f} | Bleu-2:{:0.3f} | B-Bleu-2:{:0.3f}'.format(best_bleu, bleu, b_bleu))\n",
    "            torch.save(generator.state_dict(), args.output_dir+'/generator_'+str(args.gloabl_step_eval)+'.th')\n",
    "            torch.save(critic.state_dict(), args.output_dir+'/critic_'+str(args.gloabl_step_eval)+'.th')\n",
    "            \n",
    "        \n",
    "\n",
    "#     if args.finetune_decoder: \n",
    "#         logger.info(\"Loading generator\")\n",
    "#         generator.load_state_dict(torch.load(args.output_dir+'/generator_'+str(args.gloabl_step_eval)+'.th'))\n",
    "        \n",
    "#         model_decoder.train()\n",
    "#         generator.eval()\n",
    "#         dec_optimizer = optim.Adam(model_decoder.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "#         value_loss = nn.L1Loss()\n",
    "#         B = args.batch_size_rl\n",
    "#         total_scores = 0\n",
    "#         total_entropy = 0\n",
    "#         total_values = 0\n",
    "#         total_v_loss = 0\n",
    "#         for epoch_ in range(args.epochs_rl):\n",
    "#             if epoch_ == 200:\n",
    "#                 # Finetune decoder after training of value head\n",
    "#                 dec_optimizer = optim.Adam(model_decoder.parameters(), lr=args.lr_rl, betas=(0.5, 0.999))\n",
    "#             noise = torch.from_numpy(np.random.normal(0, 1, (B, args.latent_size))).float()\n",
    "#             noise = noise.to(args.device)\n",
    "#             z_fake = generator(noise)            \n",
    "#             sents, logprobs, values, entropy = rollout(model_decoder, z_fake, tokenizer_decoder, args.max_seq_length, B, 1)\n",
    "#             p_reference = random.sample(reference, 500)\n",
    "\n",
    "#             blue = []\n",
    "#             for i in sents:\n",
    "#                 blue.append(calc_blue_parallel_func(p_reference, [i], 1, 0))\n",
    "\n",
    "#             values = torch.stack(values, dim=1)\n",
    "#             logprobs = torch.stack(logprobs, dim=1)\n",
    "#             entropy = torch.stack(entropy, dim=1)\n",
    "\n",
    "#             # Get tokens and mask of batch\n",
    "#             toks_gpt = [([50258] + tokenizer_decoder.encode(j) + [50259]) for j in sents]\n",
    "#             toks_gpt, mask = pad_seq(toks_gpt, tokenizer_decoder.encode(\"<PAD>\")[0], values.size(1)+1)\n",
    "#             toks_gpt = torch.tensor(toks_gpt).to(args.device)\n",
    "#             mask = torch.tensor(mask).to(args.device)\n",
    "              \n",
    "#             values = values * mask[:,1:]\n",
    "#             logprobs = logprobs * mask[:,1:]\n",
    "#             entropy = entropy * mask[:,1:]\n",
    "#             scores = torch.tensor(blue).to(args.device)\n",
    "#             # Get value loss\n",
    "#             v_loss = value_loss(torch.sum(values, dim=1), scores) \n",
    "              \n",
    "#             if epoch_ >= 200:\n",
    "#                 R = 0\n",
    "#                 rewards = []\n",
    "\n",
    "#                 # Discount future rewards back to the present using gamma\n",
    "#                 for j in range(len(values.tolist())):\n",
    "#                     R = 0\n",
    "#                     batch_rewards = []\n",
    "#                     for r in reversed(values.tolist()[j]):\n",
    "#                         R = r + 0.99 * R\n",
    "#                         batch_rewards.insert(0,R)\n",
    "#                     rewards.append(batch_rewards)\n",
    "\n",
    "#                 # Penalizing low entropy states\n",
    "#                 rewards = torch.FloatTensor(rewards).to(args.device)\n",
    "#                 rewards = rewards + torch.log(torch.clamp(entropy,0.2,1))\n",
    "#                 # Calculate loss\n",
    "#                 d_loss = torch.sum(torch.mul(logprobs, rewards.detach()).mul(-1))\n",
    "#             else:\n",
    "#                 d_loss = torch.tensor(0)\n",
    "\n",
    "#             # Backpropagate losses\n",
    "#             loss = v_loss + d_loss              \n",
    "#             dec_optimizer.zero_grad()              \n",
    "#             loss.backward()\n",
    "#             dec_optimizer.step()\n",
    "\n",
    "#             total_scores += torch.mean(scores).item()\n",
    "#             total_values += torch.mean(torch.sum(values,-1)).item()\n",
    "#             total_v_loss += v_loss.item()\n",
    "#             total_entropy += torch.mean(torch.mean(entropy,dim=1)).item()\n",
    "#             if (epoch_ % args.interval) == 0:\n",
    "#                 logger.info(\"Epoch {}/{} | Value Loss:{} | Mean values:{} | Mean BLEU scores:{} | Mean Entropy: {}\".format(epoch_, \n",
    "#                 args.epochs_rl, total_v_loss/args.interval, total_values/args.interval, total_scores/args.interval, total_entropy/args.interval))\n",
    "#                 total_scores = 0\n",
    "#                 total_values = 0\n",
    "#                 total_v_loss = 0\n",
    "#                 total_entropy = 0\n",
    "#         logger.info(\"Saving decoder\")\n",
    "#         output_decoder_dir = os.path.join(args.output_dir, 'checkpoint-decoder-{}'.format(global_step))\n",
    "#         if not os.path.exists(output_decoder_dir):\n",
    "#             os.makedirs(output_decoder_dir)\n",
    "#         model_decoder.save_pretrained(output_decoder_dir)\n",
    "#         torch.save(args, os.path.join(output_decoder_dir, 'training_encoder_args.bin'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60f76a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5139a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70a35fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(accuracy_array)) #0.4725 e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967334e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_array[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c07743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(accuracy_array)\n",
    "plt.title('OPTAGAN-GAN-BERT Performance over Training Epochs', fontsize=20)\n",
    "plt.xlabel('Epochs', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.xlim(0,200)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9ec8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_save = pd.DataFrame(accuracy_array)\n",
    "df_to_save.to_csv('accuracy_array_optagan_yahoo_custom_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eae8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Generating Sentences\n",
    "# from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "# import argparse\n",
    "\n",
    "# import logging\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import numpy as np\n",
    "\n",
    "# from modules.gan import Generator\n",
    "\n",
    "# import glob\n",
    "# import os\n",
    "# import pickle\n",
    "# import random\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "# from tqdm import tqdm, trange\n",
    "\n",
    "# from func import GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig, BertConfig\n",
    "# from func import GPT2LMHeadModel, GPT2Tokenizer, GPT2ForLatentConnector, GPT2ForLatentConnectorValueHead\n",
    "# from func import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer\n",
    "# from func import XLNetLMHeadModel, XLNetTokenizer\n",
    "# from func import TransfoXLLMHeadModel, TransfoXLTokenizer\n",
    "# from func import BertForLatentConnector, BertTokenizer\n",
    "\n",
    "# from collections import defaultdict\n",
    "# import pdb\n",
    "# from modules.utils import rollout_test\n",
    "\n",
    "# MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "\n",
    "# ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) for conf in (GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig)), ())\n",
    "\n",
    "# logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "#                     datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "#                     level = logging.INFO)\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# MODEL_CLASSES = {\n",
    "#     'gpt2': (GPT2Config, GPT2ForLatentConnector, GPT2Tokenizer),\n",
    "#     'bert': (BertConfig, BertForLatentConnector, BertTokenizer),\n",
    "#     'gpt2v': (GPT2Config, GPT2ForLatentConnectorValueHead, GPT2Tokenizer)\n",
    "# }\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--seed', type=int, default=0)\n",
    "#     parser.add_argument('--new_sent', type=int, default=1, help=\"Number of sentences to generate\")\n",
    "#     parser.add_argument('--n_layers', type=int, default=20, help=\"Number of layers of generator\")\n",
    "#     parser.add_argument('--block_dim', type=int, default=100)\n",
    "#     parser.add_argument('--interval', type=int, default=10)\n",
    "#     parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())\n",
    "#     parser.add_argument('--generator_dir', default=None, type=str, required=True, help=\"Directory of GAN model checkpoint\")\n",
    "#     parser.add_argument(\"--checkpoint_dir\", default=None, type=str, required=True,\n",
    "#                         help=\"The directory where checkpoints are saved.\")\n",
    "#     parser.add_argument(\"--output_dir\", default=None, type=str, required=True,\n",
    "#                         help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
    "#     parser.add_argument(\"--save\", default=False, type=bool, help=\"Save results to file.\")\n",
    "#     parser.add_argument(\"--latent_size\", default=32, type=int, help=\"Latent space dimension.\")\n",
    "#     parser.add_argument(\"--output_name\", default=\"results\", type=str, help=\"File name of output\")\n",
    "#     parser.add_argument(\"--batch_size\", default=100, type=int, help=\"Batch size to generate outputs\")\n",
    "#     ## Encoder options\n",
    "#     parser.add_argument(\"--encoder_model_type\", default=\"bert\", type=str,\n",
    "#                         help=\"The encoder model architecture to be fine-tuned.\")\n",
    "#     parser.add_argument(\"--encoder_model_name_or_path\", default=\"bert-base-cased\", type=str,\n",
    "#                         help=\"The encoder model checkpoint for weights initialization.\")\n",
    "#     parser.add_argument(\"--encoder_config_name\", default=\"\", type=str,\n",
    "#                         help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
    "#     parser.add_argument(\"--encoder_tokenizer_name\", default=\"\", type=str,\n",
    "#                         help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
    "#     ## Decoder options\n",
    "#     parser.add_argument(\"--decoder_model_type\", default=\"gpt2\", type=str,\n",
    "#                         help=\"The decoder model architecture to be fine-tuned.\")\n",
    "#     parser.add_argument(\"--decoder_model_name_or_path\", default=\"gpt2\", type=str,\n",
    "#                         help=\"The decoder model checkpoint for weights initialization.\")\n",
    "#     parser.add_argument(\"--decoder_config_name\", default=\"\", type=str,\n",
    "#                         help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
    "#     parser.add_argument(\"--decoder_tokenizer_name\", default=\"\", type=str,\n",
    "#                         help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
    "#     parser.add_argument(\"--max_seq_length\", default=512, type=int,\n",
    "#                         help=\"Optional input sequence length before tokenization. The sequence will be dropped if it is longer the max_seq_length\")\n",
    "#     parser.add_argument(\"--finetune_decoder\", default=False, type=bool,\n",
    "#                         help=\"Uses finetuned decoder in output dir if true.\")\n",
    "\n",
    "#     ## Variational auto-encoder(check this)\n",
    "#     parser.add_argument(\"--top_k\", type=int, default=0)\n",
    "#     parser.add_argument(\"--top_p\", type=float, default=1.0)\n",
    "#     parser.add_argument(\"--prompt\", type=str, default=\"\")\n",
    "#     parser.add_argument(\"--padding_text\", type=str, default=\"\")\n",
    "#     parser.add_argument(\"--length\", type=int, default=20)\n",
    "#     parser.add_argument(\"--block_size\", default=-1, type=int,\n",
    "#                         help=\"Optional input sequence length after tokenization.\"\n",
    "#                              \"The training dataset will be truncated in block of this size for training.\"\n",
    "#                              \"Default to the model max input length for single sentence inputs (take into account special tokens).\")\n",
    "#     parser.add_argument(\"--do_lower_case\", action='store_true',\n",
    "#                         help=\"Set this flag if you are using an uncased model.\")\n",
    "#     parser.add_argument(\"--use_philly\", action='store_true',\n",
    "#                         help=\"Use Philly for computing.\")\n",
    "#     parser.add_argument('--gloabl_step_eval', type=int, default=508523,\n",
    "#                         help=\"Evaluate the results at the given global step\")\n",
    "\n",
    "#     # Load a trained Encoder model and vocabulary that you have fine-tuned\n",
    "#     args = parser.parse_args(\"--checkpoint_dir=output_dir_yahoo_768_0 \\\n",
    "#     --output_dir=output_dir_yahoo_768_0 \\\n",
    "#     --generator_dir=output_dir_yahoo_768_0 \\\n",
    "#     --block_size 100 \\\n",
    "#     --max_seq_length 60 \\\n",
    "#     --gloabl_step_eval 24000 \\\n",
    "#     --latent_size 32 \\\n",
    "#     --block_dim 100 \\\n",
    "#     --new_sent 100 \\\n",
    "#     --n_layers 10 \\\n",
    "#     --top_p 0.9 \\\n",
    "#     --output_name=results \\\n",
    "#     --save True\".split())\n",
    "#     global_step = args.gloabl_step_eval\n",
    "\n",
    "#     np.random.seed(args.seed)\n",
    "#     torch.manual_seed(args.seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "#     args.n_gpu = torch.cuda.device_count()\n",
    "#     if args.n_gpu > 0:\n",
    "#         torch.cuda.manual_seed_all(args.seed)       \n",
    "    \n",
    "#     args.encoder_model_type = args.encoder_model_type.lower()\n",
    "#     args.decoder_model_type = args.decoder_model_type.lower()\n",
    "\n",
    "#     output_encoder_dir = os.path.join(args.checkpoint_dir, 'checkpoint-encoder-{}'.format(global_step))\n",
    "#     output_decoder_dir = os.path.join(args.checkpoint_dir, 'checkpoint-decoder-{}'.format(global_step))\n",
    "#     if not args.finetune_decoder:\n",
    "#         output_decoder_dir = os.path.join(args.checkpoint_dir, 'checkpoint-decoder-{}'.format(global_step))\n",
    "#     else:\n",
    "#          output_decoder_dir = os.path.join(args.output_dir, 'checkpoint-decoder-{}'.format(global_step))\n",
    "#     checkpoints = [ [output_encoder_dir, output_decoder_dir] ]\n",
    "\n",
    "#     # Load a trained Encoder model and vocabulary that you have fine-tuned\n",
    "#     encoder_config_class, encoder_model_class, encoder_tokenizer_class = MODEL_CLASSES[args.encoder_model_type]\n",
    "#     model_encoder = encoder_model_class.from_pretrained(output_encoder_dir, latent_size=args.latent_size)\n",
    "#     tokenizer_encoder = encoder_tokenizer_class.from_pretrained(args.encoder_tokenizer_name if args.encoder_tokenizer_name else args.encoder_model_name_or_path, do_lower_case=args.do_lower_case)\n",
    "\n",
    "#     model_encoder.to(args.device)\n",
    "#     if args.block_size <= 0:\n",
    "#         args.block_size = tokenizer_encoder.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
    "#     args.block_size = min(args.block_size, tokenizer_encoder.max_len_single_sentence)\n",
    "\n",
    "#     # Load a trained Decoder model and vocabulary that you have fine-tuned\n",
    "#     if not args.finetune_decoder:\n",
    "#         decoder_config_class, decoder_model_class, decoder_tokenizer_class = MODEL_CLASSES[args.decoder_model_type]\n",
    "#     else:\n",
    "#         decoder_config_class, decoder_model_class, decoder_tokenizer_class = MODEL_CLASSES[\"gpt2v\"]\n",
    "#     model_decoder = decoder_model_class.from_pretrained(output_decoder_dir, latent_size=args.latent_size)\n",
    "#     tokenizer_decoder = decoder_tokenizer_class.from_pretrained(args.decoder_tokenizer_name if args.decoder_tokenizer_name else args.decoder_model_name_or_path, do_lower_case=args.do_lower_case)\n",
    "#     model_decoder.to(args.device)\n",
    "#     if args.block_size <= 0:\n",
    "#         args.block_size = tokenizer_decoder.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
    "#     args.block_size = min(args.block_size, tokenizer_decoder.max_len_single_sentence)\n",
    "\n",
    "#     # Chunyuan: Add Padding token to GPT2\n",
    "#     special_tokens_dict = {'pad_token': '<PAD>', 'bos_token': '<BOS>', 'eos_token': '<EOS>'}\n",
    "#     num_added_toks = tokenizer_decoder.add_special_tokens(special_tokens_dict)\n",
    "#     logger.info('We have added {} tokens to GPT2'.format(num_added_toks))\n",
    "#     model_decoder.resize_token_embeddings(len(tokenizer_decoder))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n",
    "#     assert tokenizer_decoder.pad_token == '<PAD>'\n",
    "    \n",
    "#     generator = Generator(args.n_layers, args.block_dim, args.latent_size)\n",
    "\n",
    "#     if args.cuda:\n",
    "#         generator = generator.cuda()\n",
    "\n",
    "#     generator.load_state_dict(torch.load(args.generator_dir+'/generator_'+str(args.gloabl_step_eval)+'.th'))\n",
    "#     generator.eval()\n",
    "#     model_decoder.eval()\n",
    "#     model_encoder.eval()\n",
    "#     if args.save:\n",
    "#         if not os.path.exists(args.output_dir+\"/{}.txt\".format(args.output_name)):\n",
    "#             with open(args.output_dir+\"/{}.txt\".format(args.output_name), 'w'): \n",
    "#                 pass\n",
    "\n",
    "#     for i in range(int(args.new_sent/args.batch_size)):\n",
    "#         # sample noise\n",
    "#         noise = torch.Tensor(np.random.normal(0, 1, (args.batch_size, args.latent_size))).to(args.device)\n",
    "#         new_z = generator(noise).data\n",
    "\n",
    "#         # create new sent\n",
    "#         sents = rollout_test(model_decoder, new_z, tokenizer_decoder, args.max_seq_length, args.batch_size, args.top_k, args.top_p)\n",
    "\n",
    "#         if args.save:\n",
    "#             with open(args.output_dir+\"/{}.txt\".format(args.output_name), 'a') as file:\n",
    "#                 for i in sents:\n",
    "#                     file.write(i+\"\\n\")\n",
    "#         else:\n",
    "#             for i in sents:\n",
    "#                 logger.info(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab174fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
