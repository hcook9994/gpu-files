{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "673b1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils import to_gpu, train_ngram_lm, get_ppl, create_exp_dir, Dictionary, length_sort\n",
    "from models import Seq2Seq, MLP_D, MLP_D_local, MLP_G\n",
    "from bleu_self import *\n",
    "from bleu_test import *\n",
    "import datetime\n",
    "now_time = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f208b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='TILGAN for unconditional generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d96c0a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import *\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "520324a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=4\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "228c8e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path=\"data/MS_COCO\"\n",
    "#data_path=\"../../yahoo/unlabelled_small\"\n",
    "save=\"./results/yelp_merge_results\"\n",
    "maxlen=30\n",
    "batch_size=24\n",
    "eval_batch_size = 24\n",
    "noise_seq_length = 15\n",
    "add_noise=True #what does this do? - question applies to most parameters\n",
    "emsize=512\n",
    "nhidden=512\n",
    "nlayers=2\n",
    "nheads=4\n",
    "nff=1024\n",
    "aehidden=56\n",
    "noise_r=0.05\n",
    "hidden_init=True\n",
    "dropout=0.3\n",
    "gpu=True\n",
    "z_size=100\n",
    "arch_g='300-300'\n",
    "gan_g_activation=True\n",
    "arch_d='300-300'\n",
    "gan_d_local=False\n",
    "gan_d_local_windowsize=3\n",
    "arch_d_local='300-300'\n",
    "lr_ae=0.12\n",
    "lr_gan_e=1e-04\n",
    "beta1=0.5\n",
    "lr_gan_g=4e-04\n",
    "lr_gan_d=1e-04\n",
    "epochs=200\n",
    "sample=True\n",
    "clip=1\n",
    "log_interval=100\n",
    "gan_lambda=0.1\n",
    "niters_gan_d=1\n",
    "niters_gan_g=1\n",
    "niters_gan_ae=1\n",
    "niters_gan_dec=1\n",
    "niters_gan_schedule=''\n",
    "niters_ae=1\n",
    "gan_type='kl'\n",
    "enhance_dec=True\n",
    "gan_gp_lambda=1\n",
    "vocab_size=0\n",
    "lowercase=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a437b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=\"../../yahoo/yahoo_everything.csv\"\n",
    "\n",
    "label_list = [\"UNK\",1,2,3,4,5]\n",
    "\n",
    "# df_yahoo = pd.read_csv(data)\n",
    "# #df_yahoo=df_yahoo.rename(columns = {\"Unnamed: 0\":'label'})\n",
    "# df_yahoo=df_yahoo.set_index(\"Unnamed: 0\")\n",
    "# df = df_yahoo.sample(frac=1)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c82811f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbers=list(df['label'].unique())\n",
    "# list_zeros = [0]*len(numbers)\n",
    "# count_dictionary = dict(zip(numbers, list_zeros))\n",
    "\n",
    "# values_array_train_labelled=[]\n",
    "# values_array_test_labelled=[]\n",
    "# values_array_test_unlabelled=[]\n",
    "# values_array_train_unlabelled=[]\n",
    "# values_array_unlabelled=[]\n",
    "# data_all=[]\n",
    "# for index, row in df.iterrows():\n",
    "#     if count_dictionary[row['label']]<20:\n",
    "#         count_dictionary[row['label']]=count_dictionary[row['label']]+1\n",
    "#         values_array_train_labelled.append((row['question_1'],row['label']))\n",
    "#     elif count_dictionary[row['label']]<60:\n",
    "#         count_dictionary[row['label']]=count_dictionary[row['label']]+1\n",
    "#         values_array_test_labelled.append((row['question_1'],row['label']))\n",
    "#     elif count_dictionary[row['label']]<600:\n",
    "#         count_dictionary[row['label']]=count_dictionary[row['label']]+1\n",
    "#         values_array_unlabelled.append((row['question_1'],'UNK'))\n",
    "#     elif count_dictionary[row['label']]<1600:\n",
    "#         count_dictionary[row['label']]=count_dictionary[row['label']]+1\n",
    "#         values_array_test_unlabelled.append((row['question_1'],'UNK'))\n",
    "#     elif count_dictionary[row['label']]<7600:\n",
    "#         count_dictionary[row['label']]=count_dictionary[row['label']]+1\n",
    "#         values_array_train_unlabelled.append((row['question_1'],'UNK'))\n",
    "#     data_all.append(row['question_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aaa3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_l =  values_array_train_labelled\n",
    "# test_l = values_array_test_labelled\n",
    "# test_u = values_array_test_unlabelled\n",
    "# train_u = values_array_train_unlabelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe974841",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_l=pd.read_csv(\"../../yelp/assigned/train_l.csv\", index_col=\"Unnamed: 0\")\n",
    "df_test_l=pd.read_csv(\"../../yelp/assigned/test_l.csv\", index_col=\"Unnamed: 0\")\n",
    "df_u=pd.read_csv(\"../../yelp/assigned/u.csv\", index_col=\"Unnamed: 0\")\n",
    "df_train_u=pd.read_csv(\"../../yelp/assigned/train_u.csv\", index_col=\"Unnamed: 0\")#.head(30000)\n",
    "df_test_u=pd.read_csv(\"../../yelp/assigned/test_u.csv\", index_col=\"Unnamed: 0\")#.head(5000)\n",
    "df_all = pd.concat([df_train_l, df_test_l, df_u, df_train_u, df_test_u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b66567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_l =  list(df_train_l.to_records(index=False))\n",
    "test_l = list(df_test_l.to_records(index=False))\n",
    "u_list = list(df_u.to_records(index=False))\n",
    "test_u = list(df_test_u.to_records(index=False))\n",
    "train_u = list(df_train_u.to_records(index=False))\n",
    "data_all = list(df_all[\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3d439b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, data_all, train_l, test_l, train_u, test_u, maxlen, vocab_size=11000, lowercase=False):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.maxlen = maxlen\n",
    "        self.lowercase = lowercase\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.data_all = data_all\n",
    "        self.train_l = train_l\n",
    "        self.test_l = test_l\n",
    "        self.train_u = train_u\n",
    "        self.test_u = test_u\n",
    "\n",
    "        # make the vocabulary from training set\n",
    "        self.make_vocab()\n",
    "        \n",
    "        self.train_l_tok = self.tokenize(self.train_l)\n",
    "        self.test_l_tok = self.tokenize(self.test_l)\n",
    "        self.train_u_tok = self.tokenize(self.train_u)\n",
    "        self.test_u_tok = self.tokenize(self.test_u)\n",
    "\n",
    "    def make_vocab(self):\n",
    "        # Add words to the dictionary\n",
    "        print(len(self.data_all))\n",
    "        print(self.data_all[0])\n",
    "        for line in self.data_all:\n",
    "            if self.lowercase:\n",
    "                # -1 to get rid of \\n character\n",
    "                words = line[:-1].lower().split(\" \")\n",
    "            else:\n",
    "                words = line[:-1].split(\" \")\n",
    "            for word in words:\n",
    "                self.dictionary.add_word(word)\n",
    "\n",
    "        # prune the vocabulary\n",
    "        self.dictionary.prune_vocab(k=self.vocab_size, cnt=True)\n",
    "\n",
    "    def tokenize(self, data):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        dropped = 0\n",
    "        #with open(path, 'r') as f:\n",
    "        linecount = 0\n",
    "        lines = []\n",
    "        for line, label in data:\n",
    "            linecount += 1\n",
    "            if self.lowercase:\n",
    "                words = line[:-1].lower().strip().split(\" \")\n",
    "            else:\n",
    "                words = line[:-1].strip().split(\" \")\n",
    "            if len(words) > self.maxlen:\n",
    "                dropped += 1\n",
    "                continue\n",
    "            words = ['<sos>'] + words\n",
    "            words += ['<eos>']\n",
    "            # vectorize\n",
    "            vocab = self.dictionary.word2idx\n",
    "            unk_idx = vocab['<oov>']\n",
    "            indices = [vocab[w] if w in vocab else unk_idx for w in words]\n",
    "            lines.append(indices)\n",
    "\n",
    "        print(\"Number of sentences dropped: {} out of {} total\".\n",
    "              format(dropped, linecount))\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24e5f2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75300\n",
      "Huge gender imbalance and rude staff.\n",
      "original vocab 85023; pruned to 85027\n",
      "Number of sentences dropped: 0 out of 100 total\n",
      "Number of sentences dropped: 0 out of 200 total\n",
      "Number of sentences dropped: 8 out of 60000 total\n",
      "Number of sentences dropped: 1 out of 10000 total\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(data_all = data_all,\n",
    "                train_l=train_l,\n",
    "                test_l=test_l,\n",
    "                train_u=train_u,\n",
    "                test_u=test_u,\n",
    "                maxlen=maxlen,\n",
    "                vocab_size=vocab_size,\n",
    "                lowercase=lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daa4ee98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 85027\n"
     ]
    }
   ],
   "source": [
    "# save arguments\n",
    "ntokens = len(corpus.dictionary.word2idx)\n",
    "print(\"Vocabulary Size: {}\".format(ntokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f66f2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : ./results/yelp_merge_results\n"
     ]
    }
   ],
   "source": [
    "# exp dir\n",
    "create_exp_dir(os.path.join(save), ['train.py', 'models.py', 'utils.py'],\n",
    "        dict=corpus.dictionary.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a05112cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging(str, to_stdout=True):\n",
    "    with open(os.path.join(save, 'log.txt'), 'a') as f:\n",
    "        f.write(str + '\\n')\n",
    "    if to_stdout:\n",
    "        print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86dcf57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz, max_len, shuffle=False, gpu=False):\n",
    "    if shuffle:\n",
    "        random.shuffle(data)\n",
    "    nbatch = len(data) // bsz\n",
    "    batches = []\n",
    "\n",
    "    for i in range(nbatch):\n",
    "        maxlen = max_len+1\n",
    "        # Pad batches to maximum sequence length in batch\n",
    "        batch = data[i*bsz:(i+1)*bsz]\n",
    "        # subtract 1 from lengths b/c includes BOTH starts & end symbols\n",
    "        lengths = [len(x)-1 for x in batch]\n",
    "\n",
    "        # sort items by length (decreasing)\n",
    "        batch, lengths = length_sort(batch, lengths)\n",
    "\n",
    "        # source has no end symbol\n",
    "        source = [x[:-1] for x in batch]\n",
    "        # target has no start symbol\n",
    "        target = [x[1:] for x in batch]\n",
    "\n",
    "\n",
    "        for x, y in zip(source, target):\n",
    "            zeros = (maxlen-len(x))*[0]\n",
    "            x += zeros\n",
    "            y += zeros\n",
    "        source = torch.LongTensor(np.array(source))\n",
    "        target = torch.LongTensor(np.array(target)).view(-1)\n",
    "\n",
    "        if gpu:\n",
    "            source = source.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "        batches.append((source, target, lengths))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4042220b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data!\n"
     ]
    }
   ],
   "source": [
    "test_data = batchify(corpus.test_u_tok, eval_batch_size, maxlen, shuffle=False)\n",
    "train_data = batchify(corpus.train_u_tok, batch_size, maxlen,  shuffle=True)\n",
    "\n",
    "print(\"Loaded data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2bb5ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416\n",
      "2499\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51cc7067",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Build the models\n",
    "###############################################################################\n",
    "autoencoder = Seq2Seq(add_noise=add_noise,\n",
    "                      emsize=emsize,\n",
    "                      nhidden=nhidden,\n",
    "                      ntokens=ntokens,\n",
    "                      nlayers=nlayers,\n",
    "                      nheads=nheads,\n",
    "                      nff=nff,\n",
    "                      aehidden=aehidden,\n",
    "                      noise_r=noise_r,\n",
    "                      hidden_init=hidden_init,\n",
    "                      dropout=dropout,\n",
    "                      gpu=True)\n",
    "nlatent = aehidden * (maxlen+1)\n",
    "gan_gen = MLP_G(ninput=z_size, noutput=nlatent, layers=arch_g, gan_g_activation=gan_g_activation)\n",
    "gan_disc = MLP_D(ninput=nlatent, noutput=1, layers=arch_d)\n",
    "gan_disc_local = MLP_D_local(ninput=gan_d_local_windowsize * aehidden, noutput=1, layers=arch_d_local)\n",
    "\n",
    "optimizer_ae = optim.SGD(autoencoder.parameters(), lr=lr_ae)\n",
    "\n",
    "\n",
    "optimizer_gan_e = optim.Adam(autoencoder.encoder.parameters(),\n",
    "                             lr=lr_gan_e,\n",
    "                             betas=(beta1, 0.999))\n",
    "optimizer_gan_g = optim.Adam(gan_gen.parameters(),\n",
    "                             lr=lr_gan_g,\n",
    "                             betas=(beta1, 0.999))\n",
    "optimizer_gan_d = optim.Adam(gan_disc.parameters(),\n",
    "                             lr=lr_gan_d,\n",
    "                             betas=(beta1, 0.999))\n",
    "optimizer_gan_d_local = optim.Adam(gan_disc_local.parameters(),\n",
    "                             lr=lr_gan_d,\n",
    "                             betas=(beta1, 0.999))\n",
    "optimizer_gan_dec = optim.Adam(autoencoder.decoder.parameters(),\n",
    "                             lr=lr_gan_e,\n",
    "                             betas=(beta1, 0.999))\n",
    "\n",
    "autoencoder = autoencoder.to(device)\n",
    "gan_gen = gan_gen.to(device)\n",
    "gan_disc = gan_disc.to(device)\n",
    "gan_disc_local = gan_disc_local.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42a2dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    print(\"Saving models to {}\".format(save))\n",
    "    torch.save({\n",
    "        \"ae\": autoencoder.state_dict(),\n",
    "        \"gan_g\": gan_gen.state_dict(),\n",
    "        \"gan_d\": gan_disc.state_dict(),\n",
    "        \"gan_d_local\": gan_disc_local.state_dict()\n",
    "\n",
    "        },\n",
    "        os.path.join(save, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b528fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        param_norm = p.grad.data.norm(2)\n",
    "        total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** (1. / 2)\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7a9fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    model_args = json.load(open(os.path.join(save, 'options.json'), 'r'))\n",
    "    word2idx = json.load(open(os.path.join(save, 'vocab.json'), 'r'))\n",
    "    idx2word = {v: k for k, v in word2idx.items()}\n",
    "\n",
    "    print('Loading models from {}'.format(save))\n",
    "    loaded = torch.load(os.path.join(save, \"model.pt\"))\n",
    "    autoencoder.load_state_dict(loaded.get('ae'))\n",
    "    gan_gen.load_state_dict(loaded.get('gan_g'))\n",
    "    gan_disc.load_state_dict(loaded.get('gan_d'))\n",
    "    gan_disc_local.load_state_dict(loaded.get('gan_d_local'))\n",
    "    return model_args, idx2word, autoencoder, gan_gen, gan_disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03edc3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_autoencoder(data_source, epoch):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    autoencoder.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary.word2idx)\n",
    "    all_accuracies = 0\n",
    "    bcnt = 0\n",
    "    for i, batch in enumerate(data_source):\n",
    "        source, target, lengths = batch\n",
    "        with torch.no_grad():\n",
    "            source = Variable(source.to(device))\n",
    "            target = Variable(target.to(device))\n",
    "            mask = target.gt(0)\n",
    "            masked_target = target.masked_select(mask)\n",
    "            # examples x ntokens\n",
    "            output_mask = mask.unsqueeze(1).expand(mask.size(0), ntokens)\n",
    "\n",
    "            # output: batch x seq_len x ntokens\n",
    "            output = autoencoder(source, lengths, source, add_noise=add_noise, soft=False)\n",
    "            flattened_output = output.view(-1, ntokens)\n",
    "\n",
    "            masked_output = \\\n",
    "                flattened_output.masked_select(output_mask).view(-1, ntokens)\n",
    "            total_loss += F.cross_entropy(masked_output, masked_target)\n",
    "\n",
    "            # accuracy\n",
    "            max_vals, max_indices = torch.max(masked_output, 1)\n",
    "            accuracy = torch.mean(max_indices.eq(masked_target).float()).data.item()\n",
    "            all_accuracies += accuracy\n",
    "            bcnt += 1\n",
    "\n",
    "        aeoutf = os.path.join(save, \"autoencoder.txt\")\n",
    "        with open(aeoutf, \"w\") as f:\n",
    "            max_values, max_indices = torch.max(output, 2)\n",
    "            max_indices = \\\n",
    "                max_indices.view(output.size(0), -1).data.cpu().numpy()\n",
    "            target = target.view(output.size(0), -1).data.cpu().numpy()\n",
    "            for t, idx in zip(target, max_indices):\n",
    "                # real sentence\n",
    "                chars = \" \".join([corpus.dictionary.idx2word[x] for x in t])\n",
    "                f.write(chars + '\\n')\n",
    "                # autoencoder output sentence\n",
    "                chars = \" \".join([corpus.dictionary.idx2word[x] for x in idx])\n",
    "                f.write(chars + '\\n'*2)\n",
    "\n",
    "    return total_loss.item() / len(data_source), all_accuracies/bcnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b838544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fixed_noise(noise, to_save):\n",
    "    gan_gen.eval()\n",
    "    autoencoder.eval()\n",
    "\n",
    "    fake_hidden = gan_gen(noise)\n",
    "    max_indices = autoencoder.generate(fake_hidden, maxlen, sample=sample)\n",
    "\n",
    "    with open(to_save, \"w\") as f:\n",
    "        max_indices = max_indices.data.cpu().numpy()\n",
    "        for idx in max_indices:\n",
    "            # generated sentence\n",
    "            words = [corpus.dictionary.idx2word[x] for x in idx]\n",
    "            # truncate sentences to first occurrence of <eos>\n",
    "            truncated_sent = []\n",
    "            for w in words:\n",
    "                if w != '<eos>':\n",
    "                    truncated_sent.append(w)\n",
    "                else:\n",
    "                    break\n",
    "            chars = \" \".join(truncated_sent)\n",
    "            f.write(chars + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "184db856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fixed_noise_new(noise):\n",
    "    gan_gen.eval()\n",
    "    autoencoder.eval()\n",
    "\n",
    "    fake_hidden = gan_gen(noise)\n",
    "    max_indices = autoencoder.generate(fake_hidden, maxlen, sample=sample)\n",
    "    \n",
    "    sent_list = []\n",
    "    \n",
    "    #with open(to_save, \"w\") as f:\n",
    "    max_indices = max_indices.data.cpu().numpy()\n",
    "    for idx in max_indices:\n",
    "        # generated sentence\n",
    "        words = [corpus.dictionary.idx2word[x] for x in idx]\n",
    "        # truncate sentences to first occurrence of <eos>\n",
    "        truncated_sent = []\n",
    "        for w in words:\n",
    "            if w != '<eos>':\n",
    "                truncated_sent.append(w)\n",
    "            else:\n",
    "                break\n",
    "        chars = \" \".join(truncated_sent)\n",
    "        #f.write(chars + '\\n')\n",
    "        sent_list.append(chars)\n",
    "    #print(sent_list)\n",
    "    return sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58dced27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(gen_text_savepath):\n",
    "    selfbleu = bleu_self(gen_text_savepath)\n",
    "    real_text = os.path.join(data_path, \"test.txt\")\n",
    "    testbleu = bleu_test(real_text, gen_text_savepath)\n",
    "    return selfbleu, testbleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "124dcc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae(epoch, batch, total_loss_ae, start_time, i):\n",
    "    '''Train AE with the negative log-likelihood loss'''\n",
    "    autoencoder.train()\n",
    "    optimizer_ae.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = Variable(source.to(device))\n",
    "    target = Variable(target.to(device))\n",
    "    output = autoencoder(source, lengths, source, add_noise=add_noise, soft=False)\n",
    "\n",
    "    mask = target.gt(0)\n",
    "    masked_target = target.masked_select(mask)\n",
    "    output_mask = mask.unsqueeze(1).expand(mask.size(0), ntokens)\n",
    "    flat_output = output.view(-1, ntokens)\n",
    "    masked_output = flat_output.masked_select(output_mask).view(-1, ntokens)\n",
    "    loss = F.cross_entropy(masked_output, masked_target)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(autoencoder.parameters(), clip)\n",
    "    train_ae_norm = cal_norm(autoencoder)\n",
    "    optimizer_ae.step()\n",
    "\n",
    "    total_loss_ae += loss.data.item()\n",
    "    if i % log_interval == 0:\n",
    "        probs = F.softmax(masked_output, dim=-1)\n",
    "        max_vals, max_indices = torch.max(probs, 1)\n",
    "        accuracy = torch.mean(max_indices.eq(masked_target).float()).data.item()\n",
    "        cur_loss = total_loss_ae / log_interval\n",
    "        elapsed = time.time() - start_time\n",
    "        logging('| epoch {:3d} | {:5d}/{:5d} batches | lr {:08.6f} | ms/batch {:5.2f} | '\n",
    "                'loss {:5.2f} | ppl {:8.2f} | acc {:8.2f} | train_ae_norm {:8.2f}'.format(\n",
    "                epoch, i, len(train_data), 0,\n",
    "                elapsed * 1000 / log_interval,\n",
    "                cur_loss, math.exp(cur_loss), accuracy, train_ae_norm))\n",
    "\n",
    "        total_loss_ae = 0\n",
    "        start_time = time.time()\n",
    "    return total_loss_ae, start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64870e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_g(gan_type='kl'):\n",
    "    gan_gen.train()\n",
    "    optimizer_gan_g.zero_grad()\n",
    "\n",
    "    z = Variable(torch.Tensor(batch_size, z_size).normal_(0, 1).to(device))\n",
    "    fake_hidden = gan_gen(z)\n",
    "    fake_score = gan_disc(fake_hidden)\n",
    "\n",
    "    if gan_d_local:\n",
    "        idx = random.randint(0, maxlen - gan_d_local_windowsize)\n",
    "        fake_hidden_local = fake_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        fake_score_local = gan_disc_local(fake_hidden_local)\n",
    "\n",
    "        if gan_type == 'kl':\n",
    "            errG = -(torch.exp(fake_score.detach()).clamp(0.5, 2) * fake_score).mean() -(torch.exp(fake_score_local.detach()).clamp(0.5, 2) * fake_score_local).mean()\n",
    "        else:\n",
    "            errG = -fake_score.mean() -fake_score_local.mean()\n",
    "    else:\n",
    "        if gan_type == 'kl':\n",
    "            errG = -(torch.exp(fake_score.detach()).clamp(0.5, 2) * fake_score).mean()\n",
    "        else:\n",
    "            errG = -fake_score.mean()\n",
    "\n",
    "\n",
    "    errG *= gan_lambda\n",
    "    errG.backward()\n",
    "    optimizer_gan_g.step()\n",
    "\n",
    "    return errG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33244527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_dec(gan_type='kl'):\n",
    "    autoencoder.decoder.train()\n",
    "    optimizer_gan_dec.zero_grad()\n",
    "\n",
    "    z = Variable(torch.Tensor(batch_size, z_size).normal_(0, 1).to(device))\n",
    "    fake_hidden = gan_gen(z)\n",
    "\n",
    "    # 1. decoder  - soft distribution\n",
    "    enhance_source, max_indices= autoencoder.generate_enh_dec(fake_hidden, maxlen, sample=sample)\n",
    "    # 2. soft distribution - > encoder  -> fake_hidden\n",
    "    enhance_hidden = autoencoder(enhance_source, None, max_indices, add_noise=add_noise, soft=True, encode_only=True)\n",
    "    fake_score = gan_disc(enhance_hidden)\n",
    "\n",
    "    if gan_d_local:\n",
    "        idx = random.randint(0, maxlen - gan_d_local_windowsize)\n",
    "        fake_hidden_local = fake_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        fake_score_local = gan_disc_local(fake_hidden_local)\n",
    "\n",
    "        if gan_type == 'kl':\n",
    "            errG = -(torch.exp(fake_score.detach()).clamp(0.5, 2) * fake_score).mean() -(torch.exp(fake_score_local.detach()).clamp(0.5, 2) * fake_score_local).mean()\n",
    "        else:\n",
    "            errG = -fake_score.mean() -fake_score_local.mean()\n",
    "    else:\n",
    "        if gan_type == 'kl':\n",
    "            errG = -(torch.exp(fake_score.detach()).clamp(0.5, 2) * fake_score).mean()\n",
    "        else:\n",
    "            errG = -fake_score.mean()\n",
    "\n",
    "\n",
    "    errG *= gan_lambda\n",
    "    errG.backward()\n",
    "    optimizer_gan_dec.step()\n",
    "\n",
    "    return errG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce1e7869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_hook(grad):\n",
    "    return grad * gan_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5cf668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Steal from https://github.com/caogang/wgan-gp/blob/master/gan_cifar10.py '''\n",
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    bsz = real_data.size(0)\n",
    "    alpha = torch.rand(bsz, 1)\n",
    "    alpha = alpha.expand(bsz, real_data.size(1))  # only works for 2D XXX\n",
    "    alpha = alpha.to(device)\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                                    grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
    "                                    create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * gan_gp_lambda\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e65e6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_d(batch, gan_type='kl'):\n",
    "    gan_disc.train()\n",
    "    gan_disc_local.train()\n",
    "    optimizer_gan_d.zero_grad()\n",
    "    optimizer_gan_d_local.zero_grad()\n",
    "\n",
    "    # + samples\n",
    "    source, target, lengths = batch\n",
    "    source = Variable(source.to(device))\n",
    "    target = Variable(target.to(device))\n",
    "    real_hidden = autoencoder(source, lengths, source, add_noise=add_noise, soft=False, encode_only=True)\n",
    "    real_score = gan_disc(real_hidden.detach())\n",
    "\n",
    "    idx = random.randint(0, maxlen - gan_d_local_windowsize)\n",
    "    if gan_d_local:\n",
    "        real_hidden_local = real_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        real_score_local = gan_disc_local(real_hidden_local)\n",
    "        real_score += real_score_local\n",
    "\n",
    "\n",
    "    if gan_type == 'wgan':\n",
    "        errD_real = -real_score.mean()\n",
    "    else: # kl or all\n",
    "        errD_real = F.softplus(-real_score).mean()\n",
    "    errD_real.backward()\n",
    "\n",
    "    # - samples\n",
    "    z = Variable(torch.Tensor(batch_size, z_size).normal_(0, 1).to(device))\n",
    "    fake_hidden = gan_gen(z)\n",
    "    fake_score = gan_disc(fake_hidden.detach())\n",
    "\n",
    "    if gan_d_local:\n",
    "        fake_hidden_local = fake_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        fake_score_local = gan_disc_local(fake_hidden_local)\n",
    "        fake_score += fake_score_local\n",
    "\n",
    "    if gan_type == 'wgan':\n",
    "        errD_fake = fake_score.mean()\n",
    "    else:  # kl or all\n",
    "        errD_fake = F.softplus(fake_score).mean()\n",
    "    errD_fake.backward()\n",
    "\n",
    "    # gradient penalty\n",
    "    if gan_type == 'wgan':\n",
    "        gradient_penalty = calc_gradient_penalty(gan_disc, real_hidden.data, fake_hidden.data)\n",
    "        gradient_penalty.backward()\n",
    "\n",
    "    optimizer_gan_d.step()\n",
    "    optimizer_gan_d_local.step()\n",
    "    return errD_real + errD_fake, errD_real, errD_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f12ead25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_d_into_ae(batch):\n",
    "    autoencoder.train()\n",
    "    optimizer_gan_e.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = Variable(source.to(device))\n",
    "    target = Variable(target.to(device))\n",
    "    real_hidden = autoencoder(source, lengths, source, add_noise=add_noise, soft=False, encode_only=True)\n",
    "\n",
    "    if gan_d_local:\n",
    "        idx = random.randint(0, maxlen - gan_d_local_windowsize)\n",
    "        real_hidden_local = real_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        real_score_local = gan_disc_local(real_hidden_local)\n",
    "        errD_real = gan_disc(real_hidden).mean() + real_score_local.mean()\n",
    "    else:\n",
    "        errD_real = gan_disc(real_hidden).mean()\n",
    "\n",
    "    errD_real *= gan_lambda\n",
    "    errD_real.backward()\n",
    "    torch.nn.utils.clip_grad_norm(autoencoder.parameters(), clip)\n",
    "\n",
    "    optimizer_gan_e.step()\n",
    "    return errD_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd8488e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "#  Transformer parameters\n",
    "#--------------------------------\n",
    "max_seq_length = 64\n",
    "batch_size_d = 48\n",
    "\n",
    "#--------------------------------\n",
    "#  GAN-BERT specific parameters\n",
    "#--------------------------------\n",
    "# number of hidden layers in the generator, \n",
    "# each of the size of the output space\n",
    "#num_hidden_layers_g = 1; \n",
    "# number of hidden layers in the discriminator, \n",
    "# each of the size of the input space\n",
    "num_hidden_layers_d = 1; \n",
    "# size of the generator's input noisy vectors\n",
    "noise_size = 100\n",
    "# dropout to be applied to discriminator's input vectors\n",
    "out_dropout_rate = 0.2\n",
    "\n",
    "# Replicate labeled data to balance poorly represented datasets, \n",
    "# e.g., less than 1% of labeled material\n",
    "apply_balance = True\n",
    "\n",
    "#--------------------------------\n",
    "#  Optimization parameters\n",
    "#--------------------------------\n",
    "learning_rate_discriminator = 5e-6 #5e-5?\n",
    "#learning_rate_generator = 5e-5\n",
    "epsilon = 1e-8\n",
    "num_train_epochs = 50\n",
    "multi_gpu = True\n",
    "# Scheduler\n",
    "apply_scheduler = False\n",
    "warmup_proportion = 0.1\n",
    "# Print\n",
    "print_each_n_step = 10\n",
    "\n",
    "#--------------------------------\n",
    "#  Adopted Tranformer model\n",
    "#--------------------------------\n",
    "# Since this version is compatible with Huggingface transformers, you can uncomment\n",
    "# (or add) transformer models compatible with GAN\n",
    "\n",
    "#model_name = \"bert-base-cased\"\n",
    "#model_name = \"bert-base-uncased\"\n",
    "#model_name = \"roberta-base\"\n",
    "#model_name = \"albert-base-v2\"\n",
    "#model_name = \"xlm-roberta-base\"\n",
    "#model_name = \"amazon/bort\"\n",
    "#model_name=\"google/electra-large-discriminator\"\n",
    "#model_name=\"google/electra-small-discriminator\"\n",
    "#model_name=\"microsoft/deberta-v2-xxlarge\"\n",
    "#model_name=\"microsoft/deberta-v3-base\"\n",
    "model_name = \"google/electra-base-discriminator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90e59fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/electra-base-discriminator/resolve/main/config.json from cache at /home/harry/.cache/huggingface/transformers/7d1569a4df2372d67341bda716bce4e3edf3e3ffadb97251bc4b6b35d459f624.57c13443a51769ce892714c93bb3ee3952bad66d7d9662d9de382b808377c3f8\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/electra-base-discriminator/resolve/main/pytorch_model.bin from cache at /home/harry/.cache/huggingface/transformers/aed576b8aec823c870feda40d60bd803ac8e40056ecb7d7f43dd0b2bfd82e373.db390a2059e53ead2bb00e1a2f8cd50b0a47e1969d180cd70339ec3f6f29dce1\n",
      "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at google/electra-base-discriminator.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/google/electra-base-discriminator/resolve/main/config.json from cache at /home/harry/.cache/huggingface/transformers/7d1569a4df2372d67341bda716bce4e3edf3e3ffadb97251bc4b6b35d459f624.57c13443a51769ce892714c93bb3ee3952bad66d7d9662d9de382b808377c3f8\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/google/electra-base-discriminator/resolve/main/vocab.txt from cache at /home/harry/.cache/huggingface/transformers/fe616facc71d8e3afc69de3edac76bf1e4a0a741e80d9a99a2cc6a9a8f5f74b5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/google/electra-base-discriminator/resolve/main/tokenizer.json from cache at /home/harry/.cache/huggingface/transformers/81840ac426bf0d690bfb69a4ec7d706e8853d8ab309e7decb6b72ab939d6682e.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/google/electra-base-discriminator/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/google/electra-base-discriminator/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/google/electra-base-discriminator/resolve/main/tokenizer_config.json from cache at /home/harry/.cache/huggingface/transformers/6f8b3f5095b6f44f5c75cee3c56b971b3208b08132ba2f9fb775a4a7b7140942.4f2213f5603276adf12967b32e4444c0f187f34ca4f8b22a65f03e13514589e9\n",
      "loading configuration file https://huggingface.co/google/electra-base-discriminator/resolve/main/config.json from cache at /home/harry/.cache/huggingface/transformers/7d1569a4df2372d67341bda716bce4e3edf3e3ffadb97251bc4b6b35d459f624.57c13443a51769ce892714c93bb3ee3952bad66d7d9662d9de382b808377c3f8\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ea605ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_loader(input_examples, label_masks, label_map, do_shuffle = False, balance_label_examples = False):\n",
    "  '''\n",
    "  Generate a Dataloader given the input examples, eventually masked if they are \n",
    "  to be considered NOT labeled.\n",
    "  '''\n",
    "  examples = []\n",
    "\n",
    "  # Count the percentage of labeled examples  \n",
    "  num_labeled_examples = 0\n",
    "  for label_mask in label_masks:\n",
    "    if label_mask: \n",
    "      num_labeled_examples += 1\n",
    "  label_mask_rate = num_labeled_examples/len(input_examples)\n",
    "\n",
    "  # if required it applies the balance\n",
    "  for index, ex in enumerate(input_examples): \n",
    "    if label_mask_rate == 1 or not balance_label_examples:\n",
    "      examples.append((ex, label_masks[index]))\n",
    "    else:\n",
    "      # IT SIMULATE A LABELED EXAMPLE\n",
    "      if label_masks[index]:\n",
    "        balance = int(1/label_mask_rate)\n",
    "        balance = int(math.log(balance,2))\n",
    "        if balance < 1:\n",
    "          balance = 1\n",
    "        for b in range(0, int(balance)):\n",
    "          examples.append((ex, label_masks[index]))\n",
    "      else:\n",
    "        examples.append((ex, label_masks[index]))\n",
    "  \n",
    "  #-----------------------------------------------\n",
    "  # Generate input examples to the Transformer\n",
    "  #-----------------------------------------------\n",
    "  input_ids = []\n",
    "  input_mask_array = []\n",
    "  label_mask_array = []\n",
    "  label_id_array = []\n",
    "\n",
    "  # Tokenization \n",
    "  for (text, label_mask) in examples:\n",
    "    encoded_sent = tokenizer.encode(text[0], add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
    "    input_ids.append(encoded_sent)\n",
    "    label_id_array.append(label_map[text[1]])\n",
    "    label_mask_array.append(label_mask)\n",
    "  \n",
    "  # Attention to token (to ignore padded input wordpieces)\n",
    "  for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]                          \n",
    "    input_mask_array.append(att_mask)\n",
    "  # Convertion to Tensor\n",
    "  input_ids = torch.tensor(input_ids) \n",
    "  input_mask_array = torch.tensor(input_mask_array)\n",
    "  label_id_array = torch.tensor(label_id_array, dtype=torch.long)\n",
    "  label_mask_array = torch.tensor(label_mask_array)\n",
    "\n",
    "  # Building the TensorDataset\n",
    "  dataset = TensorDataset(input_ids, input_mask_array, label_id_array, label_mask_array)\n",
    "\n",
    "  if do_shuffle:\n",
    "    sampler = RandomSampler\n",
    "  else:\n",
    "    sampler = SequentialSampler\n",
    "\n",
    "  # Building the DataLoader\n",
    "  return DataLoader(\n",
    "              dataset,  # The training samples.\n",
    "              sampler = sampler(dataset), \n",
    "              batch_size = batch_size) # Trains with this batch size.\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d50e8d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_fake(input_examples):\n",
    "  '''\n",
    "  Generate a Dataloader given the input examples, eventually masked if they are \n",
    "  to be considered NOT labeled.\n",
    "  '''\n",
    "  \n",
    "  #-----------------------------------------------\n",
    "  # Generate input examples to the Transformer\n",
    "  #-----------------------------------------------\n",
    "  input_ids = []\n",
    "  input_mask_array = []\n",
    "\n",
    "  # Tokenization \n",
    "  for text in input_examples:\n",
    "    encoded_sent = tokenizer.encode(text, add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
    "    input_ids.append(encoded_sent)\n",
    "  \n",
    "  # Attention to token (to ignore padded input wordpieces)\n",
    "  for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]                          \n",
    "    input_mask_array.append(att_mask)\n",
    "  # Convertion to Tensor\n",
    "  input_ids = torch.tensor(input_ids) \n",
    "  input_mask_array = torch.tensor(input_mask_array)\n",
    "\n",
    "  # Building the DataLoader\n",
    "  return input_ids, input_mask_array # Trains with this batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3fa1e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the examples\n",
    "labeled_examples = train_l\n",
    "unlabeled_examples = u_list\n",
    "test_examples = test_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc1767ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {}\n",
    "for (i, label) in enumerate(label_list):\n",
    "  label_map[label] = i\n",
    "#------------------------------\n",
    "#   Load the train dataset\n",
    "#------------------------------\n",
    "train_examples = labeled_examples\n",
    "#The labeled (train) dataset is assigned with a mask set to True\n",
    "train_label_masks = np.ones(len(labeled_examples), dtype=bool)\n",
    "#If unlabel examples are available\n",
    "if unlabeled_examples:\n",
    "  train_examples = train_examples + unlabeled_examples\n",
    "  #The unlabeled (train) dataset is assigned with a mask set to False\n",
    "  tmp_masks = np.zeros(len(unlabeled_examples), dtype=bool)\n",
    "  train_label_masks = np.concatenate([train_label_masks,tmp_masks])\n",
    "\n",
    "train_dataloader = generate_data_loader(train_examples, train_label_masks, label_map, do_shuffle = True, balance_label_examples = apply_balance)\n",
    "\n",
    "#------------------------------\n",
    "#   Load the test dataset\n",
    "#------------------------------\n",
    "#The labeled (test) dataset is assigned with a mask set to True\n",
    "test_label_masks = np.ones(len(test_examples), dtype=bool)\n",
    "\n",
    "test_dataloader = generate_data_loader(test_examples, test_label_masks, label_map, do_shuffle = False, balance_label_examples = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "771c13ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------\n",
    "#   The Discriminator\n",
    "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
    "#   https://github.com/crux82/ganbert\n",
    "#------------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
    "        layers = []\n",
    "        hidden_sizes = [input_size] + hidden_sizes\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
    "\n",
    "        self.layers = nn.Sequential(*layers) #per il flatten\n",
    "        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1) # +1 for the probability of this sample being fake/real.\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_rep):\n",
    "        input_rep = self.input_dropout(input_rep)\n",
    "        last_rep = self.layers(input_rep)\n",
    "        logits = self.logit(last_rep)\n",
    "        probs = self.softmax(logits)\n",
    "        return last_rep, logits, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e797014b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/electra-base-discriminator/resolve/main/config.json from cache at /home/harry/.cache/huggingface/transformers/7d1569a4df2372d67341bda716bce4e3edf3e3ffadb97251bc4b6b35d459f624.57c13443a51769ce892714c93bb3ee3952bad66d7d9662d9de382b808377c3f8\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The config file is required to get the dimension of the vector produced by \n",
    "# the underlying transformer\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "hidden_size = int(config.hidden_size)\n",
    "# Define the number and width of hidden layers\n",
    "#hidden_levels_g = [hidden_size for i in range(0, num_hidden_layers_g)]\n",
    "hidden_levels_d = [hidden_size for i in range(0, num_hidden_layers_d)]\n",
    "\n",
    "#-------------------------------------------------\n",
    "#   Instantiate the Generator and Discriminator\n",
    "#-------------------------------------------------\n",
    "#generator = Generator(noise_size=noise_size, output_size=hidden_size, hidden_sizes=hidden_levels_g, dropout_rate=out_dropout_rate)\n",
    "discriminator = Discriminator(input_size=hidden_size, hidden_sizes=hidden_levels_d, num_labels=len(label_list), dropout_rate=out_dropout_rate)\n",
    "\n",
    "# Put everything in the GPU if available\n",
    "if torch.cuda.is_available():    \n",
    "  #generator.cuda()\n",
    "  discriminator.cuda()\n",
    "  transformer.cuda()\n",
    "  if multi_gpu:\n",
    "    transformer = torch.nn.DataParallel(transformer)\n",
    "\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f542a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {}\n",
    "for (i, label) in enumerate(label_list):\n",
    "  label_map[label] = i\n",
    "#------------------------------\n",
    "#   Load the train dataset\n",
    "#------------------------------\n",
    "train_examples = labeled_examples\n",
    "#The labeled (train) dataset is assigned with a mask set to True\n",
    "train_label_masks = np.ones(len(labeled_examples), dtype=bool)\n",
    "#If unlabel examples are available\n",
    "if unlabeled_examples:\n",
    "  train_examples = train_examples + unlabeled_examples\n",
    "  #The unlabeled (train) dataset is assigned with a mask set to False\n",
    "  tmp_masks = np.zeros(len(unlabeled_examples), dtype=bool)\n",
    "  train_label_masks = np.concatenate([train_label_masks,tmp_masks])\n",
    "\n",
    "train_dataloader = generate_data_loader(train_examples, train_label_masks, label_map, do_shuffle = True, balance_label_examples = apply_balance)\n",
    "\n",
    "#------------------------------\n",
    "#   Load the test dataset\n",
    "#------------------------------\n",
    "#The labeled (test) dataset is assigned with a mask set to True\n",
    "test_label_masks = np.ones(len(test_examples), dtype=bool)\n",
    "\n",
    "test_dataloader = generate_data_loader(test_examples, test_label_masks, label_map, do_shuffle = False, balance_label_examples = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad127360",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stats = []\n",
    "\n",
    "accuracy_array=[]\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "#models parameters\n",
    "transformer_vars = [i for i in transformer.parameters()]\n",
    "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
    "#g_vars = [v for v in generator.parameters()]\n",
    "\n",
    "#optimizer\n",
    "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
    "#gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator) \n",
    "\n",
    "#scheduler\n",
    "if apply_scheduler:\n",
    "  num_train_examples = len(train_examples)\n",
    "  num_train_steps = int(num_train_examples / batch_size * num_train_epochs)\n",
    "  num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
    "\n",
    "  scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n",
    "                                           num_warmup_steps = num_warmup_steps)\n",
    "  scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n",
    "                                           num_warmup_steps = num_warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44e7895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    logging(\"Training\")\n",
    "    train_data = batchify(corpus.train_u_tok, batch_size, maxlen, shuffle=True)\n",
    "\n",
    "    # gan: preparation\n",
    "    if niters_gan_schedule != \"\":\n",
    "        gan_schedule = [int(x) for x in niters_gan_schedule.split(\"-\")]\n",
    "    else:\n",
    "        gan_schedule = []\n",
    "    niter_gan = 1\n",
    "    fixed_noise = Variable(torch.ones(eval_batch_size, z_size).normal_(0, 1).to(device))\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # update gan training schedule\n",
    "        if epoch in gan_schedule:\n",
    "            niter_gan += 1\n",
    "            logging(\"GAN training loop schedule: {}\".format(niter_gan))\n",
    "\n",
    "        total_loss_ae = 0\n",
    "        epoch_start_time = time.time()\n",
    "        start_time = time.time()\n",
    "        niter = 0\n",
    "        niter_g = 1\n",
    "        print(\"Train classification discriminator\")\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        # Perform one full pass over the training set.\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        tr_g_loss = 0\n",
    "        tr_d_loss = 0\n",
    "\n",
    "        # Put the model into training mode.\n",
    "        transformer.train() \n",
    "        #generator.train()\n",
    "        discriminator.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Progress update every print_each_n_step batches.\n",
    "            if step % print_each_n_step == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            b_label_mask = batch[3].to(device)\n",
    "\n",
    "            real_batch_size = b_input_ids.shape[0]\n",
    "\n",
    "            # Encode real data in the Transformer\n",
    "            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
    "            hidden_states = model_outputs.last_hidden_state[:,0,:] \n",
    "            #hidden_states = model_outputs[-1]\n",
    "            #print(\"  Number of real sentences (labelled and unlabelled): {}\".format(len(hidden_states)))\n",
    "            \n",
    "            # Generate fake data that should have the same distribution of the ones\n",
    "            # encoded by the transformer. \n",
    "            # First noisy input are used in input to the Generator\n",
    "            fixed_noise = Variable(torch.ones(real_batch_size, 100).normal_(0, 1).to(device))\n",
    "            fake_sentences = gen_fixed_noise_new(fixed_noise)\n",
    "            #print(\"  Number of generated sentences: {}\".format(len(fake_sentences)))\n",
    "\n",
    "            b_input_ids_fake, b_input_mask_fake = generate_data_fake(fake_sentences)\n",
    "            model_outputs_fake = transformer(b_input_ids_fake, attention_mask=b_input_mask_fake)\n",
    "            hidden_states_fake = model_outputs_fake.last_hidden_state[:,0,:] \n",
    "            #hidden_states_fake = model_outputs_fake[-1]\n",
    "\n",
    "            #noise = torch.zeros(real_batch_size, noise_size, device=device).uniform_(0, 1)\n",
    "            # Gnerate Fake data\n",
    "            #gen_rep = generator(noise)\n",
    "            #print(\"Length of generator output {}\".format(len(gen_rep)))\n",
    "            #print(\"Length of single generator output {}\".format(len(gen_rep[0])))\n",
    "\n",
    "            # Generate the output of the Discriminator for real and fake data.\n",
    "            # First, we put together the output of the tranformer and the generator\n",
    "            disciminator_input = torch.cat([hidden_states, hidden_states_fake], dim=0)\n",
    "            # Then, we select the output of the disciminator\n",
    "            features, logits, probs = discriminator(disciminator_input)\n",
    "\n",
    "            # Finally, we separate the discriminator's output for the real and fake\n",
    "            # data\n",
    "            features_list = torch.split(features, real_batch_size)\n",
    "            D_real_features = features_list[0]\n",
    "            D_fake_features = features_list[1]\n",
    "\n",
    "            logits_list = torch.split(logits, real_batch_size)\n",
    "            D_real_logits = logits_list[0]\n",
    "            D_fake_logits = logits_list[1]\n",
    "\n",
    "            probs_list = torch.split(probs, real_batch_size)\n",
    "            D_real_probs = probs_list[0]\n",
    "            D_fake_probs = probs_list[1]\n",
    "\n",
    "            #---------------------------------\n",
    "            #  LOSS evaluation\n",
    "            #---------------------------------\n",
    "            # Generator's LOSS estimation\n",
    "            g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + epsilon))\n",
    "            g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n",
    "            g_loss = g_loss_d + g_feat_reg\n",
    "\n",
    "            # Disciminator's LOSS estimation\n",
    "            logits = D_real_logits[:,0:-1]\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            # The discriminator provides an output for labeled and unlabeled real data\n",
    "            # so the loss evaluated for unlabeled data is ignored (masked)\n",
    "            label2one_hot = torch.nn.functional.one_hot(b_labels, len(label_list))\n",
    "            per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n",
    "            per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n",
    "            labeled_example_count = per_example_loss.type(torch.float32).numel()\n",
    "\n",
    "            # It may be the case that a batch does not contain labeled examples, \n",
    "            # so the \"supervised loss\" in this case is not evaluated\n",
    "            if labeled_example_count == 0:\n",
    "              D_L_Supervised = 0\n",
    "            else:\n",
    "              D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n",
    "\n",
    "            D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n",
    "            D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n",
    "            d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n",
    "\n",
    "            #---------------------------------\n",
    "            #  OPTIMIZATION\n",
    "            #---------------------------------\n",
    "            # Avoid gradient accumulation\n",
    "            #gen_optimizer.zero_grad()\n",
    "            dis_optimizer.zero_grad()\n",
    "\n",
    "            # Calculate weigth updates\n",
    "            # retain_graph=True is required since the underlying graph will be deleted after backward\n",
    "            g_loss.backward(retain_graph=True)\n",
    "            d_loss.backward() \n",
    "\n",
    "            # Apply modifications\n",
    "            #gen_optimizer.step()\n",
    "            dis_optimizer.step()\n",
    "\n",
    "            # A detail log of the individual losses\n",
    "            #print(\"{0:.4f}\\t{1:.4f}\\t{2:.4f}\\t{3:.4f}\\t{4:.4f}\".\n",
    "            #      format(D_L_Supervised, D_L_unsupervised1U, D_L_unsupervised2U,\n",
    "            #             g_loss_d, g_feat_reg))\n",
    "\n",
    "            # Save the losses to print them later\n",
    "            tr_g_loss += g_loss.item()\n",
    "            tr_d_loss += d_loss.item()\n",
    "\n",
    "            # Update the learning rate with the scheduler\n",
    "            if apply_scheduler:\n",
    "              scheduler_d.step()\n",
    "              #scheduler_g.step()\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss_g = tr_g_loss / len(train_dataloader)\n",
    "        avg_train_loss_d = tr_d_loss / len(train_dataloader)             \n",
    "\n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss generetor: {0:.3f}\".format(avg_train_loss_g))\n",
    "        print(\"  Average training loss discriminator: {0:.3f}\".format(avg_train_loss_d))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "        # ========================================\n",
    "        #     TEST ON THE EVALUATION DATASET\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our test set.\n",
    "        print(\"\")\n",
    "        print(\"Running Test...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        transformer.eval() #maybe redundant\n",
    "        discriminator.eval()\n",
    "        #generator.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_test_accuracy = 0\n",
    "\n",
    "        total_test_loss = 0\n",
    "        nb_test_steps = 0\n",
    "\n",
    "        all_preds = []\n",
    "        all_labels_ids = []\n",
    "\n",
    "        #loss\n",
    "        nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in test_dataloader:\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "                model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
    "                hidden_states = model_outputs.last_hidden_state[:,0,:] \n",
    "                #hidden_states = model_outputs[-1]\n",
    "                _, logits, probs = discriminator(hidden_states)\n",
    "                ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n",
    "                filtered_logits = logits[:,0:-1]\n",
    "                # Accumulate the test loss.\n",
    "                total_test_loss += nll_loss(filtered_logits, b_labels)\n",
    "\n",
    "            # Accumulate the predictions and the input labels\n",
    "            _, preds = torch.max(filtered_logits, 1)\n",
    "            all_preds += preds.detach().cpu()\n",
    "            all_labels_ids += b_labels.detach().cpu()\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        all_preds = torch.stack(all_preds).numpy()\n",
    "        all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
    "        test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
    "        print(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "        avg_test_loss = avg_test_loss.item()\n",
    "\n",
    "        # Measure how long the validation run took.\n",
    "        test_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"  Test Loss: {0:.3f}\".format(avg_test_loss))\n",
    "        print(\"  Test took: {:}\".format(test_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch + 1,\n",
    "                'Training Loss generator': avg_train_loss_g,\n",
    "                'Training Loss discriminator': avg_train_loss_d,\n",
    "                'Valid. Loss': avg_test_loss,\n",
    "                'Valid. Accur.': test_accuracy,\n",
    "                'Training Time': training_time,\n",
    "                'Test Time': test_time\n",
    "            }\n",
    "        )\n",
    "\n",
    "        accuracy_array.append(test_accuracy)\n",
    "        print(\"Train other shit\")\n",
    "        while niter < len(train_data):\n",
    "            # train ae\n",
    "            for i in range(niters_ae):\n",
    "                if niter >= len(train_data):\n",
    "                    break  # end of epoch\n",
    "                total_loss_ae, start_time = train_ae(epoch, train_data[niter],\n",
    "                                total_loss_ae, start_time, niter)\n",
    "                niter += 1\n",
    "            # train gan\n",
    "            for k in range(niter_gan):\n",
    "                for i in range(niters_gan_d):\n",
    "                    errD, errD_real, errD_fake = train_gan_d(\n",
    "                            train_data[random.randint(0, len(train_data)-1)], gan_type)\n",
    "                for i in range(niters_gan_ae):\n",
    "                    train_gan_d_into_ae(train_data[random.randint(0, len(train_data)-1)])\n",
    "                for i in range(niters_gan_g):\n",
    "                    errG = train_gan_g(gan_type)\n",
    "                if enhance_dec:\n",
    "                    for i in range(niters_gan_dec):\n",
    "                        errG_enh_dec = train_gan_dec()\n",
    "                else:\n",
    "                    errG_enh_dec = torch.Tensor([0])\n",
    "\n",
    "            niter_g += 1\n",
    "            if niter_g % log_interval == 0:\n",
    "                logging('[{}/{}][{}/{}] Loss_D: {:.8f} (Loss_D_real: {:.8f} '\n",
    "                        'Loss_D_fake: {:.8f}) Loss_G: {:.8f} Loss_Enh_Dec: {:.8f}'.format(\n",
    "                         epoch, epochs, niter, len(train_data),\n",
    "                         errD.data.item(), errD_real.data.item(),\n",
    "                         errD_fake.data.item(), errG.data.item(), errG_enh_dec.data.item()))\n",
    "        # eval\n",
    "        test_loss, accuracy = evaluate_autoencoder(test_data, epoch)\n",
    "        logging('| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} | '\n",
    "                'test ppl {:5.2f} | acc {:3.3f}'.format(epoch,\n",
    "                (time.time() - epoch_start_time), test_loss,\n",
    "                math.exp(test_loss), accuracy))\n",
    "\n",
    "        gen_text_savepath = os.path.join(save, \"{:03d}_examplar_gen\".format(epoch))\n",
    "        gen_fixed_noise(fixed_noise, gen_text_savepath)\n",
    "#         if epoch % 5 == 0 or epoch % 4 == 0 or (epochs - epoch) <=2:\n",
    "#             selfbleu, testbleu = eval_bleu(gen_text_savepath)\n",
    "#             logging('bleu_self: [{:.8f},{:.8f},{:.8f},{:.8f},{:.8f}]'.format(selfbleu[0], selfbleu[1], selfbleu[2], selfbleu[3], selfbleu[4]))\n",
    "#             logging('bleu_test: [{:.8f},{:.8f},{:.8f},{:.8f},{:.8f}]'.format(testbleu[0], testbleu[1], testbleu[2], testbleu[3], testbleu[4]))\n",
    "\n",
    "        if epoch % 15 == 0 or epoch == epochs-1:\n",
    "            logging(\"New saving model: epoch {:03d}.\".format(epoch))\n",
    "            save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a24898f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 1 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:20.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:27.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:34.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:47.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:54.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:01.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:22.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:28.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:35.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:42.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:49.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:56.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:02.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:09.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:16.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:23.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:30.\n",
      "\n",
      "  Average training loss generetor: 0.545\n",
      "  Average training loss discriminator: 2.724\n",
      "  Training epcoh took: 0:02:36\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.365\n",
      "  Test Loss: 1.648\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   1 |     0/ 2499 batches | lr 0.000000 | ms/batch 1567.13 | loss  0.12 | ppl     1.13 | acc     0.00 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harry/venv/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/harry/venv/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/200][99/2499] Loss_D: 1.38627386 (Loss_D_real: 0.69190776 Loss_D_fake: 0.69436610) Loss_G: -0.00027885 Loss_Enh_Dec: -0.00023097\n",
      "| epoch   1 |   100/ 2499 batches | lr 0.000000 | ms/batch 1057.18 | loss  8.95 | ppl  7722.83 | acc     0.06 | train_ae_norm     1.00\n",
      "[1/200][199/2499] Loss_D: 1.38628232 (Loss_D_real: 0.69293344 Loss_D_fake: 0.69334888) Loss_G: -0.00002488 Loss_Enh_Dec: -0.00003587\n",
      "| epoch   1 |   200/ 2499 batches | lr 0.000000 | ms/batch 1053.37 | loss  7.87 | ppl  2629.67 | acc     0.07 | train_ae_norm     1.00\n",
      "[1/200][299/2499] Loss_D: 1.38652194 (Loss_D_real: 0.69352162 Loss_D_fake: 0.69300032) Loss_G: 0.00005976 Loss_Enh_Dec: 0.00005715\n",
      "| epoch   1 |   300/ 2499 batches | lr 0.000000 | ms/batch 1052.27 | loss  7.58 | ppl  1956.15 | acc     0.10 | train_ae_norm     1.00\n",
      "[1/200][399/2499] Loss_D: 1.38642275 (Loss_D_real: 0.69369590 Loss_D_fake: 0.69272685) Loss_G: 0.00008975 Loss_Enh_Dec: 0.00013494\n",
      "| epoch   1 |   400/ 2499 batches | lr 0.000000 | ms/batch 1052.67 | loss  7.33 | ppl  1525.21 | acc     0.11 | train_ae_norm     1.00\n",
      "[1/200][499/2499] Loss_D: 1.38639939 (Loss_D_real: 0.69374001 Loss_D_fake: 0.69265938) Loss_G: 0.00010085 Loss_Enh_Dec: 0.00010410\n",
      "| epoch   1 |   500/ 2499 batches | lr 0.000000 | ms/batch 1052.48 | loss  7.21 | ppl  1355.27 | acc     0.10 | train_ae_norm     1.00\n",
      "[1/200][599/2499] Loss_D: 1.38633227 (Loss_D_real: 0.69347501 Loss_D_fake: 0.69285727) Loss_G: 0.00007511 Loss_Enh_Dec: 0.00005769\n",
      "| epoch   1 |   600/ 2499 batches | lr 0.000000 | ms/batch 1052.54 | loss  7.09 | ppl  1197.25 | acc     0.10 | train_ae_norm     1.00\n",
      "[1/200][699/2499] Loss_D: 1.38648617 (Loss_D_real: 0.69315428 Loss_D_fake: 0.69333190) Loss_G: -0.00004731 Loss_Enh_Dec: -0.00001591\n",
      "| epoch   1 |   700/ 2499 batches | lr 0.000000 | ms/batch 1049.89 | loss  7.00 | ppl  1094.66 | acc     0.12 | train_ae_norm     1.00\n",
      "[1/200][799/2499] Loss_D: 1.38642061 (Loss_D_real: 0.69312990 Loss_D_fake: 0.69329071) Loss_G: -0.00001950 Loss_Enh_Dec: -0.00000267\n",
      "| epoch   1 |   800/ 2499 batches | lr 0.000000 | ms/batch 1049.90 | loss  6.91 | ppl  1001.08 | acc     0.14 | train_ae_norm     1.00\n",
      "[1/200][899/2499] Loss_D: 1.38646877 (Loss_D_real: 0.69329059 Loss_D_fake: 0.69317818) Loss_G: -0.00000133 Loss_Enh_Dec: 0.00002256\n",
      "| epoch   1 |   900/ 2499 batches | lr 0.000000 | ms/batch 1052.37 | loss  6.81 | ppl   902.69 | acc     0.12 | train_ae_norm     1.00\n",
      "[1/200][999/2499] Loss_D: 1.38633299 (Loss_D_real: 0.69317377 Loss_D_fake: 0.69315922) Loss_G: 0.00000009 Loss_Enh_Dec: 0.00000886\n",
      "| epoch   1 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1052.24 | loss  6.77 | ppl   872.54 | acc     0.13 | train_ae_norm     1.00\n",
      "[1/200][1099/2499] Loss_D: 1.38634109 (Loss_D_real: 0.69317245 Loss_D_fake: 0.69316864) Loss_G: 0.00000082 Loss_Enh_Dec: 0.00000289\n",
      "| epoch   1 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1052.60 | loss  6.68 | ppl   795.62 | acc     0.17 | train_ae_norm     1.00\n",
      "[1/200][1299/2499] Loss_D: 1.38631952 (Loss_D_real: 0.69318092 Loss_D_fake: 0.69313860) Loss_G: 0.00000552 Loss_Enh_Dec: 0.00000787\n",
      "| epoch   1 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1052.83 | loss  6.60 | ppl   732.62 | acc     0.12 | train_ae_norm     1.00\n",
      "[1/200][1399/2499] Loss_D: 1.38634050 (Loss_D_real: 0.69316518 Loss_D_fake: 0.69317532) Loss_G: -0.00000311 Loss_Enh_Dec: 0.00000421\n",
      "| epoch   1 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1050.92 | loss  6.54 | ppl   692.66 | acc     0.16 | train_ae_norm     1.00\n",
      "[1/200][1499/2499] Loss_D: 1.38633204 (Loss_D_real: 0.69308174 Loss_D_fake: 0.69325036) Loss_G: -0.00001458 Loss_Enh_Dec: -0.00001287\n",
      "| epoch   1 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1053.77 | loss  6.51 | ppl   672.89 | acc     0.16 | train_ae_norm     1.00\n",
      "[1/200][1599/2499] Loss_D: 1.38631105 (Loss_D_real: 0.69315547 Loss_D_fake: 0.69315565) Loss_G: 0.00000059 Loss_Enh_Dec: 0.00000260\n",
      "| epoch   1 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1050.67 | loss  6.50 | ppl   662.77 | acc     0.18 | train_ae_norm     1.00\n",
      "[1/200][1699/2499] Loss_D: 1.38629127 (Loss_D_real: 0.69313133 Loss_D_fake: 0.69316000) Loss_G: 0.00000141 Loss_Enh_Dec: -0.00000281\n",
      "| epoch   1 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1051.19 | loss  6.38 | ppl   592.68 | acc     0.19 | train_ae_norm     1.00\n",
      "[1/200][1799/2499] Loss_D: 1.38625813 (Loss_D_real: 0.69317961 Loss_D_fake: 0.69307852) Loss_G: 0.00001090 Loss_Enh_Dec: 0.00000532\n",
      "| epoch   1 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1056.47 | loss  6.34 | ppl   566.54 | acc     0.20 | train_ae_norm     1.00\n",
      "[1/200][1899/2499] Loss_D: 1.38633335 (Loss_D_real: 0.69315457 Loss_D_fake: 0.69317877) Loss_G: -0.00000059 Loss_Enh_Dec: 0.00000105\n",
      "| epoch   1 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1052.55 | loss  6.34 | ppl   568.33 | acc     0.14 | train_ae_norm     1.00\n",
      "[1/200][1999/2499] Loss_D: 1.38631153 (Loss_D_real: 0.69315958 Loss_D_fake: 0.69315195) Loss_G: 0.00000040 Loss_Enh_Dec: 0.00000247\n",
      "| epoch   1 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1054.72 | loss  6.33 | ppl   560.37 | acc     0.17 | train_ae_norm     1.00\n",
      "[1/200][2099/2499] Loss_D: 1.38629055 (Loss_D_real: 0.69315118 Loss_D_fake: 0.69313943) Loss_G: 0.00000438 Loss_Enh_Dec: 0.00000032\n",
      "| epoch   1 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1053.95 | loss  6.25 | ppl   518.83 | acc     0.15 | train_ae_norm     1.00\n",
      "[1/200][2199/2499] Loss_D: 1.38629508 (Loss_D_real: 0.69315159 Loss_D_fake: 0.69314349) Loss_G: 0.00000027 Loss_Enh_Dec: 0.00000137\n",
      "| epoch   1 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1053.58 | loss  6.22 | ppl   502.00 | acc     0.19 | train_ae_norm     1.00\n",
      "[1/200][2299/2499] Loss_D: 1.38630581 (Loss_D_real: 0.69314110 Loss_D_fake: 0.69316477) Loss_G: -0.00000218 Loss_Enh_Dec: -0.00000297\n",
      "| epoch   1 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1052.75 | loss  6.18 | ppl   481.05 | acc     0.18 | train_ae_norm     1.00\n",
      "[1/200][2399/2499] Loss_D: 1.38630211 (Loss_D_real: 0.69315660 Loss_D_fake: 0.69314551) Loss_G: 0.00000191 Loss_Enh_Dec: 0.00000159\n",
      "| epoch   1 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1053.73 | loss  6.17 | ppl   475.93 | acc     0.18 | train_ae_norm     1.00\n",
      "[1/200][2499/2499] Loss_D: 1.38629639 (Loss_D_real: 0.69313848 Loss_D_fake: 0.69315791) Loss_G: -0.00000363 Loss_Enh_Dec: -0.00000202\n",
      "| end of epoch   1 | time: 2807.70s | test loss  6.04 | test ppl 419.47 | acc 0.203\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 2 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:20.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:27.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:34.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:54.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:01.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:22.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:28.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:35.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:42.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:49.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:56.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:03.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:09.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:16.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:23.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:30.\n",
      "\n",
      "  Average training loss generetor: 0.655\n",
      "  Average training loss discriminator: 2.262\n",
      "  Training epcoh took: 0:02:36\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.390\n",
      "  Test Loss: 1.484\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   2 |     0/ 2499 batches | lr 0.000000 | ms/batch 1567.76 | loss  0.06 | ppl     1.06 | acc     0.19 | train_ae_norm     1.00\n",
      "[2/200][99/2499] Loss_D: 1.38631010 (Loss_D_real: 0.69317406 Loss_D_fake: 0.69313610) Loss_G: 0.00000408 Loss_Enh_Dec: 0.00000322\n",
      "| epoch   2 |   100/ 2499 batches | lr 0.000000 | ms/batch 1053.33 | loss  6.09 | ppl   439.79 | acc     0.21 | train_ae_norm     1.00\n",
      "[2/200][199/2499] Loss_D: 1.38629007 (Loss_D_real: 0.69312948 Loss_D_fake: 0.69316053) Loss_G: -0.00000049 Loss_Enh_Dec: -0.00000087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |   200/ 2499 batches | lr 0.000000 | ms/batch 1050.77 | loss  6.02 | ppl   413.60 | acc     0.20 | train_ae_norm     1.00\n",
      "[2/200][299/2499] Loss_D: 1.38628852 (Loss_D_real: 0.69313097 Loss_D_fake: 0.69315755) Loss_G: -0.00000327 Loss_Enh_Dec: -0.00000242\n",
      "| epoch   2 |   300/ 2499 batches | lr 0.000000 | ms/batch 1051.97 | loss  6.05 | ppl   424.26 | acc     0.20 | train_ae_norm     1.00\n",
      "[2/200][399/2499] Loss_D: 1.38629723 (Loss_D_real: 0.69315428 Loss_D_fake: 0.69314289) Loss_G: 0.00000110 Loss_Enh_Dec: 0.00000040\n",
      "| epoch   2 |   400/ 2499 batches | lr 0.000000 | ms/batch 1052.23 | loss  5.99 | ppl   399.06 | acc     0.23 | train_ae_norm     1.00\n",
      "[2/200][499/2499] Loss_D: 1.38631511 (Loss_D_real: 0.69313085 Loss_D_fake: 0.69318432) Loss_G: -0.00000249 Loss_Enh_Dec: -0.00000628\n",
      "| epoch   2 |   500/ 2499 batches | lr 0.000000 | ms/batch 1053.72 | loss  6.01 | ppl   409.21 | acc     0.22 | train_ae_norm     1.00\n",
      "[2/200][599/2499] Loss_D: 1.38628209 (Loss_D_real: 0.69315016 Loss_D_fake: 0.69313192) Loss_G: 0.00000094 Loss_Enh_Dec: 0.00000168\n",
      "| epoch   2 |   600/ 2499 batches | lr 0.000000 | ms/batch 1051.88 | loss  5.97 | ppl   391.82 | acc     0.18 | train_ae_norm     1.00\n",
      "[2/200][699/2499] Loss_D: 1.38630104 (Loss_D_real: 0.69315428 Loss_D_fake: 0.69314671) Loss_G: 0.00000000 Loss_Enh_Dec: -0.00000057\n",
      "| epoch   2 |   700/ 2499 batches | lr 0.000000 | ms/batch 1051.66 | loss  5.93 | ppl   376.11 | acc     0.22 | train_ae_norm     1.00\n",
      "[2/200][799/2499] Loss_D: 1.38630295 (Loss_D_real: 0.69314688 Loss_D_fake: 0.69315600) Loss_G: -0.00000137 Loss_Enh_Dec: -0.00000213\n",
      "| epoch   2 |   800/ 2499 batches | lr 0.000000 | ms/batch 1051.56 | loss  5.93 | ppl   377.60 | acc     0.23 | train_ae_norm     1.00\n",
      "[2/200][899/2499] Loss_D: 1.38629508 (Loss_D_real: 0.69313705 Loss_D_fake: 0.69315803) Loss_G: -0.00000088 Loss_Enh_Dec: -0.00000196\n",
      "| epoch   2 |   900/ 2499 batches | lr 0.000000 | ms/batch 1053.21 | loss  5.88 | ppl   358.64 | acc     0.22 | train_ae_norm     1.00\n",
      "[2/200][999/2499] Loss_D: 1.38628960 (Loss_D_real: 0.69313180 Loss_D_fake: 0.69315785) Loss_G: -0.00000214 Loss_Enh_Dec: -0.00000301\n",
      "| epoch   2 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1051.74 | loss  5.90 | ppl   364.62 | acc     0.21 | train_ae_norm     1.00\n",
      "[2/200][1099/2499] Loss_D: 1.38629448 (Loss_D_real: 0.69315064 Loss_D_fake: 0.69314384) Loss_G: 0.00000012 Loss_Enh_Dec: -0.00000086\n",
      "| epoch   2 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1050.96 | loss  5.85 | ppl   347.12 | acc     0.23 | train_ae_norm     1.00\n",
      "[2/200][1199/2499] Loss_D: 1.38629007 (Loss_D_real: 0.69313657 Loss_D_fake: 0.69315356) Loss_G: 0.00000081 Loss_Enh_Dec: 0.00000105\n",
      "| epoch   2 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1053.02 | loss  5.85 | ppl   347.60 | acc     0.21 | train_ae_norm     1.00\n",
      "[2/200][1299/2499] Loss_D: 1.38630772 (Loss_D_real: 0.69315374 Loss_D_fake: 0.69315404) Loss_G: 0.00000061 Loss_Enh_Dec: 0.00000115\n",
      "| epoch   2 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1052.37 | loss  5.87 | ppl   353.49 | acc     0.21 | train_ae_norm     1.00\n",
      "[2/200][1399/2499] Loss_D: 1.38629103 (Loss_D_real: 0.69313812 Loss_D_fake: 0.69315284) Loss_G: -0.00000157 Loss_Enh_Dec: -0.00000254\n",
      "| epoch   2 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1050.87 | loss  5.81 | ppl   334.38 | acc     0.24 | train_ae_norm     1.00\n",
      "[2/200][1499/2499] Loss_D: 1.38629699 (Loss_D_real: 0.69314492 Loss_D_fake: 0.69315213) Loss_G: -0.00000071 Loss_Enh_Dec: 0.00000059\n",
      "| epoch   2 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1050.76 | loss  5.78 | ppl   322.33 | acc     0.21 | train_ae_norm     1.00\n",
      "[2/200][1599/2499] Loss_D: 1.38629222 (Loss_D_real: 0.69315165 Loss_D_fake: 0.69314051) Loss_G: 0.00000014 Loss_Enh_Dec: 0.00000197\n",
      "| epoch   2 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1051.44 | loss  5.81 | ppl   332.35 | acc     0.25 | train_ae_norm     1.00\n",
      "[2/200][1699/2499] Loss_D: 1.38629580 (Loss_D_real: 0.69314992 Loss_D_fake: 0.69314587) Loss_G: 0.00000152 Loss_Enh_Dec: 0.00000022\n",
      "| epoch   2 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1049.52 | loss  5.73 | ppl   306.57 | acc     0.25 | train_ae_norm     1.00\n",
      "[2/200][1799/2499] Loss_D: 1.38630497 (Loss_D_real: 0.69315189 Loss_D_fake: 0.69315308) Loss_G: -0.00000148 Loss_Enh_Dec: 0.00000007\n",
      "| epoch   2 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1054.64 | loss  5.70 | ppl   298.37 | acc     0.27 | train_ae_norm     1.00\n",
      "[2/200][1899/2499] Loss_D: 1.38630354 (Loss_D_real: 0.69315594 Loss_D_fake: 0.69314760) Loss_G: 0.00000010 Loss_Enh_Dec: 0.00000086\n",
      "| epoch   2 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1049.87 | loss  5.71 | ppl   301.22 | acc     0.23 | train_ae_norm     1.00\n",
      "[2/200][1999/2499] Loss_D: 1.38628864 (Loss_D_real: 0.69314861 Loss_D_fake: 0.69314003) Loss_G: 0.00000117 Loss_Enh_Dec: 0.00000084\n",
      "| epoch   2 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1049.55 | loss  5.70 | ppl   297.90 | acc     0.25 | train_ae_norm     1.00\n",
      "[2/200][2099/2499] Loss_D: 1.38630009 (Loss_D_real: 0.69314909 Loss_D_fake: 0.69315100) Loss_G: -0.00000029 Loss_Enh_Dec: 0.00000075\n",
      "| epoch   2 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1049.76 | loss  5.65 | ppl   283.74 | acc     0.22 | train_ae_norm     1.00\n",
      "[2/200][2199/2499] Loss_D: 1.38629472 (Loss_D_real: 0.69314814 Loss_D_fake: 0.69314659) Loss_G: 0.00000091 Loss_Enh_Dec: 0.00000174\n",
      "| epoch   2 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1051.87 | loss  5.63 | ppl   277.72 | acc     0.27 | train_ae_norm     1.00\n",
      "[2/200][2299/2499] Loss_D: 1.38630152 (Loss_D_real: 0.69315171 Loss_D_fake: 0.69314975) Loss_G: -0.00000109 Loss_Enh_Dec: -0.00000119\n",
      "| epoch   2 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1051.80 | loss  5.59 | ppl   267.42 | acc     0.24 | train_ae_norm     1.00\n",
      "[2/200][2399/2499] Loss_D: 1.38629699 (Loss_D_real: 0.69315189 Loss_D_fake: 0.69314516) Loss_G: 0.00000005 Loss_Enh_Dec: -0.00000061\n",
      "| epoch   2 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1053.66 | loss  5.59 | ppl   267.83 | acc     0.25 | train_ae_norm     1.00\n",
      "[2/200][2499/2499] Loss_D: 1.38628817 (Loss_D_real: 0.69314456 Loss_D_fake: 0.69314355) Loss_G: 0.00000022 Loss_Enh_Dec: 0.00000115\n",
      "| end of epoch   2 | time: 2805.10s | test loss  5.39 | test ppl 219.97 | acc 0.287\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 3 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:20.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:27.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:34.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:54.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:01.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:22.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:28.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:35.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:42.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:49.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:56.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:02.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:09.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:16.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:23.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:29.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 1.744\n",
      "  Training epcoh took: 0:02:36\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.430\n",
      "  Test Loss: 1.279\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   3 |     0/ 2499 batches | lr 0.000000 | ms/batch 1564.91 | loss  0.06 | ppl     1.06 | acc     0.29 | train_ae_norm     1.00\n",
      "[3/200][99/2499] Loss_D: 1.38629758 (Loss_D_real: 0.69314688 Loss_D_fake: 0.69315070) Loss_G: -0.00000010 Loss_Enh_Dec: -0.00000106\n",
      "| epoch   3 |   100/ 2499 batches | lr 0.000000 | ms/batch 1051.90 | loss  5.53 | ppl   251.62 | acc     0.28 | train_ae_norm     1.00\n",
      "[3/200][199/2499] Loss_D: 1.38629651 (Loss_D_real: 0.69315052 Loss_D_fake: 0.69314599) Loss_G: 0.00000095 Loss_Enh_Dec: 0.00000107\n",
      "| epoch   3 |   200/ 2499 batches | lr 0.000000 | ms/batch 1051.63 | loss  5.48 | ppl   239.74 | acc     0.28 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/200][299/2499] Loss_D: 1.38629425 (Loss_D_real: 0.69314647 Loss_D_fake: 0.69314778) Loss_G: 0.00000021 Loss_Enh_Dec: 0.00000088\n",
      "| epoch   3 |   300/ 2499 batches | lr 0.000000 | ms/batch 1050.61 | loss  5.51 | ppl   245.95 | acc     0.27 | train_ae_norm     1.00\n",
      "[3/200][399/2499] Loss_D: 1.38629580 (Loss_D_real: 0.69315022 Loss_D_fake: 0.69314563) Loss_G: -0.00000025 Loss_Enh_Dec: 0.00000076\n",
      "| epoch   3 |   400/ 2499 batches | lr 0.000000 | ms/batch 1049.25 | loss  5.45 | ppl   232.34 | acc     0.30 | train_ae_norm     1.00\n",
      "[3/200][499/2499] Loss_D: 1.38628995 (Loss_D_real: 0.69314814 Loss_D_fake: 0.69314182) Loss_G: -0.00000052 Loss_Enh_Dec: -0.00000107\n",
      "| epoch   3 |   500/ 2499 batches | lr 0.000000 | ms/batch 1049.27 | loss  5.46 | ppl   235.45 | acc     0.29 | train_ae_norm     1.00\n",
      "[3/200][599/2499] Loss_D: 1.38629317 (Loss_D_real: 0.69315976 Loss_D_fake: 0.69313347) Loss_G: 0.00000249 Loss_Enh_Dec: -0.00000178\n",
      "| epoch   3 |   600/ 2499 batches | lr 0.000000 | ms/batch 1048.60 | loss  5.45 | ppl   233.83 | acc     0.23 | train_ae_norm     1.00\n",
      "[3/200][699/2499] Loss_D: 1.38630056 (Loss_D_real: 0.69314814 Loss_D_fake: 0.69315243) Loss_G: -0.00000003 Loss_Enh_Dec: 0.00000021\n",
      "| epoch   3 |   700/ 2499 batches | lr 0.000000 | ms/batch 1048.99 | loss  5.44 | ppl   230.36 | acc     0.28 | train_ae_norm     1.00\n",
      "[3/200][799/2499] Loss_D: 1.38630295 (Loss_D_real: 0.69315183 Loss_D_fake: 0.69315112) Loss_G: -0.00000082 Loss_Enh_Dec: 0.00000060\n",
      "| epoch   3 |   800/ 2499 batches | lr 0.000000 | ms/batch 1048.81 | loss  5.39 | ppl   219.91 | acc     0.29 | train_ae_norm     1.00\n",
      "[3/200][899/2499] Loss_D: 1.38629937 (Loss_D_real: 0.69314569 Loss_D_fake: 0.69315362) Loss_G: 0.00000060 Loss_Enh_Dec: 0.00000099\n",
      "| epoch   3 |   900/ 2499 batches | lr 0.000000 | ms/batch 1049.10 | loss  5.33 | ppl   206.74 | acc     0.28 | train_ae_norm     1.00\n",
      "[3/200][999/2499] Loss_D: 1.38629282 (Loss_D_real: 0.69315469 Loss_D_fake: 0.69313812) Loss_G: -0.00000040 Loss_Enh_Dec: 0.00000118\n",
      "| epoch   3 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1049.16 | loss  5.34 | ppl   209.19 | acc     0.28 | train_ae_norm     1.00\n",
      "[3/200][1099/2499] Loss_D: 1.38628900 (Loss_D_real: 0.69314802 Loss_D_fake: 0.69314098) Loss_G: -0.00000067 Loss_Enh_Dec: 0.00000040\n",
      "| epoch   3 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1050.58 | loss  5.27 | ppl   195.17 | acc     0.32 | train_ae_norm     1.00\n",
      "[3/200][1199/2499] Loss_D: 1.38630033 (Loss_D_real: 0.69314885 Loss_D_fake: 0.69315147) Loss_G: 0.00000068 Loss_Enh_Dec: 0.00000005\n",
      "| epoch   3 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1050.47 | loss  5.27 | ppl   194.38 | acc     0.30 | train_ae_norm     1.00\n",
      "[3/200][1299/2499] Loss_D: 1.38629937 (Loss_D_real: 0.69315112 Loss_D_fake: 0.69314831) Loss_G: 0.00000107 Loss_Enh_Dec: 0.00000111\n",
      "| epoch   3 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1049.14 | loss  5.28 | ppl   197.30 | acc     0.31 | train_ae_norm     1.00\n",
      "[3/200][1399/2499] Loss_D: 1.38629019 (Loss_D_real: 0.69314355 Loss_D_fake: 0.69314665) Loss_G: -0.00000104 Loss_Enh_Dec: -0.00000219\n",
      "| epoch   3 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1049.95 | loss  5.22 | ppl   185.42 | acc     0.32 | train_ae_norm     1.00\n",
      "[3/200][1499/2499] Loss_D: 1.38629603 (Loss_D_real: 0.69314528 Loss_D_fake: 0.69315070) Loss_G: 0.00000012 Loss_Enh_Dec: -0.00000052\n",
      "| epoch   3 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1050.82 | loss  5.17 | ppl   176.78 | acc     0.31 | train_ae_norm     1.00\n",
      "[3/200][1599/2499] Loss_D: 1.38629150 (Loss_D_real: 0.69314027 Loss_D_fake: 0.69315118) Loss_G: 0.00000065 Loss_Enh_Dec: -0.00000016\n",
      "| epoch   3 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1050.77 | loss  5.20 | ppl   181.80 | acc     0.35 | train_ae_norm     1.00\n",
      "[3/200][1699/2499] Loss_D: 1.38629949 (Loss_D_real: 0.69315356 Loss_D_fake: 0.69314593) Loss_G: 0.00000015 Loss_Enh_Dec: 0.00000079\n",
      "| epoch   3 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1050.90 | loss  5.11 | ppl   166.35 | acc     0.34 | train_ae_norm     1.00\n",
      "[3/200][1799/2499] Loss_D: 1.38630009 (Loss_D_real: 0.69314831 Loss_D_fake: 0.69315183) Loss_G: 0.00000023 Loss_Enh_Dec: 0.00000020\n",
      "| epoch   3 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1050.74 | loss  5.08 | ppl   160.16 | acc     0.36 | train_ae_norm     1.00\n",
      "[3/200][1899/2499] Loss_D: 1.38629532 (Loss_D_real: 0.69314897 Loss_D_fake: 0.69314641) Loss_G: -0.00000169 Loss_Enh_Dec: -0.00000014\n",
      "| epoch   3 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1050.89 | loss  5.08 | ppl   160.68 | acc     0.32 | train_ae_norm     1.00\n",
      "[3/200][1999/2499] Loss_D: 1.38629985 (Loss_D_real: 0.69314444 Loss_D_fake: 0.69315541) Loss_G: 0.00000004 Loss_Enh_Dec: 0.00000115\n",
      "| epoch   3 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1050.58 | loss  5.08 | ppl   161.10 | acc     0.33 | train_ae_norm     1.00\n",
      "[3/200][2099/2499] Loss_D: 1.38629568 (Loss_D_real: 0.69314587 Loss_D_fake: 0.69314981) Loss_G: 0.00000088 Loss_Enh_Dec: 0.00000274\n",
      "| epoch   3 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1050.63 | loss  5.02 | ppl   151.84 | acc     0.33 | train_ae_norm     1.00\n",
      "[3/200][2199/2499] Loss_D: 1.38628960 (Loss_D_real: 0.69314545 Loss_D_fake: 0.69314420) Loss_G: 0.00000166 Loss_Enh_Dec: 0.00000089\n",
      "| epoch   3 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1051.82 | loss  4.99 | ppl   146.44 | acc     0.36 | train_ae_norm     1.00\n",
      "[3/200][2299/2499] Loss_D: 1.38629818 (Loss_D_real: 0.69315052 Loss_D_fake: 0.69314766) Loss_G: 0.00000084 Loss_Enh_Dec: 0.00000130\n",
      "| epoch   3 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1051.12 | loss  4.94 | ppl   139.41 | acc     0.35 | train_ae_norm     1.00\n",
      "[3/200][2399/2499] Loss_D: 1.38629699 (Loss_D_real: 0.69314563 Loss_D_fake: 0.69315135) Loss_G: 0.00000132 Loss_Enh_Dec: 0.00000101\n",
      "| epoch   3 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1050.61 | loss  4.95 | ppl   141.13 | acc     0.35 | train_ae_norm     1.00\n",
      "[3/200][2499/2499] Loss_D: 1.38629484 (Loss_D_real: 0.69314826 Loss_D_fake: 0.69314659) Loss_G: -0.00000091 Loss_Enh_Dec: -0.00000090\n",
      "| end of epoch   3 | time: 2801.31s | test loss  4.63 | test ppl 102.91 | acc 0.415\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 4 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:20.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:27.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:34.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:01.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:22.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:29.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:36.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:42.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:49.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:56.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:03.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:10.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:16.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:23.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:30.\n",
      "\n",
      "  Average training loss generetor: 0.705\n",
      "  Average training loss discriminator: 1.329\n",
      "  Training epcoh took: 0:02:37\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.465\n",
      "  Test Loss: 1.203\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   4 |     0/ 2499 batches | lr 0.000000 | ms/batch 1571.51 | loss  0.05 | ppl     1.05 | acc     0.37 | train_ae_norm     1.00\n",
      "[4/200][99/2499] Loss_D: 1.38628948 (Loss_D_real: 0.69315046 Loss_D_fake: 0.69313902) Loss_G: 0.00000174 Loss_Enh_Dec: 0.00000040\n",
      "| epoch   4 |   100/ 2499 batches | lr 0.000000 | ms/batch 1052.19 | loss  4.85 | ppl   127.55 | acc     0.37 | train_ae_norm     1.00\n",
      "[4/200][199/2499] Loss_D: 1.38629079 (Loss_D_real: 0.69314659 Loss_D_fake: 0.69314420) Loss_G: 0.00000277 Loss_Enh_Dec: 0.00000184\n",
      "| epoch   4 |   200/ 2499 batches | lr 0.000000 | ms/batch 1050.44 | loss  4.79 | ppl   120.14 | acc     0.40 | train_ae_norm     1.00\n",
      "[4/200][299/2499] Loss_D: 1.38629854 (Loss_D_real: 0.69314432 Loss_D_fake: 0.69315422) Loss_G: -0.00000027 Loss_Enh_Dec: -0.00000078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |   300/ 2499 batches | lr 0.000000 | ms/batch 1050.42 | loss  4.84 | ppl   125.88 | acc     0.38 | train_ae_norm     1.00\n",
      "[4/200][399/2499] Loss_D: 1.38629401 (Loss_D_real: 0.69315708 Loss_D_fake: 0.69313693) Loss_G: -0.00000020 Loss_Enh_Dec: 0.00000064\n",
      "| epoch   4 |   400/ 2499 batches | lr 0.000000 | ms/batch 1051.72 | loss  4.78 | ppl   118.59 | acc     0.40 | train_ae_norm     1.00\n",
      "[4/200][499/2499] Loss_D: 1.38630056 (Loss_D_real: 0.69316500 Loss_D_fake: 0.69313562) Loss_G: -0.00000167 Loss_Enh_Dec: -0.00000002\n",
      "| epoch   4 |   500/ 2499 batches | lr 0.000000 | ms/batch 1050.62 | loss  4.75 | ppl   115.59 | acc     0.40 | train_ae_norm     1.00\n",
      "[4/200][599/2499] Loss_D: 1.38629246 (Loss_D_real: 0.69314140 Loss_D_fake: 0.69315112) Loss_G: 0.00000005 Loss_Enh_Dec: 0.00000136\n",
      "| epoch   4 |   600/ 2499 batches | lr 0.000000 | ms/batch 1049.91 | loss  4.69 | ppl   108.42 | acc     0.36 | train_ae_norm     1.00\n",
      "[4/200][699/2499] Loss_D: 1.38629007 (Loss_D_real: 0.69314736 Loss_D_fake: 0.69314277) Loss_G: -0.00000132 Loss_Enh_Dec: -0.00000220\n",
      "| epoch   4 |   700/ 2499 batches | lr 0.000000 | ms/batch 1050.66 | loss  4.65 | ppl   104.37 | acc     0.39 | train_ae_norm     1.00\n",
      "[4/200][799/2499] Loss_D: 1.38629019 (Loss_D_real: 0.69314754 Loss_D_fake: 0.69314265) Loss_G: 0.00000103 Loss_Enh_Dec: 0.00000032\n",
      "| epoch   4 |   800/ 2499 batches | lr 0.000000 | ms/batch 1051.59 | loss  4.64 | ppl   103.61 | acc     0.41 | train_ae_norm     1.00\n",
      "[4/200][899/2499] Loss_D: 1.38629818 (Loss_D_real: 0.69314957 Loss_D_fake: 0.69314861) Loss_G: 0.00000045 Loss_Enh_Dec: 0.00000026\n",
      "| epoch   4 |   900/ 2499 batches | lr 0.000000 | ms/batch 1049.97 | loss  4.56 | ppl    96.02 | acc     0.42 | train_ae_norm     1.00\n",
      "[4/200][999/2499] Loss_D: 1.38629413 (Loss_D_real: 0.69315785 Loss_D_fake: 0.69313633) Loss_G: -0.00000224 Loss_Enh_Dec: -0.00000264\n",
      "| epoch   4 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1050.63 | loss  4.56 | ppl    95.27 | acc     0.40 | train_ae_norm     1.00\n",
      "[4/200][1099/2499] Loss_D: 1.38629651 (Loss_D_real: 0.69314706 Loss_D_fake: 0.69314945) Loss_G: -0.00000071 Loss_Enh_Dec: -0.00000035\n",
      "| epoch   4 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1051.45 | loss  4.49 | ppl    89.49 | acc     0.42 | train_ae_norm     1.00\n",
      "[4/200][1199/2499] Loss_D: 1.38629425 (Loss_D_real: 0.69315374 Loss_D_fake: 0.69314051) Loss_G: -0.00000045 Loss_Enh_Dec: -0.00000066\n",
      "| epoch   4 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1050.54 | loss  4.46 | ppl    86.20 | acc     0.42 | train_ae_norm     1.00\n",
      "[4/200][1299/2499] Loss_D: 1.38629270 (Loss_D_real: 0.69315213 Loss_D_fake: 0.69314063) Loss_G: -0.00000123 Loss_Enh_Dec: -0.00000123\n",
      "| epoch   4 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1051.27 | loss  4.48 | ppl    88.57 | acc     0.42 | train_ae_norm     1.00\n",
      "[4/200][1399/2499] Loss_D: 1.38629675 (Loss_D_real: 0.69314015 Loss_D_fake: 0.69315666) Loss_G: 0.00000167 Loss_Enh_Dec: 0.00000143\n",
      "| epoch   4 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1050.80 | loss  4.43 | ppl    83.55 | acc     0.44 | train_ae_norm     1.00\n",
      "[4/200][1499/2499] Loss_D: 1.38630033 (Loss_D_real: 0.69314355 Loss_D_fake: 0.69315684) Loss_G: 0.00000112 Loss_Enh_Dec: 0.00000172\n",
      "| epoch   4 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1050.97 | loss  4.39 | ppl    80.28 | acc     0.43 | train_ae_norm     1.00\n",
      "[4/200][1599/2499] Loss_D: 1.38629889 (Loss_D_real: 0.69315529 Loss_D_fake: 0.69314355) Loss_G: 0.00000435 Loss_Enh_Dec: 0.00000317\n",
      "| epoch   4 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1051.75 | loss  4.39 | ppl    80.95 | acc     0.47 | train_ae_norm     1.00\n",
      "[4/200][1699/2499] Loss_D: 1.38629234 (Loss_D_real: 0.69314623 Loss_D_fake: 0.69314611) Loss_G: 0.00000011 Loss_Enh_Dec: 0.00000011\n",
      "| epoch   4 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1049.62 | loss  4.29 | ppl    72.71 | acc     0.47 | train_ae_norm     1.00\n",
      "[4/200][1799/2499] Loss_D: 1.38629305 (Loss_D_real: 0.69316220 Loss_D_fake: 0.69313085) Loss_G: -0.00000086 Loss_Enh_Dec: -0.00000158\n",
      "| epoch   4 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1050.95 | loss  4.24 | ppl    69.10 | acc     0.51 | train_ae_norm     1.00\n",
      "[4/200][1899/2499] Loss_D: 1.38629496 (Loss_D_real: 0.69313759 Loss_D_fake: 0.69315737) Loss_G: 0.00000227 Loss_Enh_Dec: 0.00000267\n",
      "| epoch   4 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1050.36 | loss  4.24 | ppl    69.45 | acc     0.45 | train_ae_norm     1.00\n",
      "[4/200][1999/2499] Loss_D: 1.38629508 (Loss_D_real: 0.69315147 Loss_D_fake: 0.69314355) Loss_G: -0.00000137 Loss_Enh_Dec: -0.00000094\n",
      "| epoch   4 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1049.72 | loss  4.21 | ppl    67.35 | acc     0.48 | train_ae_norm     1.00\n",
      "[4/200][2099/2499] Loss_D: 1.38629675 (Loss_D_real: 0.69315970 Loss_D_fake: 0.69313705) Loss_G: -0.00000235 Loss_Enh_Dec: -0.00000158\n",
      "| epoch   4 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1050.74 | loss  4.14 | ppl    62.85 | acc     0.47 | train_ae_norm     1.00\n",
      "[4/200][2199/2499] Loss_D: 1.38628948 (Loss_D_real: 0.69315612 Loss_D_fake: 0.69313335) Loss_G: -0.00000141 Loss_Enh_Dec: -0.00000171\n",
      "| epoch   4 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1050.68 | loss  4.09 | ppl    59.94 | acc     0.50 | train_ae_norm     1.00\n",
      "[4/200][2299/2499] Loss_D: 1.38630795 (Loss_D_real: 0.69315100 Loss_D_fake: 0.69315690) Loss_G: -0.00000566 Loss_Enh_Dec: -0.00000477\n",
      "| epoch   4 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1051.19 | loss  4.05 | ppl    57.56 | acc     0.45 | train_ae_norm     1.00\n",
      "[4/200][2399/2499] Loss_D: 1.38629675 (Loss_D_real: 0.69314021 Loss_D_fake: 0.69315660) Loss_G: 0.00000062 Loss_Enh_Dec: 0.00000031\n",
      "| epoch   4 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1051.32 | loss  4.05 | ppl    57.24 | acc     0.49 | train_ae_norm     1.00\n",
      "[4/200][2499/2499] Loss_D: 1.38628840 (Loss_D_real: 0.69316709 Loss_D_fake: 0.69312131) Loss_G: -0.00000529 Loss_Enh_Dec: -0.00000621\n",
      "| end of epoch   4 | time: 2803.20s | test loss  3.58 | test ppl 35.98 | acc 0.575\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 5 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:20.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:27.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:34.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:01.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:22.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:29.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:35.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:42.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:49.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:56.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:03.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:10.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:16.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:23.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:30.\n",
      "\n",
      "  Average training loss generetor: 0.710\n",
      "  Average training loss discriminator: 1.073\n",
      "  Training epcoh took: 0:02:36\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.495\n",
      "  Test Loss: 1.172\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   5 |     0/ 2499 batches | lr 0.000000 | ms/batch 1569.68 | loss  0.04 | ppl     1.04 | acc     0.53 | train_ae_norm     1.00\n",
      "[5/200][99/2499] Loss_D: 1.38629651 (Loss_D_real: 0.69316113 Loss_D_fake: 0.69313538) Loss_G: -0.00000399 Loss_Enh_Dec: -0.00000400\n",
      "| epoch   5 |   100/ 2499 batches | lr 0.000000 | ms/batch 1050.12 | loss  3.95 | ppl    51.72 | acc     0.50 | train_ae_norm     1.00\n",
      "[5/200][199/2499] Loss_D: 1.38629472 (Loss_D_real: 0.69315624 Loss_D_fake: 0.69313848) Loss_G: -0.00000182 Loss_Enh_Dec: -0.00000166\n",
      "| epoch   5 |   200/ 2499 batches | lr 0.000000 | ms/batch 1050.02 | loss  3.87 | ppl    47.99 | acc     0.54 | train_ae_norm     1.00\n",
      "[5/200][299/2499] Loss_D: 1.38629425 (Loss_D_real: 0.69313687 Loss_D_fake: 0.69315737) Loss_G: 0.00000262 Loss_Enh_Dec: 0.00000296\n",
      "| epoch   5 |   300/ 2499 batches | lr 0.000000 | ms/batch 1050.73 | loss  3.88 | ppl    48.62 | acc     0.51 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/200][399/2499] Loss_D: 1.38629425 (Loss_D_real: 0.69317019 Loss_D_fake: 0.69312406) Loss_G: -0.00000489 Loss_Enh_Dec: -0.00000420\n",
      "| epoch   5 |   400/ 2499 batches | lr 0.000000 | ms/batch 1050.82 | loss  3.80 | ppl    44.58 | acc     0.57 | train_ae_norm     1.00\n",
      "[5/200][499/2499] Loss_D: 1.38629317 (Loss_D_real: 0.69313997 Loss_D_fake: 0.69315314) Loss_G: 0.00000071 Loss_Enh_Dec: 0.00000006\n",
      "| epoch   5 |   500/ 2499 batches | lr 0.000000 | ms/batch 1051.64 | loss  3.78 | ppl    43.69 | acc     0.54 | train_ae_norm     1.00\n",
      "[5/200][599/2499] Loss_D: 1.38629282 (Loss_D_real: 0.69315851 Loss_D_fake: 0.69313431) Loss_G: -0.00000283 Loss_Enh_Dec: -0.00000213\n",
      "| epoch   5 |   600/ 2499 batches | lr 0.000000 | ms/batch 1051.94 | loss  3.72 | ppl    41.30 | acc     0.52 | train_ae_norm     1.00\n",
      "[5/200][699/2499] Loss_D: 1.38629770 (Loss_D_real: 0.69314229 Loss_D_fake: 0.69315547) Loss_G: 0.00000202 Loss_Enh_Dec: 0.00000177\n",
      "| epoch   5 |   700/ 2499 batches | lr 0.000000 | ms/batch 1051.80 | loss  3.68 | ppl    39.61 | acc     0.53 | train_ae_norm     1.00\n",
      "[5/200][799/2499] Loss_D: 1.38629055 (Loss_D_real: 0.69311905 Loss_D_fake: 0.69317150) Loss_G: 0.00000623 Loss_Enh_Dec: 0.00000663\n",
      "| epoch   5 |   800/ 2499 batches | lr 0.000000 | ms/batch 1051.38 | loss  3.66 | ppl    38.83 | acc     0.56 | train_ae_norm     1.00\n",
      "[5/200][899/2499] Loss_D: 1.38628960 (Loss_D_real: 0.69315374 Loss_D_fake: 0.69313586) Loss_G: -0.00000348 Loss_Enh_Dec: -0.00000336\n",
      "| epoch   5 |   900/ 2499 batches | lr 0.000000 | ms/batch 1050.99 | loss  3.61 | ppl    36.99 | acc     0.58 | train_ae_norm     1.00\n",
      "[5/200][999/2499] Loss_D: 1.38629878 (Loss_D_real: 0.69316292 Loss_D_fake: 0.69313586) Loss_G: -0.00000405 Loss_Enh_Dec: -0.00000316\n",
      "| epoch   5 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1050.34 | loss  3.58 | ppl    35.93 | acc     0.58 | train_ae_norm     1.00\n",
      "[5/200][1099/2499] Loss_D: 1.38629603 (Loss_D_real: 0.69314039 Loss_D_fake: 0.69315571) Loss_G: 0.00000155 Loss_Enh_Dec: 0.00000165\n",
      "| epoch   5 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1051.06 | loss  3.52 | ppl    33.80 | acc     0.58 | train_ae_norm     1.00\n",
      "[5/200][1199/2499] Loss_D: 1.38629985 (Loss_D_real: 0.69313133 Loss_D_fake: 0.69316858) Loss_G: 0.00000243 Loss_Enh_Dec: 0.00000233\n",
      "| epoch   5 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1051.89 | loss  3.50 | ppl    33.10 | acc     0.57 | train_ae_norm     1.00\n",
      "[5/200][1299/2499] Loss_D: 1.38629413 (Loss_D_real: 0.69314003 Loss_D_fake: 0.69315404) Loss_G: 0.00000091 Loss_Enh_Dec: 0.00000162\n",
      "| epoch   5 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1052.96 | loss  3.50 | ppl    33.12 | acc     0.56 | train_ae_norm     1.00\n",
      "[5/200][1399/2499] Loss_D: 1.38629270 (Loss_D_real: 0.69312620 Loss_D_fake: 0.69316643) Loss_G: 0.00000527 Loss_Enh_Dec: 0.00000467\n",
      "| epoch   5 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1051.22 | loss  3.42 | ppl    30.45 | acc     0.60 | train_ae_norm     1.00\n",
      "[5/200][1499/2499] Loss_D: 1.38629258 (Loss_D_real: 0.69316858 Loss_D_fake: 0.69312400) Loss_G: -0.00000277 Loss_Enh_Dec: -0.00000248\n",
      "| epoch   5 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1051.54 | loss  3.36 | ppl    28.77 | acc     0.59 | train_ae_norm     1.00\n",
      "[5/200][1599/2499] Loss_D: 1.38629484 (Loss_D_real: 0.69315171 Loss_D_fake: 0.69314307) Loss_G: -0.00000092 Loss_Enh_Dec: -0.00000082\n",
      "| epoch   5 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1051.46 | loss  3.38 | ppl    29.28 | acc     0.60 | train_ae_norm     1.00\n",
      "[5/200][1699/2499] Loss_D: 1.38629353 (Loss_D_real: 0.69314897 Loss_D_fake: 0.69314456) Loss_G: -0.00000000 Loss_Enh_Dec: 0.00000026\n",
      "| epoch   5 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1052.08 | loss  3.25 | ppl    25.80 | acc     0.62 | train_ae_norm     1.00\n",
      "[5/200][1799/2499] Loss_D: 1.38629293 (Loss_D_real: 0.69313622 Loss_D_fake: 0.69315666) Loss_G: 0.00000180 Loss_Enh_Dec: 0.00000206\n",
      "| epoch   5 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1051.38 | loss  3.21 | ppl    24.85 | acc     0.65 | train_ae_norm     1.00\n",
      "[5/200][1899/2499] Loss_D: 1.38629079 (Loss_D_real: 0.69310319 Loss_D_fake: 0.69318759) Loss_G: 0.00000646 Loss_Enh_Dec: 0.00000789\n",
      "| epoch   5 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1053.59 | loss  3.21 | ppl    24.68 | acc     0.58 | train_ae_norm     1.00\n",
      "[5/200][1999/2499] Loss_D: 1.38628745 (Loss_D_real: 0.69314969 Loss_D_fake: 0.69313782) Loss_G: 0.00000039 Loss_Enh_Dec: -0.00000038\n",
      "| epoch   5 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1055.28 | loss  3.24 | ppl    25.41 | acc     0.64 | train_ae_norm     1.00\n",
      "[5/200][2099/2499] Loss_D: 1.38629198 (Loss_D_real: 0.69313639 Loss_D_fake: 0.69315553) Loss_G: 0.00000204 Loss_Enh_Dec: 0.00000231\n",
      "| epoch   5 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1056.11 | loss  3.14 | ppl    23.15 | acc     0.64 | train_ae_norm     1.00\n",
      "[5/200][2199/2499] Loss_D: 1.38629580 (Loss_D_real: 0.69314069 Loss_D_fake: 0.69315505) Loss_G: 0.00000073 Loss_Enh_Dec: 0.00000125\n",
      "| epoch   5 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1055.52 | loss  3.09 | ppl    22.05 | acc     0.65 | train_ae_norm     1.00\n",
      "[5/200][2299/2499] Loss_D: 1.38629472 (Loss_D_real: 0.69316047 Loss_D_fake: 0.69313425) Loss_G: -0.00000252 Loss_Enh_Dec: -0.00000237\n",
      "| epoch   5 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1053.96 | loss  3.04 | ppl    20.96 | acc     0.63 | train_ae_norm     1.00\n",
      "[5/200][2399/2499] Loss_D: 1.38629675 (Loss_D_real: 0.69316554 Loss_D_fake: 0.69313115) Loss_G: -0.00000353 Loss_Enh_Dec: -0.00000349\n",
      "| epoch   5 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1055.92 | loss  3.01 | ppl    20.25 | acc     0.64 | train_ae_norm     1.00\n",
      "[5/200][2499/2499] Loss_D: 1.38629150 (Loss_D_real: 0.69312441 Loss_D_fake: 0.69316709) Loss_G: 0.00000420 Loss_Enh_Dec: 0.00000396\n",
      "| end of epoch   5 | time: 2806.87s | test loss  2.51 | test ppl 12.34 | acc 0.716\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 6 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:20.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:27.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:34.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:01.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:22.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:29.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:36.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:43.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:49.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:56.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:03.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:10.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:17.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:24.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:31.\n",
      "\n",
      "  Average training loss generetor: 0.712\n",
      "  Average training loss discriminator: 0.914\n",
      "  Training epcoh took: 0:02:37\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.515\n",
      "  Test Loss: 1.242\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   6 |     0/ 2499 batches | lr 0.000000 | ms/batch 1576.08 | loss  0.03 | ppl     1.03 | acc     0.65 | train_ae_norm     1.00\n",
      "[6/200][99/2499] Loss_D: 1.38629031 (Loss_D_real: 0.69315523 Loss_D_fake: 0.69313514) Loss_G: -0.00000305 Loss_Enh_Dec: -0.00000316\n",
      "| epoch   6 |   100/ 2499 batches | lr 0.000000 | ms/batch 1051.29 | loss  2.95 | ppl    19.11 | acc     0.64 | train_ae_norm     1.00\n",
      "[6/200][199/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69319028 Loss_D_fake: 0.69310427) Loss_G: -0.00000694 Loss_Enh_Dec: -0.00000687\n",
      "| epoch   6 |   200/ 2499 batches | lr 0.000000 | ms/batch 1048.91 | loss  2.90 | ppl    18.10 | acc     0.67 | train_ae_norm     1.00\n",
      "[6/200][299/2499] Loss_D: 1.38629389 (Loss_D_real: 0.69315356 Loss_D_fake: 0.69314027) Loss_G: -0.00000263 Loss_Enh_Dec: -0.00000206\n",
      "| epoch   6 |   300/ 2499 batches | lr 0.000000 | ms/batch 1050.07 | loss  2.92 | ppl    18.49 | acc     0.64 | train_ae_norm     1.00\n",
      "[6/200][399/2499] Loss_D: 1.38629484 (Loss_D_real: 0.69318390 Loss_D_fake: 0.69311094) Loss_G: -0.00000708 Loss_Enh_Dec: -0.00000703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   6 |   400/ 2499 batches | lr 0.000000 | ms/batch 1048.52 | loss  2.82 | ppl    16.78 | acc     0.69 | train_ae_norm     1.00\n",
      "[6/200][499/2499] Loss_D: 1.38629127 (Loss_D_real: 0.69314909 Loss_D_fake: 0.69314218) Loss_G: -0.00000014 Loss_Enh_Dec: 0.00000046\n",
      "| epoch   6 |   500/ 2499 batches | lr 0.000000 | ms/batch 1050.86 | loss  2.80 | ppl    16.49 | acc     0.67 | train_ae_norm     1.00\n",
      "[6/200][599/2499] Loss_D: 1.38629293 (Loss_D_real: 0.69317877 Loss_D_fake: 0.69311416) Loss_G: -0.00000605 Loss_Enh_Dec: -0.00000625\n",
      "| epoch   6 |   600/ 2499 batches | lr 0.000000 | ms/batch 1049.49 | loss  2.74 | ppl    15.53 | acc     0.67 | train_ae_norm     1.00\n",
      "[6/200][699/2499] Loss_D: 1.38629675 (Loss_D_real: 0.69314307 Loss_D_fake: 0.69315374) Loss_G: 0.00000156 Loss_Enh_Dec: 0.00000199\n",
      "| epoch   6 |   700/ 2499 batches | lr 0.000000 | ms/batch 1051.32 | loss  2.69 | ppl    14.77 | acc     0.66 | train_ae_norm     1.00\n",
      "[6/200][799/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69312584 Loss_D_fake: 0.69316858) Loss_G: 0.00000462 Loss_Enh_Dec: 0.00000457\n",
      "| epoch   6 |   800/ 2499 batches | lr 0.000000 | ms/batch 1051.14 | loss  2.68 | ppl    14.63 | acc     0.67 | train_ae_norm     1.00\n",
      "[6/200][899/2499] Loss_D: 1.38629293 (Loss_D_real: 0.69314754 Loss_D_fake: 0.69314545) Loss_G: -0.00000067 Loss_Enh_Dec: -0.00000065\n",
      "| epoch   6 |   900/ 2499 batches | lr 0.000000 | ms/batch 1050.38 | loss  2.61 | ppl    13.54 | acc     0.71 | train_ae_norm     1.00\n",
      "[6/200][999/2499] Loss_D: 1.38629365 (Loss_D_real: 0.69315767 Loss_D_fake: 0.69313592) Loss_G: -0.00000135 Loss_Enh_Dec: -0.00000125\n",
      "| epoch   6 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1051.28 | loss  2.62 | ppl    13.67 | acc     0.72 | train_ae_norm     1.00\n",
      "[6/200][1099/2499] Loss_D: 1.38629401 (Loss_D_real: 0.69316375 Loss_D_fake: 0.69313025) Loss_G: -0.00000357 Loss_Enh_Dec: -0.00000335\n",
      "| epoch   6 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1051.32 | loss  2.55 | ppl    12.85 | acc     0.72 | train_ae_norm     1.00\n",
      "[6/200][1199/2499] Loss_D: 1.38629556 (Loss_D_real: 0.69316137 Loss_D_fake: 0.69313419) Loss_G: -0.00000365 Loss_Enh_Dec: -0.00000345\n",
      "| epoch   6 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1051.66 | loss  2.51 | ppl    12.29 | acc     0.69 | train_ae_norm     1.00\n",
      "[6/200][1299/2499] Loss_D: 1.38628960 (Loss_D_real: 0.69315839 Loss_D_fake: 0.69313121) Loss_G: -0.00000117 Loss_Enh_Dec: -0.00000141\n",
      "| epoch   6 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1051.53 | loss  2.52 | ppl    12.39 | acc     0.68 | train_ae_norm     1.00\n",
      "[6/200][1399/2499] Loss_D: 1.38629484 (Loss_D_real: 0.69316167 Loss_D_fake: 0.69313323) Loss_G: -0.00000299 Loss_Enh_Dec: -0.00000297\n",
      "| epoch   6 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1053.71 | loss  2.49 | ppl    12.05 | acc     0.74 | train_ae_norm     1.00\n",
      "[6/200][1499/2499] Loss_D: 1.38629377 (Loss_D_real: 0.69311684 Loss_D_fake: 0.69317693) Loss_G: 0.00000523 Loss_Enh_Dec: 0.00000518\n",
      "| epoch   6 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1052.67 | loss  2.42 | ppl    11.26 | acc     0.71 | train_ae_norm     1.00\n",
      "[6/200][1599/2499] Loss_D: 1.38629365 (Loss_D_real: 0.69313991 Loss_D_fake: 0.69315380) Loss_G: -0.00000005 Loss_Enh_Dec: 0.00000060\n",
      "| epoch   6 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1051.01 | loss  2.46 | ppl    11.71 | acc     0.70 | train_ae_norm     1.00\n",
      "[6/200][1699/2499] Loss_D: 1.38630033 (Loss_D_real: 0.69315672 Loss_D_fake: 0.69314361) Loss_G: 0.00000026 Loss_Enh_Dec: 0.00000066\n",
      "| epoch   6 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1050.83 | loss  2.41 | ppl    11.10 | acc     0.73 | train_ae_norm     1.00\n",
      "[6/200][1799/2499] Loss_D: 1.38629246 (Loss_D_real: 0.69316143 Loss_D_fake: 0.69313097) Loss_G: -0.00000436 Loss_Enh_Dec: -0.00000418\n",
      "| epoch   6 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1052.39 | loss  2.36 | ppl    10.57 | acc     0.73 | train_ae_norm     1.00\n",
      "[6/200][1899/2499] Loss_D: 1.38629556 (Loss_D_real: 0.69312614 Loss_D_fake: 0.69316947) Loss_G: 0.00000471 Loss_Enh_Dec: 0.00000388\n",
      "| epoch   6 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1051.45 | loss  2.36 | ppl    10.62 | acc     0.70 | train_ae_norm     1.00\n",
      "[6/200][1999/2499] Loss_D: 1.38629270 (Loss_D_real: 0.69313622 Loss_D_fake: 0.69315642) Loss_G: 0.00000260 Loss_Enh_Dec: 0.00000264\n",
      "| epoch   6 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1050.45 | loss  2.35 | ppl    10.45 | acc     0.75 | train_ae_norm     1.00\n",
      "[6/200][2099/2499] Loss_D: 1.38629270 (Loss_D_real: 0.69309896 Loss_D_fake: 0.69319379) Loss_G: 0.00001076 Loss_Enh_Dec: 0.00001067\n",
      "| epoch   6 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1051.87 | loss  2.27 | ppl     9.69 | acc     0.75 | train_ae_norm     1.00\n",
      "[6/200][2199/2499] Loss_D: 1.38629556 (Loss_D_real: 0.69312942 Loss_D_fake: 0.69316614) Loss_G: 0.00000414 Loss_Enh_Dec: 0.00000478\n",
      "| epoch   6 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1051.66 | loss  2.29 | ppl     9.86 | acc     0.75 | train_ae_norm     1.00\n",
      "[6/200][2299/2499] Loss_D: 1.38629603 (Loss_D_real: 0.69316339 Loss_D_fake: 0.69313264) Loss_G: -0.00000328 Loss_Enh_Dec: -0.00000312\n",
      "| epoch   6 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1050.45 | loss  2.27 | ppl     9.72 | acc     0.74 | train_ae_norm     1.00\n",
      "[6/200][2399/2499] Loss_D: 1.38629901 (Loss_D_real: 0.69321096 Loss_D_fake: 0.69308805) Loss_G: -0.00001199 Loss_Enh_Dec: -0.00001338\n",
      "| epoch   6 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1050.95 | loss  2.22 | ppl     9.23 | acc     0.73 | train_ae_norm     1.00\n",
      "[6/200][2499/2499] Loss_D: 1.38630605 (Loss_D_real: 0.69315076 Loss_D_fake: 0.69315523) Loss_G: -0.00000014 Loss_Enh_Dec: -0.00000002\n",
      "| end of epoch   6 | time: 2804.31s | test loss  1.81 | test ppl  6.12 | acc 0.793\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 7 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:27.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:34.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:22.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:29.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:36.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:43.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:50.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:56.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:03.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:10.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:17.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:24.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:31.\n",
      "\n",
      "  Average training loss generetor: 0.710\n",
      "  Average training loss discriminator: 0.866\n",
      "  Training epcoh took: 0:02:37\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.525\n",
      "  Test Loss: 1.325\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   7 |     0/ 2499 batches | lr 0.000000 | ms/batch 1576.29 | loss  0.02 | ppl     1.02 | acc     0.74 | train_ae_norm     1.00\n",
      "[7/200][99/2499] Loss_D: 1.38628960 (Loss_D_real: 0.69309616 Loss_D_fake: 0.69319344) Loss_G: 0.00001092 Loss_Enh_Dec: 0.00000944\n",
      "| epoch   7 |   100/ 2499 batches | lr 0.000000 | ms/batch 1050.07 | loss  2.20 | ppl     9.00 | acc     0.73 | train_ae_norm     1.00\n",
      "[7/200][199/2499] Loss_D: 1.38629448 (Loss_D_real: 0.69314647 Loss_D_fake: 0.69314802) Loss_G: -0.00000020 Loss_Enh_Dec: 0.00000001\n",
      "| epoch   7 |   200/ 2499 batches | lr 0.000000 | ms/batch 1047.90 | loss  2.13 | ppl     8.38 | acc     0.76 | train_ae_norm     1.00\n",
      "[7/200][299/2499] Loss_D: 1.38629746 (Loss_D_real: 0.69312644 Loss_D_fake: 0.69317096) Loss_G: 0.00000425 Loss_Enh_Dec: 0.00000478\n",
      "| epoch   7 |   300/ 2499 batches | lr 0.000000 | ms/batch 1049.71 | loss  2.14 | ppl     8.54 | acc     0.74 | train_ae_norm     1.00\n",
      "[7/200][399/2499] Loss_D: 1.38629282 (Loss_D_real: 0.69316405 Loss_D_fake: 0.69312876) Loss_G: -0.00000388 Loss_Enh_Dec: -0.00000368\n",
      "| epoch   7 |   400/ 2499 batches | lr 0.000000 | ms/batch 1049.35 | loss  2.09 | ppl     8.09 | acc     0.76 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/200][499/2499] Loss_D: 1.38629222 (Loss_D_real: 0.69313812 Loss_D_fake: 0.69315410) Loss_G: 0.00000177 Loss_Enh_Dec: 0.00000200\n",
      "| epoch   7 |   500/ 2499 batches | lr 0.000000 | ms/batch 1050.14 | loss  2.07 | ppl     7.92 | acc     0.76 | train_ae_norm     1.00\n",
      "[7/200][599/2499] Loss_D: 1.38629353 (Loss_D_real: 0.69315612 Loss_D_fake: 0.69313741) Loss_G: -0.00000170 Loss_Enh_Dec: -0.00000201\n",
      "| epoch   7 |   600/ 2499 batches | lr 0.000000 | ms/batch 1053.34 | loss  2.04 | ppl     7.70 | acc     0.76 | train_ae_norm     1.00\n",
      "[7/200][699/2499] Loss_D: 1.38629532 (Loss_D_real: 0.69315904 Loss_D_fake: 0.69313622) Loss_G: -0.00000198 Loss_Enh_Dec: -0.00000192\n",
      "| epoch   7 |   700/ 2499 batches | lr 0.000000 | ms/batch 1050.93 | loss  1.99 | ppl     7.34 | acc     0.74 | train_ae_norm     1.00\n",
      "[7/200][799/2499] Loss_D: 1.38629425 (Loss_D_real: 0.69312418 Loss_D_fake: 0.69317007) Loss_G: 0.00000449 Loss_Enh_Dec: 0.00000457\n",
      "| epoch   7 |   800/ 2499 batches | lr 0.000000 | ms/batch 1052.64 | loss  1.99 | ppl     7.30 | acc     0.74 | train_ae_norm     1.00\n",
      "[7/200][899/2499] Loss_D: 1.38629138 (Loss_D_real: 0.69316494 Loss_D_fake: 0.69312644) Loss_G: -0.00000591 Loss_Enh_Dec: -0.00000618\n",
      "| epoch   7 |   900/ 2499 batches | lr 0.000000 | ms/batch 1052.21 | loss  1.94 | ppl     6.96 | acc     0.80 | train_ae_norm     1.00\n",
      "[7/200][999/2499] Loss_D: 1.38630188 (Loss_D_real: 0.69313020 Loss_D_fake: 0.69317168) Loss_G: 0.00000920 Loss_Enh_Dec: 0.00001065\n",
      "| epoch   7 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1052.55 | loss  1.96 | ppl     7.10 | acc     0.79 | train_ae_norm     1.00\n",
      "[7/200][1099/2499] Loss_D: 1.38629961 (Loss_D_real: 0.69316572 Loss_D_fake: 0.69313383) Loss_G: -0.00000367 Loss_Enh_Dec: -0.00000302\n",
      "| epoch   7 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1053.26 | loss  1.97 | ppl     7.19 | acc     0.79 | train_ae_norm     1.00\n",
      "[7/200][1199/2499] Loss_D: 1.38630080 (Loss_D_real: 0.69313830 Loss_D_fake: 0.69316256) Loss_G: 0.00000437 Loss_Enh_Dec: 0.00000372\n",
      "| epoch   7 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1050.63 | loss  1.98 | ppl     7.24 | acc     0.73 | train_ae_norm     1.00\n",
      "[7/200][1299/2499] Loss_D: 1.38629675 (Loss_D_real: 0.69313860 Loss_D_fake: 0.69315815) Loss_G: 0.00000095 Loss_Enh_Dec: 0.00000169\n",
      "| epoch   7 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1049.33 | loss  1.98 | ppl     7.27 | acc     0.75 | train_ae_norm     1.00\n",
      "[7/200][1399/2499] Loss_D: 1.38629389 (Loss_D_real: 0.69315159 Loss_D_fake: 0.69314229) Loss_G: -0.00000122 Loss_Enh_Dec: -0.00000042\n",
      "| epoch   7 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1048.04 | loss  1.93 | ppl     6.92 | acc     0.80 | train_ae_norm     1.00\n",
      "[7/200][1499/2499] Loss_D: 1.38630140 (Loss_D_real: 0.69316471 Loss_D_fake: 0.69313669) Loss_G: -0.00000396 Loss_Enh_Dec: -0.00000404\n",
      "| epoch   7 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1048.78 | loss  1.86 | ppl     6.40 | acc     0.79 | train_ae_norm     1.00\n",
      "[7/200][1599/2499] Loss_D: 1.38629365 (Loss_D_real: 0.69314480 Loss_D_fake: 0.69314885) Loss_G: 0.00000018 Loss_Enh_Dec: -0.00000019\n",
      "| epoch   7 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1050.09 | loss  1.90 | ppl     6.66 | acc     0.76 | train_ae_norm     1.00\n",
      "[7/200][1699/2499] Loss_D: 1.38629889 (Loss_D_real: 0.69315624 Loss_D_fake: 0.69314259) Loss_G: -0.00000112 Loss_Enh_Dec: -0.00000037\n",
      "| epoch   7 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1049.85 | loss  1.81 | ppl     6.12 | acc     0.81 | train_ae_norm     1.00\n",
      "[7/200][1799/2499] Loss_D: 1.38629603 (Loss_D_real: 0.69313782 Loss_D_fake: 0.69315827) Loss_G: 0.00000199 Loss_Enh_Dec: 0.00000215\n",
      "| epoch   7 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1051.04 | loss  1.80 | ppl     6.05 | acc     0.82 | train_ae_norm     1.00\n",
      "[7/200][1899/2499] Loss_D: 1.38629317 (Loss_D_real: 0.69312543 Loss_D_fake: 0.69316769) Loss_G: 0.00000459 Loss_Enh_Dec: 0.00000470\n",
      "| epoch   7 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1051.75 | loss  1.78 | ppl     5.93 | acc     0.76 | train_ae_norm     1.00\n",
      "[7/200][1999/2499] Loss_D: 1.38629401 (Loss_D_real: 0.69312042 Loss_D_fake: 0.69317359) Loss_G: 0.00000502 Loss_Enh_Dec: 0.00000508\n",
      "| epoch   7 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1050.99 | loss  1.78 | ppl     5.91 | acc     0.80 | train_ae_norm     1.00\n",
      "[7/200][2099/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69312590 Loss_D_fake: 0.69316840) Loss_G: 0.00000428 Loss_Enh_Dec: 0.00000444\n",
      "| epoch   7 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1053.01 | loss  1.72 | ppl     5.57 | acc     0.82 | train_ae_norm     1.00\n",
      "[7/200][2199/2499] Loss_D: 1.38629496 (Loss_D_real: 0.69316214 Loss_D_fake: 0.69313282) Loss_G: -0.00000244 Loss_Enh_Dec: -0.00000301\n",
      "| epoch   7 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1052.38 | loss  1.69 | ppl     5.39 | acc     0.82 | train_ae_norm     1.00\n",
      "[7/200][2299/2499] Loss_D: 1.38629234 (Loss_D_real: 0.69309366 Loss_D_fake: 0.69319868) Loss_G: 0.00000824 Loss_Enh_Dec: 0.00000913\n",
      "| epoch   7 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1052.49 | loss  1.69 | ppl     5.42 | acc     0.83 | train_ae_norm     1.00\n",
      "[7/200][2399/2499] Loss_D: 1.38629377 (Loss_D_real: 0.69312048 Loss_D_fake: 0.69317329) Loss_G: 0.00000394 Loss_Enh_Dec: 0.00000380\n",
      "| epoch   7 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1053.18 | loss  1.68 | ppl     5.38 | acc     0.79 | train_ae_norm     1.00\n",
      "[7/200][2499/2499] Loss_D: 1.38627887 (Loss_D_real: 0.69312114 Loss_D_fake: 0.69315767) Loss_G: 0.00000703 Loss_Enh_Dec: 0.00000379\n",
      "| end of epoch   7 | time: 2804.24s | test loss  1.38 | test ppl  3.99 | acc 0.841\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 8 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:27.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:34.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:22.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:29.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:36.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:43.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:49.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:56.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:03.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:10.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:17.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:24.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:31.\n",
      "\n",
      "  Average training loss generetor: 0.714\n",
      "  Average training loss discriminator: 0.820\n",
      "  Training epcoh took: 0:02:37\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.525\n",
      "  Test Loss: 1.425\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   8 |     0/ 2499 batches | lr 0.000000 | ms/batch 1576.28 | loss  0.02 | ppl     1.02 | acc     0.79 | train_ae_norm     1.00\n",
      "[8/200][99/2499] Loss_D: 1.38630378 (Loss_D_real: 0.69315386 Loss_D_fake: 0.69314992) Loss_G: -0.00000164 Loss_Enh_Dec: 0.00000085\n",
      "| epoch   8 |   100/ 2499 batches | lr 0.000000 | ms/batch 1051.77 | loss  1.82 | ppl     6.14 | acc     0.76 | train_ae_norm     1.00\n",
      "[8/200][199/2499] Loss_D: 1.38629889 (Loss_D_real: 0.69315547 Loss_D_fake: 0.69314337) Loss_G: 0.00000097 Loss_Enh_Dec: 0.00000232\n",
      "| epoch   8 |   200/ 2499 batches | lr 0.000000 | ms/batch 1052.25 | loss  1.77 | ppl     5.85 | acc     0.80 | train_ae_norm     1.00\n",
      "[8/200][299/2499] Loss_D: 1.38630056 (Loss_D_real: 0.69311637 Loss_D_fake: 0.69318414) Loss_G: 0.00000766 Loss_Enh_Dec: 0.00000861\n",
      "| epoch   8 |   300/ 2499 batches | lr 0.000000 | ms/batch 1052.88 | loss  1.75 | ppl     5.73 | acc     0.78 | train_ae_norm     1.00\n",
      "[8/200][399/2499] Loss_D: 1.38629222 (Loss_D_real: 0.69313598 Loss_D_fake: 0.69315618) Loss_G: 0.00000245 Loss_Enh_Dec: 0.00000338\n",
      "| epoch   8 |   400/ 2499 batches | lr 0.000000 | ms/batch 1054.92 | loss  1.68 | ppl     5.37 | acc     0.80 | train_ae_norm     1.00\n",
      "[8/200][499/2499] Loss_D: 1.38628411 (Loss_D_real: 0.69311512 Loss_D_fake: 0.69316900) Loss_G: 0.00000526 Loss_Enh_Dec: 0.00000415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   8 |   500/ 2499 batches | lr 0.000000 | ms/batch 1054.93 | loss  1.68 | ppl     5.34 | acc     0.79 | train_ae_norm     1.00\n",
      "[8/200][599/2499] Loss_D: 1.38629663 (Loss_D_real: 0.69314575 Loss_D_fake: 0.69315088) Loss_G: 0.00000088 Loss_Enh_Dec: 0.00000140\n",
      "| epoch   8 |   600/ 2499 batches | lr 0.000000 | ms/batch 1056.40 | loss  1.68 | ppl     5.35 | acc     0.82 | train_ae_norm     1.00\n",
      "[8/200][699/2499] Loss_D: 1.38629913 (Loss_D_real: 0.69316530 Loss_D_fake: 0.69313383) Loss_G: 0.00000090 Loss_Enh_Dec: 0.00000199\n",
      "| epoch   8 |   700/ 2499 batches | lr 0.000000 | ms/batch 1056.38 | loss  1.61 | ppl     4.98 | acc     0.78 | train_ae_norm     1.00\n",
      "[8/200][799/2499] Loss_D: 1.38630056 (Loss_D_real: 0.69317305 Loss_D_fake: 0.69312751) Loss_G: -0.00000240 Loss_Enh_Dec: -0.00000194\n",
      "| epoch   8 |   800/ 2499 batches | lr 0.000000 | ms/batch 1054.72 | loss  1.63 | ppl     5.11 | acc     0.78 | train_ae_norm     1.00\n",
      "[8/200][899/2499] Loss_D: 1.38628972 (Loss_D_real: 0.69319940 Loss_D_fake: 0.69309032) Loss_G: -0.00000993 Loss_Enh_Dec: -0.00001067\n",
      "| epoch   8 |   900/ 2499 batches | lr 0.000000 | ms/batch 1056.18 | loss  1.57 | ppl     4.80 | acc     0.83 | train_ae_norm     1.00\n",
      "[8/200][999/2499] Loss_D: 1.38630092 (Loss_D_real: 0.69318312 Loss_D_fake: 0.69311780) Loss_G: -0.00000496 Loss_Enh_Dec: -0.00000379\n",
      "| epoch   8 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1053.82 | loss  1.60 | ppl     4.94 | acc     0.82 | train_ae_norm     1.00\n",
      "[8/200][1099/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69313657 Loss_D_fake: 0.69315803) Loss_G: 0.00000209 Loss_Enh_Dec: 0.00000205\n",
      "| epoch   8 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1054.41 | loss  1.55 | ppl     4.71 | acc     0.84 | train_ae_norm     1.00\n",
      "[8/200][1199/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69310749 Loss_D_fake: 0.69318718) Loss_G: 0.00000628 Loss_Enh_Dec: 0.00000599\n",
      "| epoch   8 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1055.51 | loss  1.51 | ppl     4.53 | acc     0.79 | train_ae_norm     1.00\n",
      "[8/200][1399/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69314831 Loss_D_fake: 0.69314599) Loss_G: 0.00000015 Loss_Enh_Dec: 0.00000007\n",
      "| epoch   8 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1055.01 | loss  1.50 | ppl     4.48 | acc     0.84 | train_ae_norm     1.00\n",
      "[8/200][1499/2499] Loss_D: 1.38629341 (Loss_D_real: 0.69314003 Loss_D_fake: 0.69315338) Loss_G: 0.00000111 Loss_Enh_Dec: 0.00000087\n",
      "| epoch   8 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1055.50 | loss  1.47 | ppl     4.33 | acc     0.84 | train_ae_norm     1.00\n",
      "[8/200][1599/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69313276 Loss_D_fake: 0.69316185) Loss_G: 0.00000295 Loss_Enh_Dec: 0.00000357\n",
      "| epoch   8 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1055.35 | loss  1.48 | ppl     4.41 | acc     0.81 | train_ae_norm     1.00\n",
      "[8/200][1699/2499] Loss_D: 1.38629532 (Loss_D_real: 0.69310188 Loss_D_fake: 0.69319338) Loss_G: 0.00000936 Loss_Enh_Dec: 0.00000921\n",
      "| epoch   8 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1054.53 | loss  1.43 | ppl     4.16 | acc     0.86 | train_ae_norm     1.00\n",
      "[8/200][1799/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69314378 Loss_D_fake: 0.69315088) Loss_G: 0.00000020 Loss_Enh_Dec: 0.00000012\n",
      "| epoch   8 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1055.34 | loss  1.41 | ppl     4.10 | acc     0.86 | train_ae_norm     1.00\n",
      "[8/200][1899/2499] Loss_D: 1.38629377 (Loss_D_real: 0.69315696 Loss_D_fake: 0.69313681) Loss_G: -0.00000252 Loss_Enh_Dec: -0.00000243\n",
      "| epoch   8 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1056.09 | loss  1.41 | ppl     4.09 | acc     0.81 | train_ae_norm     1.00\n",
      "[8/200][1999/2499] Loss_D: 1.38629365 (Loss_D_real: 0.69317287 Loss_D_fake: 0.69312084) Loss_G: -0.00000546 Loss_Enh_Dec: -0.00000566\n",
      "| epoch   8 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1055.97 | loss  1.41 | ppl     4.10 | acc     0.85 | train_ae_norm     1.00\n",
      "[8/200][2099/2499] Loss_D: 1.38629174 (Loss_D_real: 0.69313300 Loss_D_fake: 0.69315875) Loss_G: 0.00000231 Loss_Enh_Dec: 0.00000188\n",
      "| epoch   8 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1056.43 | loss  1.37 | ppl     3.95 | acc     0.85 | train_ae_norm     1.00\n",
      "[8/200][2199/2499] Loss_D: 1.38629651 (Loss_D_real: 0.69316310 Loss_D_fake: 0.69313335) Loss_G: -0.00000354 Loss_Enh_Dec: -0.00000348\n",
      "| epoch   8 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1055.19 | loss  1.34 | ppl     3.84 | acc     0.84 | train_ae_norm     1.00\n",
      "[8/200][2299/2499] Loss_D: 1.38629365 (Loss_D_real: 0.69316816 Loss_D_fake: 0.69312549) Loss_G: -0.00000448 Loss_Enh_Dec: -0.00000459\n",
      "| epoch   8 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1055.46 | loss  1.34 | ppl     3.83 | acc     0.85 | train_ae_norm     1.00\n",
      "[8/200][2399/2499] Loss_D: 1.38629448 (Loss_D_real: 0.69311833 Loss_D_fake: 0.69317615) Loss_G: 0.00000603 Loss_Enh_Dec: 0.00000621\n",
      "| epoch   8 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1054.89 | loss  1.33 | ppl     3.79 | acc     0.83 | train_ae_norm     1.00\n",
      "[8/200][2499/2499] Loss_D: 1.38629425 (Loss_D_real: 0.69311255 Loss_D_fake: 0.69318169) Loss_G: 0.00000724 Loss_Enh_Dec: 0.00000724\n",
      "| end of epoch   8 | time: 2814.05s | test loss  1.09 | test ppl  2.96 | acc 0.874\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 9 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:27.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:34.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:22.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:29.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:36.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:43.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:50.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:57.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:03.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:10.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:17.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:24.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:31.\n",
      "\n",
      "  Average training loss generetor: 0.713\n",
      "  Average training loss discriminator: 0.785\n",
      "  Training epcoh took: 0:02:37\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.530\n",
      "  Test Loss: 1.514\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   9 |     0/ 2499 batches | lr 0.000000 | ms/batch 1579.04 | loss  0.01 | ppl     1.01 | acc     0.83 | train_ae_norm     1.00\n",
      "[9/200][99/2499] Loss_D: 1.38629448 (Loss_D_real: 0.69317836 Loss_D_fake: 0.69311613) Loss_G: -0.00000675 Loss_Enh_Dec: -0.00000669\n",
      "| epoch   9 |   100/ 2499 batches | lr 0.000000 | ms/batch 1055.03 | loss  1.32 | ppl     3.73 | acc     0.82 | train_ae_norm     1.00\n",
      "[9/200][199/2499] Loss_D: 1.38629293 (Loss_D_real: 0.69313574 Loss_D_fake: 0.69315720) Loss_G: 0.00000265 Loss_Enh_Dec: 0.00000238\n",
      "| epoch   9 |   200/ 2499 batches | lr 0.000000 | ms/batch 1054.67 | loss  1.30 | ppl     3.66 | acc     0.84 | train_ae_norm     1.00\n",
      "[9/200][299/2499] Loss_D: 1.38629413 (Loss_D_real: 0.69315684 Loss_D_fake: 0.69313729) Loss_G: -0.00000191 Loss_Enh_Dec: -0.00000196\n",
      "| epoch   9 |   300/ 2499 batches | lr 0.000000 | ms/batch 1053.62 | loss  1.31 | ppl     3.70 | acc     0.82 | train_ae_norm     1.00\n",
      "[9/200][399/2499] Loss_D: 1.38629210 (Loss_D_real: 0.69315183 Loss_D_fake: 0.69314027) Loss_G: -0.00000177 Loss_Enh_Dec: -0.00000163\n",
      "| epoch   9 |   400/ 2499 batches | lr 0.000000 | ms/batch 1053.92 | loss  1.27 | ppl     3.56 | acc     0.85 | train_ae_norm     1.00\n",
      "[9/200][499/2499] Loss_D: 1.38629413 (Loss_D_real: 0.69314402 Loss_D_fake: 0.69315016) Loss_G: 0.00000069 Loss_Enh_Dec: 0.00000069\n",
      "| epoch   9 |   500/ 2499 batches | lr 0.000000 | ms/batch 1052.80 | loss  1.25 | ppl     3.49 | acc     0.84 | train_ae_norm     1.00\n",
      "[9/200][599/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69314378 Loss_D_fake: 0.69315076) Loss_G: 0.00000059 Loss_Enh_Dec: 0.00000060\n",
      "| epoch   9 |   600/ 2499 batches | lr 0.000000 | ms/batch 1051.08 | loss  1.24 | ppl     3.45 | acc     0.87 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/200][699/2499] Loss_D: 1.38629425 (Loss_D_real: 0.69315517 Loss_D_fake: 0.69313908) Loss_G: -0.00000121 Loss_Enh_Dec: -0.00000124\n",
      "| epoch   9 |   700/ 2499 batches | lr 0.000000 | ms/batch 1051.51 | loss  1.21 | ppl     3.35 | acc     0.83 | train_ae_norm     1.00\n",
      "[9/200][799/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69310176 Loss_D_fake: 0.69319266) Loss_G: 0.00000904 Loss_Enh_Dec: 0.00000907\n",
      "| epoch   9 |   800/ 2499 batches | lr 0.000000 | ms/batch 1051.81 | loss  1.23 | ppl     3.42 | acc     0.83 | train_ae_norm     1.00\n",
      "[9/200][899/2499] Loss_D: 1.38628817 (Loss_D_real: 0.69312620 Loss_D_fake: 0.69316196) Loss_G: 0.00000205 Loss_Enh_Dec: 0.00000070\n",
      "| epoch   9 |   900/ 2499 batches | lr 0.000000 | ms/batch 1053.02 | loss  1.18 | ppl     3.26 | acc     0.89 | train_ae_norm     1.00\n",
      "[9/200][999/2499] Loss_D: 1.38629413 (Loss_D_real: 0.69313955 Loss_D_fake: 0.69315457) Loss_G: 0.00000157 Loss_Enh_Dec: 0.00000153\n",
      "| epoch   9 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1053.17 | loss  1.25 | ppl     3.49 | acc     0.86 | train_ae_norm     1.00\n",
      "[9/200][1099/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69317073 Loss_D_fake: 0.69312394) Loss_G: -0.00000466 Loss_Enh_Dec: -0.00000462\n",
      "| epoch   9 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1053.25 | loss  1.21 | ppl     3.36 | acc     0.87 | train_ae_norm     1.00\n",
      "[9/200][1199/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69316280 Loss_D_fake: 0.69313157) Loss_G: -0.00000309 Loss_Enh_Dec: -0.00000317\n",
      "| epoch   9 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1054.65 | loss  1.18 | ppl     3.24 | acc     0.84 | train_ae_norm     1.00\n",
      "[9/200][1299/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69315338 Loss_D_fake: 0.69314098) Loss_G: -0.00000136 Loss_Enh_Dec: -0.00000136\n",
      "| epoch   9 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1054.74 | loss  1.19 | ppl     3.29 | acc     0.83 | train_ae_norm     1.00\n",
      "[9/200][1399/2499] Loss_D: 1.38629174 (Loss_D_real: 0.69315779 Loss_D_fake: 0.69313395) Loss_G: -0.00000135 Loss_Enh_Dec: -0.00000226\n",
      "| epoch   9 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1055.38 | loss  1.18 | ppl     3.26 | acc     0.88 | train_ae_norm     1.00\n",
      "[9/200][1499/2499] Loss_D: 1.38629389 (Loss_D_real: 0.69312894 Loss_D_fake: 0.69316500) Loss_G: 0.00000412 Loss_Enh_Dec: 0.00000395\n",
      "| epoch   9 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1053.85 | loss  1.19 | ppl     3.29 | acc     0.86 | train_ae_norm     1.00\n",
      "[9/200][1599/2499] Loss_D: 1.38629293 (Loss_D_real: 0.69314164 Loss_D_fake: 0.69315135) Loss_G: 0.00000055 Loss_Enh_Dec: 0.00000036\n",
      "| epoch   9 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1051.75 | loss  1.18 | ppl     3.26 | acc     0.84 | train_ae_norm     1.00\n",
      "[9/200][1699/2499] Loss_D: 1.38629389 (Loss_D_real: 0.69311494 Loss_D_fake: 0.69317889) Loss_G: 0.00000454 Loss_Enh_Dec: 0.00000520\n",
      "| epoch   9 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1050.62 | loss  1.14 | ppl     3.12 | acc     0.88 | train_ae_norm     1.00\n",
      "[9/200][1799/2499] Loss_D: 1.38629282 (Loss_D_real: 0.69318402 Loss_D_fake: 0.69310880) Loss_G: -0.00000747 Loss_Enh_Dec: -0.00000787\n",
      "| epoch   9 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1051.06 | loss  1.12 | ppl     3.07 | acc     0.89 | train_ae_norm     1.00\n",
      "[9/200][1899/2499] Loss_D: 1.38629568 (Loss_D_real: 0.69313830 Loss_D_fake: 0.69315737) Loss_G: 0.00000326 Loss_Enh_Dec: 0.00000340\n",
      "| epoch   9 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1052.11 | loss  1.16 | ppl     3.18 | acc     0.86 | train_ae_norm     1.00\n",
      "[9/200][1999/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69318390 Loss_D_fake: 0.69311064) Loss_G: -0.00000825 Loss_Enh_Dec: -0.00000829\n",
      "| epoch   9 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1055.25 | loss  1.15 | ppl     3.16 | acc     0.86 | train_ae_norm     1.00\n",
      "[9/200][2099/2499] Loss_D: 1.38629639 (Loss_D_real: 0.69314730 Loss_D_fake: 0.69314909) Loss_G: 0.00000154 Loss_Enh_Dec: 0.00000093\n",
      "| epoch   9 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1054.70 | loss  1.10 | ppl     3.01 | acc     0.89 | train_ae_norm     1.00\n",
      "[9/200][2199/2499] Loss_D: 1.38629425 (Loss_D_real: 0.69313043 Loss_D_fake: 0.69316381) Loss_G: 0.00000339 Loss_Enh_Dec: 0.00000347\n",
      "| epoch   9 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1055.06 | loss  1.10 | ppl     2.99 | acc     0.88 | train_ae_norm     1.00\n",
      "[9/200][2299/2499] Loss_D: 1.38629651 (Loss_D_real: 0.69319749 Loss_D_fake: 0.69309896) Loss_G: -0.00001234 Loss_Enh_Dec: -0.00001108\n",
      "| epoch   9 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1052.24 | loss  1.09 | ppl     2.99 | acc     0.88 | train_ae_norm     1.00\n",
      "[9/200][2399/2499] Loss_D: 1.38629878 (Loss_D_real: 0.69317734 Loss_D_fake: 0.69312143) Loss_G: -0.00000418 Loss_Enh_Dec: -0.00000430\n",
      "| epoch   9 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1052.96 | loss  1.12 | ppl     3.06 | acc     0.85 | train_ae_norm     1.00\n",
      "[9/200][2499/2499] Loss_D: 1.38629329 (Loss_D_real: 0.69315422 Loss_D_fake: 0.69313908) Loss_G: -0.00000119 Loss_Enh_Dec: -0.00000126\n",
      "| end of epoch   9 | time: 2810.34s | test loss  0.92 | test ppl  2.52 | acc 0.892\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 10 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:09.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:16.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:23.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:30.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:37.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:44.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:50.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:57.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:04.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:18.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:25.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:32.\n",
      "\n",
      "  Average training loss generetor: 0.714\n",
      "  Average training loss discriminator: 0.768\n",
      "  Training epcoh took: 0:02:38\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.510\n",
      "  Test Loss: 1.678\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  10 |     0/ 2499 batches | lr 0.000000 | ms/batch 1590.16 | loss  0.01 | ppl     1.01 | acc     0.85 | train_ae_norm     1.00\n",
      "[10/200][99/2499] Loss_D: 1.38629293 (Loss_D_real: 0.69314784 Loss_D_fake: 0.69314516) Loss_G: -0.00000125 Loss_Enh_Dec: -0.00000135\n",
      "| epoch  10 |   100/ 2499 batches | lr 0.000000 | ms/batch 1055.42 | loss  1.14 | ppl     3.12 | acc     0.83 | train_ae_norm     1.00\n",
      "[10/200][199/2499] Loss_D: 1.38629377 (Loss_D_real: 0.69319808 Loss_D_fake: 0.69309568) Loss_G: -0.00001005 Loss_Enh_Dec: -0.00001021\n",
      "| epoch  10 |   200/ 2499 batches | lr 0.000000 | ms/batch 1054.75 | loss  1.11 | ppl     3.02 | acc     0.87 | train_ae_norm     1.00\n",
      "[10/200][299/2499] Loss_D: 1.38630080 (Loss_D_real: 0.69309324 Loss_D_fake: 0.69320762) Loss_G: 0.00000802 Loss_Enh_Dec: 0.00000792\n",
      "| epoch  10 |   300/ 2499 batches | lr 0.000000 | ms/batch 1056.35 | loss  1.10 | ppl     3.00 | acc     0.84 | train_ae_norm     1.00\n",
      "[10/200][399/2499] Loss_D: 1.38629496 (Loss_D_real: 0.69313586 Loss_D_fake: 0.69315910) Loss_G: 0.00000227 Loss_Enh_Dec: 0.00000247\n",
      "| epoch  10 |   400/ 2499 batches | lr 0.000000 | ms/batch 1054.65 | loss  1.08 | ppl     2.93 | acc     0.88 | train_ae_norm     1.00\n",
      "[10/200][499/2499] Loss_D: 1.38629472 (Loss_D_real: 0.69315803 Loss_D_fake: 0.69313669) Loss_G: -0.00000206 Loss_Enh_Dec: -0.00000209\n",
      "| epoch  10 |   500/ 2499 batches | lr 0.000000 | ms/batch 1055.46 | loss  1.09 | ppl     2.96 | acc     0.87 | train_ae_norm     1.00\n",
      "[10/200][599/2499] Loss_D: 1.38630748 (Loss_D_real: 0.69314927 Loss_D_fake: 0.69315815) Loss_G: -0.00000026 Loss_Enh_Dec: 0.00000061\n",
      "| epoch  10 |   600/ 2499 batches | lr 0.000000 | ms/batch 1055.63 | loss  1.06 | ppl     2.88 | acc     0.89 | train_ae_norm     1.00\n",
      "[10/200][699/2499] Loss_D: 1.38629627 (Loss_D_real: 0.69308233 Loss_D_fake: 0.69321388) Loss_G: 0.00001255 Loss_Enh_Dec: 0.00001294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  10 |   700/ 2499 batches | lr 0.000000 | ms/batch 1056.07 | loss  1.05 | ppl     2.85 | acc     0.84 | train_ae_norm     1.00\n",
      "[10/200][799/2499] Loss_D: 1.38629377 (Loss_D_real: 0.69321620 Loss_D_fake: 0.69307756) Loss_G: -0.00001311 Loss_Enh_Dec: -0.00001230\n",
      "| epoch  10 |   800/ 2499 batches | lr 0.000000 | ms/batch 1055.46 | loss  1.06 | ppl     2.88 | acc     0.84 | train_ae_norm     1.00\n",
      "[10/200][899/2499] Loss_D: 1.38629138 (Loss_D_real: 0.69312948 Loss_D_fake: 0.69316190) Loss_G: 0.00000279 Loss_Enh_Dec: 0.00000285\n",
      "| epoch  10 |   900/ 2499 batches | lr 0.000000 | ms/batch 1053.11 | loss  1.03 | ppl     2.81 | acc     0.89 | train_ae_norm     1.00\n",
      "[10/200][999/2499] Loss_D: 1.38629270 (Loss_D_real: 0.69315171 Loss_D_fake: 0.69314092) Loss_G: -0.00000127 Loss_Enh_Dec: -0.00000123\n",
      "| epoch  10 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1047.67 | loss  1.10 | ppl     3.00 | acc     0.87 | train_ae_norm     1.00\n",
      "[10/200][1099/2499] Loss_D: 1.38628602 (Loss_D_real: 0.69312733 Loss_D_fake: 0.69315875) Loss_G: 0.00000595 Loss_Enh_Dec: 0.00000403\n",
      "| epoch  10 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1049.41 | loss  1.04 | ppl     2.82 | acc     0.89 | train_ae_norm     1.00\n",
      "[10/200][1199/2499] Loss_D: 1.38630116 (Loss_D_real: 0.69318473 Loss_D_fake: 0.69311643) Loss_G: -0.00000434 Loss_Enh_Dec: -0.00000354\n",
      "| epoch  10 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1051.12 | loss  1.06 | ppl     2.89 | acc     0.84 | train_ae_norm     1.00\n",
      "[10/200][1299/2499] Loss_D: 1.38630486 (Loss_D_real: 0.69306087 Loss_D_fake: 0.69324398) Loss_G: 0.00001460 Loss_Enh_Dec: 0.00001383\n",
      "| epoch  10 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1050.48 | loss  1.10 | ppl     3.01 | acc     0.83 | train_ae_norm     1.00\n",
      "[10/200][1399/2499] Loss_D: 1.38630366 (Loss_D_real: 0.69319069 Loss_D_fake: 0.69311303) Loss_G: -0.00000365 Loss_Enh_Dec: -0.00000352\n",
      "| epoch  10 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1052.76 | loss  1.15 | ppl     3.15 | acc     0.87 | train_ae_norm     1.00\n",
      "[10/200][1499/2499] Loss_D: 1.38630664 (Loss_D_real: 0.69313562 Loss_D_fake: 0.69317102) Loss_G: 0.00000538 Loss_Enh_Dec: 0.00000435\n",
      "| epoch  10 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1051.65 | loss  1.22 | ppl     3.37 | acc     0.83 | train_ae_norm     1.00\n",
      "[10/200][1599/2499] Loss_D: 1.38630617 (Loss_D_real: 0.69325775 Loss_D_fake: 0.69304842) Loss_G: 0.00001016 Loss_Enh_Dec: 0.00001685\n",
      "| epoch  10 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1048.38 | loss  1.45 | ppl     4.25 | acc     0.81 | train_ae_norm     1.00\n",
      "[10/200][1699/2499] Loss_D: 1.38631892 (Loss_D_real: 0.69311655 Loss_D_fake: 0.69320238) Loss_G: -0.00000104 Loss_Enh_Dec: 0.00000594\n",
      "| epoch  10 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1048.62 | loss  1.40 | ppl     4.05 | acc     0.85 | train_ae_norm     1.00\n",
      "[10/200][1799/2499] Loss_D: 1.38633811 (Loss_D_real: 0.69316739 Loss_D_fake: 0.69317073) Loss_G: -0.00000570 Loss_Enh_Dec: -0.00000428\n",
      "| epoch  10 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1049.22 | loss  1.43 | ppl     4.16 | acc     0.85 | train_ae_norm     1.00\n",
      "[10/200][1899/2499] Loss_D: 1.38626456 (Loss_D_real: 0.69315404 Loss_D_fake: 0.69311047) Loss_G: 0.00000692 Loss_Enh_Dec: 0.00000575\n",
      "| epoch  10 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1052.39 | loss  1.42 | ppl     4.12 | acc     0.79 | train_ae_norm     1.00\n",
      "[10/200][1999/2499] Loss_D: 1.38631964 (Loss_D_real: 0.69316864 Loss_D_fake: 0.69315100) Loss_G: -0.00000850 Loss_Enh_Dec: 0.00000080\n",
      "| epoch  10 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1050.38 | loss  1.38 | ppl     3.96 | acc     0.85 | train_ae_norm     1.00\n",
      "[10/200][2099/2499] Loss_D: 1.38631356 (Loss_D_real: 0.69315857 Loss_D_fake: 0.69315499) Loss_G: 0.00000133 Loss_Enh_Dec: 0.00000201\n",
      "| epoch  10 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1049.81 | loss  1.25 | ppl     3.48 | acc     0.86 | train_ae_norm     1.00\n",
      "[10/200][2199/2499] Loss_D: 1.38628650 (Loss_D_real: 0.69311988 Loss_D_fake: 0.69316661) Loss_G: -0.00000100 Loss_Enh_Dec: 0.00000230\n",
      "| epoch  10 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1050.19 | loss  1.18 | ppl     3.26 | acc     0.86 | train_ae_norm     1.00\n",
      "[10/200][2299/2499] Loss_D: 1.38631701 (Loss_D_real: 0.69315195 Loss_D_fake: 0.69316500) Loss_G: -0.00000861 Loss_Enh_Dec: -0.00000204\n",
      "| epoch  10 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1052.21 | loss  1.15 | ppl     3.14 | acc     0.87 | train_ae_norm     1.00\n",
      "[10/200][2399/2499] Loss_D: 1.38632393 (Loss_D_real: 0.69318426 Loss_D_fake: 0.69313967) Loss_G: 0.00000145 Loss_Enh_Dec: 0.00000921\n",
      "| epoch  10 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1051.21 | loss  1.16 | ppl     3.17 | acc     0.83 | train_ae_norm     1.00\n",
      "[10/200][2499/2499] Loss_D: 1.38629663 (Loss_D_real: 0.69314498 Loss_D_fake: 0.69315165) Loss_G: -0.00000029 Loss_Enh_Dec: 0.00000403\n",
      "| end of epoch  10 | time: 2808.43s | test loss  0.92 | test ppl  2.51 | acc 0.889\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 11 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:09.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:16.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:23.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:30.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:37.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:44.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:51.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:58.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:04.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:18.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:25.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:32.\n",
      "\n",
      "  Average training loss generetor: 0.711\n",
      "  Average training loss discriminator: 0.762\n",
      "  Training epcoh took: 0:02:39\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.515\n",
      "  Test Loss: 1.765\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  11 |     0/ 2499 batches | lr 0.000000 | ms/batch 1592.46 | loss  0.01 | ppl     1.01 | acc     0.85 | train_ae_norm     1.00\n",
      "[11/200][99/2499] Loss_D: 1.38627887 (Loss_D_real: 0.69315052 Loss_D_fake: 0.69312829) Loss_G: -0.00000110 Loss_Enh_Dec: 0.00000403\n",
      "| epoch  11 |   100/ 2499 batches | lr 0.000000 | ms/batch 1049.57 | loss  1.13 | ppl     3.08 | acc     0.82 | train_ae_norm     1.00\n",
      "[11/200][299/2499] Loss_D: 1.38627267 (Loss_D_real: 0.69312018 Loss_D_fake: 0.69315255) Loss_G: -0.00000627 Loss_Enh_Dec: -0.00000381\n",
      "| epoch  11 |   300/ 2499 batches | lr 0.000000 | ms/batch 1048.04 | loss  1.14 | ppl     3.12 | acc     0.84 | train_ae_norm     1.00\n",
      "[11/200][399/2499] Loss_D: 1.38633418 (Loss_D_real: 0.69314945 Loss_D_fake: 0.69318479) Loss_G: -0.00000237 Loss_Enh_Dec: -0.00000163\n",
      "| epoch  11 |   400/ 2499 batches | lr 0.000000 | ms/batch 1047.74 | loss  1.14 | ppl     3.14 | acc     0.86 | train_ae_norm     1.00\n",
      "[11/200][499/2499] Loss_D: 1.38633060 (Loss_D_real: 0.69317329 Loss_D_fake: 0.69315737) Loss_G: -0.00000062 Loss_Enh_Dec: -0.00000030\n",
      "| epoch  11 |   500/ 2499 batches | lr 0.000000 | ms/batch 1043.53 | loss  1.14 | ppl     3.13 | acc     0.86 | train_ae_norm     1.00\n",
      "[11/200][599/2499] Loss_D: 1.38634181 (Loss_D_real: 0.69319761 Loss_D_fake: 0.69314426) Loss_G: -0.00000093 Loss_Enh_Dec: -0.00000074\n",
      "| epoch  11 |   600/ 2499 batches | lr 0.000000 | ms/batch 1045.44 | loss  1.11 | ppl     3.02 | acc     0.88 | train_ae_norm     1.00\n",
      "[11/200][699/2499] Loss_D: 1.38630843 (Loss_D_real: 0.69314718 Loss_D_fake: 0.69316119) Loss_G: 0.00000565 Loss_Enh_Dec: 0.00000167\n",
      "| epoch  11 |   700/ 2499 batches | lr 0.000000 | ms/batch 1048.07 | loss  1.09 | ppl     2.99 | acc     0.84 | train_ae_norm     1.00\n",
      "[11/200][799/2499] Loss_D: 1.38629067 (Loss_D_real: 0.69314516 Loss_D_fake: 0.69314551) Loss_G: 0.00000221 Loss_Enh_Dec: -0.00000068\n",
      "| epoch  11 |   800/ 2499 batches | lr 0.000000 | ms/batch 1046.10 | loss  1.10 | ppl     2.99 | acc     0.85 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/200][899/2499] Loss_D: 1.38629961 (Loss_D_real: 0.69313860 Loss_D_fake: 0.69316095) Loss_G: -0.00000142 Loss_Enh_Dec: 0.00000231\n",
      "| epoch  11 |   900/ 2499 batches | lr 0.000000 | ms/batch 1046.25 | loss  1.04 | ppl     2.83 | acc     0.89 | train_ae_norm     1.00\n",
      "[11/200][999/2499] Loss_D: 1.38628376 (Loss_D_real: 0.69313180 Loss_D_fake: 0.69315195) Loss_G: 0.00000236 Loss_Enh_Dec: -0.00000142\n",
      "| epoch  11 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1044.26 | loss  1.07 | ppl     2.90 | acc     0.87 | train_ae_norm     1.00\n",
      "[11/200][1099/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69314682 Loss_D_fake: 0.69314760) Loss_G: 0.00000238 Loss_Enh_Dec: 0.00000225\n",
      "| epoch  11 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1045.03 | loss  1.04 | ppl     2.84 | acc     0.89 | train_ae_norm     1.00\n",
      "[11/200][1199/2499] Loss_D: 1.38629484 (Loss_D_real: 0.69314945 Loss_D_fake: 0.69314545) Loss_G: 0.00000182 Loss_Enh_Dec: 0.00000288\n",
      "| epoch  11 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1045.78 | loss  1.01 | ppl     2.76 | acc     0.85 | train_ae_norm     1.00\n",
      "[11/200][1299/2499] Loss_D: 1.38630390 (Loss_D_real: 0.69314170 Loss_D_fake: 0.69316220) Loss_G: 0.00000125 Loss_Enh_Dec: 0.00000170\n",
      "| epoch  11 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1044.87 | loss  1.03 | ppl     2.80 | acc     0.84 | train_ae_norm     1.00\n",
      "[11/200][1399/2499] Loss_D: 1.38629365 (Loss_D_real: 0.69314241 Loss_D_fake: 0.69315118) Loss_G: 0.00000151 Loss_Enh_Dec: 0.00000172\n",
      "| epoch  11 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1043.71 | loss  1.02 | ppl     2.77 | acc     0.88 | train_ae_norm     1.00\n",
      "[11/200][1499/2499] Loss_D: 1.38629901 (Loss_D_real: 0.69314826 Loss_D_fake: 0.69315076) Loss_G: -0.00000006 Loss_Enh_Dec: 0.00000046\n",
      "| epoch  11 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1043.95 | loss  0.99 | ppl     2.70 | acc     0.88 | train_ae_norm     1.00\n",
      "[11/200][1599/2499] Loss_D: 1.38631034 (Loss_D_real: 0.69317138 Loss_D_fake: 0.69313896) Loss_G: -0.00000045 Loss_Enh_Dec: -0.00000136\n",
      "| epoch  11 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1044.03 | loss  1.01 | ppl     2.73 | acc     0.85 | train_ae_norm     1.00\n",
      "[11/200][1699/2499] Loss_D: 1.38629997 (Loss_D_real: 0.69309062 Loss_D_fake: 0.69320935) Loss_G: 0.00000557 Loss_Enh_Dec: 0.00000353\n",
      "| epoch  11 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1043.60 | loss  0.98 | ppl     2.66 | acc     0.89 | train_ae_norm     1.00\n",
      "[11/200][1799/2499] Loss_D: 1.38630271 (Loss_D_real: 0.69315290 Loss_D_fake: 0.69314975) Loss_G: 0.00000164 Loss_Enh_Dec: 0.00000253\n",
      "| epoch  11 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1045.63 | loss  0.98 | ppl     2.68 | acc     0.91 | train_ae_norm     1.00\n",
      "[11/200][1899/2499] Loss_D: 1.38629138 (Loss_D_real: 0.69313836 Loss_D_fake: 0.69315302) Loss_G: 0.00000193 Loss_Enh_Dec: -0.00000159\n",
      "| epoch  11 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1052.13 | loss  0.98 | ppl     2.67 | acc     0.86 | train_ae_norm     1.00\n",
      "[11/200][1999/2499] Loss_D: 1.38629222 (Loss_D_real: 0.69313323 Loss_D_fake: 0.69315904) Loss_G: 0.00000168 Loss_Enh_Dec: 0.00000234\n",
      "| epoch  11 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1053.44 | loss  0.98 | ppl     2.67 | acc     0.88 | train_ae_norm     1.00\n",
      "[11/200][2099/2499] Loss_D: 1.38629222 (Loss_D_real: 0.69313002 Loss_D_fake: 0.69316214) Loss_G: 0.00000108 Loss_Enh_Dec: 0.00000187\n",
      "| epoch  11 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1052.52 | loss  0.95 | ppl     2.58 | acc     0.89 | train_ae_norm     1.00\n",
      "[11/200][2199/2499] Loss_D: 1.38629746 (Loss_D_real: 0.69315004 Loss_D_fake: 0.69314736) Loss_G: 0.00000100 Loss_Enh_Dec: -0.00000009\n",
      "| epoch  11 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1053.19 | loss  0.93 | ppl     2.53 | acc     0.90 | train_ae_norm     1.00\n",
      "[11/200][2299/2499] Loss_D: 1.38629270 (Loss_D_real: 0.69315004 Loss_D_fake: 0.69314259) Loss_G: -0.00000026 Loss_Enh_Dec: -0.00000057\n",
      "| epoch  11 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1052.51 | loss  0.92 | ppl     2.51 | acc     0.91 | train_ae_norm     1.00\n",
      "[11/200][2399/2499] Loss_D: 1.38629246 (Loss_D_real: 0.69313538 Loss_D_fake: 0.69315714) Loss_G: 0.00000237 Loss_Enh_Dec: 0.00000207\n",
      "| epoch  11 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1052.44 | loss  0.91 | ppl     2.49 | acc     0.86 | train_ae_norm     1.00\n",
      "[11/200][2499/2499] Loss_D: 1.38629186 (Loss_D_real: 0.69314814 Loss_D_fake: 0.69314373) Loss_G: -0.00000061 Loss_Enh_Dec: -0.00000066\n",
      "| end of epoch  11 | time: 2797.81s | test loss  0.79 | test ppl  2.20 | acc 0.908\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 12 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:09.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:16.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:23.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:30.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:37.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:44.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:51.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:58.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:05.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:19.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:25.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:32.\n",
      "\n",
      "  Average training loss generetor: 0.712\n",
      "  Average training loss discriminator: 0.749\n",
      "  Training epcoh took: 0:02:39\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.515\n",
      "  Test Loss: 1.912\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  12 |     0/ 2499 batches | lr 0.000000 | ms/batch 1594.77 | loss  0.01 | ppl     1.01 | acc     0.88 | train_ae_norm     1.00\n",
      "[12/200][99/2499] Loss_D: 1.38629222 (Loss_D_real: 0.69314468 Loss_D_fake: 0.69314760) Loss_G: 0.00000010 Loss_Enh_Dec: -0.00000016\n",
      "| epoch  12 |   100/ 2499 batches | lr 0.000000 | ms/batch 1052.95 | loss  0.91 | ppl     2.48 | acc     0.86 | train_ae_norm     1.00\n",
      "[12/200][199/2499] Loss_D: 1.38629484 (Loss_D_real: 0.69311327 Loss_D_fake: 0.69318163) Loss_G: 0.00000461 Loss_Enh_Dec: 0.00000491\n",
      "| epoch  12 |   200/ 2499 batches | lr 0.000000 | ms/batch 1051.69 | loss  0.89 | ppl     2.44 | acc     0.90 | train_ae_norm     1.00\n",
      "[12/200][299/2499] Loss_D: 1.38629603 (Loss_D_real: 0.69312876 Loss_D_fake: 0.69316721) Loss_G: 0.00000390 Loss_Enh_Dec: 0.00000330\n",
      "| epoch  12 |   300/ 2499 batches | lr 0.000000 | ms/batch 1052.46 | loss  0.90 | ppl     2.47 | acc     0.87 | train_ae_norm     1.00\n",
      "[12/200][399/2499] Loss_D: 1.38629651 (Loss_D_real: 0.69316739 Loss_D_fake: 0.69312906) Loss_G: -0.00000304 Loss_Enh_Dec: -0.00000334\n",
      "| epoch  12 |   400/ 2499 batches | lr 0.000000 | ms/batch 1052.09 | loss  0.88 | ppl     2.40 | acc     0.90 | train_ae_norm     1.00\n",
      "[12/200][499/2499] Loss_D: 1.38629818 (Loss_D_real: 0.69311452 Loss_D_fake: 0.69318360) Loss_G: 0.00000734 Loss_Enh_Dec: 0.00000675\n",
      "| epoch  12 |   500/ 2499 batches | lr 0.000000 | ms/batch 1051.67 | loss  0.85 | ppl     2.35 | acc     0.89 | train_ae_norm     1.00\n",
      "[12/200][599/2499] Loss_D: 1.38629603 (Loss_D_real: 0.69309497 Loss_D_fake: 0.69320101) Loss_G: 0.00001195 Loss_Enh_Dec: 0.00000663\n",
      "| epoch  12 |   600/ 2499 batches | lr 0.000000 | ms/batch 1052.70 | loss  0.86 | ppl     2.36 | acc     0.90 | train_ae_norm     1.00\n",
      "[12/200][699/2499] Loss_D: 1.38630342 (Loss_D_real: 0.69317448 Loss_D_fake: 0.69312900) Loss_G: -0.00000489 Loss_Enh_Dec: -0.00000633\n",
      "| epoch  12 |   700/ 2499 batches | lr 0.000000 | ms/batch 1052.41 | loss  0.85 | ppl     2.35 | acc     0.88 | train_ae_norm     1.00\n",
      "[12/200][799/2499] Loss_D: 1.38629282 (Loss_D_real: 0.69311321 Loss_D_fake: 0.69317961) Loss_G: 0.00000853 Loss_Enh_Dec: 0.00000646\n",
      "| epoch  12 |   800/ 2499 batches | lr 0.000000 | ms/batch 1053.12 | loss  0.87 | ppl     2.38 | acc     0.88 | train_ae_norm     1.00\n",
      "[12/200][899/2499] Loss_D: 1.38629222 (Loss_D_real: 0.69311941 Loss_D_fake: 0.69317287) Loss_G: 0.00000527 Loss_Enh_Dec: 0.00000276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  12 |   900/ 2499 batches | lr 0.000000 | ms/batch 1052.01 | loss  0.84 | ppl     2.31 | acc     0.91 | train_ae_norm     1.00\n",
      "[12/200][999/2499] Loss_D: 1.38631499 (Loss_D_real: 0.69315362 Loss_D_fake: 0.69316137) Loss_G: -0.00000301 Loss_Enh_Dec: -0.00000210\n",
      "| epoch  12 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1047.57 | loss  0.87 | ppl     2.38 | acc     0.90 | train_ae_norm     1.00\n",
      "[12/200][1099/2499] Loss_D: 1.38630652 (Loss_D_real: 0.69313180 Loss_D_fake: 0.69317472) Loss_G: 0.00000480 Loss_Enh_Dec: 0.00000315\n",
      "| epoch  12 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1046.64 | loss  0.84 | ppl     2.31 | acc     0.91 | train_ae_norm     1.00\n",
      "[12/200][1199/2499] Loss_D: 1.38630033 (Loss_D_real: 0.69314021 Loss_D_fake: 0.69316006) Loss_G: 0.00000255 Loss_Enh_Dec: 0.00000283\n",
      "| epoch  12 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1047.11 | loss  0.81 | ppl     2.26 | acc     0.87 | train_ae_norm     1.00\n",
      "[12/200][1299/2499] Loss_D: 1.38630176 (Loss_D_real: 0.69318694 Loss_D_fake: 0.69311476) Loss_G: -0.00000006 Loss_Enh_Dec: -0.00000090\n",
      "| epoch  12 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1046.69 | loss  0.85 | ppl     2.34 | acc     0.86 | train_ae_norm     1.00\n",
      "[12/200][1399/2499] Loss_D: 1.38629150 (Loss_D_real: 0.69314617 Loss_D_fake: 0.69314528) Loss_G: 0.00000164 Loss_Enh_Dec: 0.00000169\n",
      "| epoch  12 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1045.08 | loss  0.85 | ppl     2.35 | acc     0.91 | train_ae_norm     1.00\n",
      "[12/200][1499/2499] Loss_D: 1.38629341 (Loss_D_real: 0.69318163 Loss_D_fake: 0.69311178) Loss_G: -0.00000735 Loss_Enh_Dec: -0.00000707\n",
      "| epoch  12 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1045.71 | loss  0.82 | ppl     2.28 | acc     0.90 | train_ae_norm     1.00\n",
      "[12/200][1599/2499] Loss_D: 1.38629651 (Loss_D_real: 0.69313782 Loss_D_fake: 0.69315875) Loss_G: 0.00000238 Loss_Enh_Dec: 0.00000237\n",
      "| epoch  12 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1044.83 | loss  0.84 | ppl     2.31 | acc     0.88 | train_ae_norm     1.00\n",
      "[12/200][1699/2499] Loss_D: 1.38629317 (Loss_D_real: 0.69314957 Loss_D_fake: 0.69314361) Loss_G: -0.00000018 Loss_Enh_Dec: -0.00000077\n",
      "| epoch  12 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1044.33 | loss  0.81 | ppl     2.25 | acc     0.91 | train_ae_norm     1.00\n",
      "[12/200][1799/2499] Loss_D: 1.38629365 (Loss_D_real: 0.69315213 Loss_D_fake: 0.69314146) Loss_G: -0.00000034 Loss_Enh_Dec: -0.00000063\n",
      "| epoch  12 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1044.92 | loss  0.79 | ppl     2.21 | acc     0.93 | train_ae_norm     1.00\n",
      "[12/200][1899/2499] Loss_D: 1.38629842 (Loss_D_real: 0.69316638 Loss_D_fake: 0.69313204) Loss_G: -0.00000432 Loss_Enh_Dec: -0.00000427\n",
      "| epoch  12 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1045.69 | loss  0.79 | ppl     2.21 | acc     0.90 | train_ae_norm     1.00\n",
      "[12/200][1999/2499] Loss_D: 1.38629997 (Loss_D_real: 0.69319999 Loss_D_fake: 0.69309998) Loss_G: -0.00001487 Loss_Enh_Dec: -0.00001618\n",
      "| epoch  12 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1044.37 | loss  0.80 | ppl     2.23 | acc     0.90 | train_ae_norm     1.00\n",
      "[12/200][2099/2499] Loss_D: 1.38629425 (Loss_D_real: 0.69319248 Loss_D_fake: 0.69310176) Loss_G: -0.00000894 Loss_Enh_Dec: -0.00000890\n",
      "| epoch  12 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1045.37 | loss  0.78 | ppl     2.19 | acc     0.92 | train_ae_norm     1.00\n",
      "[12/200][2199/2499] Loss_D: 1.38629174 (Loss_D_real: 0.69313329 Loss_D_fake: 0.69315851) Loss_G: 0.00000283 Loss_Enh_Dec: 0.00000180\n",
      "| epoch  12 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1044.35 | loss  0.76 | ppl     2.14 | acc     0.91 | train_ae_norm     1.00\n",
      "[12/200][2299/2499] Loss_D: 1.38629603 (Loss_D_real: 0.69315213 Loss_D_fake: 0.69314384) Loss_G: -0.00000108 Loss_Enh_Dec: -0.00000117\n",
      "| epoch  12 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1047.76 | loss  0.77 | ppl     2.16 | acc     0.91 | train_ae_norm     1.00\n",
      "[12/200][2399/2499] Loss_D: 1.38629651 (Loss_D_real: 0.69315565 Loss_D_fake: 0.69314092) Loss_G: -0.00000173 Loss_Enh_Dec: -0.00000224\n",
      "| epoch  12 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1046.75 | loss  0.77 | ppl     2.16 | acc     0.87 | train_ae_norm     1.00\n",
      "[12/200][2499/2499] Loss_D: 1.38629568 (Loss_D_real: 0.69312131 Loss_D_fake: 0.69317436) Loss_G: 0.00000357 Loss_Enh_Dec: 0.00000335\n",
      "| end of epoch  12 | time: 2799.06s | test loss  0.69 | test ppl  2.00 | acc 0.922\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 13 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:27.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:34.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:22.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:29.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:36.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:43.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:50.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:56.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:03.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:10.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:17.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:24.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:31.\n",
      "\n",
      "  Average training loss generetor: 0.710\n",
      "  Average training loss discriminator: 0.743\n",
      "  Training epcoh took: 0:02:37\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.510\n",
      "  Test Loss: 1.973\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  13 |     0/ 2499 batches | lr 0.000000 | ms/batch 1578.26 | loss  0.01 | ppl     1.01 | acc     0.89 | train_ae_norm     1.00\n",
      "[13/200][99/2499] Loss_D: 1.38629627 (Loss_D_real: 0.69316602 Loss_D_fake: 0.69313025) Loss_G: -0.00000304 Loss_Enh_Dec: -0.00000310\n",
      "| epoch  13 |   100/ 2499 batches | lr 0.000000 | ms/batch 1047.22 | loss  0.77 | ppl     2.16 | acc     0.88 | train_ae_norm     1.00\n",
      "[13/200][199/2499] Loss_D: 1.38630569 (Loss_D_real: 0.69324148 Loss_D_fake: 0.69306421) Loss_G: -0.00001531 Loss_Enh_Dec: -0.00001495\n",
      "| epoch  13 |   200/ 2499 batches | lr 0.000000 | ms/batch 1046.30 | loss  0.77 | ppl     2.16 | acc     0.91 | train_ae_norm     1.00\n",
      "[13/200][299/2499] Loss_D: 1.38629532 (Loss_D_real: 0.69313788 Loss_D_fake: 0.69315737) Loss_G: 0.00000233 Loss_Enh_Dec: 0.00000184\n",
      "| epoch  13 |   300/ 2499 batches | lr 0.000000 | ms/batch 1045.71 | loss  0.78 | ppl     2.18 | acc     0.89 | train_ae_norm     1.00\n",
      "[13/200][399/2499] Loss_D: 1.38629413 (Loss_D_real: 0.69314039 Loss_D_fake: 0.69315374) Loss_G: 0.00000133 Loss_Enh_Dec: 0.00000123\n",
      "| epoch  13 |   400/ 2499 batches | lr 0.000000 | ms/batch 1044.69 | loss  0.76 | ppl     2.13 | acc     0.92 | train_ae_norm     1.00\n",
      "[13/200][499/2499] Loss_D: 1.38629341 (Loss_D_real: 0.69319171 Loss_D_fake: 0.69310176) Loss_G: -0.00000770 Loss_Enh_Dec: -0.00000680\n",
      "| epoch  13 |   500/ 2499 batches | lr 0.000000 | ms/batch 1045.71 | loss  0.74 | ppl     2.09 | acc     0.91 | train_ae_norm     1.00\n",
      "[13/200][599/2499] Loss_D: 1.38629568 (Loss_D_real: 0.69316113 Loss_D_fake: 0.69313455) Loss_G: -0.00000279 Loss_Enh_Dec: -0.00000243\n",
      "| epoch  13 |   600/ 2499 batches | lr 0.000000 | ms/batch 1046.21 | loss  0.74 | ppl     2.09 | acc     0.92 | train_ae_norm     1.00\n",
      "[13/200][699/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69310617 Loss_D_fake: 0.69318819) Loss_G: 0.00000844 Loss_Enh_Dec: 0.00000826\n",
      "| epoch  13 |   700/ 2499 batches | lr 0.000000 | ms/batch 1044.82 | loss  0.72 | ppl     2.05 | acc     0.89 | train_ae_norm     1.00\n",
      "[13/200][799/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69314444 Loss_D_fake: 0.69314998) Loss_G: 0.00000016 Loss_Enh_Dec: 0.00000022\n",
      "| epoch  13 |   800/ 2499 batches | lr 0.000000 | ms/batch 1046.76 | loss  0.75 | ppl     2.11 | acc     0.90 | train_ae_norm     1.00\n",
      "[13/200][899/2499] Loss_D: 1.38628912 (Loss_D_real: 0.69314355 Loss_D_fake: 0.69314563) Loss_G: 0.00000148 Loss_Enh_Dec: 0.00000276\n",
      "| epoch  13 |   900/ 2499 batches | lr 0.000000 | ms/batch 1046.60 | loss  0.71 | ppl     2.04 | acc     0.92 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/200][999/2499] Loss_D: 1.38628912 (Loss_D_real: 0.69312710 Loss_D_fake: 0.69316196) Loss_G: 0.00000636 Loss_Enh_Dec: 0.00000338\n",
      "| epoch  13 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1043.82 | loss  0.76 | ppl     2.13 | acc     0.90 | train_ae_norm     1.00\n",
      "[13/200][1099/2499] Loss_D: 1.38630247 (Loss_D_real: 0.69316834 Loss_D_fake: 0.69313419) Loss_G: -0.00000537 Loss_Enh_Dec: -0.00000557\n",
      "| epoch  13 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1046.11 | loss  0.76 | ppl     2.14 | acc     0.92 | train_ae_norm     1.00\n",
      "[13/200][1199/2499] Loss_D: 1.38636029 (Loss_D_real: 0.69323719 Loss_D_fake: 0.69312310) Loss_G: -0.00001437 Loss_Enh_Dec: -0.00001120\n",
      "| epoch  13 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1051.51 | loss  0.80 | ppl     2.23 | acc     0.87 | train_ae_norm     1.00\n",
      "[13/200][1299/2499] Loss_D: 1.38629663 (Loss_D_real: 0.69313687 Loss_D_fake: 0.69315976) Loss_G: 0.00000300 Loss_Enh_Dec: 0.00000343\n",
      "| epoch  13 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1046.90 | loss  0.81 | ppl     2.25 | acc     0.87 | train_ae_norm     1.00\n",
      "[13/200][1399/2499] Loss_D: 1.38630307 (Loss_D_real: 0.69316769 Loss_D_fake: 0.69313538) Loss_G: -0.00000422 Loss_Enh_Dec: -0.00000259\n",
      "| epoch  13 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1043.52 | loss  0.78 | ppl     2.19 | acc     0.91 | train_ae_norm     1.00\n",
      "[13/200][1499/2499] Loss_D: 1.38629448 (Loss_D_real: 0.69314408 Loss_D_fake: 0.69315040) Loss_G: 0.00000089 Loss_Enh_Dec: -0.00000042\n",
      "| epoch  13 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1044.39 | loss  0.76 | ppl     2.13 | acc     0.89 | train_ae_norm     1.00\n",
      "[13/200][1599/2499] Loss_D: 1.38629234 (Loss_D_real: 0.69312811 Loss_D_fake: 0.69316423) Loss_G: 0.00000311 Loss_Enh_Dec: 0.00000277\n",
      "| epoch  13 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1044.72 | loss  0.75 | ppl     2.12 | acc     0.89 | train_ae_norm     1.00\n",
      "[13/200][1699/2499] Loss_D: 1.38629591 (Loss_D_real: 0.69321847 Loss_D_fake: 0.69307745) Loss_G: -0.00001598 Loss_Enh_Dec: -0.00001546\n",
      "| epoch  13 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1043.55 | loss  0.72 | ppl     2.06 | acc     0.92 | train_ae_norm     1.00\n",
      "[13/200][1799/2499] Loss_D: 1.38629127 (Loss_D_real: 0.69314277 Loss_D_fake: 0.69314855) Loss_G: 0.00000058 Loss_Enh_Dec: 0.00000039\n",
      "| epoch  13 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1045.77 | loss  0.72 | ppl     2.05 | acc     0.93 | train_ae_norm     1.00\n",
      "[13/200][1899/2499] Loss_D: 1.38629174 (Loss_D_real: 0.69311589 Loss_D_fake: 0.69317591) Loss_G: 0.00000566 Loss_Enh_Dec: 0.00000585\n",
      "| epoch  13 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1044.17 | loss  0.71 | ppl     2.04 | acc     0.88 | train_ae_norm     1.00\n",
      "[13/200][1999/2499] Loss_D: 1.38629341 (Loss_D_real: 0.69310302 Loss_D_fake: 0.69319046) Loss_G: 0.00000883 Loss_Enh_Dec: 0.00000866\n",
      "| epoch  13 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1044.69 | loss  0.71 | ppl     2.04 | acc     0.91 | train_ae_norm     1.00\n",
      "[13/200][2099/2499] Loss_D: 1.38629556 (Loss_D_real: 0.69314235 Loss_D_fake: 0.69315326) Loss_G: 0.00000208 Loss_Enh_Dec: 0.00000180\n",
      "| epoch  13 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1044.71 | loss  0.69 | ppl     1.99 | acc     0.92 | train_ae_norm     1.00\n",
      "[13/200][2199/2499] Loss_D: 1.38629365 (Loss_D_real: 0.69322175 Loss_D_fake: 0.69307184) Loss_G: -0.00001519 Loss_Enh_Dec: -0.00001525\n",
      "| epoch  13 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1046.00 | loss  0.67 | ppl     1.96 | acc     0.92 | train_ae_norm     1.00\n",
      "[13/200][2299/2499] Loss_D: 1.38629341 (Loss_D_real: 0.69308555 Loss_D_fake: 0.69320792) Loss_G: 0.00001185 Loss_Enh_Dec: 0.00001273\n",
      "| epoch  13 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1045.09 | loss  0.68 | ppl     1.98 | acc     0.92 | train_ae_norm     1.00\n",
      "[13/200][2399/2499] Loss_D: 1.38629389 (Loss_D_real: 0.69313765 Loss_D_fake: 0.69315624) Loss_G: 0.00000165 Loss_Enh_Dec: 0.00000141\n",
      "| epoch  13 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1045.29 | loss  0.69 | ppl     2.00 | acc     0.88 | train_ae_norm     1.00\n",
      "[13/200][2499/2499] Loss_D: 1.38629377 (Loss_D_real: 0.69312322 Loss_D_fake: 0.69317055) Loss_G: 0.00000531 Loss_Enh_Dec: 0.00000463\n",
      "| end of epoch  13 | time: 2790.94s | test loss  0.65 | test ppl  1.91 | acc 0.928\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 14 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:20.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:27.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:34.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:01.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:22.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:29.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:36.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:42.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:49.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:56.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:03.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:10.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:17.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:24.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:30.\n",
      "\n",
      "  Average training loss generetor: 0.709\n",
      "  Average training loss discriminator: 0.738\n",
      "  Training epcoh took: 0:02:37\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.500\n",
      "  Test Loss: 2.230\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  14 |     0/ 2499 batches | lr 0.000000 | ms/batch 1573.72 | loss  0.01 | ppl     1.01 | acc     0.89 | train_ae_norm     1.00\n",
      "[14/200][99/2499] Loss_D: 1.38629484 (Loss_D_real: 0.69319600 Loss_D_fake: 0.69309890) Loss_G: -0.00000969 Loss_Enh_Dec: -0.00000972\n",
      "| epoch  14 |   100/ 2499 batches | lr 0.000000 | ms/batch 1046.72 | loss  0.70 | ppl     2.02 | acc     0.88 | train_ae_norm     1.00\n",
      "[14/200][199/2499] Loss_D: 1.38629675 (Loss_D_real: 0.69321251 Loss_D_fake: 0.69308418) Loss_G: -0.00001251 Loss_Enh_Dec: -0.00001265\n",
      "| epoch  14 |   200/ 2499 batches | lr 0.000000 | ms/batch 1045.23 | loss  0.68 | ppl     1.98 | acc     0.92 | train_ae_norm     1.00\n",
      "[14/200][299/2499] Loss_D: 1.38629532 (Loss_D_real: 0.69314277 Loss_D_fake: 0.69315261) Loss_G: 0.00000057 Loss_Enh_Dec: 0.00000073\n",
      "| epoch  14 |   300/ 2499 batches | lr 0.000000 | ms/batch 1046.51 | loss  0.68 | ppl     1.98 | acc     0.91 | train_ae_norm     1.00\n",
      "[14/200][399/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69315946 Loss_D_fake: 0.69313520) Loss_G: -0.00000226 Loss_Enh_Dec: -0.00000253\n",
      "| epoch  14 |   400/ 2499 batches | lr 0.000000 | ms/batch 1045.72 | loss  0.66 | ppl     1.94 | acc     0.91 | train_ae_norm     1.00\n",
      "[14/200][499/2499] Loss_D: 1.38629484 (Loss_D_real: 0.69320071 Loss_D_fake: 0.69309413) Loss_G: -0.00001046 Loss_Enh_Dec: -0.00001060\n",
      "| epoch  14 |   500/ 2499 batches | lr 0.000000 | ms/batch 1045.86 | loss  0.64 | ppl     1.90 | acc     0.91 | train_ae_norm     1.00\n",
      "[14/200][599/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69319236 Loss_D_fake: 0.69310200) Loss_G: -0.00000746 Loss_Enh_Dec: -0.00000751\n",
      "| epoch  14 |   600/ 2499 batches | lr 0.000000 | ms/batch 1046.49 | loss  0.63 | ppl     1.89 | acc     0.93 | train_ae_norm     1.00\n",
      "[14/200][699/2499] Loss_D: 1.38629615 (Loss_D_real: 0.69321150 Loss_D_fake: 0.69308466) Loss_G: -0.00001353 Loss_Enh_Dec: -0.00001374\n",
      "| epoch  14 |   700/ 2499 batches | lr 0.000000 | ms/batch 1046.91 | loss  0.62 | ppl     1.86 | acc     0.92 | train_ae_norm     1.00\n",
      "[14/200][799/2499] Loss_D: 1.38629293 (Loss_D_real: 0.69316101 Loss_D_fake: 0.69313186) Loss_G: -0.00000290 Loss_Enh_Dec: -0.00000334\n",
      "| epoch  14 |   800/ 2499 batches | lr 0.000000 | ms/batch 1047.20 | loss  0.64 | ppl     1.91 | acc     0.92 | train_ae_norm     1.00\n",
      "[14/200][899/2499] Loss_D: 1.38629234 (Loss_D_real: 0.69317377 Loss_D_fake: 0.69311857) Loss_G: -0.00000502 Loss_Enh_Dec: -0.00000558\n",
      "| epoch  14 |   900/ 2499 batches | lr 0.000000 | ms/batch 1048.50 | loss  0.64 | ppl     1.90 | acc     0.92 | train_ae_norm     1.00\n",
      "[14/200][999/2499] Loss_D: 1.38629913 (Loss_D_real: 0.69321394 Loss_D_fake: 0.69308519) Loss_G: -0.00001384 Loss_Enh_Dec: -0.00001390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  14 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1046.31 | loss  0.68 | ppl     1.97 | acc     0.91 | train_ae_norm     1.00\n",
      "[14/200][1099/2499] Loss_D: 1.38629282 (Loss_D_real: 0.69310224 Loss_D_fake: 0.69319057) Loss_G: 0.00000970 Loss_Enh_Dec: 0.00000995\n",
      "| epoch  14 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1046.42 | loss  0.64 | ppl     1.90 | acc     0.93 | train_ae_norm     1.00\n",
      "[14/200][1199/2499] Loss_D: 1.38629532 (Loss_D_real: 0.69314784 Loss_D_fake: 0.69314754) Loss_G: -0.00000098 Loss_Enh_Dec: -0.00000065\n",
      "| epoch  14 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1046.09 | loss  0.63 | ppl     1.88 | acc     0.89 | train_ae_norm     1.00\n",
      "[14/200][1299/2499] Loss_D: 1.38629663 (Loss_D_real: 0.69315135 Loss_D_fake: 0.69314528) Loss_G: -0.00000081 Loss_Enh_Dec: -0.00000075\n",
      "| epoch  14 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1046.85 | loss  0.65 | ppl     1.91 | acc     0.89 | train_ae_norm     1.00\n",
      "[14/200][1399/2499] Loss_D: 1.38629377 (Loss_D_real: 0.69314229 Loss_D_fake: 0.69315147) Loss_G: 0.00000151 Loss_Enh_Dec: 0.00000115\n",
      "| epoch  14 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1048.31 | loss  0.65 | ppl     1.91 | acc     0.93 | train_ae_norm     1.00\n",
      "[14/200][1499/2499] Loss_D: 1.38629341 (Loss_D_real: 0.69326675 Loss_D_fake: 0.69302672) Loss_G: -0.00002269 Loss_Enh_Dec: -0.00002296\n",
      "| epoch  14 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1048.62 | loss  0.62 | ppl     1.86 | acc     0.91 | train_ae_norm     1.00\n",
      "[14/200][1599/2499] Loss_D: 1.38627958 (Loss_D_real: 0.69322693 Loss_D_fake: 0.69305265) Loss_G: -0.00001478 Loss_Enh_Dec: -0.00001672\n",
      "| epoch  14 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1048.11 | loss  0.63 | ppl     1.87 | acc     0.91 | train_ae_norm     1.00\n",
      "[14/200][1699/2499] Loss_D: 1.38629484 (Loss_D_real: 0.69312191 Loss_D_fake: 0.69317287) Loss_G: 0.00000530 Loss_Enh_Dec: 0.00000582\n",
      "| epoch  14 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1051.92 | loss  0.65 | ppl     1.92 | acc     0.93 | train_ae_norm     1.00\n",
      "[14/200][1799/2499] Loss_D: 1.38629532 (Loss_D_real: 0.69318390 Loss_D_fake: 0.69311136) Loss_G: -0.00000741 Loss_Enh_Dec: -0.00000752\n",
      "| epoch  14 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1051.01 | loss  0.63 | ppl     1.88 | acc     0.94 | train_ae_norm     1.00\n",
      "[14/200][1899/2499] Loss_D: 1.38629496 (Loss_D_real: 0.69320220 Loss_D_fake: 0.69309276) Loss_G: -0.00001069 Loss_Enh_Dec: -0.00001077\n",
      "| epoch  14 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1050.63 | loss  0.61 | ppl     1.84 | acc     0.91 | train_ae_norm     1.00\n",
      "[14/200][1999/2499] Loss_D: 1.38629413 (Loss_D_real: 0.69310725 Loss_D_fake: 0.69318694) Loss_G: 0.00000863 Loss_Enh_Dec: 0.00000863\n",
      "| epoch  14 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1050.99 | loss  0.62 | ppl     1.86 | acc     0.93 | train_ae_norm     1.00\n",
      "[14/200][2099/2499] Loss_D: 1.38629413 (Loss_D_real: 0.69311535 Loss_D_fake: 0.69317883) Loss_G: 0.00000704 Loss_Enh_Dec: 0.00000693\n",
      "| epoch  14 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1051.83 | loss  0.59 | ppl     1.81 | acc     0.94 | train_ae_norm     1.00\n",
      "[14/200][2199/2499] Loss_D: 1.38629532 (Loss_D_real: 0.69311303 Loss_D_fake: 0.69318235) Loss_G: 0.00000737 Loss_Enh_Dec: 0.00000746\n",
      "| epoch  14 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1051.53 | loss  0.58 | ppl     1.78 | acc     0.93 | train_ae_norm     1.00\n",
      "[14/200][2299/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69312727 Loss_D_fake: 0.69316709) Loss_G: 0.00000404 Loss_Enh_Dec: 0.00000417\n",
      "| epoch  14 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1051.13 | loss  0.58 | ppl     1.79 | acc     0.94 | train_ae_norm     1.00\n",
      "[14/200][2399/2499] Loss_D: 1.38629401 (Loss_D_real: 0.69317663 Loss_D_fake: 0.69311738) Loss_G: -0.00000578 Loss_Enh_Dec: -0.00000583\n",
      "| epoch  14 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1051.12 | loss  0.58 | ppl     1.79 | acc     0.90 | train_ae_norm     1.00\n",
      "[14/200][2499/2499] Loss_D: 1.38629365 (Loss_D_real: 0.69317836 Loss_D_fake: 0.69311523) Loss_G: -0.00000653 Loss_Enh_Dec: -0.00000638\n",
      "| end of epoch  14 | time: 2797.71s | test loss  0.58 | test ppl  1.79 | acc 0.938\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 15 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:09.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:16.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:23.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:30.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:37.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:44.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:51.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:58.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:04.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:18.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:25.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:32.\n",
      "\n",
      "  Average training loss generetor: 0.710\n",
      "  Average training loss discriminator: 0.733\n",
      "  Training epcoh took: 0:02:39\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.510\n",
      "  Test Loss: 2.180\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  15 |     0/ 2499 batches | lr 0.000000 | ms/batch 1592.81 | loss  0.01 | ppl     1.01 | acc     0.92 | train_ae_norm     1.00\n",
      "[15/200][99/2499] Loss_D: 1.38629580 (Loss_D_real: 0.69316149 Loss_D_fake: 0.69313431) Loss_G: -0.00000324 Loss_Enh_Dec: -0.00000327\n",
      "| epoch  15 |   100/ 2499 batches | lr 0.000000 | ms/batch 1052.57 | loss  0.59 | ppl     1.80 | acc     0.91 | train_ae_norm     1.00\n",
      "[15/200][199/2499] Loss_D: 1.38629389 (Loss_D_real: 0.69316310 Loss_D_fake: 0.69313085) Loss_G: -0.00000222 Loss_Enh_Dec: -0.00000161\n",
      "| epoch  15 |   200/ 2499 batches | lr 0.000000 | ms/batch 1049.68 | loss  0.58 | ppl     1.78 | acc     0.93 | train_ae_norm     1.00\n",
      "[15/200][299/2499] Loss_D: 1.38629484 (Loss_D_real: 0.69315213 Loss_D_fake: 0.69314277) Loss_G: -0.00000157 Loss_Enh_Dec: -0.00000113\n",
      "| epoch  15 |   300/ 2499 batches | lr 0.000000 | ms/batch 1052.24 | loss  0.58 | ppl     1.79 | acc     0.92 | train_ae_norm     1.00\n",
      "[15/200][499/2499] Loss_D: 1.38628674 (Loss_D_real: 0.69311953 Loss_D_fake: 0.69316721) Loss_G: 0.00000368 Loss_Enh_Dec: 0.00000295\n",
      "| epoch  15 |   500/ 2499 batches | lr 0.000000 | ms/batch 1054.16 | loss  0.58 | ppl     1.78 | acc     0.92 | train_ae_norm     1.00\n",
      "[15/200][599/2499] Loss_D: 1.38629603 (Loss_D_real: 0.69314307 Loss_D_fake: 0.69315290) Loss_G: 0.00000104 Loss_Enh_Dec: 0.00000056\n",
      "| epoch  15 |   600/ 2499 batches | lr 0.000000 | ms/batch 1051.99 | loss  0.60 | ppl     1.82 | acc     0.93 | train_ae_norm     1.00\n",
      "[15/200][699/2499] Loss_D: 1.38629413 (Loss_D_real: 0.69317245 Loss_D_fake: 0.69312161) Loss_G: -0.00000442 Loss_Enh_Dec: -0.00000477\n",
      "| epoch  15 |   700/ 2499 batches | lr 0.000000 | ms/batch 1052.10 | loss  0.58 | ppl     1.79 | acc     0.93 | train_ae_norm     1.00\n",
      "[15/200][799/2499] Loss_D: 1.38629282 (Loss_D_real: 0.69314849 Loss_D_fake: 0.69314432) Loss_G: -0.00000003 Loss_Enh_Dec: 0.00000029\n",
      "| epoch  15 |   800/ 2499 batches | lr 0.000000 | ms/batch 1048.55 | loss  0.58 | ppl     1.79 | acc     0.92 | train_ae_norm     1.00\n",
      "[15/200][899/2499] Loss_D: 1.38629043 (Loss_D_real: 0.69322288 Loss_D_fake: 0.69306755) Loss_G: -0.00001411 Loss_Enh_Dec: -0.00001516\n",
      "| epoch  15 |   900/ 2499 batches | lr 0.000000 | ms/batch 1049.02 | loss  0.56 | ppl     1.74 | acc     0.93 | train_ae_norm     1.00\n",
      "[15/200][999/2499] Loss_D: 1.38629270 (Loss_D_real: 0.69316423 Loss_D_fake: 0.69312853) Loss_G: -0.00000302 Loss_Enh_Dec: -0.00000348\n",
      "| epoch  15 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1047.64 | loss  0.59 | ppl     1.80 | acc     0.93 | train_ae_norm     1.00\n",
      "[15/200][1099/2499] Loss_D: 1.38629580 (Loss_D_real: 0.69314766 Loss_D_fake: 0.69314814) Loss_G: -0.00000006 Loss_Enh_Dec: -0.00000004\n",
      "| epoch  15 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1046.55 | loss  0.56 | ppl     1.75 | acc     0.94 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/200][1199/2499] Loss_D: 1.38629413 (Loss_D_real: 0.69314545 Loss_D_fake: 0.69314873) Loss_G: 0.00000051 Loss_Enh_Dec: 0.00000054\n",
      "| epoch  15 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1046.38 | loss  0.55 | ppl     1.73 | acc     0.91 | train_ae_norm     1.00\n",
      "[15/200][1299/2499] Loss_D: 1.38629365 (Loss_D_real: 0.69314975 Loss_D_fake: 0.69314384) Loss_G: -0.00000101 Loss_Enh_Dec: -0.00000109\n",
      "| epoch  15 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1050.16 | loss  0.56 | ppl     1.74 | acc     0.90 | train_ae_norm     1.00\n",
      "[15/200][1399/2499] Loss_D: 1.38629580 (Loss_D_real: 0.69328016 Loss_D_fake: 0.69301569) Loss_G: -0.00002666 Loss_Enh_Dec: -0.00002676\n",
      "| epoch  15 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1054.22 | loss  0.57 | ppl     1.76 | acc     0.93 | train_ae_norm     1.00\n",
      "[15/200][1499/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69311494 Loss_D_fake: 0.69317973) Loss_G: 0.00000724 Loss_Enh_Dec: 0.00000735\n",
      "| epoch  15 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1052.42 | loss  0.54 | ppl     1.71 | acc     0.92 | train_ae_norm     1.00\n",
      "[15/200][1599/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69313163 Loss_D_fake: 0.69316280) Loss_G: 0.00000274 Loss_Enh_Dec: 0.00000286\n",
      "| epoch  15 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1048.22 | loss  0.54 | ppl     1.72 | acc     0.91 | train_ae_norm     1.00\n",
      "[15/200][1699/2499] Loss_D: 1.38629603 (Loss_D_real: 0.69315356 Loss_D_fake: 0.69314241) Loss_G: -0.00000125 Loss_Enh_Dec: -0.00000181\n",
      "| epoch  15 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1050.87 | loss  0.53 | ppl     1.70 | acc     0.94 | train_ae_norm     1.00\n",
      "[15/200][1799/2499] Loss_D: 1.38630354 (Loss_D_real: 0.69314831 Loss_D_fake: 0.69315523) Loss_G: -0.00000100 Loss_Enh_Dec: 0.00000170\n",
      "| epoch  15 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1047.55 | loss  0.55 | ppl     1.74 | acc     0.94 | train_ae_norm     1.00\n",
      "[15/200][1899/2499] Loss_D: 1.38629556 (Loss_D_real: 0.69309092 Loss_D_fake: 0.69320458) Loss_G: 0.00001158 Loss_Enh_Dec: 0.00001174\n",
      "| epoch  15 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1045.39 | loss  0.54 | ppl     1.72 | acc     0.92 | train_ae_norm     1.00\n",
      "[15/200][1999/2499] Loss_D: 1.38629389 (Loss_D_real: 0.69308233 Loss_D_fake: 0.69321156) Loss_G: 0.00001264 Loss_Enh_Dec: 0.00001273\n",
      "| epoch  15 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1045.72 | loss  0.54 | ppl     1.71 | acc     0.94 | train_ae_norm     1.00\n",
      "[15/200][2099/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69310749 Loss_D_fake: 0.69318694) Loss_G: 0.00000772 Loss_Enh_Dec: 0.00000771\n",
      "| epoch  15 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1045.80 | loss  0.51 | ppl     1.67 | acc     0.94 | train_ae_norm     1.00\n",
      "[15/200][2199/2499] Loss_D: 1.38629520 (Loss_D_real: 0.69311571 Loss_D_fake: 0.69317949) Loss_G: 0.00000638 Loss_Enh_Dec: 0.00000635\n",
      "| epoch  15 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1047.59 | loss  0.50 | ppl     1.65 | acc     0.93 | train_ae_norm     1.00\n",
      "[15/200][2299/2499] Loss_D: 1.38629472 (Loss_D_real: 0.69311756 Loss_D_fake: 0.69317716) Loss_G: 0.00000601 Loss_Enh_Dec: 0.00000599\n",
      "| epoch  15 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1045.96 | loss  0.50 | ppl     1.65 | acc     0.94 | train_ae_norm     1.00\n",
      "[15/200][2399/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69309795 Loss_D_fake: 0.69319642) Loss_G: 0.00001023 Loss_Enh_Dec: 0.00001020\n",
      "| epoch  15 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1047.04 | loss  0.50 | ppl     1.64 | acc     0.91 | train_ae_norm     1.00\n",
      "[15/200][2499/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69306749 Loss_D_fake: 0.69322681) Loss_G: 0.00001658 Loss_Enh_Dec: 0.00001656\n",
      "| end of epoch  15 | time: 2801.49s | test loss  0.54 | test ppl  1.72 | acc 0.945\n",
      "New saving model: epoch 015.\n",
      "Saving models to ./results/yelp_merge_results\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 16 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:09.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:16.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:23.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:30.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:37.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:44.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:51.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:58.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:05.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:18.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:25.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:32.\n",
      "\n",
      "  Average training loss generetor: 0.709\n",
      "  Average training loss discriminator: 0.728\n",
      "  Training epcoh took: 0:02:39\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.505\n",
      "  Test Loss: 2.335\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  16 |     0/ 2499 batches | lr 0.000000 | ms/batch 1593.38 | loss  0.01 | ppl     1.01 | acc     0.93 | train_ae_norm     1.00\n",
      "[16/200][99/2499] Loss_D: 1.38629425 (Loss_D_real: 0.69324613 Loss_D_fake: 0.69304812) Loss_G: -0.00001959 Loss_Enh_Dec: -0.00001967\n",
      "| epoch  16 |   100/ 2499 batches | lr 0.000000 | ms/batch 1046.86 | loss  0.50 | ppl     1.65 | acc     0.92 | train_ae_norm     1.00\n",
      "[16/200][199/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69321555 Loss_D_fake: 0.69307899) Loss_G: -0.00001299 Loss_Enh_Dec: -0.00001298\n",
      "| epoch  16 |   200/ 2499 batches | lr 0.000000 | ms/batch 1045.64 | loss  0.49 | ppl     1.64 | acc     0.95 | train_ae_norm     1.00\n",
      "[16/200][299/2499] Loss_D: 1.38629413 (Loss_D_real: 0.69317615 Loss_D_fake: 0.69311804) Loss_G: -0.00000539 Loss_Enh_Dec: -0.00000546\n",
      "| epoch  16 |   300/ 2499 batches | lr 0.000000 | ms/batch 1045.50 | loss  0.50 | ppl     1.64 | acc     0.93 | train_ae_norm     1.00\n",
      "[16/200][399/2499] Loss_D: 1.38629389 (Loss_D_real: 0.69315743 Loss_D_fake: 0.69313639) Loss_G: -0.00000178 Loss_Enh_Dec: -0.00000183\n",
      "| epoch  16 |   400/ 2499 batches | lr 0.000000 | ms/batch 1048.11 | loss  0.49 | ppl     1.63 | acc     0.93 | train_ae_norm     1.00\n",
      "[16/200][499/2499] Loss_D: 1.38629317 (Loss_D_real: 0.69324923 Loss_D_fake: 0.69304389) Loss_G: -0.00002005 Loss_Enh_Dec: -0.00002063\n",
      "| epoch  16 |   500/ 2499 batches | lr 0.000000 | ms/batch 1045.28 | loss  0.48 | ppl     1.62 | acc     0.92 | train_ae_norm     1.00\n",
      "[16/200][599/2499] Loss_D: 1.38629329 (Loss_D_real: 0.69318426 Loss_D_fake: 0.69310904) Loss_G: -0.00000710 Loss_Enh_Dec: -0.00000725\n",
      "| epoch  16 |   600/ 2499 batches | lr 0.000000 | ms/batch 1046.16 | loss  0.50 | ppl     1.64 | acc     0.94 | train_ae_norm     1.00\n",
      "[16/200][699/2499] Loss_D: 1.38630521 (Loss_D_real: 0.69312048 Loss_D_fake: 0.69318473) Loss_G: 0.00000570 Loss_Enh_Dec: 0.00000723\n",
      "| epoch  16 |   700/ 2499 batches | lr 0.000000 | ms/batch 1046.06 | loss  0.49 | ppl     1.64 | acc     0.92 | train_ae_norm     1.00\n",
      "[16/200][799/2499] Loss_D: 1.38629174 (Loss_D_real: 0.69310635 Loss_D_fake: 0.69318533) Loss_G: 0.00000697 Loss_Enh_Dec: 0.00000828\n",
      "| epoch  16 |   800/ 2499 batches | lr 0.000000 | ms/batch 1045.51 | loss  0.51 | ppl     1.66 | acc     0.91 | train_ae_norm     1.00\n",
      "[16/200][899/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69314754 Loss_D_fake: 0.69314688) Loss_G: -0.00000003 Loss_Enh_Dec: -0.00000004\n",
      "| epoch  16 |   900/ 2499 batches | lr 0.000000 | ms/batch 1044.97 | loss  0.52 | ppl     1.68 | acc     0.95 | train_ae_norm     1.00\n",
      "[16/200][999/2499] Loss_D: 1.38629401 (Loss_D_real: 0.69315422 Loss_D_fake: 0.69313979) Loss_G: -0.00000237 Loss_Enh_Dec: -0.00000243\n",
      "| epoch  16 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1046.43 | loss  0.51 | ppl     1.67 | acc     0.93 | train_ae_norm     1.00\n",
      "[16/200][1099/2499] Loss_D: 1.38629580 (Loss_D_real: 0.69318581 Loss_D_fake: 0.69310999) Loss_G: -0.00000847 Loss_Enh_Dec: -0.00000938\n",
      "| epoch  16 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1050.36 | loss  0.52 | ppl     1.68 | acc     0.94 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/200][1199/2499] Loss_D: 1.38629270 (Loss_D_real: 0.69316095 Loss_D_fake: 0.69313180) Loss_G: -0.00000275 Loss_Enh_Dec: -0.00000386\n",
      "| epoch  16 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1053.92 | loss  0.51 | ppl     1.67 | acc     0.91 | train_ae_norm     1.00\n",
      "[16/200][1299/2499] Loss_D: 1.38629603 (Loss_D_real: 0.69326591 Loss_D_fake: 0.69303012) Loss_G: -0.00002204 Loss_Enh_Dec: -0.00002271\n",
      "| epoch  16 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1052.43 | loss  0.52 | ppl     1.69 | acc     0.90 | train_ae_norm     1.00\n",
      "[16/200][1399/2499] Loss_D: 1.38630235 (Loss_D_real: 0.69304943 Loss_D_fake: 0.69325292) Loss_G: 0.00001820 Loss_Enh_Dec: 0.00001803\n",
      "| epoch  16 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1047.04 | loss  0.54 | ppl     1.71 | acc     0.93 | train_ae_norm     1.00\n",
      "[16/200][1499/2499] Loss_D: 1.38629627 (Loss_D_real: 0.69317955 Loss_D_fake: 0.69311666) Loss_G: -0.00000568 Loss_Enh_Dec: -0.00000470\n",
      "| epoch  16 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1045.05 | loss  0.51 | ppl     1.66 | acc     0.92 | train_ae_norm     1.00\n",
      "[16/200][1599/2499] Loss_D: 1.38629675 (Loss_D_real: 0.69311953 Loss_D_fake: 0.69317722) Loss_G: 0.00000730 Loss_Enh_Dec: 0.00000679\n",
      "| epoch  16 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1045.23 | loss  0.51 | ppl     1.66 | acc     0.92 | train_ae_norm     1.00\n",
      "[16/200][1699/2499] Loss_D: 1.38629234 (Loss_D_real: 0.69311500 Loss_D_fake: 0.69317734) Loss_G: 0.00000572 Loss_Enh_Dec: 0.00000619\n",
      "| epoch  16 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1045.28 | loss  0.50 | ppl     1.64 | acc     0.94 | train_ae_norm     1.00\n",
      "[16/200][1799/2499] Loss_D: 1.38629723 (Loss_D_real: 0.69329262 Loss_D_fake: 0.69300461) Loss_G: -0.00002829 Loss_Enh_Dec: -0.00002833\n",
      "| epoch  16 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1043.79 | loss  0.49 | ppl     1.63 | acc     0.94 | train_ae_norm     1.00\n",
      "[16/200][1899/2499] Loss_D: 1.38628793 (Loss_D_real: 0.69310904 Loss_D_fake: 0.69317889) Loss_G: 0.00000731 Loss_Enh_Dec: 0.00000761\n",
      "| epoch  16 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1043.66 | loss  0.48 | ppl     1.62 | acc     0.93 | train_ae_norm     1.00\n",
      "[16/200][1999/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69314754 Loss_D_fake: 0.69314688) Loss_G: 0.00000012 Loss_Enh_Dec: 0.00000016\n",
      "| epoch  16 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1044.71 | loss  0.49 | ppl     1.63 | acc     0.93 | train_ae_norm     1.00\n",
      "[16/200][2099/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69314718 Loss_D_fake: 0.69314736) Loss_G: -0.00000006 Loss_Enh_Dec: -0.00000005\n",
      "| epoch  16 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1047.05 | loss  0.47 | ppl     1.60 | acc     0.95 | train_ae_norm     1.00\n",
      "[16/200][2199/2499] Loss_D: 1.38629425 (Loss_D_real: 0.69314671 Loss_D_fake: 0.69314754) Loss_G: -0.00000008 Loss_Enh_Dec: -0.00000011\n",
      "| epoch  16 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1045.27 | loss  0.45 | ppl     1.57 | acc     0.95 | train_ae_norm     1.00\n",
      "[16/200][2299/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69312310 Loss_D_fake: 0.69317150) Loss_G: 0.00000516 Loss_Enh_Dec: 0.00000525\n",
      "| epoch  16 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1043.94 | loss  0.45 | ppl     1.56 | acc     0.95 | train_ae_norm     1.00\n",
      "[16/200][2399/2499] Loss_D: 1.38629341 (Loss_D_real: 0.69315952 Loss_D_fake: 0.69313395) Loss_G: -0.00000187 Loss_Enh_Dec: -0.00000226\n",
      "| epoch  16 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1045.18 | loss  0.46 | ppl     1.58 | acc     0.91 | train_ae_norm     1.00\n",
      "[16/200][2499/2499] Loss_D: 1.38629282 (Loss_D_real: 0.69316041 Loss_D_fake: 0.69313240) Loss_G: -0.00000222 Loss_Enh_Dec: -0.00000272\n",
      "| end of epoch  16 | time: 2794.33s | test loss  0.52 | test ppl  1.68 | acc 0.949\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 17 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:20.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:27.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:34.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:01.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:22.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:29.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:36.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:42.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:49.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:56.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:03.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:10.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:17.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:23.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:30.\n",
      "\n",
      "  Average training loss generetor: 0.708\n",
      "  Average training loss discriminator: 0.727\n",
      "  Training epcoh took: 0:02:37\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.500\n",
      "  Test Loss: 2.503\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  17 |     0/ 2499 batches | lr 0.000000 | ms/batch 1573.39 | loss  0.01 | ppl     1.01 | acc     0.94 | train_ae_norm     1.00\n",
      "[17/200][99/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69314927 Loss_D_fake: 0.69314516) Loss_G: -0.00000046 Loss_Enh_Dec: -0.00000086\n",
      "| epoch  17 |   100/ 2499 batches | lr 0.000000 | ms/batch 1043.97 | loss  0.46 | ppl     1.59 | acc     0.92 | train_ae_norm     1.00\n",
      "[17/200][199/2499] Loss_D: 1.38630438 (Loss_D_real: 0.69310975 Loss_D_fake: 0.69319457) Loss_G: 0.00000718 Loss_Enh_Dec: 0.00000710\n",
      "| epoch  17 |   200/ 2499 batches | lr 0.000000 | ms/batch 1044.99 | loss  0.46 | ppl     1.58 | acc     0.94 | train_ae_norm     1.00\n",
      "[17/200][299/2499] Loss_D: 1.38629508 (Loss_D_real: 0.69315040 Loss_D_fake: 0.69314468) Loss_G: -0.00000086 Loss_Enh_Dec: -0.00000095\n",
      "| epoch  17 |   300/ 2499 batches | lr 0.000000 | ms/batch 1045.31 | loss  0.47 | ppl     1.61 | acc     0.94 | train_ae_norm     1.00\n",
      "[17/200][399/2499] Loss_D: 1.38629341 (Loss_D_real: 0.69315195 Loss_D_fake: 0.69314140) Loss_G: -0.00000158 Loss_Enh_Dec: -0.00000207\n",
      "| epoch  17 |   400/ 2499 batches | lr 0.000000 | ms/batch 1046.60 | loss  0.46 | ppl     1.58 | acc     0.94 | train_ae_norm     1.00\n",
      "[17/200][499/2499] Loss_D: 1.38629425 (Loss_D_real: 0.69314021 Loss_D_fake: 0.69315404) Loss_G: 0.00000145 Loss_Enh_Dec: 0.00000143\n",
      "| epoch  17 |   500/ 2499 batches | lr 0.000000 | ms/batch 1048.07 | loss  0.44 | ppl     1.55 | acc     0.93 | train_ae_norm     1.00\n",
      "[17/200][599/2499] Loss_D: 1.38629508 (Loss_D_real: 0.69315088 Loss_D_fake: 0.69314426) Loss_G: -0.00000019 Loss_Enh_Dec: -0.00000030\n",
      "| epoch  17 |   600/ 2499 batches | lr 0.000000 | ms/batch 1045.86 | loss  0.43 | ppl     1.54 | acc     0.95 | train_ae_norm     1.00\n",
      "[17/200][699/2499] Loss_D: 1.38629293 (Loss_D_real: 0.69325078 Loss_D_fake: 0.69304216) Loss_G: -0.00001751 Loss_Enh_Dec: -0.00001765\n",
      "| epoch  17 |   700/ 2499 batches | lr 0.000000 | ms/batch 1045.68 | loss  0.43 | ppl     1.54 | acc     0.94 | train_ae_norm     1.00\n",
      "[17/200][799/2499] Loss_D: 1.38629472 (Loss_D_real: 0.69304597 Loss_D_fake: 0.69324875) Loss_G: 0.00002045 Loss_Enh_Dec: 0.00002014\n",
      "| epoch  17 |   800/ 2499 batches | lr 0.000000 | ms/batch 1046.87 | loss  0.44 | ppl     1.56 | acc     0.92 | train_ae_norm     1.00\n",
      "[17/200][899/2499] Loss_D: 1.38629484 (Loss_D_real: 0.69317579 Loss_D_fake: 0.69311905) Loss_G: -0.00000633 Loss_Enh_Dec: -0.00000645\n",
      "| epoch  17 |   900/ 2499 batches | lr 0.000000 | ms/batch 1046.37 | loss  0.42 | ppl     1.52 | acc     0.96 | train_ae_norm     1.00\n",
      "[17/200][999/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69314516 Loss_D_fake: 0.69314921) Loss_G: 0.00000046 Loss_Enh_Dec: 0.00000033\n",
      "| epoch  17 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1047.26 | loss  0.44 | ppl     1.55 | acc     0.94 | train_ae_norm     1.00\n",
      "[17/200][1099/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69314879 Loss_D_fake: 0.69314575) Loss_G: -0.00000035 Loss_Enh_Dec: -0.00000035\n",
      "| epoch  17 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1047.93 | loss  0.43 | ppl     1.53 | acc     0.94 | train_ae_norm     1.00\n",
      "[17/200][1199/2499] Loss_D: 1.38629472 (Loss_D_real: 0.69315755 Loss_D_fake: 0.69313717) Loss_G: -0.00000185 Loss_Enh_Dec: -0.00000192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  17 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1047.96 | loss  0.41 | ppl     1.51 | acc     0.92 | train_ae_norm     1.00\n",
      "[17/200][1299/2499] Loss_D: 1.38629377 (Loss_D_real: 0.69301295 Loss_D_fake: 0.69328082) Loss_G: 0.00002753 Loss_Enh_Dec: 0.00002743\n",
      "| epoch  17 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1045.66 | loss  0.42 | ppl     1.52 | acc     0.92 | train_ae_norm     1.00\n",
      "[17/200][1399/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69319409 Loss_D_fake: 0.69310045) Loss_G: -0.00001051 Loss_Enh_Dec: -0.00001045\n",
      "| epoch  17 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1047.92 | loss  0.43 | ppl     1.54 | acc     0.95 | train_ae_norm     1.00\n",
      "[17/200][1499/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69310820 Loss_D_fake: 0.69318622) Loss_G: 0.00000833 Loss_Enh_Dec: 0.00000836\n",
      "| epoch  17 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1050.10 | loss  0.41 | ppl     1.50 | acc     0.93 | train_ae_norm     1.00\n",
      "[17/200][1599/2499] Loss_D: 1.38629484 (Loss_D_real: 0.69308758 Loss_D_fake: 0.69320720) Loss_G: 0.00001251 Loss_Enh_Dec: 0.00001227\n",
      "| epoch  17 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1049.06 | loss  0.41 | ppl     1.51 | acc     0.93 | train_ae_norm     1.00\n",
      "[17/200][1699/2499] Loss_D: 1.38629389 (Loss_D_real: 0.69306439 Loss_D_fake: 0.69322944) Loss_G: 0.00001717 Loss_Enh_Dec: 0.00001673\n",
      "| epoch  17 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1048.65 | loss  0.41 | ppl     1.50 | acc     0.94 | train_ae_norm     1.00\n",
      "[17/200][1799/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69304788 Loss_D_fake: 0.69324648) Loss_G: 0.00001953 Loss_Enh_Dec: 0.00001957\n",
      "| epoch  17 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1051.43 | loss  0.40 | ppl     1.50 | acc     0.96 | train_ae_norm     1.00\n",
      "[17/200][1899/2499] Loss_D: 1.38629389 (Loss_D_real: 0.69294643 Loss_D_fake: 0.69334739) Loss_G: 0.00003585 Loss_Enh_Dec: 0.00003528\n",
      "| epoch  17 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1049.09 | loss  0.40 | ppl     1.49 | acc     0.94 | train_ae_norm     1.00\n",
      "[17/200][1999/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69316262 Loss_D_fake: 0.69313180) Loss_G: -0.00000336 Loss_Enh_Dec: -0.00000348\n",
      "| epoch  17 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1049.44 | loss  0.41 | ppl     1.50 | acc     0.95 | train_ae_norm     1.00\n",
      "[17/200][2099/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69315732 Loss_D_fake: 0.69313711) Loss_G: -0.00000203 Loss_Enh_Dec: -0.00000216\n",
      "| epoch  17 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1049.72 | loss  0.39 | ppl     1.48 | acc     0.96 | train_ae_norm     1.00\n",
      "[17/200][2199/2499] Loss_D: 1.38629472 (Loss_D_real: 0.69314194 Loss_D_fake: 0.69315279) Loss_G: 0.00000119 Loss_Enh_Dec: 0.00000114\n",
      "| epoch  17 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1048.93 | loss  0.38 | ppl     1.47 | acc     0.95 | train_ae_norm     1.00\n",
      "[17/200][2299/2499] Loss_D: 1.38629448 (Loss_D_real: 0.69314837 Loss_D_fake: 0.69314611) Loss_G: -0.00000025 Loss_Enh_Dec: -0.00000023\n",
      "| epoch  17 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1047.85 | loss  0.38 | ppl     1.46 | acc     0.96 | train_ae_norm     1.00\n",
      "[17/200][2399/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69314909 Loss_D_fake: 0.69314545) Loss_G: -0.00000034 Loss_Enh_Dec: -0.00000043\n",
      "| epoch  17 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1050.65 | loss  0.38 | ppl     1.47 | acc     0.92 | train_ae_norm     1.00\n",
      "[17/200][2499/2499] Loss_D: 1.38629496 (Loss_D_real: 0.69318432 Loss_D_fake: 0.69311064) Loss_G: -0.00000644 Loss_Enh_Dec: -0.00000648\n",
      "| end of epoch  17 | time: 2795.92s | test loss  0.49 | test ppl  1.63 | acc 0.955\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 18 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:27.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:34.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:09.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:22.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:29.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:36.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:43.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:50.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:57.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:04.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:10.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:17.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:24.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:31.\n",
      "\n",
      "  Average training loss generetor: 0.708\n",
      "  Average training loss discriminator: 0.723\n",
      "  Training epcoh took: 0:02:38\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.505\n",
      "  Test Loss: 2.474\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  18 |     0/ 2499 batches | lr 0.000000 | ms/batch 1580.70 | loss  0.00 | ppl     1.00 | acc     0.94 | train_ae_norm     1.00\n",
      "[18/200][99/2499] Loss_D: 1.38629365 (Loss_D_real: 0.69307393 Loss_D_fake: 0.69321966) Loss_G: 0.00001311 Loss_Enh_Dec: 0.00001307\n",
      "| epoch  18 |   100/ 2499 batches | lr 0.000000 | ms/batch 1049.71 | loss  0.38 | ppl     1.47 | acc     0.92 | train_ae_norm     1.00\n",
      "[18/200][199/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69301522 Loss_D_fake: 0.69327945) Loss_G: 0.00002548 Loss_Enh_Dec: 0.00002545\n",
      "| epoch  18 |   200/ 2499 batches | lr 0.000000 | ms/batch 1049.01 | loss  0.38 | ppl     1.46 | acc     0.96 | train_ae_norm     1.00\n",
      "[18/200][299/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69306970 Loss_D_fake: 0.69322467) Loss_G: 0.00001697 Loss_Enh_Dec: 0.00001708\n",
      "| epoch  18 |   300/ 2499 batches | lr 0.000000 | ms/batch 1047.09 | loss  0.38 | ppl     1.47 | acc     0.95 | train_ae_norm     1.00\n",
      "[18/200][399/2499] Loss_D: 1.38629448 (Loss_D_real: 0.69312561 Loss_D_fake: 0.69316888) Loss_G: 0.00000475 Loss_Enh_Dec: 0.00000477\n",
      "| epoch  18 |   400/ 2499 batches | lr 0.000000 | ms/batch 1049.82 | loss  0.37 | ppl     1.45 | acc     0.94 | train_ae_norm     1.00\n",
      "[18/200][499/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69312847 Loss_D_fake: 0.69316596) Loss_G: 0.00000395 Loss_Enh_Dec: 0.00000392\n",
      "| epoch  18 |   500/ 2499 batches | lr 0.000000 | ms/batch 1047.40 | loss  0.36 | ppl     1.43 | acc     0.94 | train_ae_norm     1.00\n",
      "[18/200][599/2499] Loss_D: 1.38629353 (Loss_D_real: 0.69316697 Loss_D_fake: 0.69312656) Loss_G: -0.00000448 Loss_Enh_Dec: -0.00000475\n",
      "| epoch  18 |   600/ 2499 batches | lr 0.000000 | ms/batch 1048.23 | loss  0.38 | ppl     1.46 | acc     0.95 | train_ae_norm     1.00\n",
      "[18/200][699/2499] Loss_D: 1.38629413 (Loss_D_real: 0.69315517 Loss_D_fake: 0.69313896) Loss_G: -0.00000178 Loss_Enh_Dec: -0.00000202\n",
      "| epoch  18 |   700/ 2499 batches | lr 0.000000 | ms/batch 1047.41 | loss  0.36 | ppl     1.44 | acc     0.94 | train_ae_norm     1.00\n",
      "[18/200][799/2499] Loss_D: 1.38629448 (Loss_D_real: 0.69315052 Loss_D_fake: 0.69314396) Loss_G: -0.00000067 Loss_Enh_Dec: -0.00000069\n",
      "| epoch  18 |   800/ 2499 batches | lr 0.000000 | ms/batch 1047.19 | loss  0.38 | ppl     1.46 | acc     0.93 | train_ae_norm     1.00\n",
      "[18/200][899/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69314814 Loss_D_fake: 0.69314623) Loss_G: -0.00000016 Loss_Enh_Dec: -0.00000016\n",
      "| epoch  18 |   900/ 2499 batches | lr 0.000000 | ms/batch 1045.79 | loss  0.36 | ppl     1.43 | acc     0.95 | train_ae_norm     1.00\n",
      "[18/200][999/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69307768 Loss_D_fake: 0.69321668) Loss_G: 0.00001600 Loss_Enh_Dec: 0.00001600\n",
      "| epoch  18 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1047.47 | loss  0.37 | ppl     1.45 | acc     0.95 | train_ae_norm     1.00\n",
      "[18/200][1099/2499] Loss_D: 1.38629472 (Loss_D_real: 0.69315767 Loss_D_fake: 0.69313705) Loss_G: -0.00000370 Loss_Enh_Dec: -0.00000335\n",
      "| epoch  18 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1047.03 | loss  0.36 | ppl     1.44 | acc     0.95 | train_ae_norm     1.00\n",
      "[18/200][1199/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69312191 Loss_D_fake: 0.69317245) Loss_G: 0.00000506 Loss_Enh_Dec: 0.00000506\n",
      "| epoch  18 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1046.26 | loss  0.37 | ppl     1.45 | acc     0.93 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18/200][1299/2499] Loss_D: 1.38629472 (Loss_D_real: 0.69313192 Loss_D_fake: 0.69316280) Loss_G: 0.00000329 Loss_Enh_Dec: 0.00000327\n",
      "| epoch  18 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1046.33 | loss  0.37 | ppl     1.44 | acc     0.92 | train_ae_norm     1.00\n",
      "[18/200][1399/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69316959 Loss_D_fake: 0.69312477) Loss_G: -0.00000422 Loss_Enh_Dec: -0.00000422\n",
      "| epoch  18 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1043.01 | loss  0.38 | ppl     1.47 | acc     0.95 | train_ae_norm     1.00\n",
      "[18/200][1499/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69312233 Loss_D_fake: 0.69317198) Loss_G: 0.00000453 Loss_Enh_Dec: 0.00000453\n",
      "| epoch  18 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1043.40 | loss  0.37 | ppl     1.45 | acc     0.94 | train_ae_norm     1.00\n",
      "[18/200][1599/2499] Loss_D: 1.38629580 (Loss_D_real: 0.69311756 Loss_D_fake: 0.69317818) Loss_G: 0.00000494 Loss_Enh_Dec: 0.00000493\n",
      "| epoch  18 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1042.42 | loss  0.38 | ppl     1.46 | acc     0.93 | train_ae_norm     1.00\n",
      "[18/200][1699/2499] Loss_D: 1.38629448 (Loss_D_real: 0.69310033 Loss_D_fake: 0.69319415) Loss_G: 0.00001093 Loss_Enh_Dec: 0.00001093\n",
      "| epoch  18 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1041.25 | loss  0.36 | ppl     1.44 | acc     0.96 | train_ae_norm     1.00\n",
      "[18/200][1799/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69314408 Loss_D_fake: 0.69315028) Loss_G: 0.00000060 Loss_Enh_Dec: 0.00000060\n",
      "| epoch  18 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1042.44 | loss  0.36 | ppl     1.43 | acc     0.96 | train_ae_norm     1.00\n",
      "[18/200][1899/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69315767 Loss_D_fake: 0.69313669) Loss_G: -0.00000202 Loss_Enh_Dec: -0.00000202\n",
      "| epoch  18 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1041.98 | loss  0.35 | ppl     1.42 | acc     0.95 | train_ae_norm     1.00\n",
      "[18/200][1999/2499] Loss_D: 1.38629305 (Loss_D_real: 0.69313765 Loss_D_fake: 0.69315541) Loss_G: 0.00000238 Loss_Enh_Dec: 0.00000157\n",
      "| epoch  18 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1042.02 | loss  0.36 | ppl     1.43 | acc     0.95 | train_ae_norm     1.00\n",
      "[18/200][2099/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69314575 Loss_D_fake: 0.69314855) Loss_G: 0.00000026 Loss_Enh_Dec: 0.00000026\n",
      "| epoch  18 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1041.95 | loss  0.37 | ppl     1.45 | acc     0.96 | train_ae_norm     1.00\n",
      "[18/200][2199/2499] Loss_D: 1.38629532 (Loss_D_real: 0.69314164 Loss_D_fake: 0.69315362) Loss_G: 0.00000143 Loss_Enh_Dec: 0.00000159\n",
      "| epoch  18 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1042.51 | loss  0.34 | ppl     1.41 | acc     0.96 | train_ae_norm     1.00\n",
      "[18/200][2299/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69321227 Loss_D_fake: 0.69308209) Loss_G: -0.00001395 Loss_Enh_Dec: -0.00001394\n",
      "| epoch  18 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1042.25 | loss  0.35 | ppl     1.41 | acc     0.96 | train_ae_norm     1.00\n",
      "[18/200][2399/2499] Loss_D: 1.38629222 (Loss_D_real: 0.69314736 Loss_D_fake: 0.69314492) Loss_G: -0.00000165 Loss_Enh_Dec: -0.00000131\n",
      "| epoch  18 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1042.26 | loss  0.34 | ppl     1.41 | acc     0.92 | train_ae_norm     1.00\n",
      "[18/200][2499/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69313782 Loss_D_fake: 0.69315672) Loss_G: 0.00000226 Loss_Enh_Dec: 0.00000234\n",
      "| end of epoch  18 | time: 2789.84s | test loss  0.48 | test ppl  1.61 | acc 0.958\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 19 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:20.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:27.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:34.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:54.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:01.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:22.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:29.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:35.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:42.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:49.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:56.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:03.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:10.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:16.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:23.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:30.\n",
      "\n",
      "  Average training loss generetor: 0.692\n",
      "  Average training loss discriminator: 0.879\n",
      "  Training epcoh took: 0:02:37\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.540\n",
      "  Test Loss: 2.071\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  19 |     0/ 2499 batches | lr 0.000000 | ms/batch 1570.98 | loss  0.00 | ppl     1.00 | acc     0.94 | train_ae_norm     1.00\n",
      "[19/200][99/2499] Loss_D: 1.38629591 (Loss_D_real: 0.69316030 Loss_D_fake: 0.69313562) Loss_G: -0.00000209 Loss_Enh_Dec: -0.00000191\n",
      "| epoch  19 |   100/ 2499 batches | lr 0.000000 | ms/batch 1043.12 | loss  0.34 | ppl     1.41 | acc     0.93 | train_ae_norm     1.00\n",
      "[19/200][199/2499] Loss_D: 1.38629413 (Loss_D_real: 0.69312924 Loss_D_fake: 0.69316483) Loss_G: 0.00000333 Loss_Enh_Dec: 0.00000329\n",
      "| epoch  19 |   200/ 2499 batches | lr 0.000000 | ms/batch 1041.87 | loss  0.34 | ppl     1.41 | acc     0.96 | train_ae_norm     1.00\n",
      "[19/200][299/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69316429 Loss_D_fake: 0.69313014) Loss_G: -0.00000320 Loss_Enh_Dec: -0.00000324\n",
      "| epoch  19 |   300/ 2499 batches | lr 0.000000 | ms/batch 1042.38 | loss  0.34 | ppl     1.40 | acc     0.96 | train_ae_norm     1.00\n",
      "[19/200][399/2499] Loss_D: 1.38629472 (Loss_D_real: 0.69316673 Loss_D_fake: 0.69312799) Loss_G: -0.00000401 Loss_Enh_Dec: -0.00000401\n",
      "| epoch  19 |   400/ 2499 batches | lr 0.000000 | ms/batch 1043.10 | loss  0.33 | ppl     1.39 | acc     0.95 | train_ae_norm     1.00\n",
      "[19/200][499/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69316006 Loss_D_fake: 0.69313425) Loss_G: -0.00000262 Loss_Enh_Dec: -0.00000262\n",
      "| epoch  19 |   500/ 2499 batches | lr 0.000000 | ms/batch 1041.18 | loss  0.32 | ppl     1.38 | acc     0.94 | train_ae_norm     1.00\n",
      "[19/200][599/2499] Loss_D: 1.38629484 (Loss_D_real: 0.69312495 Loss_D_fake: 0.69316983) Loss_G: 0.00000424 Loss_Enh_Dec: 0.00000402\n",
      "| epoch  19 |   600/ 2499 batches | lr 0.000000 | ms/batch 1043.00 | loss  0.32 | ppl     1.38 | acc     0.97 | train_ae_norm     1.00\n",
      "[19/200][699/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69313455 Loss_D_fake: 0.69315982) Loss_G: 0.00000257 Loss_Enh_Dec: 0.00000257\n",
      "| epoch  19 |   700/ 2499 batches | lr 0.000000 | ms/batch 1041.31 | loss  0.32 | ppl     1.38 | acc     0.95 | train_ae_norm     1.00\n",
      "[19/200][799/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69314021 Loss_D_fake: 0.69315410) Loss_G: 0.00000142 Loss_Enh_Dec: 0.00000142\n",
      "| epoch  19 |   800/ 2499 batches | lr 0.000000 | ms/batch 1042.23 | loss  0.33 | ppl     1.39 | acc     0.94 | train_ae_norm     1.00\n",
      "[19/200][899/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69314784 Loss_D_fake: 0.69314671) Loss_G: -0.00000002 Loss_Enh_Dec: -0.00000012\n",
      "| epoch  19 |   900/ 2499 batches | lr 0.000000 | ms/batch 1042.34 | loss  0.31 | ppl     1.36 | acc     0.96 | train_ae_norm     1.00\n",
      "[19/200][999/2499] Loss_D: 1.38629413 (Loss_D_real: 0.69314897 Loss_D_fake: 0.69314516) Loss_G: -0.00000029 Loss_Enh_Dec: -0.00000029\n",
      "| epoch  19 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1043.27 | loss  0.33 | ppl     1.39 | acc     0.95 | train_ae_norm     1.00\n",
      "[19/200][1099/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69317055 Loss_D_fake: 0.69312382) Loss_G: -0.00000358 Loss_Enh_Dec: -0.00000358\n",
      "| epoch  19 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1041.47 | loss  0.32 | ppl     1.38 | acc     0.95 | train_ae_norm     1.00\n",
      "[19/200][1199/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69316351 Loss_D_fake: 0.69313091) Loss_G: -0.00000361 Loss_Enh_Dec: -0.00000361\n",
      "| epoch  19 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1041.97 | loss  0.31 | ppl     1.37 | acc     0.94 | train_ae_norm     1.00\n",
      "[19/200][1299/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69314760 Loss_D_fake: 0.69314671) Loss_G: -0.00000007 Loss_Enh_Dec: -0.00000007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  19 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1042.06 | loss  0.31 | ppl     1.36 | acc     0.93 | train_ae_norm     1.00\n",
      "[19/200][1399/2499] Loss_D: 1.38629270 (Loss_D_real: 0.69320220 Loss_D_fake: 0.69309056) Loss_G: -0.00001405 Loss_Enh_Dec: -0.00001439\n",
      "| epoch  19 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1042.34 | loss  0.34 | ppl     1.41 | acc     0.96 | train_ae_norm     1.00\n",
      "[19/200][1499/2499] Loss_D: 1.38629413 (Loss_D_real: 0.69311237 Loss_D_fake: 0.69318175) Loss_G: 0.00000777 Loss_Enh_Dec: 0.00000781\n",
      "| epoch  19 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1043.19 | loss  0.35 | ppl     1.41 | acc     0.94 | train_ae_norm     1.00\n",
      "[19/200][1599/2499] Loss_D: 1.38629568 (Loss_D_real: 0.69316769 Loss_D_fake: 0.69312799) Loss_G: -0.00000487 Loss_Enh_Dec: -0.00000468\n",
      "| epoch  19 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1042.83 | loss  0.34 | ppl     1.40 | acc     0.94 | train_ae_norm     1.00\n",
      "[19/200][1699/2499] Loss_D: 1.38629663 (Loss_D_real: 0.69317740 Loss_D_fake: 0.69311923) Loss_G: -0.00000574 Loss_Enh_Dec: -0.00000528\n",
      "| epoch  19 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1043.06 | loss  0.32 | ppl     1.38 | acc     0.96 | train_ae_norm     1.00\n",
      "[19/200][1799/2499] Loss_D: 1.38629365 (Loss_D_real: 0.69318384 Loss_D_fake: 0.69310987) Loss_G: -0.00000741 Loss_Enh_Dec: -0.00000754\n",
      "| epoch  19 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1042.45 | loss  0.32 | ppl     1.38 | acc     0.95 | train_ae_norm     1.00\n",
      "[19/200][1899/2499] Loss_D: 1.38629341 (Loss_D_real: 0.69322175 Loss_D_fake: 0.69307172) Loss_G: -0.00001481 Loss_Enh_Dec: -0.00001485\n",
      "| epoch  19 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1041.84 | loss  0.32 | ppl     1.38 | acc     0.94 | train_ae_norm     1.00\n",
      "[19/200][1999/2499] Loss_D: 1.38629222 (Loss_D_real: 0.69319284 Loss_D_fake: 0.69309938) Loss_G: -0.00000980 Loss_Enh_Dec: -0.00000963\n",
      "| epoch  19 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1050.58 | loss  0.35 | ppl     1.42 | acc     0.95 | train_ae_norm     1.00\n",
      "[19/200][2099/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69313407 Loss_D_fake: 0.69316053) Loss_G: 0.00000362 Loss_Enh_Dec: 0.00000338\n",
      "| epoch  19 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1053.25 | loss  0.32 | ppl     1.38 | acc     0.97 | train_ae_norm     1.00\n",
      "[19/200][2199/2499] Loss_D: 1.38629866 (Loss_D_real: 0.69310033 Loss_D_fake: 0.69319832) Loss_G: 0.00000970 Loss_Enh_Dec: 0.00000942\n",
      "| epoch  19 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1052.04 | loss  0.33 | ppl     1.40 | acc     0.96 | train_ae_norm     1.00\n",
      "[19/200][2299/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69312495 Loss_D_fake: 0.69316947) Loss_G: 0.00000449 Loss_Enh_Dec: 0.00000449\n",
      "| epoch  19 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1047.55 | loss  0.32 | ppl     1.37 | acc     0.96 | train_ae_norm     1.00\n",
      "[19/200][2399/2499] Loss_D: 1.38629413 (Loss_D_real: 0.69313276 Loss_D_fake: 0.69316143) Loss_G: 0.00000335 Loss_Enh_Dec: 0.00000335\n",
      "| epoch  19 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1048.32 | loss  0.31 | ppl     1.37 | acc     0.94 | train_ae_norm     1.00\n",
      "[19/200][2499/2499] Loss_D: 1.38629448 (Loss_D_real: 0.69313562 Loss_D_fake: 0.69315886) Loss_G: 0.00000238 Loss_Enh_Dec: 0.00000237\n",
      "| end of epoch  19 | time: 2786.79s | test loss  0.46 | test ppl  1.59 | acc 0.961\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 20 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:27.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:34.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:22.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:29.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:36.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:43.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:50.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:56.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:03.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:10.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:17.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:24.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:31.\n",
      "\n",
      "  Average training loss generetor: 0.705\n",
      "  Average training loss discriminator: 0.735\n",
      "  Training epcoh took: 0:02:37\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.525\n",
      "  Test Loss: 2.261\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  20 |     0/ 2499 batches | lr 0.000000 | ms/batch 1576.45 | loss  0.00 | ppl     1.00 | acc     0.95 | train_ae_norm     1.00\n",
      "[20/200][99/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69315147 Loss_D_fake: 0.69314289) Loss_G: -0.00000091 Loss_Enh_Dec: -0.00000094\n",
      "| epoch  20 |   100/ 2499 batches | lr 0.000000 | ms/batch 1049.46 | loss  0.31 | ppl     1.36 | acc     0.94 | train_ae_norm     1.00\n",
      "[20/200][199/2499] Loss_D: 1.38629425 (Loss_D_real: 0.69314194 Loss_D_fake: 0.69315231) Loss_G: 0.00000156 Loss_Enh_Dec: 0.00000146\n",
      "| epoch  20 |   200/ 2499 batches | lr 0.000000 | ms/batch 1048.91 | loss  0.30 | ppl     1.35 | acc     0.97 | train_ae_norm     1.00\n",
      "[20/200][299/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69315112 Loss_D_fake: 0.69314331) Loss_G: -0.00000069 Loss_Enh_Dec: -0.00000070\n",
      "| epoch  20 |   300/ 2499 batches | lr 0.000000 | ms/batch 1050.07 | loss  0.30 | ppl     1.35 | acc     0.97 | train_ae_norm     1.00\n",
      "[20/200][399/2499] Loss_D: 1.38629425 (Loss_D_real: 0.69315040 Loss_D_fake: 0.69314384) Loss_G: -0.00000033 Loss_Enh_Dec: -0.00000034\n",
      "| epoch  20 |   400/ 2499 batches | lr 0.000000 | ms/batch 1049.48 | loss  0.29 | ppl     1.34 | acc     0.96 | train_ae_norm     1.00\n",
      "[20/200][499/2499] Loss_D: 1.38629770 (Loss_D_real: 0.69316137 Loss_D_fake: 0.69313633) Loss_G: -0.00000302 Loss_Enh_Dec: -0.00000320\n",
      "| epoch  20 |   500/ 2499 batches | lr 0.000000 | ms/batch 1049.86 | loss  0.29 | ppl     1.34 | acc     0.95 | train_ae_norm     1.00\n",
      "[20/200][599/2499] Loss_D: 1.38629448 (Loss_D_real: 0.69316685 Loss_D_fake: 0.69312763) Loss_G: -0.00000396 Loss_Enh_Dec: -0.00000401\n",
      "| epoch  20 |   600/ 2499 batches | lr 0.000000 | ms/batch 1052.51 | loss  0.29 | ppl     1.34 | acc     0.96 | train_ae_norm     1.00\n",
      "[20/200][699/2499] Loss_D: 1.38629377 (Loss_D_real: 0.69313449 Loss_D_fake: 0.69315928) Loss_G: 0.00000163 Loss_Enh_Dec: 0.00000131\n",
      "| epoch  20 |   700/ 2499 batches | lr 0.000000 | ms/batch 1052.22 | loss  0.30 | ppl     1.34 | acc     0.95 | train_ae_norm     1.00\n",
      "[20/200][799/2499] Loss_D: 1.38629937 (Loss_D_real: 0.69308031 Loss_D_fake: 0.69321907) Loss_G: 0.00002088 Loss_Enh_Dec: 0.00001634\n",
      "| epoch  20 |   800/ 2499 batches | lr 0.000000 | ms/batch 1051.46 | loss  0.33 | ppl     1.40 | acc     0.92 | train_ae_norm     1.00\n",
      "[20/200][899/2499] Loss_D: 1.38628936 (Loss_D_real: 0.69320631 Loss_D_fake: 0.69308299) Loss_G: -0.00001247 Loss_Enh_Dec: -0.00001341\n",
      "| epoch  20 |   900/ 2499 batches | lr 0.000000 | ms/batch 1048.71 | loss  0.31 | ppl     1.36 | acc     0.96 | train_ae_norm     1.00\n",
      "[20/200][999/2499] Loss_D: 1.38629365 (Loss_D_real: 0.69320458 Loss_D_fake: 0.69308901) Loss_G: -0.00001094 Loss_Enh_Dec: -0.00001191\n",
      "| epoch  20 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1045.28 | loss  0.38 | ppl     1.47 | acc     0.95 | train_ae_norm     1.00\n",
      "[20/200][1099/2499] Loss_D: 1.38629341 (Loss_D_real: 0.69312638 Loss_D_fake: 0.69316709) Loss_G: 0.00000382 Loss_Enh_Dec: 0.00000316\n",
      "| epoch  20 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1042.33 | loss  0.36 | ppl     1.44 | acc     0.95 | train_ae_norm     1.00\n",
      "[20/200][1199/2499] Loss_D: 1.38629317 (Loss_D_real: 0.69311726 Loss_D_fake: 0.69317591) Loss_G: 0.00000554 Loss_Enh_Dec: 0.00000577\n",
      "| epoch  20 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1043.48 | loss  0.33 | ppl     1.39 | acc     0.94 | train_ae_norm     1.00\n",
      "[20/200][1299/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69314158 Loss_D_fake: 0.69315308) Loss_G: 0.00000103 Loss_Enh_Dec: 0.00000114\n",
      "| epoch  20 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1042.05 | loss  0.32 | ppl     1.37 | acc     0.94 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/200][1399/2499] Loss_D: 1.38628292 (Loss_D_real: 0.69313812 Loss_D_fake: 0.69314480) Loss_G: 0.00000259 Loss_Enh_Dec: 0.00000155\n",
      "| epoch  20 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1043.37 | loss  0.32 | ppl     1.38 | acc     0.96 | train_ae_norm     1.00\n",
      "[20/200][1499/2499] Loss_D: 1.38629007 (Loss_D_real: 0.69322717 Loss_D_fake: 0.69306296) Loss_G: -0.00001819 Loss_Enh_Dec: -0.00001820\n",
      "| epoch  20 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1045.19 | loss  0.33 | ppl     1.39 | acc     0.94 | train_ae_norm     1.00\n",
      "[20/200][1599/2499] Loss_D: 1.38629413 (Loss_D_real: 0.69310564 Loss_D_fake: 0.69318855) Loss_G: 0.00000837 Loss_Enh_Dec: 0.00000855\n",
      "| epoch  20 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1041.30 | loss  0.34 | ppl     1.40 | acc     0.94 | train_ae_norm     1.00\n",
      "[20/200][1699/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69312799 Loss_D_fake: 0.69316643) Loss_G: 0.00000406 Loss_Enh_Dec: 0.00000405\n",
      "| epoch  20 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1042.38 | loss  0.31 | ppl     1.36 | acc     0.96 | train_ae_norm     1.00\n",
      "[20/200][1799/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69313800 Loss_D_fake: 0.69315636) Loss_G: 0.00000192 Loss_Enh_Dec: 0.00000191\n",
      "| epoch  20 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1042.76 | loss  0.30 | ppl     1.35 | acc     0.97 | train_ae_norm     1.00\n",
      "[20/200][1899/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69314432 Loss_D_fake: 0.69315004) Loss_G: 0.00000058 Loss_Enh_Dec: 0.00000058\n",
      "| epoch  20 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1042.42 | loss  0.29 | ppl     1.34 | acc     0.96 | train_ae_norm     1.00\n",
      "[20/200][1999/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69314641 Loss_D_fake: 0.69314802) Loss_G: 0.00000015 Loss_Enh_Dec: 0.00000015\n",
      "| epoch  20 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1043.04 | loss  0.30 | ppl     1.35 | acc     0.96 | train_ae_norm     1.00\n",
      "[20/200][2099/2499] Loss_D: 1.38629770 (Loss_D_real: 0.69320148 Loss_D_fake: 0.69309616) Loss_G: -0.00000976 Loss_Enh_Dec: -0.00000657\n",
      "| epoch  20 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1044.11 | loss  0.30 | ppl     1.35 | acc     0.96 | train_ae_norm     1.00\n",
      "[20/200][2199/2499] Loss_D: 1.38629532 (Loss_D_real: 0.69319093 Loss_D_fake: 0.69310445) Loss_G: -0.00000861 Loss_Enh_Dec: -0.00000864\n",
      "| epoch  20 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1047.64 | loss  0.31 | ppl     1.36 | acc     0.96 | train_ae_norm     1.00\n",
      "[20/200][2299/2499] Loss_D: 1.38629270 (Loss_D_real: 0.69312942 Loss_D_fake: 0.69316334) Loss_G: 0.00000336 Loss_Enh_Dec: 0.00000198\n",
      "| epoch  20 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1049.73 | loss  0.31 | ppl     1.36 | acc     0.97 | train_ae_norm     1.00\n",
      "[20/200][2399/2499] Loss_D: 1.38629603 (Loss_D_real: 0.69311297 Loss_D_fake: 0.69318312) Loss_G: 0.00000766 Loss_Enh_Dec: 0.00000796\n",
      "| epoch  20 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1051.65 | loss  0.30 | ppl     1.36 | acc     0.93 | train_ae_norm     1.00\n",
      "[20/200][2499/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69314855 Loss_D_fake: 0.69314587) Loss_G: -0.00000034 Loss_Enh_Dec: -0.00000034\n",
      "| end of epoch  20 | time: 2793.71s | test loss  0.46 | test ppl  1.59 | acc 0.962\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 21 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:49.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:56.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:09.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:16.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:23.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:30.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:37.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:44.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:51.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:58.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:05.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:19.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:26.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:33.\n",
      "\n",
      "  Average training loss generetor: 0.707\n",
      "  Average training loss discriminator: 0.725\n",
      "  Training epcoh took: 0:02:39\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.520\n",
      "  Test Loss: 2.386\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  21 |     0/ 2499 batches | lr 0.000000 | ms/batch 1598.64 | loss  0.00 | ppl     1.00 | acc     0.95 | train_ae_norm     1.00\n",
      "[21/200][99/2499] Loss_D: 1.38629580 (Loss_D_real: 0.69316494 Loss_D_fake: 0.69313091) Loss_G: -0.00000319 Loss_Enh_Dec: -0.00000327\n",
      "| epoch  21 |   100/ 2499 batches | lr 0.000000 | ms/batch 1050.26 | loss  0.29 | ppl     1.34 | acc     0.95 | train_ae_norm     1.00\n",
      "[21/200][199/2499] Loss_D: 1.38629603 (Loss_D_real: 0.69319427 Loss_D_fake: 0.69310182) Loss_G: -0.00000951 Loss_Enh_Dec: -0.00000922\n",
      "| epoch  21 |   200/ 2499 batches | lr 0.000000 | ms/batch 1049.34 | loss  0.29 | ppl     1.33 | acc     0.97 | train_ae_norm     1.00\n",
      "[21/200][299/2499] Loss_D: 1.38629544 (Loss_D_real: 0.69312495 Loss_D_fake: 0.69317049) Loss_G: 0.00000485 Loss_Enh_Dec: 0.00000483\n",
      "| epoch  21 |   300/ 2499 batches | lr 0.000000 | ms/batch 1050.98 | loss  0.28 | ppl     1.32 | acc     0.97 | train_ae_norm     1.00\n",
      "[21/200][399/2499] Loss_D: 1.38628674 (Loss_D_real: 0.69312739 Loss_D_fake: 0.69315928) Loss_G: 0.00000293 Loss_Enh_Dec: 0.00000222\n",
      "| epoch  21 |   400/ 2499 batches | lr 0.000000 | ms/batch 1049.37 | loss  0.28 | ppl     1.32 | acc     0.96 | train_ae_norm     1.00\n",
      "[21/200][499/2499] Loss_D: 1.38629413 (Loss_D_real: 0.69314182 Loss_D_fake: 0.69315237) Loss_G: 0.00000072 Loss_Enh_Dec: 0.00000036\n",
      "| epoch  21 |   500/ 2499 batches | lr 0.000000 | ms/batch 1050.34 | loss  0.27 | ppl     1.31 | acc     0.96 | train_ae_norm     1.00\n",
      "[21/200][599/2499] Loss_D: 1.38629389 (Loss_D_real: 0.69315374 Loss_D_fake: 0.69314021) Loss_G: -0.00000133 Loss_Enh_Dec: -0.00000132\n",
      "| epoch  21 |   600/ 2499 batches | lr 0.000000 | ms/batch 1050.04 | loss  0.27 | ppl     1.31 | acc     0.97 | train_ae_norm     1.00\n",
      "[21/200][699/2499] Loss_D: 1.38629913 (Loss_D_real: 0.69309974 Loss_D_fake: 0.69319934) Loss_G: 0.00001088 Loss_Enh_Dec: 0.00000947\n",
      "| epoch  21 |   700/ 2499 batches | lr 0.000000 | ms/batch 1049.02 | loss  0.27 | ppl     1.31 | acc     0.96 | train_ae_norm     1.00\n",
      "[21/200][799/2499] Loss_D: 1.38629246 (Loss_D_real: 0.69318807 Loss_D_fake: 0.69310445) Loss_G: -0.00000659 Loss_Enh_Dec: -0.00000636\n",
      "| epoch  21 |   800/ 2499 batches | lr 0.000000 | ms/batch 1047.24 | loss  0.29 | ppl     1.34 | acc     0.93 | train_ae_norm     1.00\n",
      "[21/200][899/2499] Loss_D: 1.38629639 (Loss_D_real: 0.69316858 Loss_D_fake: 0.69312781) Loss_G: -0.00000457 Loss_Enh_Dec: -0.00000406\n",
      "| epoch  21 |   900/ 2499 batches | lr 0.000000 | ms/batch 1049.98 | loss  0.29 | ppl     1.33 | acc     0.96 | train_ae_norm     1.00\n",
      "[21/200][999/2499] Loss_D: 1.38629472 (Loss_D_real: 0.69320482 Loss_D_fake: 0.69308990) Loss_G: -0.00001106 Loss_Enh_Dec: -0.00001058\n",
      "| epoch  21 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1050.03 | loss  0.29 | ppl     1.34 | acc     0.96 | train_ae_norm     1.00\n",
      "[21/200][1099/2499] Loss_D: 1.38629448 (Loss_D_real: 0.69319266 Loss_D_fake: 0.69310182) Loss_G: -0.00000898 Loss_Enh_Dec: -0.00000886\n",
      "| epoch  21 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1049.76 | loss  0.28 | ppl     1.32 | acc     0.95 | train_ae_norm     1.00\n",
      "[21/200][1199/2499] Loss_D: 1.38629496 (Loss_D_real: 0.69319546 Loss_D_fake: 0.69309950) Loss_G: -0.00000971 Loss_Enh_Dec: -0.00000955\n",
      "| epoch  21 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1049.96 | loss  0.27 | ppl     1.31 | acc     0.94 | train_ae_norm     1.00\n",
      "[21/200][1299/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69319451 Loss_D_fake: 0.69309986) Loss_G: -0.00000952 Loss_Enh_Dec: -0.00000960\n",
      "| epoch  21 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1049.05 | loss  0.27 | ppl     1.31 | acc     0.95 | train_ae_norm     1.00\n",
      "[21/200][1399/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69320649 Loss_D_fake: 0.69308794) Loss_G: -0.00001217 Loss_Enh_Dec: -0.00001201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  21 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1049.40 | loss  0.27 | ppl     1.31 | acc     0.96 | train_ae_norm     1.00\n",
      "[21/200][1499/2499] Loss_D: 1.38629520 (Loss_D_real: 0.69322193 Loss_D_fake: 0.69307327) Loss_G: -0.00001458 Loss_Enh_Dec: -0.00001492\n",
      "| epoch  21 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1049.31 | loss  0.27 | ppl     1.31 | acc     0.95 | train_ae_norm     1.00\n",
      "[21/200][1599/2499] Loss_D: 1.38629556 (Loss_D_real: 0.69320452 Loss_D_fake: 0.69309109) Loss_G: -0.00001077 Loss_Enh_Dec: -0.00001091\n",
      "| epoch  21 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1049.38 | loss  0.27 | ppl     1.31 | acc     0.95 | train_ae_norm     1.00\n",
      "[21/200][1699/2499] Loss_D: 1.38629401 (Loss_D_real: 0.69317365 Loss_D_fake: 0.69312036) Loss_G: -0.00000495 Loss_Enh_Dec: -0.00000526\n",
      "| epoch  21 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1049.39 | loss  0.26 | ppl     1.30 | acc     0.97 | train_ae_norm     1.00\n",
      "[21/200][1799/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69317186 Loss_D_fake: 0.69312280) Loss_G: -0.00000500 Loss_Enh_Dec: -0.00000505\n",
      "| epoch  21 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1050.14 | loss  0.26 | ppl     1.30 | acc     0.96 | train_ae_norm     1.00\n",
      "[21/200][1899/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69317693 Loss_D_fake: 0.69311750) Loss_G: -0.00000618 Loss_Enh_Dec: -0.00000614\n",
      "| epoch  21 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1049.30 | loss  0.25 | ppl     1.29 | acc     0.96 | train_ae_norm     1.00\n",
      "[21/200][1999/2499] Loss_D: 1.38629496 (Loss_D_real: 0.69312638 Loss_D_fake: 0.69316858) Loss_G: 0.00000466 Loss_Enh_Dec: 0.00000473\n",
      "| epoch  21 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1050.55 | loss  0.29 | ppl     1.34 | acc     0.96 | train_ae_norm     1.00\n",
      "[21/200][2099/2499] Loss_D: 1.38629401 (Loss_D_real: 0.69313741 Loss_D_fake: 0.69315660) Loss_G: 0.00000203 Loss_Enh_Dec: 0.00000208\n",
      "| epoch  21 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1052.54 | loss  0.27 | ppl     1.31 | acc     0.98 | train_ae_norm     1.00\n",
      "[21/200][2199/2499] Loss_D: 1.38629556 (Loss_D_real: 0.69315851 Loss_D_fake: 0.69313711) Loss_G: -0.00000185 Loss_Enh_Dec: -0.00000241\n",
      "| epoch  21 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1052.96 | loss  0.26 | ppl     1.30 | acc     0.97 | train_ae_norm     1.00\n",
      "[21/200][2299/2499] Loss_D: 1.38629377 (Loss_D_real: 0.69314283 Loss_D_fake: 0.69315094) Loss_G: 0.00000088 Loss_Enh_Dec: 0.00000117\n",
      "| epoch  21 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1052.11 | loss  0.26 | ppl     1.30 | acc     0.97 | train_ae_norm     1.00\n",
      "[21/200][2399/2499] Loss_D: 1.38628983 (Loss_D_real: 0.69316739 Loss_D_fake: 0.69312239) Loss_G: -0.00000557 Loss_Enh_Dec: -0.00000419\n",
      "| epoch  21 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1051.13 | loss  0.27 | ppl     1.31 | acc     0.95 | train_ae_norm     1.00\n",
      "[21/200][2499/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69313306 Loss_D_fake: 0.69316137) Loss_G: 0.00000301 Loss_Enh_Dec: 0.00000301\n",
      "| end of epoch  21 | time: 2803.97s | test loss  0.46 | test ppl  1.58 | acc 0.964\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 22 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:09.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:16.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:23.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:30.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:37.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:44.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:51.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:58.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:05.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:19.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:25.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:32.\n",
      "\n",
      "  Average training loss generetor: 0.707\n",
      "  Average training loss discriminator: 0.721\n",
      "  Training epcoh took: 0:02:39\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.510\n",
      "  Test Loss: 2.486\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  22 |     0/ 2499 batches | lr 0.000000 | ms/batch 1595.34 | loss  0.00 | ppl     1.00 | acc     0.96 | train_ae_norm     1.00\n",
      "[22/200][99/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69315481 Loss_D_fake: 0.69313955) Loss_G: -0.00000160 Loss_Enh_Dec: -0.00000160\n",
      "| epoch  22 |   100/ 2499 batches | lr 0.000000 | ms/batch 1050.49 | loss  0.26 | ppl     1.30 | acc     0.95 | train_ae_norm     1.00\n",
      "[22/200][199/2499] Loss_D: 1.38628960 (Loss_D_real: 0.69315028 Loss_D_fake: 0.69313926) Loss_G: -0.00000285 Loss_Enh_Dec: -0.00000532\n",
      "| epoch  22 |   200/ 2499 batches | lr 0.000000 | ms/batch 1049.04 | loss  0.26 | ppl     1.29 | acc     0.96 | train_ae_norm     1.00\n",
      "[22/200][299/2499] Loss_D: 1.38630795 (Loss_D_real: 0.69309229 Loss_D_fake: 0.69321573) Loss_G: 0.00001109 Loss_Enh_Dec: 0.00001334\n",
      "| epoch  22 |   300/ 2499 batches | lr 0.000000 | ms/batch 1049.81 | loss  0.29 | ppl     1.33 | acc     0.96 | train_ae_norm     1.00\n",
      "[22/200][399/2499] Loss_D: 1.38630176 (Loss_D_real: 0.69313955 Loss_D_fake: 0.69316220) Loss_G: 0.00000329 Loss_Enh_Dec: 0.00000442\n",
      "| epoch  22 |   400/ 2499 batches | lr 0.000000 | ms/batch 1049.09 | loss  0.31 | ppl     1.36 | acc     0.97 | train_ae_norm     1.00\n",
      "[22/200][499/2499] Loss_D: 1.38629758 (Loss_D_real: 0.69321746 Loss_D_fake: 0.69308013) Loss_G: -0.00001388 Loss_Enh_Dec: -0.00001444\n",
      "| epoch  22 |   500/ 2499 batches | lr 0.000000 | ms/batch 1048.96 | loss  0.26 | ppl     1.30 | acc     0.95 | train_ae_norm     1.00\n",
      "[22/200][599/2499] Loss_D: 1.38629580 (Loss_D_real: 0.69314444 Loss_D_fake: 0.69315135) Loss_G: 0.00000061 Loss_Enh_Dec: 0.00000070\n",
      "| epoch  22 |   600/ 2499 batches | lr 0.000000 | ms/batch 1049.76 | loss  0.26 | ppl     1.29 | acc     0.97 | train_ae_norm     1.00\n",
      "[22/200][699/2499] Loss_D: 1.38630438 (Loss_D_real: 0.69318241 Loss_D_fake: 0.69312191) Loss_G: -0.00000507 Loss_Enh_Dec: -0.00000490\n",
      "| epoch  22 |   700/ 2499 batches | lr 0.000000 | ms/batch 1049.22 | loss  0.26 | ppl     1.30 | acc     0.95 | train_ae_norm     1.00\n",
      "[22/200][799/2499] Loss_D: 1.38629317 (Loss_D_real: 0.69304883 Loss_D_fake: 0.69324434) Loss_G: 0.00001941 Loss_Enh_Dec: 0.00001946\n",
      "| epoch  22 |   800/ 2499 batches | lr 0.000000 | ms/batch 1048.93 | loss  0.27 | ppl     1.31 | acc     0.94 | train_ae_norm     1.00\n",
      "[22/200][899/2499] Loss_D: 1.38629448 (Loss_D_real: 0.69311565 Loss_D_fake: 0.69317883) Loss_G: 0.00000522 Loss_Enh_Dec: 0.00000513\n",
      "| epoch  22 |   900/ 2499 batches | lr 0.000000 | ms/batch 1048.61 | loss  0.26 | ppl     1.30 | acc     0.97 | train_ae_norm     1.00\n",
      "[22/200][999/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69302249 Loss_D_fake: 0.69327211) Loss_G: 0.00002622 Loss_Enh_Dec: 0.00002606\n",
      "| epoch  22 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1049.15 | loss  0.26 | ppl     1.30 | acc     0.97 | train_ae_norm     1.00\n",
      "[22/200][1099/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69312572 Loss_D_fake: 0.69316864) Loss_G: 0.00000459 Loss_Enh_Dec: 0.00000448\n",
      "| epoch  22 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1049.38 | loss  0.25 | ppl     1.28 | acc     0.96 | train_ae_norm     1.00\n",
      "[22/200][1199/2499] Loss_D: 1.38629484 (Loss_D_real: 0.69318217 Loss_D_fake: 0.69311273) Loss_G: -0.00000684 Loss_Enh_Dec: -0.00000680\n",
      "| epoch  22 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1049.39 | loss  0.24 | ppl     1.27 | acc     0.95 | train_ae_norm     1.00\n",
      "[22/200][1299/2499] Loss_D: 1.38629413 (Loss_D_real: 0.69313574 Loss_D_fake: 0.69315833) Loss_G: 0.00000258 Loss_Enh_Dec: 0.00000222\n",
      "| epoch  22 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1049.77 | loss  0.25 | ppl     1.29 | acc     0.95 | train_ae_norm     1.00\n",
      "[22/200][1399/2499] Loss_D: 1.38629496 (Loss_D_real: 0.69313568 Loss_D_fake: 0.69315928) Loss_G: 0.00000189 Loss_Enh_Dec: 0.00000234\n",
      "| epoch  22 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1053.59 | loss  0.26 | ppl     1.29 | acc     0.96 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22/200][1499/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69313288 Loss_D_fake: 0.69316149) Loss_G: 0.00000295 Loss_Enh_Dec: 0.00000295\n",
      "| epoch  22 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1051.86 | loss  0.24 | ppl     1.27 | acc     0.95 | train_ae_norm     1.00\n",
      "[22/200][1599/2499] Loss_D: 1.38629448 (Loss_D_real: 0.69314069 Loss_D_fake: 0.69315380) Loss_G: 0.00000128 Loss_Enh_Dec: 0.00000141\n",
      "| epoch  22 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1049.71 | loss  0.24 | ppl     1.28 | acc     0.96 | train_ae_norm     1.00\n",
      "[22/200][1699/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69314539 Loss_D_fake: 0.69314897) Loss_G: 0.00000032 Loss_Enh_Dec: 0.00000029\n",
      "| epoch  22 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1048.29 | loss  0.23 | ppl     1.26 | acc     0.96 | train_ae_norm     1.00\n",
      "[22/200][1799/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69318664 Loss_D_fake: 0.69310796) Loss_G: -0.00000547 Loss_Enh_Dec: -0.00000550\n",
      "| epoch  22 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1052.68 | loss  0.24 | ppl     1.27 | acc     0.97 | train_ae_norm     1.00\n",
      "[22/200][1899/2499] Loss_D: 1.38629460 (Loss_D_real: 0.69314569 Loss_D_fake: 0.69314897) Loss_G: 0.00000039 Loss_Enh_Dec: 0.00000022\n",
      "| epoch  22 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1051.54 | loss  0.22 | ppl     1.25 | acc     0.97 | train_ae_norm     1.00\n",
      "[22/200][1999/2499] Loss_D: 1.38629413 (Loss_D_real: 0.69314373 Loss_D_fake: 0.69315046) Loss_G: 0.00000062 Loss_Enh_Dec: 0.00000029\n",
      "| epoch  22 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1051.21 | loss  0.23 | ppl     1.26 | acc     0.97 | train_ae_norm     1.00\n",
      "[22/200][2099/2499] Loss_D: 1.38629508 (Loss_D_real: 0.69324017 Loss_D_fake: 0.69305485) Loss_G: -0.00001945 Loss_Enh_Dec: -0.00001986\n",
      "| epoch  22 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1051.77 | loss  0.23 | ppl     1.26 | acc     0.98 | train_ae_norm     1.00\n",
      "[22/200][2199/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69326210 Loss_D_fake: 0.69303226) Loss_G: -0.00002300 Loss_Enh_Dec: -0.00002299\n",
      "| epoch  22 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1050.63 | loss  0.22 | ppl     1.25 | acc     0.98 | train_ae_norm     1.00\n",
      "[22/200][2299/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69327724 Loss_D_fake: 0.69301713) Loss_G: -0.00002634 Loss_Enh_Dec: -0.00002625\n",
      "| epoch  22 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1051.20 | loss  0.23 | ppl     1.26 | acc     0.97 | train_ae_norm     1.00\n",
      "[22/200][2399/2499] Loss_D: 1.38629472 (Loss_D_real: 0.69319785 Loss_D_fake: 0.69309688) Loss_G: -0.00001123 Loss_Enh_Dec: -0.00001129\n",
      "| epoch  22 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1051.29 | loss  0.22 | ppl     1.25 | acc     0.96 | train_ae_norm     1.00\n",
      "[22/200][2499/2499] Loss_D: 1.38629401 (Loss_D_real: 0.69314468 Loss_D_fake: 0.69314933) Loss_G: 0.00000051 Loss_Enh_Dec: 0.00000036\n",
      "| end of epoch  22 | time: 2804.16s | test loss  0.45 | test ppl  1.57 | acc 0.966\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 23 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:49.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:09.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:16.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:23.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:30.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:37.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:44.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:51.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:58.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:05.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:19.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:26.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:32.\n",
      "\n",
      "  Average training loss generetor: 0.706\n",
      "  Average training loss discriminator: 0.719\n",
      "  Training epcoh took: 0:02:39\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.500\n",
      "  Test Loss: 2.648\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  23 |     0/ 2499 batches | lr 0.000000 | ms/batch 1595.97 | loss  0.00 | ppl     1.00 | acc     0.96 | train_ae_norm     1.00\n",
      "[23/200][99/2499] Loss_D: 1.38628948 (Loss_D_real: 0.69311160 Loss_D_fake: 0.69317788) Loss_G: 0.00000712 Loss_Enh_Dec: 0.00000887\n",
      "| epoch  23 |   100/ 2499 batches | lr 0.000000 | ms/batch 1053.94 | loss  0.23 | ppl     1.25 | acc     0.94 | train_ae_norm     1.00\n",
      "[23/200][199/2499] Loss_D: 1.38629448 (Loss_D_real: 0.69311994 Loss_D_fake: 0.69317454) Loss_G: 0.00000518 Loss_Enh_Dec: 0.00000499\n",
      "| epoch  23 |   200/ 2499 batches | lr 0.000000 | ms/batch 1051.14 | loss  0.23 | ppl     1.26 | acc     0.97 | train_ae_norm     1.00\n",
      "[23/200][299/2499] Loss_D: 1.38629436 (Loss_D_real: 0.69312930 Loss_D_fake: 0.69316500) Loss_G: 0.00000300 Loss_Enh_Dec: 0.00000125\n",
      "| epoch  23 |   300/ 2499 batches | lr 0.000000 | ms/batch 1051.70 | loss  0.23 | ppl     1.25 | acc     0.98 | train_ae_norm     1.00\n",
      "[23/200][399/2499] Loss_D: 1.38632822 (Loss_D_real: 0.69321126 Loss_D_fake: 0.69311702) Loss_G: -0.00001410 Loss_Enh_Dec: -0.00000697\n",
      "| epoch  23 |   400/ 2499 batches | lr 0.000000 | ms/batch 1051.16 | loss  0.24 | ppl     1.27 | acc     0.96 | train_ae_norm     1.00\n",
      "[23/200][499/2499] Loss_D: 1.38629472 (Loss_D_real: 0.69312620 Loss_D_fake: 0.69316852) Loss_G: 0.00000377 Loss_Enh_Dec: 0.00000307\n",
      "| epoch  23 |   500/ 2499 batches | lr 0.000000 | ms/batch 1050.34 | loss  0.27 | ppl     1.31 | acc     0.96 | train_ae_norm     1.00\n",
      "[23/200][599/2499] Loss_D: 1.38629723 (Loss_D_real: 0.69311208 Loss_D_fake: 0.69318521) Loss_G: 0.00000578 Loss_Enh_Dec: 0.00001174\n",
      "| epoch  23 |   600/ 2499 batches | lr 0.000000 | ms/batch 1049.40 | loss  0.30 | ppl     1.35 | acc     0.96 | train_ae_norm     1.00\n",
      "[23/200][699/2499] Loss_D: 1.38628674 (Loss_D_real: 0.69311923 Loss_D_fake: 0.69316745) Loss_G: 0.00001230 Loss_Enh_Dec: 0.00001515\n",
      "| epoch  23 |   700/ 2499 batches | lr 0.000000 | ms/batch 1050.40 | loss  0.33 | ppl     1.39 | acc     0.94 | train_ae_norm     1.00\n",
      "[23/200][799/2499] Loss_D: 1.38627076 (Loss_D_real: 0.69311273 Loss_D_fake: 0.69315803) Loss_G: -0.00000814 Loss_Enh_Dec: -0.00001805\n",
      "| epoch  23 |   800/ 2499 batches | lr 0.000000 | ms/batch 1049.16 | loss  0.38 | ppl     1.46 | acc     0.91 | train_ae_norm     1.00\n",
      "[23/200][899/2499] Loss_D: 1.38640428 (Loss_D_real: 0.69338340 Loss_D_fake: 0.69302082) Loss_G: 0.00001257 Loss_Enh_Dec: 0.00001880\n",
      "| epoch  23 |   900/ 2499 batches | lr 0.000000 | ms/batch 1051.84 | loss  0.65 | ppl     1.91 | acc     0.89 | train_ae_norm     1.00\n",
      "[23/200][999/2499] Loss_D: 1.38634646 (Loss_D_real: 0.69318461 Loss_D_fake: 0.69316185) Loss_G: 0.00000847 Loss_Enh_Dec: -0.00000251\n",
      "| epoch  23 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1051.37 | loss  0.75 | ppl     2.12 | acc     0.90 | train_ae_norm     1.00\n",
      "[23/200][1099/2499] Loss_D: 1.38632512 (Loss_D_real: 0.69311702 Loss_D_fake: 0.69320810) Loss_G: -0.00000382 Loss_Enh_Dec: 0.00001199\n",
      "| epoch  23 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1047.72 | loss  0.80 | ppl     2.22 | acc     0.89 | train_ae_norm     1.00\n",
      "[23/200][1199/2499] Loss_D: 1.38633060 (Loss_D_real: 0.69318837 Loss_D_fake: 0.69314218) Loss_G: -0.00002826 Loss_Enh_Dec: -0.00000918\n",
      "| epoch  23 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1048.26 | loss  0.81 | ppl     2.26 | acc     0.87 | train_ae_norm     1.00\n",
      "[23/200][1299/2499] Loss_D: 1.38600504 (Loss_D_real: 0.69297189 Loss_D_fake: 0.69303316) Loss_G: 0.00000862 Loss_Enh_Dec: 0.00002515\n",
      "| epoch  23 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1046.13 | loss  0.90 | ppl     2.46 | acc     0.86 | train_ae_norm     1.00\n",
      "[23/200][1399/2499] Loss_D: 1.38630199 (Loss_D_real: 0.69327182 Loss_D_fake: 0.69303024) Loss_G: 0.00001400 Loss_Enh_Dec: 0.00003430\n",
      "| epoch  23 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1047.74 | loss  1.22 | ppl     3.38 | acc     0.84 | train_ae_norm     1.00\n",
      "[23/200][1499/2499] Loss_D: 1.38641882 (Loss_D_real: 0.69318742 Loss_D_fake: 0.69323146) Loss_G: -0.00002288 Loss_Enh_Dec: -0.00003317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  23 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1044.47 | loss  1.06 | ppl     2.88 | acc     0.87 | train_ae_norm     1.00\n",
      "[23/200][1599/2499] Loss_D: 1.38637078 (Loss_D_real: 0.69324243 Loss_D_fake: 0.69312835) Loss_G: 0.00002056 Loss_Enh_Dec: 0.00000118\n",
      "| epoch  23 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1046.91 | loss  0.85 | ppl     2.35 | acc     0.87 | train_ae_norm     1.00\n",
      "[23/200][1699/2499] Loss_D: 1.38633621 (Loss_D_real: 0.69302845 Loss_D_fake: 0.69330776) Loss_G: -0.00003126 Loss_Enh_Dec: 0.00002679\n",
      "| epoch  23 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1046.39 | loss  0.76 | ppl     2.15 | acc     0.89 | train_ae_norm     1.00\n",
      "[23/200][1799/2499] Loss_D: 1.38650036 (Loss_D_real: 0.69317055 Loss_D_fake: 0.69332981) Loss_G: 0.00001054 Loss_Enh_Dec: 0.00002975\n",
      "| epoch  23 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1048.07 | loss  0.67 | ppl     1.96 | acc     0.92 | train_ae_norm     1.00\n",
      "[23/200][1899/2499] Loss_D: 1.38642359 (Loss_D_real: 0.69324052 Loss_D_fake: 0.69318312) Loss_G: -0.00000171 Loss_Enh_Dec: 0.00001312\n",
      "| epoch  23 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1046.86 | loss  0.62 | ppl     1.86 | acc     0.88 | train_ae_norm     1.00\n",
      "[23/200][1999/2499] Loss_D: 1.38634777 (Loss_D_real: 0.69311875 Loss_D_fake: 0.69322908) Loss_G: -0.00002684 Loss_Enh_Dec: 0.00001051\n",
      "| epoch  23 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1047.05 | loss  0.60 | ppl     1.83 | acc     0.92 | train_ae_norm     1.00\n",
      "[23/200][2099/2499] Loss_D: 1.38625824 (Loss_D_real: 0.69302458 Loss_D_fake: 0.69323367) Loss_G: -0.00003957 Loss_Enh_Dec: 0.00001123\n",
      "| epoch  23 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1046.90 | loss  0.57 | ppl     1.77 | acc     0.93 | train_ae_norm     1.00\n",
      "[23/200][2199/2499] Loss_D: 1.38660717 (Loss_D_real: 0.69335479 Loss_D_fake: 0.69325244) Loss_G: -0.00001553 Loss_Enh_Dec: -0.00000031\n",
      "| epoch  23 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1049.34 | loss  0.55 | ppl     1.74 | acc     0.91 | train_ae_norm     1.00\n",
      "[23/200][2299/2499] Loss_D: 1.38632786 (Loss_D_real: 0.69307250 Loss_D_fake: 0.69325536) Loss_G: 0.00001042 Loss_Enh_Dec: -0.00018276\n",
      "| epoch  23 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1050.20 | loss  0.68 | ppl     1.98 | acc     0.91 | train_ae_norm     1.00\n",
      "[23/200][2399/2499] Loss_D: 1.38636637 (Loss_D_real: 0.69311869 Loss_D_fake: 0.69324768) Loss_G: 0.00001432 Loss_Enh_Dec: -0.00006291\n",
      "| epoch  23 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1050.88 | loss  0.70 | ppl     2.01 | acc     0.88 | train_ae_norm     1.00\n",
      "[23/200][2499/2499] Loss_D: 1.38638377 (Loss_D_real: 0.69310015 Loss_D_fake: 0.69328368) Loss_G: -0.00000715 Loss_Enh_Dec: -0.00005646\n",
      "| end of epoch  23 | time: 2801.31s | test loss  0.63 | test ppl  1.87 | acc 0.936\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 24 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:27.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:34.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:22.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:29.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:36.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:43.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:50.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:56.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:03.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:10.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:17.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:24.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:31.\n",
      "\n",
      "  Average training loss generetor: 0.706\n",
      "  Average training loss discriminator: 0.718\n",
      "  Training epcoh took: 0:02:37\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.500\n",
      "  Test Loss: 2.794\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  24 |     0/ 2499 batches | lr 0.000000 | ms/batch 1577.77 | loss  0.01 | ppl     1.01 | acc     0.88 | train_ae_norm     1.00\n",
      "[24/200][99/2499] Loss_D: 1.38629913 (Loss_D_real: 0.69312990 Loss_D_fake: 0.69316930) Loss_G: 0.00001687 Loss_Enh_Dec: 0.00000954\n",
      "| epoch  24 |   100/ 2499 batches | lr 0.000000 | ms/batch 1046.28 | loss  0.71 | ppl     2.04 | acc     0.87 | train_ae_norm     1.00\n",
      "[24/200][199/2499] Loss_D: 1.38626027 (Loss_D_real: 0.69298112 Loss_D_fake: 0.69327915) Loss_G: -0.00004873 Loss_Enh_Dec: 0.00001093\n",
      "| epoch  24 |   200/ 2499 batches | lr 0.000000 | ms/batch 1044.02 | loss  0.65 | ppl     1.92 | acc     0.91 | train_ae_norm     1.00\n",
      "[24/200][299/2499] Loss_D: 1.38640010 (Loss_D_real: 0.69332641 Loss_D_fake: 0.69307369) Loss_G: -0.00003867 Loss_Enh_Dec: -0.00001417\n",
      "| epoch  24 |   300/ 2499 batches | lr 0.000000 | ms/batch 1043.60 | loss  0.65 | ppl     1.92 | acc     0.90 | train_ae_norm     1.00\n",
      "[24/200][399/2499] Loss_D: 1.38614404 (Loss_D_real: 0.69293469 Loss_D_fake: 0.69320935) Loss_G: 0.00003390 Loss_Enh_Dec: 0.00000647\n",
      "| epoch  24 |   400/ 2499 batches | lr 0.000000 | ms/batch 1043.83 | loss  0.62 | ppl     1.86 | acc     0.92 | train_ae_norm     1.00\n",
      "[24/200][499/2499] Loss_D: 1.38642001 (Loss_D_real: 0.69313323 Loss_D_fake: 0.69328684) Loss_G: -0.00002053 Loss_Enh_Dec: 0.00001527\n",
      "| epoch  24 |   500/ 2499 batches | lr 0.000000 | ms/batch 1044.08 | loss  0.63 | ppl     1.87 | acc     0.91 | train_ae_norm     1.00\n",
      "[24/200][599/2499] Loss_D: 1.38622141 (Loss_D_real: 0.69319743 Loss_D_fake: 0.69302392) Loss_G: 0.00005236 Loss_Enh_Dec: 0.00003438\n",
      "| epoch  24 |   600/ 2499 batches | lr 0.000000 | ms/batch 1043.76 | loss  0.61 | ppl     1.84 | acc     0.91 | train_ae_norm     1.00\n",
      "[24/200][699/2499] Loss_D: 1.38635778 (Loss_D_real: 0.69305611 Loss_D_fake: 0.69330168) Loss_G: -0.00001692 Loss_Enh_Dec: -0.00001431\n",
      "| epoch  24 |   700/ 2499 batches | lr 0.000000 | ms/batch 1043.83 | loss  0.60 | ppl     1.82 | acc     0.90 | train_ae_norm     1.00\n",
      "[24/200][799/2499] Loss_D: 1.38679647 (Loss_D_real: 0.69355190 Loss_D_fake: 0.69324458) Loss_G: -0.00000968 Loss_Enh_Dec: 0.00001911\n",
      "| epoch  24 |   800/ 2499 batches | lr 0.000000 | ms/batch 1044.00 | loss  0.61 | ppl     1.83 | acc     0.89 | train_ae_norm     1.00\n",
      "[24/200][899/2499] Loss_D: 1.38627601 (Loss_D_real: 0.69300961 Loss_D_fake: 0.69326639) Loss_G: -0.00000023 Loss_Enh_Dec: -0.00000999\n",
      "| epoch  24 |   900/ 2499 batches | lr 0.000000 | ms/batch 1043.15 | loss  0.58 | ppl     1.79 | acc     0.93 | train_ae_norm     1.00\n",
      "[24/200][999/2499] Loss_D: 1.38647556 (Loss_D_real: 0.69311333 Loss_D_fake: 0.69336224) Loss_G: -0.00002289 Loss_Enh_Dec: -0.00001069\n",
      "| epoch  24 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1043.71 | loss  0.61 | ppl     1.84 | acc     0.91 | train_ae_norm     1.00\n",
      "[24/200][1099/2499] Loss_D: 1.38644898 (Loss_D_real: 0.69330382 Loss_D_fake: 0.69314516) Loss_G: 0.00001890 Loss_Enh_Dec: 0.00003331\n",
      "| epoch  24 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1043.47 | loss  0.57 | ppl     1.77 | acc     0.93 | train_ae_norm     1.00\n",
      "[24/200][1199/2499] Loss_D: 1.38655818 (Loss_D_real: 0.69331902 Loss_D_fake: 0.69323915) Loss_G: 0.00002069 Loss_Enh_Dec: 0.00004302\n",
      "| epoch  24 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1043.72 | loss  0.56 | ppl     1.75 | acc     0.90 | train_ae_norm     1.00\n",
      "[24/200][1299/2499] Loss_D: 1.38629723 (Loss_D_real: 0.69300050 Loss_D_fake: 0.69329679) Loss_G: -0.00003826 Loss_Enh_Dec: 0.00001335\n",
      "| epoch  24 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1043.11 | loss  0.59 | ppl     1.81 | acc     0.90 | train_ae_norm     1.00\n",
      "[24/200][1399/2499] Loss_D: 1.38661098 (Loss_D_real: 0.69320345 Loss_D_fake: 0.69340754) Loss_G: -0.00002028 Loss_Enh_Dec: 0.00000225\n",
      "| epoch  24 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1044.45 | loss  0.58 | ppl     1.78 | acc     0.92 | train_ae_norm     1.00\n",
      "[24/200][1499/2499] Loss_D: 1.38643289 (Loss_D_real: 0.69317758 Loss_D_fake: 0.69325531) Loss_G: -0.00004573 Loss_Enh_Dec: -0.00000694\n",
      "| epoch  24 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1043.62 | loss  0.56 | ppl     1.75 | acc     0.92 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24/200][1599/2499] Loss_D: 1.38656378 (Loss_D_real: 0.69330400 Loss_D_fake: 0.69325972) Loss_G: -0.00000310 Loss_Enh_Dec: 0.00000780\n",
      "| epoch  24 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1043.88 | loss  0.56 | ppl     1.75 | acc     0.89 | train_ae_norm     1.00\n",
      "[24/200][1699/2499] Loss_D: 1.38644433 (Loss_D_real: 0.69306779 Loss_D_fake: 0.69337654) Loss_G: -0.00000812 Loss_Enh_Dec: -0.00002756\n",
      "| epoch  24 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1043.78 | loss  0.54 | ppl     1.71 | acc     0.94 | train_ae_norm     1.00\n",
      "[24/200][1799/2499] Loss_D: 1.38657510 (Loss_D_real: 0.69332451 Loss_D_fake: 0.69325060) Loss_G: 0.00002516 Loss_Enh_Dec: 0.00002203\n",
      "| epoch  24 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1044.22 | loss  0.56 | ppl     1.75 | acc     0.91 | train_ae_norm     1.00\n",
      "[24/200][1899/2499] Loss_D: 1.38656974 (Loss_D_real: 0.69336081 Loss_D_fake: 0.69320887) Loss_G: -0.00002468 Loss_Enh_Dec: 0.00001198\n",
      "| epoch  24 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1043.97 | loss  0.58 | ppl     1.78 | acc     0.90 | train_ae_norm     1.00\n",
      "[24/200][1999/2499] Loss_D: 1.38621545 (Loss_D_real: 0.69307512 Loss_D_fake: 0.69314027) Loss_G: -0.00003352 Loss_Enh_Dec: 0.00000112\n",
      "| epoch  24 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1044.19 | loss  0.57 | ppl     1.76 | acc     0.93 | train_ae_norm     1.00\n",
      "[24/200][2099/2499] Loss_D: 1.38650024 (Loss_D_real: 0.69335717 Loss_D_fake: 0.69314307) Loss_G: -0.00001985 Loss_Enh_Dec: 0.00003799\n",
      "| epoch  24 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1045.21 | loss  0.54 | ppl     1.72 | acc     0.93 | train_ae_norm     1.00\n",
      "[24/200][2199/2499] Loss_D: 1.38654065 (Loss_D_real: 0.69328207 Loss_D_fake: 0.69325852) Loss_G: -0.00000746 Loss_Enh_Dec: 0.00002133\n",
      "| epoch  24 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1044.83 | loss  0.52 | ppl     1.68 | acc     0.94 | train_ae_norm     1.00\n",
      "[24/200][2299/2499] Loss_D: 1.38641334 (Loss_D_real: 0.69320345 Loss_D_fake: 0.69320989) Loss_G: -0.00000354 Loss_Enh_Dec: 0.00000435\n",
      "| epoch  24 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1045.29 | loss  0.57 | ppl     1.77 | acc     0.92 | train_ae_norm     1.00\n",
      "[24/200][2399/2499] Loss_D: 1.38648355 (Loss_D_real: 0.69320536 Loss_D_fake: 0.69327819) Loss_G: 0.00001306 Loss_Enh_Dec: 0.00000286\n",
      "| epoch  24 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1048.38 | loss  0.57 | ppl     1.78 | acc     0.89 | train_ae_norm     1.00\n",
      "[24/200][2499/2499] Loss_D: 1.38646841 (Loss_D_real: 0.69324058 Loss_D_fake: 0.69322777) Loss_G: -0.00000999 Loss_Enh_Dec: 0.00004053\n",
      "| end of epoch  24 | time: 2787.81s | test loss  0.57 | test ppl  1.77 | acc 0.942\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 25 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:27.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:34.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:09.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:22.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:29.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:36.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:43.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:50.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:57.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:03.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:10.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:17.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:24.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:31.\n",
      "\n",
      "  Average training loss generetor: 0.705\n",
      "  Average training loss discriminator: 0.717\n",
      "  Training epcoh took: 0:02:37\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.510\n",
      "  Test Loss: 2.826\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  25 |     0/ 2499 batches | lr 0.000000 | ms/batch 1578.55 | loss  0.01 | ppl     1.01 | acc     0.89 | train_ae_norm     1.00\n",
      "[25/200][99/2499] Loss_D: 1.38627124 (Loss_D_real: 0.69315374 Loss_D_fake: 0.69311750) Loss_G: 0.00000097 Loss_Enh_Dec: 0.00000904\n",
      "| epoch  25 |   100/ 2499 batches | lr 0.000000 | ms/batch 1045.75 | loss  0.58 | ppl     1.79 | acc     0.87 | train_ae_norm     1.00\n",
      "[25/200][199/2499] Loss_D: 1.38627338 (Loss_D_real: 0.69306797 Loss_D_fake: 0.69320536) Loss_G: -0.00001049 Loss_Enh_Dec: 0.00005528\n",
      "| epoch  25 |   200/ 2499 batches | lr 0.000000 | ms/batch 1042.67 | loss  0.57 | ppl     1.76 | acc     0.93 | train_ae_norm     1.00\n",
      "[25/200][299/2499] Loss_D: 1.38625884 (Loss_D_real: 0.69311571 Loss_D_fake: 0.69314307) Loss_G: -0.00000239 Loss_Enh_Dec: 0.00000624\n",
      "| epoch  25 |   300/ 2499 batches | lr 0.000000 | ms/batch 1045.46 | loss  0.58 | ppl     1.79 | acc     0.92 | train_ae_norm     1.00\n",
      "[25/200][399/2499] Loss_D: 1.38619304 (Loss_D_real: 0.69311923 Loss_D_fake: 0.69307375) Loss_G: 0.00003000 Loss_Enh_Dec: 0.00001133\n",
      "| epoch  25 |   400/ 2499 batches | lr 0.000000 | ms/batch 1045.15 | loss  0.55 | ppl     1.73 | acc     0.92 | train_ae_norm     1.00\n",
      "[25/200][499/2499] Loss_D: 1.38617444 (Loss_D_real: 0.69308078 Loss_D_fake: 0.69309372) Loss_G: 0.00001655 Loss_Enh_Dec: -0.00004071\n",
      "| epoch  25 |   500/ 2499 batches | lr 0.000000 | ms/batch 1045.92 | loss  0.55 | ppl     1.74 | acc     0.90 | train_ae_norm     1.00\n",
      "[25/200][599/2499] Loss_D: 1.38631582 (Loss_D_real: 0.69310844 Loss_D_fake: 0.69320744) Loss_G: -0.00000699 Loss_Enh_Dec: -0.00004462\n",
      "| epoch  25 |   600/ 2499 batches | lr 0.000000 | ms/batch 1045.87 | loss  0.55 | ppl     1.74 | acc     0.93 | train_ae_norm     1.00\n",
      "[25/200][699/2499] Loss_D: 1.38632751 (Loss_D_real: 0.69320571 Loss_D_fake: 0.69312179) Loss_G: 0.00000275 Loss_Enh_Dec: -0.00005542\n",
      "| epoch  25 |   700/ 2499 batches | lr 0.000000 | ms/batch 1043.57 | loss  0.54 | ppl     1.72 | acc     0.90 | train_ae_norm     1.00\n",
      "[25/200][799/2499] Loss_D: 1.38623905 (Loss_D_real: 0.69315040 Loss_D_fake: 0.69308865) Loss_G: -0.00002391 Loss_Enh_Dec: -0.00001202\n",
      "| epoch  25 |   800/ 2499 batches | lr 0.000000 | ms/batch 1045.93 | loss  0.57 | ppl     1.77 | acc     0.88 | train_ae_norm     1.00\n",
      "[25/200][899/2499] Loss_D: 1.38641977 (Loss_D_real: 0.69322848 Loss_D_fake: 0.69319123) Loss_G: -0.00002054 Loss_Enh_Dec: 0.00001601\n",
      "| epoch  25 |   900/ 2499 batches | lr 0.000000 | ms/batch 1044.29 | loss  0.54 | ppl     1.72 | acc     0.93 | train_ae_norm     1.00\n",
      "[25/200][999/2499] Loss_D: 1.38632107 (Loss_D_real: 0.69315588 Loss_D_fake: 0.69316518) Loss_G: 0.00000110 Loss_Enh_Dec: -0.00001336\n",
      "| epoch  25 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1045.53 | loss  0.57 | ppl     1.78 | acc     0.92 | train_ae_norm     1.00\n",
      "[25/200][1099/2499] Loss_D: 1.38631177 (Loss_D_real: 0.69320571 Loss_D_fake: 0.69310606) Loss_G: -0.00000106 Loss_Enh_Dec: 0.00003634\n",
      "| epoch  25 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1044.37 | loss  0.55 | ppl     1.73 | acc     0.93 | train_ae_norm     1.00\n",
      "[25/200][1199/2499] Loss_D: 1.38635612 (Loss_D_real: 0.69322771 Loss_D_fake: 0.69312835) Loss_G: 0.00001368 Loss_Enh_Dec: 0.00001583\n",
      "| epoch  25 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1044.27 | loss  0.54 | ppl     1.71 | acc     0.91 | train_ae_norm     1.00\n",
      "[25/200][1299/2499] Loss_D: 1.38631177 (Loss_D_real: 0.69321269 Loss_D_fake: 0.69309914) Loss_G: -0.00000984 Loss_Enh_Dec: -0.00001031\n",
      "| epoch  25 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1043.47 | loss  0.53 | ppl     1.71 | acc     0.90 | train_ae_norm     1.00\n",
      "[25/200][1399/2499] Loss_D: 1.38629150 (Loss_D_real: 0.69316292 Loss_D_fake: 0.69312859) Loss_G: -0.00000600 Loss_Enh_Dec: 0.00001794\n",
      "| epoch  25 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1043.58 | loss  0.52 | ppl     1.69 | acc     0.92 | train_ae_norm     1.00\n",
      "[25/200][1499/2499] Loss_D: 1.38638997 (Loss_D_real: 0.69325387 Loss_D_fake: 0.69313616) Loss_G: 0.00000236 Loss_Enh_Dec: 0.00000869\n",
      "| epoch  25 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1044.40 | loss  0.49 | ppl     1.64 | acc     0.92 | train_ae_norm     1.00\n",
      "[25/200][1599/2499] Loss_D: 1.38623929 (Loss_D_real: 0.69302696 Loss_D_fake: 0.69321239) Loss_G: 0.00000509 Loss_Enh_Dec: -0.00001059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  25 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1043.98 | loss  0.52 | ppl     1.68 | acc     0.91 | train_ae_norm     1.00\n",
      "[25/200][1699/2499] Loss_D: 1.38634062 (Loss_D_real: 0.69316596 Loss_D_fake: 0.69317472) Loss_G: -0.00002569 Loss_Enh_Dec: 0.00000685\n",
      "| epoch  25 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1043.98 | loss  0.49 | ppl     1.64 | acc     0.94 | train_ae_norm     1.00\n",
      "[25/200][1799/2499] Loss_D: 1.38632941 (Loss_D_real: 0.69314539 Loss_D_fake: 0.69318408) Loss_G: -0.00001273 Loss_Enh_Dec: 0.00001472\n",
      "| epoch  25 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1044.89 | loss  0.50 | ppl     1.65 | acc     0.95 | train_ae_norm     1.00\n",
      "[25/200][1899/2499] Loss_D: 1.38638580 (Loss_D_real: 0.69313663 Loss_D_fake: 0.69324917) Loss_G: -0.00000409 Loss_Enh_Dec: 0.00000496\n",
      "| epoch  25 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1045.78 | loss  0.52 | ppl     1.68 | acc     0.91 | train_ae_norm     1.00\n",
      "[25/200][1999/2499] Loss_D: 1.38647914 (Loss_D_real: 0.69324780 Loss_D_fake: 0.69323128) Loss_G: 0.00000877 Loss_Enh_Dec: -0.00000108\n",
      "| epoch  25 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1045.44 | loss  0.54 | ppl     1.72 | acc     0.93 | train_ae_norm     1.00\n",
      "[25/200][2099/2499] Loss_D: 1.38643372 (Loss_D_real: 0.69316316 Loss_D_fake: 0.69327056) Loss_G: 0.00001276 Loss_Enh_Dec: 0.00000109\n",
      "| epoch  25 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1045.60 | loss  0.53 | ppl     1.71 | acc     0.93 | train_ae_norm     1.00\n",
      "[25/200][2199/2499] Loss_D: 1.38623357 (Loss_D_real: 0.69302952 Loss_D_fake: 0.69320405) Loss_G: 0.00000800 Loss_Enh_Dec: 0.00001328\n",
      "| epoch  25 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1044.88 | loss  0.51 | ppl     1.67 | acc     0.94 | train_ae_norm     1.00\n",
      "[25/200][2299/2499] Loss_D: 1.38630962 (Loss_D_real: 0.69318426 Loss_D_fake: 0.69312537) Loss_G: -0.00000254 Loss_Enh_Dec: 0.00000819\n",
      "| epoch  25 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1044.78 | loss  0.52 | ppl     1.69 | acc     0.93 | train_ae_norm     1.00\n",
      "[25/200][2399/2499] Loss_D: 1.38634253 (Loss_D_real: 0.69319892 Loss_D_fake: 0.69314355) Loss_G: -0.00002723 Loss_Enh_Dec: -0.00003024\n",
      "| epoch  25 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1046.35 | loss  0.51 | ppl     1.66 | acc     0.91 | train_ae_norm     1.00\n",
      "[25/200][2499/2499] Loss_D: 1.38629794 (Loss_D_real: 0.69314706 Loss_D_fake: 0.69315088) Loss_G: 0.00000052 Loss_Enh_Dec: -0.00002578\n",
      "| end of epoch  25 | time: 2789.05s | test loss  0.55 | test ppl  1.73 | acc 0.946\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 26 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:27.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:34.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:09.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:22.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:29.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:36.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:43.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:50.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:56.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:03.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:10.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:17.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:24.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:31.\n",
      "\n",
      "  Average training loss generetor: 0.706\n",
      "  Average training loss discriminator: 0.715\n",
      "  Training epcoh took: 0:02:37\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.490\n",
      "  Test Loss: 3.062\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  26 |     0/ 2499 batches | lr 0.000000 | ms/batch 1577.78 | loss  0.01 | ppl     1.01 | acc     0.92 | train_ae_norm     1.00\n",
      "[26/200][99/2499] Loss_D: 1.38641655 (Loss_D_real: 0.69313467 Loss_D_fake: 0.69328189) Loss_G: 0.00000741 Loss_Enh_Dec: -0.00002623\n",
      "| epoch  26 |   100/ 2499 batches | lr 0.000000 | ms/batch 1044.70 | loss  0.50 | ppl     1.65 | acc     0.91 | train_ae_norm     1.00\n",
      "[26/200][199/2499] Loss_D: 1.38639784 (Loss_D_real: 0.69320631 Loss_D_fake: 0.69319153) Loss_G: -0.00000246 Loss_Enh_Dec: -0.00003221\n",
      "| epoch  26 |   200/ 2499 batches | lr 0.000000 | ms/batch 1044.92 | loss  0.50 | ppl     1.66 | acc     0.93 | train_ae_norm     1.00\n",
      "[26/200][299/2499] Loss_D: 1.38634253 (Loss_D_real: 0.69307160 Loss_D_fake: 0.69327092) Loss_G: 0.00002468 Loss_Enh_Dec: -0.00001980\n",
      "| epoch  26 |   300/ 2499 batches | lr 0.000000 | ms/batch 1045.76 | loss  0.51 | ppl     1.67 | acc     0.93 | train_ae_norm     1.00\n",
      "[26/200][499/2499] Loss_D: 1.38622248 (Loss_D_real: 0.69316292 Loss_D_fake: 0.69305956) Loss_G: -0.00000818 Loss_Enh_Dec: 0.00006449\n",
      "| epoch  26 |   500/ 2499 batches | lr 0.000000 | ms/batch 1043.77 | loss  0.49 | ppl     1.63 | acc     0.92 | train_ae_norm     1.00\n",
      "[26/200][599/2499] Loss_D: 1.38647175 (Loss_D_real: 0.69327927 Loss_D_fake: 0.69319248) Loss_G: 0.00000185 Loss_Enh_Dec: -0.00000496\n",
      "| epoch  26 |   600/ 2499 batches | lr 0.000000 | ms/batch 1043.97 | loss  0.49 | ppl     1.63 | acc     0.95 | train_ae_norm     1.00\n",
      "[26/200][699/2499] Loss_D: 1.38638973 (Loss_D_real: 0.69322419 Loss_D_fake: 0.69316548) Loss_G: 0.00002142 Loss_Enh_Dec: 0.00000315\n",
      "| epoch  26 |   700/ 2499 batches | lr 0.000000 | ms/batch 1044.40 | loss  0.48 | ppl     1.61 | acc     0.92 | train_ae_norm     1.00\n",
      "[26/200][799/2499] Loss_D: 1.38626504 (Loss_D_real: 0.69314003 Loss_D_fake: 0.69312495) Loss_G: -0.00000159 Loss_Enh_Dec: 0.00001600\n",
      "| epoch  26 |   800/ 2499 batches | lr 0.000000 | ms/batch 1043.63 | loss  0.50 | ppl     1.64 | acc     0.91 | train_ae_norm     1.00\n",
      "[26/200][899/2499] Loss_D: 1.38623691 (Loss_D_real: 0.69306439 Loss_D_fake: 0.69317257) Loss_G: -0.00000552 Loss_Enh_Dec: -0.00000383\n",
      "| epoch  26 |   900/ 2499 batches | lr 0.000000 | ms/batch 1044.26 | loss  0.48 | ppl     1.61 | acc     0.94 | train_ae_norm     1.00\n",
      "[26/200][999/2499] Loss_D: 1.38625598 (Loss_D_real: 0.69323409 Loss_D_fake: 0.69302195) Loss_G: 0.00001026 Loss_Enh_Dec: -0.00000515\n",
      "| epoch  26 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1043.50 | loss  0.49 | ppl     1.64 | acc     0.93 | train_ae_norm     1.00\n",
      "[26/200][1099/2499] Loss_D: 1.38631892 (Loss_D_real: 0.69312954 Loss_D_fake: 0.69318932) Loss_G: 0.00002109 Loss_Enh_Dec: 0.00002049\n",
      "| epoch  26 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1044.51 | loss  0.47 | ppl     1.60 | acc     0.93 | train_ae_norm     1.00\n",
      "[26/200][1199/2499] Loss_D: 1.38642490 (Loss_D_real: 0.69314933 Loss_D_fake: 0.69327557) Loss_G: -0.00001319 Loss_Enh_Dec: -0.00000861\n",
      "| epoch  26 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1043.34 | loss  0.48 | ppl     1.62 | acc     0.91 | train_ae_norm     1.00\n",
      "[26/200][1299/2499] Loss_D: 1.38632202 (Loss_D_real: 0.69315690 Loss_D_fake: 0.69316518) Loss_G: 0.00000960 Loss_Enh_Dec: 0.00000536\n",
      "| epoch  26 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1045.27 | loss  0.50 | ppl     1.65 | acc     0.91 | train_ae_norm     1.00\n",
      "[26/200][1399/2499] Loss_D: 1.38632154 (Loss_D_real: 0.69316882 Loss_D_fake: 0.69315279) Loss_G: -0.00000154 Loss_Enh_Dec: 0.00001145\n",
      "| epoch  26 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1043.19 | loss  0.49 | ppl     1.63 | acc     0.93 | train_ae_norm     1.00\n",
      "[26/200][1499/2499] Loss_D: 1.38633347 (Loss_D_real: 0.69322455 Loss_D_fake: 0.69310898) Loss_G: -0.00000113 Loss_Enh_Dec: 0.00001016\n",
      "| epoch  26 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1044.73 | loss  0.47 | ppl     1.60 | acc     0.93 | train_ae_norm     1.00\n",
      "[26/200][1599/2499] Loss_D: 1.38631284 (Loss_D_real: 0.69311512 Loss_D_fake: 0.69319773) Loss_G: 0.00001126 Loss_Enh_Dec: -0.00001701\n",
      "| epoch  26 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1044.02 | loss  0.50 | ppl     1.65 | acc     0.92 | train_ae_norm     1.00\n",
      "[26/200][1699/2499] Loss_D: 1.38637972 (Loss_D_real: 0.69312334 Loss_D_fake: 0.69325632) Loss_G: 0.00000166 Loss_Enh_Dec: 0.00003454\n",
      "| epoch  26 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1044.50 | loss  0.47 | ppl     1.61 | acc     0.94 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26/200][1799/2499] Loss_D: 1.38623571 (Loss_D_real: 0.69316739 Loss_D_fake: 0.69306839) Loss_G: -0.00002085 Loss_Enh_Dec: 0.00000199\n",
      "| epoch  26 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1044.27 | loss  0.48 | ppl     1.61 | acc     0.95 | train_ae_norm     1.00\n",
      "[26/200][1899/2499] Loss_D: 1.38606858 (Loss_D_real: 0.69294614 Loss_D_fake: 0.69312239) Loss_G: 0.00002463 Loss_Enh_Dec: -0.00003099\n",
      "| epoch  26 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1043.33 | loss  0.48 | ppl     1.61 | acc     0.91 | train_ae_norm     1.00\n",
      "[26/200][1999/2499] Loss_D: 1.38623309 (Loss_D_real: 0.69304752 Loss_D_fake: 0.69318551) Loss_G: -0.00000435 Loss_Enh_Dec: 0.00001641\n",
      "| epoch  26 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1044.25 | loss  0.50 | ppl     1.65 | acc     0.92 | train_ae_norm     1.00\n",
      "[26/200][2099/2499] Loss_D: 1.38652110 (Loss_D_real: 0.69331378 Loss_D_fake: 0.69320738) Loss_G: -0.00004453 Loss_Enh_Dec: -0.00000929\n",
      "| epoch  26 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1045.05 | loss  0.51 | ppl     1.66 | acc     0.93 | train_ae_norm     1.00\n",
      "[26/200][2199/2499] Loss_D: 1.38613653 (Loss_D_real: 0.69303197 Loss_D_fake: 0.69310462) Loss_G: 0.00002829 Loss_Enh_Dec: 0.00002599\n",
      "| epoch  26 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1044.67 | loss  0.49 | ppl     1.64 | acc     0.94 | train_ae_norm     1.00\n",
      "[26/200][2299/2499] Loss_D: 1.38636136 (Loss_D_real: 0.69322681 Loss_D_fake: 0.69313449) Loss_G: 0.00002602 Loss_Enh_Dec: 0.00001146\n",
      "| epoch  26 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1043.75 | loss  0.50 | ppl     1.65 | acc     0.94 | train_ae_norm     1.00\n",
      "[26/200][2399/2499] Loss_D: 1.38641608 (Loss_D_real: 0.69332337 Loss_D_fake: 0.69309270) Loss_G: -0.00000590 Loss_Enh_Dec: 0.00000581\n",
      "| epoch  26 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1044.17 | loss  0.48 | ppl     1.62 | acc     0.90 | train_ae_norm     1.00\n",
      "[26/200][2499/2499] Loss_D: 1.38622820 (Loss_D_real: 0.69317722 Loss_D_fake: 0.69305098) Loss_G: 0.00002471 Loss_Enh_Dec: 0.00003037\n",
      "| end of epoch  26 | time: 2787.69s | test loss  0.54 | test ppl  1.71 | acc 0.948\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 27 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:20.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:27.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:34.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:22.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:29.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:36.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:43.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:50.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:56.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:03.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:10.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:17.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:24.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:31.\n",
      "\n",
      "  Average training loss generetor: 0.705\n",
      "  Average training loss discriminator: 0.716\n",
      "  Training epcoh took: 0:02:37\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.495\n",
      "  Test Loss: 3.161\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  27 |     0/ 2499 batches | lr 0.000000 | ms/batch 1576.39 | loss  0.01 | ppl     1.01 | acc     0.92 | train_ae_norm     1.00\n",
      "[27/200][99/2499] Loss_D: 1.38652945 (Loss_D_real: 0.69330406 Loss_D_fake: 0.69322532) Loss_G: -0.00001461 Loss_Enh_Dec: 0.00001903\n",
      "| epoch  27 |   100/ 2499 batches | lr 0.000000 | ms/batch 1044.10 | loss  0.49 | ppl     1.64 | acc     0.91 | train_ae_norm     1.00\n",
      "[27/200][199/2499] Loss_D: 1.38624990 (Loss_D_real: 0.69313681 Loss_D_fake: 0.69311309) Loss_G: 0.00000618 Loss_Enh_Dec: 0.00000795\n",
      "| epoch  27 |   200/ 2499 batches | lr 0.000000 | ms/batch 1043.93 | loss  0.48 | ppl     1.62 | acc     0.94 | train_ae_norm     1.00\n",
      "[27/200][299/2499] Loss_D: 1.38637793 (Loss_D_real: 0.69314110 Loss_D_fake: 0.69323683) Loss_G: 0.00000078 Loss_Enh_Dec: 0.00001743\n",
      "| epoch  27 |   300/ 2499 batches | lr 0.000000 | ms/batch 1044.65 | loss  0.48 | ppl     1.61 | acc     0.93 | train_ae_norm     1.00\n",
      "[27/200][399/2499] Loss_D: 1.38633943 (Loss_D_real: 0.69316834 Loss_D_fake: 0.69317102) Loss_G: 0.00002157 Loss_Enh_Dec: 0.00003963\n",
      "| epoch  27 |   400/ 2499 batches | lr 0.000000 | ms/batch 1043.90 | loss  0.47 | ppl     1.60 | acc     0.93 | train_ae_norm     1.00\n",
      "[27/200][499/2499] Loss_D: 1.38634157 (Loss_D_real: 0.69317478 Loss_D_fake: 0.69316673) Loss_G: -0.00001415 Loss_Enh_Dec: -0.00002053\n",
      "| epoch  27 |   500/ 2499 batches | lr 0.000000 | ms/batch 1043.06 | loss  0.46 | ppl     1.59 | acc     0.94 | train_ae_norm     1.00\n",
      "[27/200][599/2499] Loss_D: 1.38648808 (Loss_D_real: 0.69323504 Loss_D_fake: 0.69325304) Loss_G: 0.00000548 Loss_Enh_Dec: 0.00000100\n",
      "| epoch  27 |   600/ 2499 batches | lr 0.000000 | ms/batch 1043.39 | loss  0.47 | ppl     1.60 | acc     0.94 | train_ae_norm     1.00\n",
      "[27/200][699/2499] Loss_D: 1.38622606 (Loss_D_real: 0.69318378 Loss_D_fake: 0.69304228) Loss_G: 0.00001684 Loss_Enh_Dec: -0.00001899\n",
      "| epoch  27 |   700/ 2499 batches | lr 0.000000 | ms/batch 1044.82 | loss  0.46 | ppl     1.59 | acc     0.92 | train_ae_norm     1.00\n",
      "[27/200][799/2499] Loss_D: 1.38623261 (Loss_D_real: 0.69325352 Loss_D_fake: 0.69297904) Loss_G: 0.00000518 Loss_Enh_Dec: -0.00001144\n",
      "| epoch  27 |   800/ 2499 batches | lr 0.000000 | ms/batch 1045.29 | loss  0.48 | ppl     1.61 | acc     0.92 | train_ae_norm     1.00\n",
      "[27/200][899/2499] Loss_D: 1.38637674 (Loss_D_real: 0.69310588 Loss_D_fake: 0.69327086) Loss_G: -0.00000489 Loss_Enh_Dec: -0.00002177\n",
      "| epoch  27 |   900/ 2499 batches | lr 0.000000 | ms/batch 1044.86 | loss  0.52 | ppl     1.69 | acc     0.92 | train_ae_norm     1.00\n",
      "[27/200][999/2499] Loss_D: 1.38619399 (Loss_D_real: 0.69312137 Loss_D_fake: 0.69307268) Loss_G: 0.00001240 Loss_Enh_Dec: -0.00004876\n",
      "| epoch  27 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1044.75 | loss  0.60 | ppl     1.83 | acc     0.92 | train_ae_norm     1.00\n",
      "[27/200][1099/2499] Loss_D: 1.38621807 (Loss_D_real: 0.69307256 Loss_D_fake: 0.69314545) Loss_G: -0.00001628 Loss_Enh_Dec: -0.00005088\n",
      "| epoch  27 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1044.25 | loss  0.58 | ppl     1.78 | acc     0.92 | train_ae_norm     1.00\n",
      "[27/200][1199/2499] Loss_D: 1.38625240 (Loss_D_real: 0.69309831 Loss_D_fake: 0.69315404) Loss_G: -0.00001971 Loss_Enh_Dec: -0.00005493\n",
      "| epoch  27 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1046.05 | loss  0.59 | ppl     1.80 | acc     0.90 | train_ae_norm     1.00\n",
      "[27/200][1299/2499] Loss_D: 1.38642728 (Loss_D_real: 0.69320333 Loss_D_fake: 0.69322395) Loss_G: 0.00001893 Loss_Enh_Dec: -0.00001790\n",
      "| epoch  27 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1044.70 | loss  0.60 | ppl     1.82 | acc     0.88 | train_ae_norm     1.00\n",
      "[27/200][1399/2499] Loss_D: 1.38657403 (Loss_D_real: 0.69329214 Loss_D_fake: 0.69328189) Loss_G: -0.00002294 Loss_Enh_Dec: -0.00007658\n",
      "| epoch  27 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1045.96 | loss  0.58 | ppl     1.78 | acc     0.92 | train_ae_norm     1.00\n",
      "[27/200][1499/2499] Loss_D: 1.38619101 (Loss_D_real: 0.69287264 Loss_D_fake: 0.69331837) Loss_G: 0.00000823 Loss_Enh_Dec: -0.00008357\n",
      "| epoch  27 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1046.88 | loss  0.56 | ppl     1.74 | acc     0.90 | train_ae_norm     1.00\n",
      "[27/200][1599/2499] Loss_D: 1.38633442 (Loss_D_real: 0.69318646 Loss_D_fake: 0.69314802) Loss_G: -0.00000085 Loss_Enh_Dec: -0.00009313\n",
      "| epoch  27 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1047.05 | loss  0.59 | ppl     1.81 | acc     0.90 | train_ae_norm     1.00\n",
      "[27/200][1699/2499] Loss_D: 1.38639283 (Loss_D_real: 0.69325066 Loss_D_fake: 0.69314212) Loss_G: 0.00004313 Loss_Enh_Dec: -0.00014713\n",
      "| epoch  27 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1048.10 | loss  0.54 | ppl     1.72 | acc     0.93 | train_ae_norm     1.00\n",
      "[27/200][1799/2499] Loss_D: 1.38608646 (Loss_D_real: 0.69297808 Loss_D_fake: 0.69310844) Loss_G: 0.00003914 Loss_Enh_Dec: 0.00004757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  27 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1045.76 | loss  0.55 | ppl     1.72 | acc     0.94 | train_ae_norm     1.00\n",
      "[27/200][1899/2499] Loss_D: 1.38613403 (Loss_D_real: 0.69326389 Loss_D_fake: 0.69287014) Loss_G: 0.00000886 Loss_Enh_Dec: 0.00008865\n",
      "| epoch  27 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1043.85 | loss  0.56 | ppl     1.75 | acc     0.91 | train_ae_norm     1.00\n",
      "[27/200][1999/2499] Loss_D: 1.38639069 (Loss_D_real: 0.69324958 Loss_D_fake: 0.69314116) Loss_G: 0.00006184 Loss_Enh_Dec: -0.00005206\n",
      "| epoch  27 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1043.35 | loss  0.60 | ppl     1.81 | acc     0.91 | train_ae_norm     1.00\n",
      "[27/200][2099/2499] Loss_D: 1.38616550 (Loss_D_real: 0.69303960 Loss_D_fake: 0.69312590) Loss_G: -0.00001251 Loss_Enh_Dec: -0.00001365\n",
      "| epoch  27 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1048.91 | loss  0.59 | ppl     1.81 | acc     0.93 | train_ae_norm     1.00\n",
      "[27/200][2199/2499] Loss_D: 1.38612413 (Loss_D_real: 0.69298637 Loss_D_fake: 0.69313776) Loss_G: -0.00000073 Loss_Enh_Dec: -0.00005179\n",
      "| epoch  27 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1045.89 | loss  0.57 | ppl     1.78 | acc     0.94 | train_ae_norm     1.00\n",
      "[27/200][2299/2499] Loss_D: 1.38620949 (Loss_D_real: 0.69293898 Loss_D_fake: 0.69327056) Loss_G: 0.00003926 Loss_Enh_Dec: -0.00014614\n",
      "| epoch  27 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1048.87 | loss  0.58 | ppl     1.79 | acc     0.93 | train_ae_norm     1.00\n",
      "[27/200][2399/2499] Loss_D: 1.38668454 (Loss_D_real: 0.69331807 Loss_D_fake: 0.69336647) Loss_G: -0.00003235 Loss_Enh_Dec: -0.00021827\n",
      "| epoch  27 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1049.04 | loss  0.57 | ppl     1.77 | acc     0.89 | train_ae_norm     1.00\n",
      "[27/200][2499/2499] Loss_D: 1.38655460 (Loss_D_real: 0.69342345 Loss_D_fake: 0.69313115) Loss_G: 0.00000594 Loss_Enh_Dec: -0.00014794\n",
      "| end of epoch  27 | time: 2790.65s | test loss  0.56 | test ppl  1.76 | acc 0.944\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 28 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:20.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:27.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:34.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:01.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:22.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:29.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:36.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:42.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:49.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:56.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:03.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:10.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:17.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:24.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:30.\n",
      "\n",
      "  Average training loss generetor: 0.703\n",
      "  Average training loss discriminator: 0.722\n",
      "  Training epcoh took: 0:02:37\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.500\n",
      "  Test Loss: 3.068\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  28 |     0/ 2499 batches | lr 0.000000 | ms/batch 1574.38 | loss  0.01 | ppl     1.01 | acc     0.91 | train_ae_norm     1.00\n",
      "[28/200][99/2499] Loss_D: 1.38649988 (Loss_D_real: 0.69329208 Loss_D_fake: 0.69320774) Loss_G: -0.00003116 Loss_Enh_Dec: -0.00032915\n",
      "| epoch  28 |   100/ 2499 batches | lr 0.000000 | ms/batch 1047.82 | loss  0.56 | ppl     1.74 | acc     0.89 | train_ae_norm     1.00\n",
      "[28/200][199/2499] Loss_D: 1.38604724 (Loss_D_real: 0.69307268 Loss_D_fake: 0.69297457) Loss_G: 0.00005924 Loss_Enh_Dec: 0.00001041\n",
      "| epoch  28 |   200/ 2499 batches | lr 0.000000 | ms/batch 1044.66 | loss  0.56 | ppl     1.75 | acc     0.93 | train_ae_norm     1.00\n",
      "[28/200][299/2499] Loss_D: 1.38637757 (Loss_D_real: 0.69328833 Loss_D_fake: 0.69308925) Loss_G: 0.00001657 Loss_Enh_Dec: -0.00011093\n",
      "| epoch  28 |   300/ 2499 batches | lr 0.000000 | ms/batch 1045.28 | loss  0.57 | ppl     1.77 | acc     0.91 | train_ae_norm     1.00\n",
      "[28/200][399/2499] Loss_D: 1.38600659 (Loss_D_real: 0.69311059 Loss_D_fake: 0.69289607) Loss_G: 0.00003687 Loss_Enh_Dec: -0.00013529\n",
      "| epoch  28 |   400/ 2499 batches | lr 0.000000 | ms/batch 1044.57 | loss  0.57 | ppl     1.77 | acc     0.93 | train_ae_norm     1.00\n",
      "[28/200][499/2499] Loss_D: 1.38608313 (Loss_D_real: 0.69307035 Loss_D_fake: 0.69301271) Loss_G: -0.00001538 Loss_Enh_Dec: -0.00007712\n",
      "| epoch  28 |   500/ 2499 batches | lr 0.000000 | ms/batch 1046.55 | loss  0.58 | ppl     1.78 | acc     0.91 | train_ae_norm     1.00\n",
      "[28/200][599/2499] Loss_D: 1.38594079 (Loss_D_real: 0.69307297 Loss_D_fake: 0.69286776) Loss_G: -0.00001471 Loss_Enh_Dec: -0.00031611\n",
      "| epoch  28 |   600/ 2499 batches | lr 0.000000 | ms/batch 1048.01 | loss  0.56 | ppl     1.75 | acc     0.92 | train_ae_norm     1.00\n",
      "[28/200][699/2499] Loss_D: 1.38602185 (Loss_D_real: 0.69311023 Loss_D_fake: 0.69291162) Loss_G: -0.00001049 Loss_Enh_Dec: -0.00035387\n",
      "| epoch  28 |   700/ 2499 batches | lr 0.000000 | ms/batch 1049.54 | loss  0.57 | ppl     1.76 | acc     0.90 | train_ae_norm     1.00\n",
      "[28/200][799/2499] Loss_D: 1.38626873 (Loss_D_real: 0.69330710 Loss_D_fake: 0.69296163) Loss_G: -0.00004020 Loss_Enh_Dec: -0.00030657\n",
      "| epoch  28 |   800/ 2499 batches | lr 0.000000 | ms/batch 1048.87 | loss  0.60 | ppl     1.83 | acc     0.90 | train_ae_norm     1.00\n",
      "[28/200][899/2499] Loss_D: 1.38634562 (Loss_D_real: 0.69332206 Loss_D_fake: 0.69302362) Loss_G: 0.00000148 Loss_Enh_Dec: 0.00017559\n",
      "| epoch  28 |   900/ 2499 batches | lr 0.000000 | ms/batch 1048.18 | loss  0.57 | ppl     1.76 | acc     0.92 | train_ae_norm     1.00\n",
      "[28/200][999/2499] Loss_D: 1.38635826 (Loss_D_real: 0.69313610 Loss_D_fake: 0.69322222) Loss_G: -0.00001657 Loss_Enh_Dec: -0.00003270\n",
      "| epoch  28 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1044.70 | loss  0.63 | ppl     1.88 | acc     0.91 | train_ae_norm     1.00\n",
      "[28/200][1099/2499] Loss_D: 1.38606846 (Loss_D_real: 0.69319952 Loss_D_fake: 0.69286895) Loss_G: 0.00000909 Loss_Enh_Dec: -0.00017129\n",
      "| epoch  28 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1047.75 | loss  0.69 | ppl     2.00 | acc     0.90 | train_ae_norm     1.00\n",
      "[28/200][1199/2499] Loss_D: 1.38648307 (Loss_D_real: 0.69314188 Loss_D_fake: 0.69334120) Loss_G: -0.00006674 Loss_Enh_Dec: -0.00017528\n",
      "| epoch  28 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1047.40 | loss  0.68 | ppl     1.97 | acc     0.90 | train_ae_norm     1.00\n",
      "[28/200][1299/2499] Loss_D: 1.38619900 (Loss_D_real: 0.69284958 Loss_D_fake: 0.69334936) Loss_G: 0.00001107 Loss_Enh_Dec: -0.00041244\n",
      "| epoch  28 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1048.16 | loss  0.72 | ppl     2.06 | acc     0.88 | train_ae_norm     1.00\n",
      "[28/200][1399/2499] Loss_D: 1.38594544 (Loss_D_real: 0.69299591 Loss_D_fake: 0.69294953) Loss_G: 0.00002603 Loss_Enh_Dec: -0.00011632\n",
      "| epoch  28 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1050.53 | loss  0.68 | ppl     1.97 | acc     0.91 | train_ae_norm     1.00\n",
      "[28/200][1499/2499] Loss_D: 1.38667119 (Loss_D_real: 0.69343650 Loss_D_fake: 0.69323468) Loss_G: 0.00005080 Loss_Enh_Dec: -0.00012551\n",
      "| epoch  28 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1051.23 | loss  0.65 | ppl     1.91 | acc     0.89 | train_ae_norm     1.00\n",
      "[28/200][1599/2499] Loss_D: 1.38603950 (Loss_D_real: 0.69307005 Loss_D_fake: 0.69296950) Loss_G: -0.00002687 Loss_Enh_Dec: -0.00024903\n",
      "| epoch  28 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1050.35 | loss  0.68 | ppl     1.96 | acc     0.87 | train_ae_norm     1.00\n",
      "[28/200][1699/2499] Loss_D: 1.38645935 (Loss_D_real: 0.69275475 Loss_D_fake: 0.69370461) Loss_G: 0.00006092 Loss_Enh_Dec: -0.00049072\n",
      "| epoch  28 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1048.63 | loss  0.66 | ppl     1.94 | acc     0.91 | train_ae_norm     1.00\n",
      "[28/200][1799/2499] Loss_D: 1.38552427 (Loss_D_real: 0.69283813 Loss_D_fake: 0.69268608) Loss_G: -0.00003488 Loss_Enh_Dec: -0.00037484\n",
      "| epoch  28 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1050.57 | loss  0.66 | ppl     1.94 | acc     0.93 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28/200][1899/2499] Loss_D: 1.38566542 (Loss_D_real: 0.69298100 Loss_D_fake: 0.69268435) Loss_G: 0.00004915 Loss_Enh_Dec: -0.00030171\n",
      "| epoch  28 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1046.41 | loss  0.69 | ppl     1.99 | acc     0.89 | train_ae_norm     1.00\n",
      "[28/200][1999/2499] Loss_D: 1.38573956 (Loss_D_real: 0.69308519 Loss_D_fake: 0.69265431) Loss_G: -0.00005812 Loss_Enh_Dec: -0.00058572\n",
      "| epoch  28 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1048.96 | loss  0.69 | ppl     1.99 | acc     0.91 | train_ae_norm     1.00\n",
      "[28/200][2099/2499] Loss_D: 1.38649607 (Loss_D_real: 0.69352239 Loss_D_fake: 0.69297361) Loss_G: 0.00008188 Loss_Enh_Dec: -0.00038696\n",
      "| epoch  28 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1048.56 | loss  0.66 | ppl     1.94 | acc     0.91 | train_ae_norm     1.00\n",
      "[28/200][2199/2499] Loss_D: 1.38632822 (Loss_D_real: 0.69305253 Loss_D_fake: 0.69327563) Loss_G: 0.00004025 Loss_Enh_Dec: -0.00085205\n",
      "| epoch  28 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1049.80 | loss  0.64 | ppl     1.90 | acc     0.92 | train_ae_norm     1.00\n",
      "[28/200][2299/2499] Loss_D: 1.38685966 (Loss_D_real: 0.69353455 Loss_D_fake: 0.69332516) Loss_G: -0.00002991 Loss_Enh_Dec: -0.00052712\n",
      "| epoch  28 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1049.31 | loss  0.64 | ppl     1.90 | acc     0.92 | train_ae_norm     1.00\n",
      "[28/200][2399/2499] Loss_D: 1.38558185 (Loss_D_real: 0.69254607 Loss_D_fake: 0.69303578) Loss_G: 0.00012448 Loss_Enh_Dec: -0.00068640\n",
      "| epoch  28 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1050.09 | loss  0.63 | ppl     1.88 | acc     0.89 | train_ae_norm     1.00\n",
      "[28/200][2499/2499] Loss_D: 1.38628638 (Loss_D_real: 0.69290197 Loss_D_fake: 0.69338441) Loss_G: 0.00005759 Loss_Enh_Dec: -0.00066332\n",
      "| end of epoch  28 | time: 2796.97s | test loss  0.61 | test ppl  1.83 | acc 0.938\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 29 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:27.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:34.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:22.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:29.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:36.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:43.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:49.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:56.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:03.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:10.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:17.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:24.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:30.\n",
      "\n",
      "  Average training loss generetor: 0.705\n",
      "  Average training loss discriminator: 0.713\n",
      "  Training epcoh took: 0:02:37\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.500\n",
      "  Test Loss: 3.200\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  29 |     0/ 2499 batches | lr 0.000000 | ms/batch 1574.27 | loss  0.01 | ppl     1.01 | acc     0.89 | train_ae_norm     1.00\n",
      "[29/200][99/2499] Loss_D: 1.38622069 (Loss_D_real: 0.69322717 Loss_D_fake: 0.69299352) Loss_G: 0.00008170 Loss_Enh_Dec: -0.00058769\n",
      "| epoch  29 |   100/ 2499 batches | lr 0.000000 | ms/batch 1048.42 | loss  0.65 | ppl     1.91 | acc     0.89 | train_ae_norm     1.00\n",
      "[29/200][199/2499] Loss_D: 1.38539779 (Loss_D_real: 0.69311923 Loss_D_fake: 0.69227856) Loss_G: 0.00005265 Loss_Enh_Dec: -0.00087484\n",
      "| epoch  29 |   200/ 2499 batches | lr 0.000000 | ms/batch 1048.79 | loss  0.64 | ppl     1.91 | acc     0.92 | train_ae_norm     1.00\n",
      "[29/200][299/2499] Loss_D: 1.38592172 (Loss_D_real: 0.69293880 Loss_D_fake: 0.69298285) Loss_G: -0.00003318 Loss_Enh_Dec: -0.00077466\n",
      "| epoch  29 |   300/ 2499 batches | lr 0.000000 | ms/batch 1050.89 | loss  0.65 | ppl     1.92 | acc     0.90 | train_ae_norm     1.00\n",
      "[29/200][399/2499] Loss_D: 1.38661754 (Loss_D_real: 0.69313186 Loss_D_fake: 0.69348568) Loss_G: -0.00001964 Loss_Enh_Dec: -0.00083118\n",
      "| epoch  29 |   400/ 2499 batches | lr 0.000000 | ms/batch 1048.45 | loss  0.63 | ppl     1.87 | acc     0.90 | train_ae_norm     1.00\n",
      "[29/200][499/2499] Loss_D: 1.38607097 (Loss_D_real: 0.69277662 Loss_D_fake: 0.69329441) Loss_G: -0.00003426 Loss_Enh_Dec: -0.00109423\n",
      "| epoch  29 |   500/ 2499 batches | lr 0.000000 | ms/batch 1049.14 | loss  0.61 | ppl     1.84 | acc     0.91 | train_ae_norm     1.00\n",
      "[29/200][599/2499] Loss_D: 1.38630962 (Loss_D_real: 0.69323587 Loss_D_fake: 0.69307375) Loss_G: -0.00009721 Loss_Enh_Dec: -0.00129731\n",
      "| epoch  29 |   600/ 2499 batches | lr 0.000000 | ms/batch 1048.40 | loss  0.60 | ppl     1.82 | acc     0.93 | train_ae_norm     1.00\n",
      "[29/200][699/2499] Loss_D: 1.38687074 (Loss_D_real: 0.69349355 Loss_D_fake: 0.69337720) Loss_G: 0.00002953 Loss_Enh_Dec: -0.00042038\n",
      "| epoch  29 |   700/ 2499 batches | lr 0.000000 | ms/batch 1048.83 | loss  0.61 | ppl     1.83 | acc     0.91 | train_ae_norm     1.00\n",
      "[29/200][799/2499] Loss_D: 1.38593686 (Loss_D_real: 0.69322050 Loss_D_fake: 0.69271636) Loss_G: 0.00004523 Loss_Enh_Dec: 0.00027578\n",
      "| epoch  29 |   800/ 2499 batches | lr 0.000000 | ms/batch 1052.54 | loss  0.62 | ppl     1.86 | acc     0.89 | train_ae_norm     1.00\n",
      "[29/200][899/2499] Loss_D: 1.38547206 (Loss_D_real: 0.69219869 Loss_D_fake: 0.69327331) Loss_G: 0.00002493 Loss_Enh_Dec: 0.00013708\n",
      "| epoch  29 |   900/ 2499 batches | lr 0.000000 | ms/batch 1049.74 | loss  0.61 | ppl     1.84 | acc     0.92 | train_ae_norm     1.00\n",
      "[29/200][999/2499] Loss_D: 1.38591731 (Loss_D_real: 0.69300354 Loss_D_fake: 0.69291377) Loss_G: 0.00001921 Loss_Enh_Dec: 0.00005397\n",
      "| epoch  29 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1044.04 | loss  0.65 | ppl     1.91 | acc     0.92 | train_ae_norm     1.00\n",
      "[29/200][1099/2499] Loss_D: 1.38626194 (Loss_D_real: 0.69307750 Loss_D_fake: 0.69318438) Loss_G: 0.00003304 Loss_Enh_Dec: 0.00008950\n",
      "| epoch  29 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1044.29 | loss  0.60 | ppl     1.83 | acc     0.91 | train_ae_norm     1.00\n",
      "[29/200][1199/2499] Loss_D: 1.38579333 (Loss_D_real: 0.69310230 Loss_D_fake: 0.69269103) Loss_G: 0.00014432 Loss_Enh_Dec: -0.00001581\n",
      "| epoch  29 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1045.93 | loss  0.61 | ppl     1.84 | acc     0.89 | train_ae_norm     1.00\n",
      "[29/200][1299/2499] Loss_D: 1.38652873 (Loss_D_real: 0.69298309 Loss_D_fake: 0.69354558) Loss_G: 0.00008137 Loss_Enh_Dec: -0.00010881\n",
      "| epoch  29 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1043.90 | loss  0.63 | ppl     1.87 | acc     0.87 | train_ae_norm     1.00\n",
      "[29/200][1399/2499] Loss_D: 1.38626564 (Loss_D_real: 0.69280303 Loss_D_fake: 0.69346261) Loss_G: 0.00012775 Loss_Enh_Dec: -0.00042875\n",
      "| epoch  29 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1049.16 | loss  0.64 | ppl     1.89 | acc     0.91 | train_ae_norm     1.00\n",
      "[29/200][1499/2499] Loss_D: 1.38566363 (Loss_D_real: 0.69271052 Loss_D_fake: 0.69295311) Loss_G: 0.00005399 Loss_Enh_Dec: -0.00115722\n",
      "| epoch  29 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1051.96 | loss  0.67 | ppl     1.96 | acc     0.88 | train_ae_norm     1.00\n",
      "[29/200][1599/2499] Loss_D: 1.38602400 (Loss_D_real: 0.69291532 Loss_D_fake: 0.69310874) Loss_G: 0.00002826 Loss_Enh_Dec: -0.00042435\n",
      "| epoch  29 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1050.75 | loss  0.71 | ppl     2.03 | acc     0.88 | train_ae_norm     1.00\n",
      "[29/200][1699/2499] Loss_D: 1.38649333 (Loss_D_real: 0.69318473 Loss_D_fake: 0.69330859) Loss_G: -0.00001137 Loss_Enh_Dec: -0.00157071\n",
      "| epoch  29 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1050.98 | loss  0.69 | ppl     1.98 | acc     0.91 | train_ae_norm     1.00\n",
      "[29/200][1799/2499] Loss_D: 1.38635826 (Loss_D_real: 0.69347155 Loss_D_fake: 0.69288677) Loss_G: 0.00002800 Loss_Enh_Dec: -0.00116468\n",
      "| epoch  29 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1050.05 | loss  0.69 | ppl     2.00 | acc     0.93 | train_ae_norm     1.00\n",
      "[29/200][1899/2499] Loss_D: 1.38606715 (Loss_D_real: 0.69297338 Loss_D_fake: 0.69309378) Loss_G: -0.00001010 Loss_Enh_Dec: -0.00017010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  29 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1051.69 | loss  0.68 | ppl     1.98 | acc     0.89 | train_ae_norm     1.00\n",
      "[29/200][1999/2499] Loss_D: 1.38559723 (Loss_D_real: 0.69255424 Loss_D_fake: 0.69304299) Loss_G: 0.00020386 Loss_Enh_Dec: -0.00000242\n",
      "| epoch  29 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1052.55 | loss  0.71 | ppl     2.03 | acc     0.90 | train_ae_norm     1.00\n",
      "[29/200][2099/2499] Loss_D: 1.38611007 (Loss_D_real: 0.69303721 Loss_D_fake: 0.69307292) Loss_G: 0.00006956 Loss_Enh_Dec: -0.00020597\n",
      "| epoch  29 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1052.29 | loss  0.69 | ppl     2.00 | acc     0.92 | train_ae_norm     1.00\n",
      "[29/200][2199/2499] Loss_D: 1.38517833 (Loss_D_real: 0.69261491 Loss_D_fake: 0.69256341) Loss_G: 0.00005538 Loss_Enh_Dec: -0.00064206\n",
      "| epoch  29 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1054.63 | loss  0.71 | ppl     2.04 | acc     0.91 | train_ae_norm     1.00\n",
      "[29/200][2299/2499] Loss_D: 1.38587618 (Loss_D_real: 0.69335079 Loss_D_fake: 0.69252533) Loss_G: 0.00003647 Loss_Enh_Dec: -0.00002066\n",
      "| epoch  29 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1052.32 | loss  0.75 | ppl     2.11 | acc     0.90 | train_ae_norm     1.00\n",
      "[29/200][2399/2499] Loss_D: 1.38602710 (Loss_D_real: 0.69285905 Loss_D_fake: 0.69316810) Loss_G: 0.00003251 Loss_Enh_Dec: -0.00031514\n",
      "| epoch  29 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1052.73 | loss  0.71 | ppl     2.04 | acc     0.86 | train_ae_norm     1.00\n",
      "[29/200][2499/2499] Loss_D: 1.38574314 (Loss_D_real: 0.69286937 Loss_D_fake: 0.69287384) Loss_G: 0.00016960 Loss_Enh_Dec: -0.00070186\n",
      "| end of epoch  29 | time: 2801.04s | test loss  0.64 | test ppl  1.90 | acc 0.932\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 30 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:41.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:09.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:16.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:23.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:30.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:37.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:44.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:51.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:58.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:04.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:18.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:25.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:32.\n",
      "\n",
      "  Average training loss generetor: 0.705\n",
      "  Average training loss discriminator: 0.711\n",
      "  Training epcoh took: 0:02:39\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.495\n",
      "  Test Loss: 3.218\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  30 |     0/ 2499 batches | lr 0.000000 | ms/batch 1592.84 | loss  0.01 | ppl     1.01 | acc     0.89 | train_ae_norm     1.00\n",
      "[30/200][99/2499] Loss_D: 1.38477349 (Loss_D_real: 0.69193792 Loss_D_fake: 0.69283557) Loss_G: 0.00002963 Loss_Enh_Dec: -0.00018163\n",
      "| epoch  30 |   100/ 2499 batches | lr 0.000000 | ms/batch 1052.89 | loss  0.71 | ppl     2.04 | acc     0.87 | train_ae_norm     1.00\n",
      "[30/200][199/2499] Loss_D: 1.38639855 (Loss_D_real: 0.69353139 Loss_D_fake: 0.69286722) Loss_G: -0.00010178 Loss_Enh_Dec: -0.00092141\n",
      "| epoch  30 |   200/ 2499 batches | lr 0.000000 | ms/batch 1052.26 | loss  0.78 | ppl     2.18 | acc     0.90 | train_ae_norm     1.00\n",
      "[30/200][299/2499] Loss_D: 1.38599551 (Loss_D_real: 0.69289088 Loss_D_fake: 0.69310462) Loss_G: 0.00005722 Loss_Enh_Dec: -0.00091097\n",
      "| epoch  30 |   300/ 2499 batches | lr 0.000000 | ms/batch 1051.02 | loss  0.81 | ppl     2.25 | acc     0.87 | train_ae_norm     1.00\n",
      "[30/200][399/2499] Loss_D: 1.38508391 (Loss_D_real: 0.69179577 Loss_D_fake: 0.69328821) Loss_G: 0.00013601 Loss_Enh_Dec: -0.00105679\n",
      "| epoch  30 |   400/ 2499 batches | lr 0.000000 | ms/batch 1052.93 | loss  0.80 | ppl     2.23 | acc     0.89 | train_ae_norm     1.00\n",
      "[30/200][499/2499] Loss_D: 1.38621140 (Loss_D_real: 0.69281673 Loss_D_fake: 0.69339466) Loss_G: 0.00000769 Loss_Enh_Dec: -0.00061154\n",
      "| epoch  30 |   500/ 2499 batches | lr 0.000000 | ms/batch 1053.12 | loss  0.78 | ppl     2.19 | acc     0.88 | train_ae_norm     1.00\n",
      "[30/200][599/2499] Loss_D: 1.38611090 (Loss_D_real: 0.69270837 Loss_D_fake: 0.69340253) Loss_G: -0.00009835 Loss_Enh_Dec: -0.00107227\n",
      "| epoch  30 |   600/ 2499 batches | lr 0.000000 | ms/batch 1053.72 | loss  0.76 | ppl     2.15 | acc     0.90 | train_ae_norm     1.00\n",
      "[30/200][699/2499] Loss_D: 1.38615775 (Loss_D_real: 0.69315052 Loss_D_fake: 0.69300717) Loss_G: 0.00010767 Loss_Enh_Dec: -0.00090202\n",
      "| epoch  30 |   700/ 2499 batches | lr 0.000000 | ms/batch 1053.31 | loss  0.74 | ppl     2.10 | acc     0.89 | train_ae_norm     1.00\n",
      "[30/200][799/2499] Loss_D: 1.38677919 (Loss_D_real: 0.69321823 Loss_D_fake: 0.69356096) Loss_G: 0.00003689 Loss_Enh_Dec: -0.00014351\n",
      "| epoch  30 |   800/ 2499 batches | lr 0.000000 | ms/batch 1053.90 | loss  0.78 | ppl     2.17 | acc     0.86 | train_ae_norm     1.00\n",
      "[30/200][899/2499] Loss_D: 1.38477755 (Loss_D_real: 0.69147885 Loss_D_fake: 0.69329870) Loss_G: 0.00027888 Loss_Enh_Dec: -0.00110463\n",
      "| epoch  30 |   900/ 2499 batches | lr 0.000000 | ms/batch 1054.26 | loss  0.77 | ppl     2.15 | acc     0.90 | train_ae_norm     1.00\n",
      "[30/200][999/2499] Loss_D: 1.38514471 (Loss_D_real: 0.69209599 Loss_D_fake: 0.69304866) Loss_G: -0.00006705 Loss_Enh_Dec: -0.00064671\n",
      "| epoch  30 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1053.86 | loss  0.81 | ppl     2.25 | acc     0.90 | train_ae_norm     1.00\n",
      "[30/200][1099/2499] Loss_D: 1.38587773 (Loss_D_real: 0.69285345 Loss_D_fake: 0.69302428) Loss_G: 0.00017270 Loss_Enh_Dec: -0.00153095\n",
      "| epoch  30 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1053.28 | loss  0.79 | ppl     2.20 | acc     0.90 | train_ae_norm     1.00\n",
      "[30/200][1199/2499] Loss_D: 1.38645101 (Loss_D_real: 0.69308579 Loss_D_fake: 0.69336528) Loss_G: 0.00022401 Loss_Enh_Dec: -0.00204952\n",
      "| epoch  30 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1053.42 | loss  0.77 | ppl     2.16 | acc     0.88 | train_ae_norm     1.00\n",
      "[30/200][1299/2499] Loss_D: 1.38683343 (Loss_D_real: 0.69299716 Loss_D_fake: 0.69383633) Loss_G: 0.00002350 Loss_Enh_Dec: -0.00240051\n",
      "| epoch  30 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1053.61 | loss  0.79 | ppl     2.20 | acc     0.87 | train_ae_norm     1.00\n",
      "[30/200][1399/2499] Loss_D: 1.38562846 (Loss_D_real: 0.69288862 Loss_D_fake: 0.69273990) Loss_G: 0.00000861 Loss_Enh_Dec: -0.00238140\n",
      "| epoch  30 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1053.43 | loss  0.80 | ppl     2.22 | acc     0.90 | train_ae_norm     1.00\n",
      "[30/200][1499/2499] Loss_D: 1.38668406 (Loss_D_real: 0.69393867 Loss_D_fake: 0.69274539) Loss_G: 0.00032559 Loss_Enh_Dec: -0.00269104\n",
      "| epoch  30 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1054.30 | loss  0.75 | ppl     2.13 | acc     0.90 | train_ae_norm     1.00\n",
      "[30/200][1599/2499] Loss_D: 1.38488984 (Loss_D_real: 0.69220471 Loss_D_fake: 0.69268513) Loss_G: 0.00007874 Loss_Enh_Dec: -0.00017435\n",
      "| epoch  30 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1054.22 | loss  0.79 | ppl     2.20 | acc     0.87 | train_ae_norm     1.00\n",
      "[30/200][1699/2499] Loss_D: 1.38643408 (Loss_D_real: 0.69345319 Loss_D_fake: 0.69298095) Loss_G: 0.00024428 Loss_Enh_Dec: -0.00291948\n",
      "| epoch  30 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1052.95 | loss  0.75 | ppl     2.11 | acc     0.90 | train_ae_norm     1.00\n",
      "[30/200][1799/2499] Loss_D: 1.38485503 (Loss_D_real: 0.69302666 Loss_D_fake: 0.69182837) Loss_G: 0.00017962 Loss_Enh_Dec: -0.00398498\n",
      "| epoch  30 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1052.54 | loss  0.75 | ppl     2.12 | acc     0.91 | train_ae_norm     1.00\n",
      "[30/200][1899/2499] Loss_D: 1.38562191 (Loss_D_real: 0.69365710 Loss_D_fake: 0.69196481) Loss_G: 0.00010139 Loss_Enh_Dec: -0.00401374\n",
      "| epoch  30 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1051.96 | loss  0.75 | ppl     2.13 | acc     0.89 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30/200][1999/2499] Loss_D: 1.38656807 (Loss_D_real: 0.69285727 Loss_D_fake: 0.69371074) Loss_G: 0.00007176 Loss_Enh_Dec: -0.00309373\n",
      "| epoch  30 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1052.65 | loss  0.76 | ppl     2.13 | acc     0.91 | train_ae_norm     1.00\n",
      "[30/200][2099/2499] Loss_D: 1.38307285 (Loss_D_real: 0.69078541 Loss_D_fake: 0.69228745) Loss_G: 0.00013979 Loss_Enh_Dec: -0.00402206\n",
      "| epoch  30 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1053.86 | loss  0.79 | ppl     2.19 | acc     0.91 | train_ae_norm     1.00\n",
      "[30/200][2199/2499] Loss_D: 1.38460445 (Loss_D_real: 0.69161195 Loss_D_fake: 0.69299257) Loss_G: 0.00032130 Loss_Enh_Dec: -0.00435783\n",
      "| epoch  30 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1053.21 | loss  0.75 | ppl     2.12 | acc     0.91 | train_ae_norm     1.00\n",
      "[30/200][2299/2499] Loss_D: 1.38428521 (Loss_D_real: 0.69256371 Loss_D_fake: 0.69172144) Loss_G: -0.00005091 Loss_Enh_Dec: -0.00008989\n",
      "| epoch  30 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1053.38 | loss  0.75 | ppl     2.12 | acc     0.90 | train_ae_norm     1.00\n",
      "[30/200][2399/2499] Loss_D: 1.38473821 (Loss_D_real: 0.69208878 Loss_D_fake: 0.69264936) Loss_G: 0.00016156 Loss_Enh_Dec: -0.00383591\n",
      "| epoch  30 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1053.96 | loss  0.77 | ppl     2.15 | acc     0.88 | train_ae_norm     1.00\n",
      "[30/200][2499/2499] Loss_D: 1.38644552 (Loss_D_real: 0.69247121 Loss_D_fake: 0.69397438) Loss_G: 0.00039097 Loss_Enh_Dec: -0.00489350\n",
      "| end of epoch  30 | time: 2811.47s | test loss  0.66 | test ppl  1.94 | acc 0.929\n",
      "New saving model: epoch 030.\n",
      "Saving models to ./results/yelp_merge_results\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 31 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:09.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:16.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:23.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:30.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:37.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:44.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:51.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:58.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:05.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:19.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:26.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:33.\n",
      "\n",
      "  Average training loss generetor: 0.708\n",
      "  Average training loss discriminator: 0.794\n",
      "  Training epcoh took: 0:02:39\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.485\n",
      "  Test Loss: 2.685\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  31 |     0/ 2499 batches | lr 0.000000 | ms/batch 1597.28 | loss  0.01 | ppl     1.01 | acc     0.88 | train_ae_norm     1.00\n",
      "[31/200][99/2499] Loss_D: 1.38432193 (Loss_D_real: 0.69140565 Loss_D_fake: 0.69291633) Loss_G: -0.00002131 Loss_Enh_Dec: -0.00411685\n",
      "| epoch  31 |   100/ 2499 batches | lr 0.000000 | ms/batch 1052.97 | loss  0.76 | ppl     2.14 | acc     0.85 | train_ae_norm     1.00\n",
      "[31/200][199/2499] Loss_D: 1.38396478 (Loss_D_real: 0.69194317 Loss_D_fake: 0.69202155) Loss_G: 0.00020027 Loss_Enh_Dec: -0.00474931\n",
      "| epoch  31 |   200/ 2499 batches | lr 0.000000 | ms/batch 1052.31 | loss  0.79 | ppl     2.19 | acc     0.89 | train_ae_norm     1.00\n",
      "[31/200][299/2499] Loss_D: 1.38366151 (Loss_D_real: 0.69140887 Loss_D_fake: 0.69225264) Loss_G: 0.00010887 Loss_Enh_Dec: -0.00446915\n",
      "| epoch  31 |   300/ 2499 batches | lr 0.000000 | ms/batch 1053.36 | loss  0.81 | ppl     2.24 | acc     0.89 | train_ae_norm     1.00\n",
      "[31/200][399/2499] Loss_D: 1.38151860 (Loss_D_real: 0.69028205 Loss_D_fake: 0.69123662) Loss_G: 0.00018481 Loss_Enh_Dec: -0.00398339\n",
      "| epoch  31 |   400/ 2499 batches | lr 0.000000 | ms/batch 1052.43 | loss  0.80 | ppl     2.23 | acc     0.90 | train_ae_norm     1.00\n",
      "[31/200][499/2499] Loss_D: 1.38285863 (Loss_D_real: 0.69146961 Loss_D_fake: 0.69138902) Loss_G: 0.00021939 Loss_Enh_Dec: -0.00617867\n",
      "| epoch  31 |   500/ 2499 batches | lr 0.000000 | ms/batch 1053.53 | loss  0.80 | ppl     2.22 | acc     0.87 | train_ae_norm     1.00\n",
      "[31/200][599/2499] Loss_D: 1.38317978 (Loss_D_real: 0.69093412 Loss_D_fake: 0.69224566) Loss_G: 0.00032908 Loss_Enh_Dec: -0.00549587\n",
      "| epoch  31 |   600/ 2499 batches | lr 0.000000 | ms/batch 1052.33 | loss  0.80 | ppl     2.24 | acc     0.90 | train_ae_norm     1.00\n",
      "[31/200][699/2499] Loss_D: 1.38194299 (Loss_D_real: 0.69089311 Loss_D_fake: 0.69104981) Loss_G: 0.00057284 Loss_Enh_Dec: -0.00526796\n",
      "| epoch  31 |   700/ 2499 batches | lr 0.000000 | ms/batch 1054.01 | loss  0.80 | ppl     2.23 | acc     0.87 | train_ae_norm     1.00\n",
      "[31/200][799/2499] Loss_D: 1.38121510 (Loss_D_real: 0.69052899 Loss_D_fake: 0.69068617) Loss_G: 0.00054238 Loss_Enh_Dec: -0.00623635\n",
      "| epoch  31 |   800/ 2499 batches | lr 0.000000 | ms/batch 1052.44 | loss  0.84 | ppl     2.30 | acc     0.86 | train_ae_norm     1.00\n",
      "[31/200][899/2499] Loss_D: 1.38439322 (Loss_D_real: 0.69205707 Loss_D_fake: 0.69233608) Loss_G: 0.00060462 Loss_Enh_Dec: -0.00427866\n",
      "| epoch  31 |   900/ 2499 batches | lr 0.000000 | ms/batch 1057.50 | loss  0.80 | ppl     2.23 | acc     0.90 | train_ae_norm     1.00\n",
      "[31/200][999/2499] Loss_D: 1.38551748 (Loss_D_real: 0.69275737 Loss_D_fake: 0.69276011) Loss_G: 0.00014741 Loss_Enh_Dec: -0.00159760\n",
      "| epoch  31 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1051.68 | loss  0.85 | ppl     2.34 | acc     0.89 | train_ae_norm     1.00\n",
      "[31/200][1099/2499] Loss_D: 1.38303542 (Loss_D_real: 0.69091338 Loss_D_fake: 0.69212198) Loss_G: 0.00018832 Loss_Enh_Dec: -0.00524395\n",
      "| epoch  31 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1052.10 | loss  0.84 | ppl     2.32 | acc     0.90 | train_ae_norm     1.00\n",
      "[31/200][1199/2499] Loss_D: 1.37894630 (Loss_D_real: 0.68995452 Loss_D_fake: 0.68899173) Loss_G: 0.00084389 Loss_Enh_Dec: -0.00526289\n",
      "| epoch  31 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1053.34 | loss  0.84 | ppl     2.32 | acc     0.87 | train_ae_norm     1.00\n",
      "[31/200][1299/2499] Loss_D: 1.38339520 (Loss_D_real: 0.69103003 Loss_D_fake: 0.69236511) Loss_G: 0.00022000 Loss_Enh_Dec: -0.00575273\n",
      "| epoch  31 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1051.86 | loss  0.90 | ppl     2.47 | acc     0.85 | train_ae_norm     1.00\n",
      "[31/200][1399/2499] Loss_D: 1.37845671 (Loss_D_real: 0.68978000 Loss_D_fake: 0.68867671) Loss_G: 0.00067840 Loss_Enh_Dec: -0.00738843\n",
      "| epoch  31 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1052.00 | loss  0.93 | ppl     2.54 | acc     0.89 | train_ae_norm     1.00\n",
      "[31/200][1499/2499] Loss_D: 1.38145924 (Loss_D_real: 0.68908656 Loss_D_fake: 0.69237268) Loss_G: 0.00044396 Loss_Enh_Dec: -0.00814840\n",
      "| epoch  31 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1053.09 | loss  0.87 | ppl     2.40 | acc     0.88 | train_ae_norm     1.00\n",
      "[31/200][1599/2499] Loss_D: 1.37935197 (Loss_D_real: 0.69078541 Loss_D_fake: 0.68856657) Loss_G: 0.00054900 Loss_Enh_Dec: -0.00813974\n",
      "| epoch  31 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1053.14 | loss  0.93 | ppl     2.52 | acc     0.87 | train_ae_norm     1.00\n",
      "[31/200][1699/2499] Loss_D: 1.37770784 (Loss_D_real: 0.68625927 Loss_D_fake: 0.69144857) Loss_G: 0.00085320 Loss_Enh_Dec: -0.00500988\n",
      "| epoch  31 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1054.30 | loss  0.88 | ppl     2.42 | acc     0.90 | train_ae_norm     1.00\n",
      "[31/200][1799/2499] Loss_D: 1.38435268 (Loss_D_real: 0.69138449 Loss_D_fake: 0.69296813) Loss_G: 0.00061300 Loss_Enh_Dec: -0.00795208\n",
      "| epoch  31 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1053.98 | loss  0.89 | ppl     2.44 | acc     0.91 | train_ae_norm     1.00\n",
      "[31/200][1899/2499] Loss_D: 1.37981367 (Loss_D_real: 0.68793738 Loss_D_fake: 0.69187629) Loss_G: 0.00047866 Loss_Enh_Dec: -0.00535312\n",
      "| epoch  31 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1054.02 | loss  0.93 | ppl     2.53 | acc     0.85 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31/200][1999/2499] Loss_D: 1.38221526 (Loss_D_real: 0.69047904 Loss_D_fake: 0.69173622) Loss_G: 0.00104809 Loss_Enh_Dec: -0.00758551\n",
      "| epoch  31 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1055.20 | loss  0.96 | ppl     2.60 | acc     0.89 | train_ae_norm     1.00\n",
      "[31/200][2099/2499] Loss_D: 1.38223302 (Loss_D_real: 0.69034576 Loss_D_fake: 0.69188726) Loss_G: 0.00049247 Loss_Enh_Dec: -0.00769348\n",
      "| epoch  31 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1055.75 | loss  0.97 | ppl     2.64 | acc     0.88 | train_ae_norm     1.00\n",
      "[31/200][2199/2499] Loss_D: 1.37480688 (Loss_D_real: 0.68579525 Loss_D_fake: 0.68901157) Loss_G: 0.00105093 Loss_Enh_Dec: -0.01020570\n",
      "| epoch  31 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1055.25 | loss  0.94 | ppl     2.55 | acc     0.89 | train_ae_norm     1.00\n",
      "[31/200][2299/2499] Loss_D: 1.37858772 (Loss_D_real: 0.68992186 Loss_D_fake: 0.68866587) Loss_G: 0.00058474 Loss_Enh_Dec: -0.00917894\n",
      "| epoch  31 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1054.03 | loss  0.98 | ppl     2.66 | acc     0.88 | train_ae_norm     1.00\n",
      "[31/200][2399/2499] Loss_D: 1.37683952 (Loss_D_real: 0.68952847 Loss_D_fake: 0.68731105) Loss_G: 0.00099700 Loss_Enh_Dec: -0.01422846\n",
      "| epoch  31 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1053.89 | loss  0.95 | ppl     2.59 | acc     0.85 | train_ae_norm     1.00\n",
      "[31/200][2499/2499] Loss_D: 1.38256598 (Loss_D_real: 0.69362372 Loss_D_fake: 0.68894231) Loss_G: 0.00070230 Loss_Enh_Dec: -0.01285773\n",
      "| end of epoch  31 | time: 2812.57s | test loss  0.74 | test ppl  2.10 | acc 0.922\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 32 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:09.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:16.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:23.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:30.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:37.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:44.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:51.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:58.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:05.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:18.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:25.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:32.\n",
      "\n",
      "  Average training loss generetor: 0.706\n",
      "  Average training loss discriminator: 0.738\n",
      "  Training epcoh took: 0:02:39\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.535\n",
      "  Test Loss: 2.528\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  32 |     0/ 2499 batches | lr 0.000000 | ms/batch 1593.16 | loss  0.01 | ppl     1.01 | acc     0.85 | train_ae_norm     1.00\n",
      "[32/200][99/2499] Loss_D: 1.37469864 (Loss_D_real: 0.68778956 Loss_D_fake: 0.68690908) Loss_G: 0.00095647 Loss_Enh_Dec: -0.01301567\n",
      "| epoch  32 |   100/ 2499 batches | lr 0.000000 | ms/batch 1054.62 | loss  0.95 | ppl     2.58 | acc     0.84 | train_ae_norm     1.00\n",
      "[32/200][199/2499] Loss_D: 1.36861944 (Loss_D_real: 0.68175626 Loss_D_fake: 0.68686318) Loss_G: 0.00120163 Loss_Enh_Dec: -0.01398120\n",
      "| epoch  32 |   200/ 2499 batches | lr 0.000000 | ms/batch 1052.69 | loss  0.95 | ppl     2.59 | acc     0.88 | train_ae_norm     1.00\n",
      "[32/200][299/2499] Loss_D: 1.37448204 (Loss_D_real: 0.68606830 Loss_D_fake: 0.68841374) Loss_G: 0.00201153 Loss_Enh_Dec: -0.01312771\n",
      "| epoch  32 |   300/ 2499 batches | lr 0.000000 | ms/batch 1053.63 | loss  1.00 | ppl     2.73 | acc     0.85 | train_ae_norm     1.00\n",
      "[32/200][399/2499] Loss_D: 1.36997461 (Loss_D_real: 0.68002194 Loss_D_fake: 0.68995273) Loss_G: 0.00221503 Loss_Enh_Dec: -0.01803734\n",
      "| epoch  32 |   400/ 2499 batches | lr 0.000000 | ms/batch 1053.66 | loss  0.98 | ppl     2.65 | acc     0.87 | train_ae_norm     1.00\n",
      "[32/200][499/2499] Loss_D: 1.36456847 (Loss_D_real: 0.68072438 Loss_D_fake: 0.68384403) Loss_G: 0.00253663 Loss_Enh_Dec: -0.02001106\n",
      "| epoch  32 |   500/ 2499 batches | lr 0.000000 | ms/batch 1054.37 | loss  0.96 | ppl     2.61 | acc     0.87 | train_ae_norm     1.00\n",
      "[32/200][599/2499] Loss_D: 1.36201215 (Loss_D_real: 0.68076134 Loss_D_fake: 0.68125075) Loss_G: 0.00198979 Loss_Enh_Dec: -0.02175297\n",
      "| epoch  32 |   600/ 2499 batches | lr 0.000000 | ms/batch 1054.19 | loss  0.95 | ppl     2.59 | acc     0.88 | train_ae_norm     1.00\n",
      "[32/200][699/2499] Loss_D: 1.36037326 (Loss_D_real: 0.67848921 Loss_D_fake: 0.68188399) Loss_G: 0.00202090 Loss_Enh_Dec: -0.02035360\n",
      "| epoch  32 |   700/ 2499 batches | lr 0.000000 | ms/batch 1054.01 | loss  1.02 | ppl     2.77 | acc     0.83 | train_ae_norm     1.00\n",
      "[32/200][799/2499] Loss_D: 1.35469913 (Loss_D_real: 0.67994881 Loss_D_fake: 0.67475027) Loss_G: 0.00333396 Loss_Enh_Dec: -0.01951323\n",
      "| epoch  32 |   800/ 2499 batches | lr 0.000000 | ms/batch 1055.53 | loss  1.01 | ppl     2.75 | acc     0.84 | train_ae_norm     1.00\n",
      "[32/200][899/2499] Loss_D: 1.35489058 (Loss_D_real: 0.67278743 Loss_D_fake: 0.68210316) Loss_G: 0.00261308 Loss_Enh_Dec: -0.00438135\n",
      "| epoch  32 |   900/ 2499 batches | lr 0.000000 | ms/batch 1054.32 | loss  0.99 | ppl     2.70 | acc     0.89 | train_ae_norm     1.00\n",
      "[32/200][999/2499] Loss_D: 1.35470104 (Loss_D_real: 0.67904347 Loss_D_fake: 0.67565763) Loss_G: 0.00240273 Loss_Enh_Dec: -0.01939681\n",
      "| epoch  32 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1053.42 | loss  1.03 | ppl     2.80 | acc     0.87 | train_ae_norm     1.00\n",
      "[32/200][1099/2499] Loss_D: 1.34653127 (Loss_D_real: 0.67282236 Loss_D_fake: 0.67370892) Loss_G: 0.00272716 Loss_Enh_Dec: -0.02493960\n",
      "| epoch  32 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1054.72 | loss  0.98 | ppl     2.66 | acc     0.87 | train_ae_norm     1.00\n",
      "[32/200][1199/2499] Loss_D: 1.35112667 (Loss_D_real: 0.66882914 Loss_D_fake: 0.68229759) Loss_G: 0.00208999 Loss_Enh_Dec: -0.03449012\n",
      "| epoch  32 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1054.39 | loss  1.00 | ppl     2.72 | acc     0.84 | train_ae_norm     1.00\n",
      "[32/200][1299/2499] Loss_D: 1.33397686 (Loss_D_real: 0.66597664 Loss_D_fake: 0.66800022) Loss_G: 0.00396737 Loss_Enh_Dec: -0.03252044\n",
      "| epoch  32 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1053.65 | loss  1.05 | ppl     2.87 | acc     0.83 | train_ae_norm     1.00\n",
      "[32/200][1399/2499] Loss_D: 1.35184717 (Loss_D_real: 0.67645693 Loss_D_fake: 0.67539024) Loss_G: 0.00398549 Loss_Enh_Dec: -0.03009873\n",
      "| epoch  32 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1055.08 | loss  1.07 | ppl     2.91 | acc     0.87 | train_ae_norm     1.00\n",
      "[32/200][1499/2499] Loss_D: 1.31698477 (Loss_D_real: 0.65540332 Loss_D_fake: 0.66158146) Loss_G: 0.00567207 Loss_Enh_Dec: -0.02615105\n",
      "| epoch  32 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1055.46 | loss  1.08 | ppl     2.95 | acc     0.85 | train_ae_norm     1.00\n",
      "[32/200][1599/2499] Loss_D: 1.34078705 (Loss_D_real: 0.67487729 Loss_D_fake: 0.66590977) Loss_G: 0.00449658 Loss_Enh_Dec: -0.04659503\n",
      "| epoch  32 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1053.70 | loss  1.16 | ppl     3.19 | acc     0.83 | train_ae_norm     1.00\n",
      "[32/200][1699/2499] Loss_D: 1.32854331 (Loss_D_real: 0.66966343 Loss_D_fake: 0.65887988) Loss_G: 0.00695688 Loss_Enh_Dec: -0.06031405\n",
      "| epoch  32 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1055.48 | loss  1.13 | ppl     3.11 | acc     0.89 | train_ae_norm     1.00\n",
      "[32/200][1799/2499] Loss_D: 1.32270944 (Loss_D_real: 0.65752852 Loss_D_fake: 0.66518092) Loss_G: 0.00455646 Loss_Enh_Dec: -0.06488787\n",
      "| epoch  32 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1055.65 | loss  1.13 | ppl     3.09 | acc     0.88 | train_ae_norm     1.00\n",
      "[32/200][1899/2499] Loss_D: 1.31811678 (Loss_D_real: 0.65986747 Loss_D_fake: 0.65824932) Loss_G: 0.00570975 Loss_Enh_Dec: -0.04329221\n",
      "| epoch  32 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1055.06 | loss  1.21 | ppl     3.35 | acc     0.82 | train_ae_norm     1.00\n",
      "[32/200][1999/2499] Loss_D: 1.31735754 (Loss_D_real: 0.65753996 Loss_D_fake: 0.65981758) Loss_G: 0.00538479 Loss_Enh_Dec: -0.05201983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  32 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1055.81 | loss  1.22 | ppl     3.37 | acc     0.87 | train_ae_norm     1.00\n",
      "[32/200][2099/2499] Loss_D: 1.28335071 (Loss_D_real: 0.63792145 Loss_D_fake: 0.64542925) Loss_G: 0.00939712 Loss_Enh_Dec: -0.05702458\n",
      "| epoch  32 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1055.53 | loss  1.18 | ppl     3.26 | acc     0.88 | train_ae_norm     1.00\n",
      "[32/200][2199/2499] Loss_D: 1.28982425 (Loss_D_real: 0.64672673 Loss_D_fake: 0.64309746) Loss_G: 0.00814705 Loss_Enh_Dec: -0.05777296\n",
      "| epoch  32 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1056.23 | loss  1.17 | ppl     3.22 | acc     0.86 | train_ae_norm     1.00\n",
      "[32/200][2299/2499] Loss_D: 1.27803683 (Loss_D_real: 0.64004165 Loss_D_fake: 0.63799524) Loss_G: 0.00685243 Loss_Enh_Dec: -0.07679480\n",
      "| epoch  32 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1055.06 | loss  1.17 | ppl     3.22 | acc     0.85 | train_ae_norm     1.00\n",
      "[32/200][2399/2499] Loss_D: 1.28460371 (Loss_D_real: 0.63999099 Loss_D_fake: 0.64461273) Loss_G: 0.00767609 Loss_Enh_Dec: -0.05845277\n",
      "| epoch  32 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1056.38 | loss  1.19 | ppl     3.27 | acc     0.81 | train_ae_norm     1.00\n",
      "[32/200][2499/2499] Loss_D: 1.28871560 (Loss_D_real: 0.65267599 Loss_D_fake: 0.63603967) Loss_G: 0.01034139 Loss_Enh_Dec: -0.05712472\n",
      "| end of epoch  32 | time: 2815.29s | test loss  0.92 | test ppl  2.51 | acc 0.903\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 33 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:09.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:16.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:23.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:30.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:37.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:44.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:51.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:58.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:04.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:18.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:25.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:32.\n",
      "\n",
      "  Average training loss generetor: 0.704\n",
      "  Average training loss discriminator: 0.713\n",
      "  Training epcoh took: 0:02:39\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.535\n",
      "  Test Loss: 2.770\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  33 |     0/ 2499 batches | lr 0.000000 | ms/batch 1592.54 | loss  0.01 | ppl     1.01 | acc     0.83 | train_ae_norm     1.00\n",
      "[33/200][99/2499] Loss_D: 1.28086543 (Loss_D_real: 0.62748271 Loss_D_fake: 0.65338266) Loss_G: 0.01105712 Loss_Enh_Dec: -0.02428357\n",
      "| epoch  33 |   100/ 2499 batches | lr 0.000000 | ms/batch 1055.72 | loss  1.34 | ppl     3.81 | acc     0.79 | train_ae_norm     1.00\n",
      "[33/200][199/2499] Loss_D: 1.31146812 (Loss_D_real: 0.65201497 Loss_D_fake: 0.65945315) Loss_G: 0.00861014 Loss_Enh_Dec: -0.06601955\n",
      "| epoch  33 |   200/ 2499 batches | lr 0.000000 | ms/batch 1055.92 | loss  1.43 | ppl     4.20 | acc     0.82 | train_ae_norm     1.00\n",
      "[33/200][299/2499] Loss_D: 1.26713705 (Loss_D_real: 0.64336872 Loss_D_fake: 0.62376827) Loss_G: 0.01244823 Loss_Enh_Dec: -0.08263306\n",
      "| epoch  33 |   300/ 2499 batches | lr 0.000000 | ms/batch 1055.69 | loss  1.33 | ppl     3.79 | acc     0.81 | train_ae_norm     1.00\n",
      "[33/200][399/2499] Loss_D: 1.26724672 (Loss_D_real: 0.62926185 Loss_D_fake: 0.63798493) Loss_G: 0.01216859 Loss_Enh_Dec: -0.08476092\n",
      "| epoch  33 |   400/ 2499 batches | lr 0.000000 | ms/batch 1056.33 | loss  1.29 | ppl     3.63 | acc     0.84 | train_ae_norm     1.00\n",
      "[33/200][499/2499] Loss_D: 1.22888136 (Loss_D_real: 0.61788028 Loss_D_fake: 0.61100113) Loss_G: 0.01606098 Loss_Enh_Dec: -0.09751099\n",
      "| epoch  33 |   500/ 2499 batches | lr 0.000000 | ms/batch 1056.30 | loss  1.25 | ppl     3.49 | acc     0.84 | train_ae_norm     1.00\n",
      "[33/200][599/2499] Loss_D: 1.28552699 (Loss_D_real: 0.65306032 Loss_D_fake: 0.63246673) Loss_G: 0.00974029 Loss_Enh_Dec: -0.12219720\n",
      "| epoch  33 |   600/ 2499 batches | lr 0.000000 | ms/batch 1056.05 | loss  1.25 | ppl     3.49 | acc     0.86 | train_ae_norm     1.00\n",
      "[33/200][699/2499] Loss_D: 1.24424362 (Loss_D_real: 0.62860173 Loss_D_fake: 0.61564195) Loss_G: 0.01184767 Loss_Enh_Dec: -0.11167014\n",
      "| epoch  33 |   700/ 2499 batches | lr 0.000000 | ms/batch 1056.46 | loss  1.23 | ppl     3.40 | acc     0.81 | train_ae_norm     1.00\n",
      "[33/200][799/2499] Loss_D: 1.21911144 (Loss_D_real: 0.60453057 Loss_D_fake: 0.61458087) Loss_G: 0.01551856 Loss_Enh_Dec: -0.07577280\n",
      "| epoch  33 |   800/ 2499 batches | lr 0.000000 | ms/batch 1056.17 | loss  1.24 | ppl     3.47 | acc     0.83 | train_ae_norm     1.00\n",
      "[33/200][899/2499] Loss_D: 1.21132362 (Loss_D_real: 0.59134698 Loss_D_fake: 0.61997664) Loss_G: 0.01668295 Loss_Enh_Dec: -0.13382743\n",
      "| epoch  33 |   900/ 2499 batches | lr 0.000000 | ms/batch 1056.44 | loss  1.20 | ppl     3.31 | acc     0.86 | train_ae_norm     1.00\n",
      "[33/200][999/2499] Loss_D: 1.22295082 (Loss_D_real: 0.60146487 Loss_D_fake: 0.62148595) Loss_G: 0.01209740 Loss_Enh_Dec: -0.12083954\n",
      "| epoch  33 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1056.39 | loss  1.22 | ppl     3.40 | acc     0.86 | train_ae_norm     1.00\n",
      "[33/200][1099/2499] Loss_D: 1.17729616 (Loss_D_real: 0.57296765 Loss_D_fake: 0.60432851) Loss_G: 0.01500754 Loss_Enh_Dec: -0.12283190\n",
      "| epoch  33 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1055.64 | loss  1.22 | ppl     3.38 | acc     0.85 | train_ae_norm     1.00\n",
      "[33/200][1199/2499] Loss_D: 1.21496320 (Loss_D_real: 0.61362505 Loss_D_fake: 0.60133809) Loss_G: 0.01515771 Loss_Enh_Dec: -0.12395010\n",
      "| epoch  33 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1055.33 | loss  1.22 | ppl     3.38 | acc     0.81 | train_ae_norm     1.00\n",
      "[33/200][1299/2499] Loss_D: 1.21749783 (Loss_D_real: 0.60182977 Loss_D_fake: 0.61566806) Loss_G: 0.01601457 Loss_Enh_Dec: -0.15658240\n",
      "| epoch  33 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1056.67 | loss  1.21 | ppl     3.35 | acc     0.81 | train_ae_norm     1.00\n",
      "[33/200][1399/2499] Loss_D: 1.20187628 (Loss_D_real: 0.58365220 Loss_D_fake: 0.61822408) Loss_G: 0.01946465 Loss_Enh_Dec: -0.16684936\n",
      "| epoch  33 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1056.36 | loss  1.20 | ppl     3.32 | acc     0.87 | train_ae_norm     1.00\n",
      "[33/200][1499/2499] Loss_D: 1.19326544 (Loss_D_real: 0.58191979 Loss_D_fake: 0.61134571) Loss_G: 0.01917005 Loss_Enh_Dec: -0.17329350\n",
      "| epoch  33 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1055.20 | loss  1.16 | ppl     3.19 | acc     0.86 | train_ae_norm     1.00\n",
      "[33/200][1599/2499] Loss_D: 1.17875278 (Loss_D_real: 0.59350002 Loss_D_fake: 0.58525276) Loss_G: 0.01864386 Loss_Enh_Dec: -0.19778918\n",
      "| epoch  33 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1055.61 | loss  1.18 | ppl     3.27 | acc     0.85 | train_ae_norm     1.00\n",
      "[33/200][1699/2499] Loss_D: 1.16712916 (Loss_D_real: 0.60235977 Loss_D_fake: 0.56476939) Loss_G: 0.01413682 Loss_Enh_Dec: -0.20687699\n",
      "| epoch  33 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1055.50 | loss  1.16 | ppl     3.18 | acc     0.87 | train_ae_norm     1.00\n",
      "[33/200][1799/2499] Loss_D: 1.19212425 (Loss_D_real: 0.59573829 Loss_D_fake: 0.59638596) Loss_G: 0.00494249 Loss_Enh_Dec: -0.18985976\n",
      "| epoch  33 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1056.39 | loss  1.13 | ppl     3.11 | acc     0.87 | train_ae_norm     1.00\n",
      "[33/200][1899/2499] Loss_D: 1.21017873 (Loss_D_real: 0.60074818 Loss_D_fake: 0.60943055) Loss_G: 0.01876018 Loss_Enh_Dec: -0.22835730\n",
      "| epoch  33 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1055.61 | loss  1.13 | ppl     3.09 | acc     0.84 | train_ae_norm     1.00\n",
      "[33/200][1999/2499] Loss_D: 1.08620560 (Loss_D_real: 0.52847409 Loss_D_fake: 0.55773151) Loss_G: 0.01765266 Loss_Enh_Dec: -0.19675985\n",
      "| epoch  33 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1055.17 | loss  1.13 | ppl     3.09 | acc     0.87 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33/200][2099/2499] Loss_D: 1.07552075 (Loss_D_real: 0.52090430 Loss_D_fake: 0.55461639) Loss_G: 0.02115051 Loss_Enh_Dec: -0.19321831\n",
      "| epoch  33 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1056.01 | loss  1.10 | ppl     3.02 | acc     0.89 | train_ae_norm     1.00\n",
      "[33/200][2199/2499] Loss_D: 1.15316737 (Loss_D_real: 0.59019363 Loss_D_fake: 0.56297374) Loss_G: 0.01951799 Loss_Enh_Dec: -0.22567074\n",
      "| epoch  33 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1056.88 | loss  1.09 | ppl     2.96 | acc     0.89 | train_ae_norm     1.00\n",
      "[33/200][2299/2499] Loss_D: 1.13836074 (Loss_D_real: 0.56318188 Loss_D_fake: 0.57517880) Loss_G: 0.01220406 Loss_Enh_Dec: -0.23356234\n",
      "| epoch  33 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1056.50 | loss  1.08 | ppl     2.96 | acc     0.87 | train_ae_norm     1.00\n",
      "[33/200][2399/2499] Loss_D: 1.14051974 (Loss_D_real: 0.60409808 Loss_D_fake: 0.53642166) Loss_G: 0.02256117 Loss_Enh_Dec: -0.25494751\n",
      "| epoch  33 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1056.90 | loss  1.09 | ppl     2.98 | acc     0.84 | train_ae_norm     1.00\n",
      "[33/200][2499/2499] Loss_D: 1.08851647 (Loss_D_real: 0.55759740 Loss_D_fake: 0.53091908) Loss_G: 0.01914950 Loss_Enh_Dec: -0.24544954\n",
      "| end of epoch  33 | time: 2818.53s | test loss  0.85 | test ppl  2.33 | acc 0.914\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 34 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:49.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:56.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:09.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:16.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:23.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:30.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:37.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:44.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:51.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:58.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:05.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:19.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:26.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:33.\n",
      "\n",
      "  Average training loss generetor: 0.704\n",
      "  Average training loss discriminator: 0.711\n",
      "  Training epcoh took: 0:02:40\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.530\n",
      "  Test Loss: 2.853\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  34 |     0/ 2499 batches | lr 0.000000 | ms/batch 1600.45 | loss  0.01 | ppl     1.01 | acc     0.87 | train_ae_norm     1.00\n",
      "[34/200][99/2499] Loss_D: 1.09515250 (Loss_D_real: 0.56450701 Loss_D_fake: 0.53064549) Loss_G: 0.02663979 Loss_Enh_Dec: -0.27026662\n",
      "| epoch  34 |   100/ 2499 batches | lr 0.000000 | ms/batch 1055.63 | loss  1.09 | ppl     2.97 | acc     0.84 | train_ae_norm     1.00\n",
      "[34/200][199/2499] Loss_D: 1.04298496 (Loss_D_real: 0.50779265 Loss_D_fake: 0.53519231) Loss_G: 0.02106044 Loss_Enh_Dec: -0.24693386\n",
      "| epoch  34 |   200/ 2499 batches | lr 0.000000 | ms/batch 1056.30 | loss  1.07 | ppl     2.91 | acc     0.87 | train_ae_norm     1.00\n",
      "[34/200][299/2499] Loss_D: 1.04786289 (Loss_D_real: 0.54720044 Loss_D_fake: 0.50066245) Loss_G: 0.02911216 Loss_Enh_Dec: -0.28712440\n",
      "| epoch  34 |   300/ 2499 batches | lr 0.000000 | ms/batch 1055.43 | loss  1.08 | ppl     2.94 | acc     0.87 | train_ae_norm     1.00\n",
      "[34/200][399/2499] Loss_D: 1.00837350 (Loss_D_real: 0.50519073 Loss_D_fake: 0.50318277) Loss_G: 0.02746845 Loss_Enh_Dec: -0.28335619\n",
      "| epoch  34 |   400/ 2499 batches | lr 0.000000 | ms/batch 1054.76 | loss  1.07 | ppl     2.90 | acc     0.86 | train_ae_norm     1.00\n",
      "[34/200][499/2499] Loss_D: 1.03338575 (Loss_D_real: 0.54474306 Loss_D_fake: 0.48864269) Loss_G: 0.03053401 Loss_Enh_Dec: -0.28261182\n",
      "| epoch  34 |   500/ 2499 batches | lr 0.000000 | ms/batch 1056.11 | loss  1.06 | ppl     2.87 | acc     0.86 | train_ae_norm     1.00\n",
      "[34/200][599/2499] Loss_D: 0.96571827 (Loss_D_real: 0.47899434 Loss_D_fake: 0.48672390) Loss_G: 0.03132165 Loss_Enh_Dec: -0.29791179\n",
      "| epoch  34 |   600/ 2499 batches | lr 0.000000 | ms/batch 1056.10 | loss  1.03 | ppl     2.79 | acc     0.89 | train_ae_norm     1.00\n",
      "[34/200][699/2499] Loss_D: 0.96974844 (Loss_D_real: 0.48454642 Loss_D_fake: 0.48520201) Loss_G: 0.02849876 Loss_Enh_Dec: -0.29915729\n",
      "| epoch  34 |   700/ 2499 batches | lr 0.000000 | ms/batch 1056.87 | loss  1.04 | ppl     2.82 | acc     0.85 | train_ae_norm     1.00\n",
      "[34/200][799/2499] Loss_D: 1.00357795 (Loss_D_real: 0.53483361 Loss_D_fake: 0.46874437) Loss_G: 0.03138542 Loss_Enh_Dec: -0.27411801\n",
      "| epoch  34 |   800/ 2499 batches | lr 0.000000 | ms/batch 1056.62 | loss  1.04 | ppl     2.83 | acc     0.85 | train_ae_norm     1.00\n",
      "[34/200][899/2499] Loss_D: 0.91016942 (Loss_D_real: 0.47176525 Loss_D_fake: 0.43840417) Loss_G: 0.02971458 Loss_Enh_Dec: -0.30582047\n",
      "| epoch  34 |   900/ 2499 batches | lr 0.000000 | ms/batch 1056.89 | loss  1.01 | ppl     2.76 | acc     0.89 | train_ae_norm     1.00\n",
      "[34/200][999/2499] Loss_D: 0.87548649 (Loss_D_real: 0.42071888 Loss_D_fake: 0.45476764) Loss_G: 0.03169420 Loss_Enh_Dec: -0.34155488\n",
      "| epoch  34 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1056.73 | loss  1.05 | ppl     2.86 | acc     0.87 | train_ae_norm     1.00\n",
      "[34/200][1099/2499] Loss_D: 0.92041522 (Loss_D_real: 0.47150254 Loss_D_fake: 0.44891268) Loss_G: 0.03132626 Loss_Enh_Dec: -0.36256239\n",
      "| epoch  34 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1062.27 | loss  1.00 | ppl     2.73 | acc     0.90 | train_ae_norm     1.00\n",
      "[34/200][1199/2499] Loss_D: 0.86356515 (Loss_D_real: 0.42801613 Loss_D_fake: 0.43554902) Loss_G: 0.03496471 Loss_Enh_Dec: -0.32969418\n",
      "| epoch  34 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1056.56 | loss  1.02 | ppl     2.78 | acc     0.86 | train_ae_norm     1.00\n",
      "[34/200][1299/2499] Loss_D: 0.91362256 (Loss_D_real: 0.47678828 Loss_D_fake: 0.43683428) Loss_G: 0.03598423 Loss_Enh_Dec: -0.31736496\n",
      "| epoch  34 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1055.97 | loss  1.06 | ppl     2.89 | acc     0.82 | train_ae_norm     1.00\n",
      "[34/200][1399/2499] Loss_D: 0.88438541 (Loss_D_real: 0.44740462 Loss_D_fake: 0.43698078) Loss_G: 0.03404240 Loss_Enh_Dec: -0.32185382\n",
      "| epoch  34 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1057.11 | loss  1.07 | ppl     2.91 | acc     0.87 | train_ae_norm     1.00\n",
      "[34/200][1499/2499] Loss_D: 0.89786810 (Loss_D_real: 0.45269603 Loss_D_fake: 0.44517207) Loss_G: 0.03637324 Loss_Enh_Dec: -0.39451790\n",
      "| epoch  34 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1056.94 | loss  1.01 | ppl     2.76 | acc     0.86 | train_ae_norm     1.00\n",
      "[34/200][1599/2499] Loss_D: 0.80817580 (Loss_D_real: 0.39454466 Loss_D_fake: 0.41363117) Loss_G: 0.03741261 Loss_Enh_Dec: -0.31608093\n",
      "| epoch  34 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1056.73 | loss  1.04 | ppl     2.82 | acc     0.86 | train_ae_norm     1.00\n",
      "[34/200][1699/2499] Loss_D: 0.87468749 (Loss_D_real: 0.46318394 Loss_D_fake: 0.41150355) Loss_G: 0.03796970 Loss_Enh_Dec: -0.32168674\n",
      "| epoch  34 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1056.60 | loss  0.98 | ppl     2.67 | acc     0.90 | train_ae_norm     1.00\n",
      "[34/200][1799/2499] Loss_D: 0.81097078 (Loss_D_real: 0.41195720 Loss_D_fake: 0.39901361) Loss_G: 0.03818271 Loss_Enh_Dec: -0.35471436\n",
      "| epoch  34 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1057.06 | loss  0.99 | ppl     2.68 | acc     0.90 | train_ae_norm     1.00\n",
      "[34/200][1899/2499] Loss_D: 0.90387571 (Loss_D_real: 0.46655297 Loss_D_fake: 0.43732271) Loss_G: 0.03319487 Loss_Enh_Dec: -0.42042246\n",
      "| epoch  34 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1056.08 | loss  1.01 | ppl     2.75 | acc     0.87 | train_ae_norm     1.00\n",
      "[34/200][1999/2499] Loss_D: 0.84833527 (Loss_D_real: 0.41026384 Loss_D_fake: 0.43807143) Loss_G: 0.03678002 Loss_Enh_Dec: -0.42961761\n",
      "| epoch  34 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1056.46 | loss  1.01 | ppl     2.75 | acc     0.88 | train_ae_norm     1.00\n",
      "[34/200][2099/2499] Loss_D: 0.95558953 (Loss_D_real: 0.51068974 Loss_D_fake: 0.44489980) Loss_G: 0.03203722 Loss_Enh_Dec: -0.39352101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  34 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1056.62 | loss  0.99 | ppl     2.69 | acc     0.90 | train_ae_norm     1.00\n",
      "[34/200][2199/2499] Loss_D: 0.86026514 (Loss_D_real: 0.45413455 Loss_D_fake: 0.40613061) Loss_G: 0.02846937 Loss_Enh_Dec: -0.44213817\n",
      "| epoch  34 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1056.96 | loss  0.95 | ppl     2.59 | acc     0.90 | train_ae_norm     1.00\n",
      "[34/200][2299/2499] Loss_D: 0.81663465 (Loss_D_real: 0.46554884 Loss_D_fake: 0.35108584) Loss_G: 0.04367068 Loss_Enh_Dec: -0.38029751\n",
      "| epoch  34 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1055.87 | loss  0.96 | ppl     2.62 | acc     0.89 | train_ae_norm     1.00\n",
      "[34/200][2399/2499] Loss_D: 0.79621798 (Loss_D_real: 0.43550861 Loss_D_fake: 0.36070937) Loss_G: 0.04294430 Loss_Enh_Dec: -0.48851204\n",
      "| epoch  34 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1056.49 | loss  0.98 | ppl     2.66 | acc     0.85 | train_ae_norm     1.00\n",
      "[34/200][2499/2499] Loss_D: 0.91258258 (Loss_D_real: 0.50864649 Loss_D_fake: 0.40393609) Loss_G: 0.03910974 Loss_Enh_Dec: -0.42576322\n",
      "| end of epoch  34 | time: 2820.74s | test loss  0.79 | test ppl  2.20 | acc 0.922\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 35 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:49.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:56.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:10.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:17.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:23.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:30.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:37.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:44.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:51.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:58.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:05.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:19.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:26.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:33.\n",
      "\n",
      "  Average training loss generetor: 0.705\n",
      "  Average training loss discriminator: 0.710\n",
      "  Training epcoh took: 0:02:40\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.500\n",
      "  Test Loss: 3.012\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  35 |     0/ 2499 batches | lr 0.000000 | ms/batch 1601.30 | loss  0.01 | ppl     1.01 | acc     0.88 | train_ae_norm     1.00\n",
      "[35/200][99/2499] Loss_D: 0.74838990 (Loss_D_real: 0.34564781 Loss_D_fake: 0.40274209) Loss_G: 0.03919043 Loss_Enh_Dec: -0.50093764\n",
      "| epoch  35 |   100/ 2499 batches | lr 0.000000 | ms/batch 1056.53 | loss  0.97 | ppl     2.65 | acc     0.85 | train_ae_norm     1.00\n",
      "[35/200][199/2499] Loss_D: 0.76940262 (Loss_D_real: 0.40538007 Loss_D_fake: 0.36402258) Loss_G: 0.03889355 Loss_Enh_Dec: -0.50028926\n",
      "| epoch  35 |   200/ 2499 batches | lr 0.000000 | ms/batch 1055.60 | loss  0.99 | ppl     2.68 | acc     0.89 | train_ae_norm     1.00\n",
      "[35/200][299/2499] Loss_D: 0.73499966 (Loss_D_real: 0.35638961 Loss_D_fake: 0.37861007) Loss_G: 0.04760090 Loss_Enh_Dec: -0.48539692\n",
      "| epoch  35 |   300/ 2499 batches | lr 0.000000 | ms/batch 1056.27 | loss  1.03 | ppl     2.79 | acc     0.85 | train_ae_norm     1.00\n",
      "[35/200][399/2499] Loss_D: 0.87937546 (Loss_D_real: 0.47669625 Loss_D_fake: 0.40267918) Loss_G: 0.04176780 Loss_Enh_Dec: -0.48899919\n",
      "| epoch  35 |   400/ 2499 batches | lr 0.000000 | ms/batch 1056.51 | loss  1.08 | ppl     2.93 | acc     0.86 | train_ae_norm     1.00\n",
      "[35/200][499/2499] Loss_D: 0.68833423 (Loss_D_real: 0.31462955 Loss_D_fake: 0.37370467) Loss_G: 0.04001790 Loss_Enh_Dec: -0.51785851\n",
      "| epoch  35 |   500/ 2499 batches | lr 0.000000 | ms/batch 1056.49 | loss  1.02 | ppl     2.77 | acc     0.85 | train_ae_norm     1.00\n",
      "[35/200][599/2499] Loss_D: 0.68376970 (Loss_D_real: 0.35351539 Loss_D_fake: 0.33025432) Loss_G: 0.05006334 Loss_Enh_Dec: -0.54968262\n",
      "| epoch  35 |   600/ 2499 batches | lr 0.000000 | ms/batch 1056.10 | loss  1.02 | ppl     2.76 | acc     0.86 | train_ae_norm     1.00\n",
      "[35/200][699/2499] Loss_D: 0.77749109 (Loss_D_real: 0.43462163 Loss_D_fake: 0.34286946) Loss_G: 0.04781329 Loss_Enh_Dec: -0.50819629\n",
      "| epoch  35 |   700/ 2499 batches | lr 0.000000 | ms/batch 1057.19 | loss  1.00 | ppl     2.71 | acc     0.85 | train_ae_norm     1.00\n",
      "[35/200][799/2499] Loss_D: 0.69647670 (Loss_D_real: 0.28721631 Loss_D_fake: 0.40926036) Loss_G: 0.04604077 Loss_Enh_Dec: -0.54305422\n",
      "| epoch  35 |   800/ 2499 batches | lr 0.000000 | ms/batch 1055.60 | loss  1.04 | ppl     2.82 | acc     0.86 | train_ae_norm     1.00\n",
      "[35/200][899/2499] Loss_D: 0.60479426 (Loss_D_real: 0.28760755 Loss_D_fake: 0.31718668) Loss_G: 0.05102716 Loss_Enh_Dec: -0.53953856\n",
      "| epoch  35 |   900/ 2499 batches | lr 0.000000 | ms/batch 1056.70 | loss  0.96 | ppl     2.60 | acc     0.90 | train_ae_norm     1.00\n",
      "[35/200][999/2499] Loss_D: 0.75599533 (Loss_D_real: 0.41145974 Loss_D_fake: 0.34453559) Loss_G: 0.05049620 Loss_Enh_Dec: -0.56058115\n",
      "| epoch  35 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1055.02 | loss  1.00 | ppl     2.73 | acc     0.89 | train_ae_norm     1.00\n",
      "[35/200][1099/2499] Loss_D: 0.69923973 (Loss_D_real: 0.35188371 Loss_D_fake: 0.34735605) Loss_G: 0.05283235 Loss_Enh_Dec: -0.60608149\n",
      "| epoch  35 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1055.93 | loss  0.98 | ppl     2.66 | acc     0.89 | train_ae_norm     1.00\n",
      "[35/200][1199/2499] Loss_D: 0.76000011 (Loss_D_real: 0.45868033 Loss_D_fake: 0.30131981) Loss_G: 0.05327233 Loss_Enh_Dec: -0.61528152\n",
      "| epoch  35 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1056.72 | loss  1.01 | ppl     2.73 | acc     0.85 | train_ae_norm     1.00\n",
      "[35/200][1299/2499] Loss_D: 0.66143376 (Loss_D_real: 0.35764322 Loss_D_fake: 0.30379054) Loss_G: 0.05298923 Loss_Enh_Dec: -0.59133124\n",
      "| epoch  35 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1055.00 | loss  1.03 | ppl     2.79 | acc     0.85 | train_ae_norm     1.00\n",
      "[35/200][1399/2499] Loss_D: 0.72288823 (Loss_D_real: 0.38961333 Loss_D_fake: 0.33327490) Loss_G: 0.05070442 Loss_Enh_Dec: -0.28482369\n",
      "| epoch  35 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1054.89 | loss  1.01 | ppl     2.73 | acc     0.89 | train_ae_norm     1.00\n",
      "[35/200][1499/2499] Loss_D: 0.66327280 (Loss_D_real: 0.35255170 Loss_D_fake: 0.31072110) Loss_G: 0.05724156 Loss_Enh_Dec: -0.36704221\n",
      "| epoch  35 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1057.29 | loss  1.00 | ppl     2.72 | acc     0.88 | train_ae_norm     1.00\n",
      "[35/200][1599/2499] Loss_D: 0.68839759 (Loss_D_real: 0.30088139 Loss_D_fake: 0.38751620) Loss_G: 0.05698708 Loss_Enh_Dec: -0.56947571\n",
      "| epoch  35 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1057.36 | loss  1.02 | ppl     2.78 | acc     0.86 | train_ae_norm     1.00\n",
      "[35/200][1699/2499] Loss_D: 0.68473363 (Loss_D_real: 0.39743227 Loss_D_fake: 0.28730139) Loss_G: 0.05874942 Loss_Enh_Dec: -0.60015643\n",
      "| epoch  35 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1056.76 | loss  0.95 | ppl     2.59 | acc     0.90 | train_ae_norm     1.00\n",
      "[35/200][1799/2499] Loss_D: 0.60887611 (Loss_D_real: 0.31414440 Loss_D_fake: 0.29473174) Loss_G: 0.05650901 Loss_Enh_Dec: -0.62309742\n",
      "| epoch  35 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1057.06 | loss  0.96 | ppl     2.62 | acc     0.92 | train_ae_norm     1.00\n",
      "[35/200][1899/2499] Loss_D: 0.56017172 (Loss_D_real: 0.26442990 Loss_D_fake: 0.29574183) Loss_G: 0.05633476 Loss_Enh_Dec: -0.59030908\n",
      "| epoch  35 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1056.64 | loss  0.96 | ppl     2.60 | acc     0.85 | train_ae_norm     1.00\n",
      "[35/200][1999/2499] Loss_D: 0.65550292 (Loss_D_real: 0.36290377 Loss_D_fake: 0.29259914) Loss_G: 0.05734780 Loss_Enh_Dec: -0.65010911\n",
      "| epoch  35 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1056.34 | loss  1.01 | ppl     2.75 | acc     0.89 | train_ae_norm     1.00\n",
      "[35/200][2099/2499] Loss_D: 0.72498721 (Loss_D_real: 0.35467052 Loss_D_fake: 0.37031668) Loss_G: 0.04144320 Loss_Enh_Dec: -0.58603227\n",
      "| epoch  35 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1055.92 | loss  0.96 | ppl     2.62 | acc     0.90 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35/200][2199/2499] Loss_D: 0.74389523 (Loss_D_real: 0.39764541 Loss_D_fake: 0.34624982) Loss_G: 0.05596375 Loss_Enh_Dec: -0.66905421\n",
      "| epoch  35 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1057.50 | loss  0.93 | ppl     2.54 | acc     0.90 | train_ae_norm     1.00\n",
      "[35/200][2299/2499] Loss_D: 0.64486980 (Loss_D_real: 0.31928906 Loss_D_fake: 0.32558075) Loss_G: 0.04695737 Loss_Enh_Dec: -0.68079060\n",
      "| epoch  35 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1057.43 | loss  0.93 | ppl     2.55 | acc     0.90 | train_ae_norm     1.00\n",
      "[35/200][2399/2499] Loss_D: 0.67101479 (Loss_D_real: 0.31062284 Loss_D_fake: 0.36039191) Loss_G: 0.04918303 Loss_Enh_Dec: -0.64374822\n",
      "| epoch  35 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1056.26 | loss  0.94 | ppl     2.56 | acc     0.86 | train_ae_norm     1.00\n",
      "[35/200][2499/2499] Loss_D: 0.61286193 (Loss_D_real: 0.30852807 Loss_D_fake: 0.30433387) Loss_G: 0.05847390 Loss_Enh_Dec: -0.70313776\n",
      "| end of epoch  35 | time: 2820.12s | test loss  0.78 | test ppl  2.17 | acc 0.923\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 36 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:09.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:16.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:23.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:30.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:37.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:44.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:51.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:58.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:05.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:18.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:25.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:32.\n",
      "\n",
      "  Average training loss generetor: 0.704\n",
      "  Average training loss discriminator: 0.709\n",
      "  Training epcoh took: 0:02:39\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.495\n",
      "  Test Loss: 3.117\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  36 |     0/ 2499 batches | lr 0.000000 | ms/batch 1594.63 | loss  0.01 | ppl     1.01 | acc     0.88 | train_ae_norm     1.00\n",
      "[36/200][99/2499] Loss_D: 0.56701690 (Loss_D_real: 0.28155577 Loss_D_fake: 0.28546113) Loss_G: 0.05970747 Loss_Enh_Dec: -0.69495535\n",
      "| epoch  36 |   100/ 2499 batches | lr 0.000000 | ms/batch 1055.96 | loss  0.95 | ppl     2.58 | acc     0.84 | train_ae_norm     1.00\n",
      "[36/200][199/2499] Loss_D: 0.64310282 (Loss_D_real: 0.34916696 Loss_D_fake: 0.29393587) Loss_G: 0.06048412 Loss_Enh_Dec: -0.65465546\n",
      "| epoch  36 |   200/ 2499 batches | lr 0.000000 | ms/batch 1054.40 | loss  0.93 | ppl     2.54 | acc     0.89 | train_ae_norm     1.00\n",
      "[36/200][299/2499] Loss_D: 0.53202140 (Loss_D_real: 0.23883997 Loss_D_fake: 0.29318142) Loss_G: 0.06006939 Loss_Enh_Dec: -0.57607615\n",
      "| epoch  36 |   300/ 2499 batches | lr 0.000000 | ms/batch 1055.08 | loss  0.94 | ppl     2.56 | acc     0.88 | train_ae_norm     1.00\n",
      "[36/200][399/2499] Loss_D: 0.74374056 (Loss_D_real: 0.40195137 Loss_D_fake: 0.34178922) Loss_G: 0.05924461 Loss_Enh_Dec: -0.52321070\n",
      "| epoch  36 |   400/ 2499 batches | lr 0.000000 | ms/batch 1055.54 | loss  0.93 | ppl     2.53 | acc     0.90 | train_ae_norm     1.00\n",
      "[36/200][499/2499] Loss_D: 0.54727441 (Loss_D_real: 0.26361114 Loss_D_fake: 0.28366327) Loss_G: 0.06218193 Loss_Enh_Dec: -0.63794327\n",
      "| epoch  36 |   500/ 2499 batches | lr 0.000000 | ms/batch 1056.17 | loss  0.90 | ppl     2.47 | acc     0.89 | train_ae_norm     1.00\n",
      "[36/200][599/2499] Loss_D: 0.52813709 (Loss_D_real: 0.25500482 Loss_D_fake: 0.27313226) Loss_G: 0.06187456 Loss_Enh_Dec: -0.76564711\n",
      "| epoch  36 |   600/ 2499 batches | lr 0.000000 | ms/batch 1055.97 | loss  0.93 | ppl     2.52 | acc     0.88 | train_ae_norm     1.00\n",
      "[36/200][699/2499] Loss_D: 0.73517287 (Loss_D_real: 0.31990385 Loss_D_fake: 0.41526902) Loss_G: 0.03980437 Loss_Enh_Dec: -0.64513457\n",
      "| epoch  36 |   700/ 2499 batches | lr 0.000000 | ms/batch 1055.52 | loss  0.91 | ppl     2.49 | acc     0.89 | train_ae_norm     1.00\n",
      "[36/200][799/2499] Loss_D: 0.64239967 (Loss_D_real: 0.33225751 Loss_D_fake: 0.31014216) Loss_G: 0.05833554 Loss_Enh_Dec: -0.63829184\n",
      "| epoch  36 |   800/ 2499 batches | lr 0.000000 | ms/batch 1054.50 | loss  0.95 | ppl     2.58 | acc     0.86 | train_ae_norm     1.00\n",
      "[36/200][899/2499] Loss_D: 0.74077612 (Loss_D_real: 0.40111977 Loss_D_fake: 0.33965635) Loss_G: 0.06041782 Loss_Enh_Dec: -0.72933465\n",
      "| epoch  36 |   900/ 2499 batches | lr 0.000000 | ms/batch 1055.56 | loss  0.91 | ppl     2.48 | acc     0.90 | train_ae_norm     1.00\n",
      "[36/200][999/2499] Loss_D: 0.60098386 (Loss_D_real: 0.30558982 Loss_D_fake: 0.29539403) Loss_G: 0.05798682 Loss_Enh_Dec: -0.76187056\n",
      "| epoch  36 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1053.34 | loss  0.95 | ppl     2.59 | acc     0.90 | train_ae_norm     1.00\n",
      "[36/200][1099/2499] Loss_D: 0.66842997 (Loss_D_real: 0.34423900 Loss_D_fake: 0.32419097) Loss_G: 0.05484629 Loss_Enh_Dec: -0.75740236\n",
      "| epoch  36 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1056.53 | loss  0.90 | ppl     2.45 | acc     0.90 | train_ae_norm     1.00\n",
      "[36/200][1199/2499] Loss_D: 0.46296525 (Loss_D_real: 0.18179363 Loss_D_fake: 0.28117162) Loss_G: 0.06369080 Loss_Enh_Dec: -0.71116346\n",
      "| epoch  36 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1056.31 | loss  0.93 | ppl     2.52 | acc     0.85 | train_ae_norm     1.00\n",
      "[36/200][1299/2499] Loss_D: 0.59992099 (Loss_D_real: 0.28906751 Loss_D_fake: 0.31085348) Loss_G: 0.05675060 Loss_Enh_Dec: -0.61969644\n",
      "| epoch  36 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1055.41 | loss  0.99 | ppl     2.70 | acc     0.85 | train_ae_norm     1.00\n",
      "[36/200][1399/2499] Loss_D: 0.55684149 (Loss_D_real: 0.24267046 Loss_D_fake: 0.31417105) Loss_G: 0.06142164 Loss_Enh_Dec: -0.77547342\n",
      "| epoch  36 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1055.07 | loss  0.97 | ppl     2.63 | acc     0.90 | train_ae_norm     1.00\n",
      "[36/200][1499/2499] Loss_D: 0.54730415 (Loss_D_real: 0.27898234 Loss_D_fake: 0.26832181) Loss_G: 0.06104104 Loss_Enh_Dec: -0.72176182\n",
      "| epoch  36 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1056.01 | loss  0.92 | ppl     2.50 | acc     0.88 | train_ae_norm     1.00\n",
      "[36/200][1599/2499] Loss_D: 0.43910238 (Loss_D_real: 0.20997798 Loss_D_fake: 0.22912440) Loss_G: 0.06446622 Loss_Enh_Dec: -0.77750218\n",
      "| epoch  36 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1055.84 | loss  0.96 | ppl     2.60 | acc     0.88 | train_ae_norm     1.00\n",
      "[36/200][1699/2499] Loss_D: 0.56915164 (Loss_D_real: 0.30824462 Loss_D_fake: 0.26090702) Loss_G: 0.06603328 Loss_Enh_Dec: -0.71620172\n",
      "| epoch  36 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1055.66 | loss  0.91 | ppl     2.49 | acc     0.90 | train_ae_norm     1.00\n",
      "[36/200][1799/2499] Loss_D: 0.54067391 (Loss_D_real: 0.26013213 Loss_D_fake: 0.28054178) Loss_G: 0.06914332 Loss_Enh_Dec: -0.60691833\n",
      "| epoch  36 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1057.04 | loss  0.95 | ppl     2.59 | acc     0.91 | train_ae_norm     1.00\n",
      "[36/200][1899/2499] Loss_D: 0.57801080 (Loss_D_real: 0.29813972 Loss_D_fake: 0.27987105) Loss_G: 0.05857584 Loss_Enh_Dec: -0.84010643\n",
      "| epoch  36 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1054.46 | loss  0.97 | ppl     2.63 | acc     0.87 | train_ae_norm     1.00\n",
      "[36/200][1999/2499] Loss_D: 0.47036999 (Loss_D_real: 0.22825414 Loss_D_fake: 0.24211586) Loss_G: 0.06881275 Loss_Enh_Dec: -0.80664885\n",
      "| epoch  36 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1055.98 | loss  0.97 | ppl     2.65 | acc     0.89 | train_ae_norm     1.00\n",
      "[36/200][2099/2499] Loss_D: 0.43460256 (Loss_D_real: 0.21646504 Loss_D_fake: 0.21813750) Loss_G: 0.07407176 Loss_Enh_Dec: -0.66359150\n",
      "| epoch  36 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1055.23 | loss  0.95 | ppl     2.58 | acc     0.91 | train_ae_norm     1.00\n",
      "[36/200][2199/2499] Loss_D: 0.54858112 (Loss_D_real: 0.23266064 Loss_D_fake: 0.31592047) Loss_G: 0.06396890 Loss_Enh_Dec: -0.71874392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  36 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1055.69 | loss  0.93 | ppl     2.53 | acc     0.90 | train_ae_norm     1.00\n",
      "[36/200][2299/2499] Loss_D: 0.55866122 (Loss_D_real: 0.29901683 Loss_D_fake: 0.25964439) Loss_G: 0.07021236 Loss_Enh_Dec: -0.78453398\n",
      "| epoch  36 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1055.64 | loss  0.95 | ppl     2.58 | acc     0.88 | train_ae_norm     1.00\n",
      "[36/200][2399/2499] Loss_D: 0.46705055 (Loss_D_real: 0.25927398 Loss_D_fake: 0.20777658) Loss_G: 0.07595996 Loss_Enh_Dec: -0.72431999\n",
      "| epoch  36 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1056.19 | loss  0.96 | ppl     2.61 | acc     0.85 | train_ae_norm     1.00\n",
      "[36/200][2499/2499] Loss_D: 0.49811834 (Loss_D_real: 0.23668222 Loss_D_fake: 0.26143610) Loss_G: 0.07843017 Loss_Enh_Dec: -0.47742423\n",
      "| end of epoch  36 | time: 2817.48s | test loss  0.81 | test ppl  2.25 | acc 0.922\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 37 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:55.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:09.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:16.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:23.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:30.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:37.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:44.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:51.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:58.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:05.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:18.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:25.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:32.\n",
      "\n",
      "  Average training loss generetor: 0.704\n",
      "  Average training loss discriminator: 0.709\n",
      "  Training epcoh took: 0:02:39\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.505\n",
      "  Test Loss: 3.203\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  37 |     0/ 2499 batches | lr 0.000000 | ms/batch 1593.55 | loss  0.01 | ppl     1.01 | acc     0.87 | train_ae_norm     1.00\n",
      "[37/200][99/2499] Loss_D: 0.39009717 (Loss_D_real: 0.19366422 Loss_D_fake: 0.19643295) Loss_G: 0.07204906 Loss_Enh_Dec: -0.57133532\n",
      "| epoch  37 |   100/ 2499 batches | lr 0.000000 | ms/batch 1056.29 | loss  0.98 | ppl     2.66 | acc     0.85 | train_ae_norm     1.00\n",
      "[37/200][199/2499] Loss_D: 0.53493929 (Loss_D_real: 0.30845952 Loss_D_fake: 0.22647980) Loss_G: 0.07683844 Loss_Enh_Dec: -0.55895245\n",
      "| epoch  37 |   200/ 2499 batches | lr 0.000000 | ms/batch 1055.40 | loss  0.99 | ppl     2.69 | acc     0.90 | train_ae_norm     1.00\n",
      "[37/200][299/2499] Loss_D: 0.52788997 (Loss_D_real: 0.28312570 Loss_D_fake: 0.24476424) Loss_G: 0.06398613 Loss_Enh_Dec: -0.59152943\n",
      "| epoch  37 |   300/ 2499 batches | lr 0.000000 | ms/batch 1055.73 | loss  0.98 | ppl     2.66 | acc     0.87 | train_ae_norm     1.00\n",
      "[37/200][399/2499] Loss_D: 0.64505428 (Loss_D_real: 0.36043286 Loss_D_fake: 0.28462142) Loss_G: 0.06941544 Loss_Enh_Dec: -0.71572280\n",
      "| epoch  37 |   400/ 2499 batches | lr 0.000000 | ms/batch 1056.66 | loss  1.00 | ppl     2.72 | acc     0.88 | train_ae_norm     1.00\n",
      "[37/200][499/2499] Loss_D: 0.47367144 (Loss_D_real: 0.21826276 Loss_D_fake: 0.25540867) Loss_G: 0.06813617 Loss_Enh_Dec: -0.70628303\n",
      "| epoch  37 |   500/ 2499 batches | lr 0.000000 | ms/batch 1057.59 | loss  0.96 | ppl     2.60 | acc     0.89 | train_ae_norm     1.00\n",
      "[37/200][599/2499] Loss_D: 0.57218564 (Loss_D_real: 0.24956131 Loss_D_fake: 0.32262433) Loss_G: 0.05434125 Loss_Enh_Dec: -0.61791986\n",
      "| epoch  37 |   600/ 2499 batches | lr 0.000000 | ms/batch 1056.71 | loss  0.96 | ppl     2.60 | acc     0.87 | train_ae_norm     1.00\n",
      "[37/200][699/2499] Loss_D: 0.58825499 (Loss_D_real: 0.26789045 Loss_D_fake: 0.32036453) Loss_G: 0.07247156 Loss_Enh_Dec: -0.65940571\n",
      "| epoch  37 |   700/ 2499 batches | lr 0.000000 | ms/batch 1056.19 | loss  0.98 | ppl     2.66 | acc     0.88 | train_ae_norm     1.00\n",
      "[37/200][799/2499] Loss_D: 0.39798445 (Loss_D_real: 0.21409437 Loss_D_fake: 0.18389007) Loss_G: 0.07565333 Loss_Enh_Dec: -0.47713038\n",
      "| epoch  37 |   800/ 2499 batches | lr 0.000000 | ms/batch 1056.87 | loss  1.00 | ppl     2.72 | acc     0.86 | train_ae_norm     1.00\n",
      "[37/200][899/2499] Loss_D: 0.42471921 (Loss_D_real: 0.21206453 Loss_D_fake: 0.21265469) Loss_G: 0.07801284 Loss_Enh_Dec: -0.21001354\n",
      "| epoch  37 |   900/ 2499 batches | lr 0.000000 | ms/batch 1056.44 | loss  0.99 | ppl     2.70 | acc     0.90 | train_ae_norm     1.00\n",
      "[37/200][999/2499] Loss_D: 0.46856925 (Loss_D_real: 0.26643297 Loss_D_fake: 0.20213628) Loss_G: 0.08297624 Loss_Enh_Dec: -0.30460954\n",
      "| epoch  37 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1055.83 | loss  1.11 | ppl     3.03 | acc     0.86 | train_ae_norm     1.00\n",
      "[37/200][1099/2499] Loss_D: 0.50712413 (Loss_D_real: 0.26322293 Loss_D_fake: 0.24390119) Loss_G: 0.07274017 Loss_Enh_Dec: -0.20762797\n",
      "| epoch  37 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1053.12 | loss  1.15 | ppl     3.15 | acc     0.86 | train_ae_norm     1.00\n",
      "[37/200][1199/2499] Loss_D: 0.50058359 (Loss_D_real: 0.25944436 Loss_D_fake: 0.24113922) Loss_G: 0.07625680 Loss_Enh_Dec: -0.20778790\n",
      "| epoch  37 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1051.30 | loss  1.15 | ppl     3.15 | acc     0.84 | train_ae_norm     1.00\n",
      "[37/200][1299/2499] Loss_D: 0.67410636 (Loss_D_real: 0.41314718 Loss_D_fake: 0.26095915) Loss_G: 0.06633073 Loss_Enh_Dec: -0.33039024\n",
      "| epoch  37 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1051.04 | loss  1.10 | ppl     3.01 | acc     0.83 | train_ae_norm     1.00\n",
      "[37/200][1399/2499] Loss_D: 0.56861645 (Loss_D_real: 0.25069356 Loss_D_fake: 0.31792289) Loss_G: 0.07578573 Loss_Enh_Dec: -0.56797808\n",
      "| epoch  37 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1051.18 | loss  1.09 | ppl     2.99 | acc     0.88 | train_ae_norm     1.00\n",
      "[37/200][1499/2499] Loss_D: 0.50608170 (Loss_D_real: 0.30998093 Loss_D_fake: 0.19610076) Loss_G: 0.07643320 Loss_Enh_Dec: -0.66716760\n",
      "| epoch  37 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1052.65 | loss  1.05 | ppl     2.87 | acc     0.87 | train_ae_norm     1.00\n",
      "[37/200][1599/2499] Loss_D: 0.39802152 (Loss_D_real: 0.20561635 Loss_D_fake: 0.19240515) Loss_G: 0.08296680 Loss_Enh_Dec: -0.58025593\n",
      "| epoch  37 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1051.54 | loss  1.06 | ppl     2.90 | acc     0.88 | train_ae_norm     1.00\n",
      "[37/200][1699/2499] Loss_D: 0.49457294 (Loss_D_real: 0.24369907 Loss_D_fake: 0.25087386) Loss_G: 0.06779473 Loss_Enh_Dec: -0.63659173\n",
      "| epoch  37 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1051.32 | loss  1.05 | ppl     2.85 | acc     0.90 | train_ae_norm     1.00\n",
      "[37/200][1799/2499] Loss_D: 0.45960838 (Loss_D_real: 0.25646332 Loss_D_fake: 0.20314507) Loss_G: 0.08367318 Loss_Enh_Dec: -0.65105468\n",
      "| epoch  37 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1051.56 | loss  1.01 | ppl     2.74 | acc     0.91 | train_ae_norm     1.00\n",
      "[37/200][1899/2499] Loss_D: 0.43204457 (Loss_D_real: 0.24565664 Loss_D_fake: 0.18638793) Loss_G: 0.08267871 Loss_Enh_Dec: -0.75452632\n",
      "| epoch  37 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1050.86 | loss  1.01 | ppl     2.76 | acc     0.85 | train_ae_norm     1.00\n",
      "[37/200][1999/2499] Loss_D: 0.52950776 (Loss_D_real: 0.26428711 Loss_D_fake: 0.26522061) Loss_G: 0.06738658 Loss_Enh_Dec: -0.64888734\n",
      "| epoch  37 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1050.13 | loss  1.02 | ppl     2.77 | acc     0.89 | train_ae_norm     1.00\n",
      "[37/200][2099/2499] Loss_D: 0.56221640 (Loss_D_real: 0.35626829 Loss_D_fake: 0.20594811) Loss_G: 0.07867839 Loss_Enh_Dec: -0.75270528\n",
      "| epoch  37 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1051.50 | loss  0.99 | ppl     2.69 | acc     0.90 | train_ae_norm     1.00\n",
      "[37/200][2199/2499] Loss_D: 0.50934047 (Loss_D_real: 0.26608738 Loss_D_fake: 0.24325308) Loss_G: 0.07660300 Loss_Enh_Dec: -0.79005110\n",
      "| epoch  37 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1051.22 | loss  0.96 | ppl     2.60 | acc     0.89 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37/200][2299/2499] Loss_D: 0.30945948 (Loss_D_real: 0.15088020 Loss_D_fake: 0.15857928) Loss_G: 0.09167407 Loss_Enh_Dec: -0.67023677\n",
      "| epoch  37 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1051.46 | loss  0.96 | ppl     2.61 | acc     0.89 | train_ae_norm     1.00\n",
      "[37/200][2399/2499] Loss_D: 0.45527390 (Loss_D_real: 0.27901012 Loss_D_fake: 0.17626378) Loss_G: 0.08555748 Loss_Enh_Dec: -0.70860910\n",
      "| epoch  37 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1051.41 | loss  0.97 | ppl     2.63 | acc     0.85 | train_ae_norm     1.00\n",
      "[37/200][2499/2499] Loss_D: 0.32465145 (Loss_D_real: 0.13444224 Loss_D_fake: 0.19020921) Loss_G: 0.09006863 Loss_Enh_Dec: -0.75881445\n",
      "| end of epoch  37 | time: 2812.08s | test loss  0.83 | test ppl  2.29 | acc 0.920\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 38 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:49.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:56.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:10.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:17.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:24.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:31.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:38.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:45.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:52.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:59.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:06.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:19.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:26.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:33.\n",
      "\n",
      "  Average training loss generetor: 0.705\n",
      "  Average training loss discriminator: 0.708\n",
      "  Training epcoh took: 0:02:40\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.500\n",
      "  Test Loss: 3.239\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  38 |     0/ 2499 batches | lr 0.000000 | ms/batch 1605.29 | loss  0.01 | ppl     1.01 | acc     0.86 | train_ae_norm     1.00\n",
      "[38/200][99/2499] Loss_D: 0.44708008 (Loss_D_real: 0.15525645 Loss_D_fake: 0.29182363) Loss_G: 0.05002902 Loss_Enh_Dec: -0.79590297\n",
      "| epoch  38 |   100/ 2499 batches | lr 0.000000 | ms/batch 1052.69 | loss  0.97 | ppl     2.64 | acc     0.86 | train_ae_norm     1.00\n",
      "[38/200][199/2499] Loss_D: 0.52497143 (Loss_D_real: 0.24065767 Loss_D_fake: 0.28431377) Loss_G: 0.06751245 Loss_Enh_Dec: -0.68725914\n",
      "| epoch  38 |   200/ 2499 batches | lr 0.000000 | ms/batch 1051.81 | loss  0.94 | ppl     2.57 | acc     0.90 | train_ae_norm     1.00\n",
      "[38/200][299/2499] Loss_D: 0.47245049 (Loss_D_real: 0.24049008 Loss_D_fake: 0.23196040) Loss_G: 0.07982893 Loss_Enh_Dec: -0.71914458\n",
      "| epoch  38 |   300/ 2499 batches | lr 0.000000 | ms/batch 1052.32 | loss  0.97 | ppl     2.64 | acc     0.89 | train_ae_norm     1.00\n",
      "[38/200][399/2499] Loss_D: 0.62797922 (Loss_D_real: 0.31972688 Loss_D_fake: 0.30825233) Loss_G: 0.06683876 Loss_Enh_Dec: -0.73209381\n",
      "| epoch  38 |   400/ 2499 batches | lr 0.000000 | ms/batch 1053.01 | loss  0.96 | ppl     2.60 | acc     0.90 | train_ae_norm     1.00\n",
      "[38/200][499/2499] Loss_D: 0.38587767 (Loss_D_real: 0.19098829 Loss_D_fake: 0.19488940) Loss_G: 0.07689270 Loss_Enh_Dec: -0.84847087\n",
      "| epoch  38 |   500/ 2499 batches | lr 0.000000 | ms/batch 1053.72 | loss  0.93 | ppl     2.55 | acc     0.89 | train_ae_norm     1.00\n",
      "[38/200][599/2499] Loss_D: 0.43193284 (Loss_D_real: 0.24017803 Loss_D_fake: 0.19175480) Loss_G: 0.08407476 Loss_Enh_Dec: -0.90921575\n",
      "| epoch  38 |   600/ 2499 batches | lr 0.000000 | ms/batch 1052.32 | loss  0.92 | ppl     2.50 | acc     0.89 | train_ae_norm     1.00\n",
      "[38/200][699/2499] Loss_D: 0.34399611 (Loss_D_real: 0.17470822 Loss_D_fake: 0.16928789) Loss_G: 0.09793701 Loss_Enh_Dec: -0.74300832\n",
      "| epoch  38 |   700/ 2499 batches | lr 0.000000 | ms/batch 1053.89 | loss  0.92 | ppl     2.50 | acc     0.87 | train_ae_norm     1.00\n",
      "[38/200][799/2499] Loss_D: 0.43412942 (Loss_D_real: 0.19306797 Loss_D_fake: 0.24106145) Loss_G: 0.08584749 Loss_Enh_Dec: -0.96021885\n",
      "| epoch  38 |   800/ 2499 batches | lr 0.000000 | ms/batch 1053.07 | loss  0.95 | ppl     2.58 | acc     0.88 | train_ae_norm     1.00\n",
      "[38/200][899/2499] Loss_D: 0.39176190 (Loss_D_real: 0.13438015 Loss_D_fake: 0.25738174) Loss_G: 0.08050126 Loss_Enh_Dec: -0.82753414\n",
      "| epoch  38 |   900/ 2499 batches | lr 0.000000 | ms/batch 1052.83 | loss  0.91 | ppl     2.47 | acc     0.91 | train_ae_norm     1.00\n",
      "[38/200][999/2499] Loss_D: 0.66046649 (Loss_D_real: 0.32980812 Loss_D_fake: 0.33065838) Loss_G: 0.07417830 Loss_Enh_Dec: -0.78966880\n",
      "| epoch  38 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1052.38 | loss  0.94 | ppl     2.56 | acc     0.91 | train_ae_norm     1.00\n",
      "[38/200][1099/2499] Loss_D: 0.27904412 (Loss_D_real: 0.13448647 Loss_D_fake: 0.14455765) Loss_G: 0.09491322 Loss_Enh_Dec: -0.84890556\n",
      "| epoch  38 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1052.07 | loss  0.90 | ppl     2.45 | acc     0.90 | train_ae_norm     1.00\n",
      "[38/200][1199/2499] Loss_D: 0.32491103 (Loss_D_real: 0.16647652 Loss_D_fake: 0.15843451) Loss_G: 0.09453365 Loss_Enh_Dec: -0.76945728\n",
      "| epoch  38 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1052.70 | loss  0.93 | ppl     2.54 | acc     0.86 | train_ae_norm     1.00\n",
      "[38/200][1299/2499] Loss_D: 0.27152961 (Loss_D_real: 0.11680378 Loss_D_fake: 0.15472582) Loss_G: 0.09278457 Loss_Enh_Dec: -0.92471069\n",
      "| epoch  38 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1053.02 | loss  0.93 | ppl     2.54 | acc     0.86 | train_ae_norm     1.00\n",
      "[38/200][1399/2499] Loss_D: 0.52664793 (Loss_D_real: 0.17451623 Loss_D_fake: 0.35213166) Loss_G: 0.05317928 Loss_Enh_Dec: -0.75192434\n",
      "| epoch  38 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1052.17 | loss  0.95 | ppl     2.59 | acc     0.89 | train_ae_norm     1.00\n",
      "[38/200][1499/2499] Loss_D: 0.41472632 (Loss_D_real: 0.18949085 Loss_D_fake: 0.22523545) Loss_G: 0.06720702 Loss_Enh_Dec: -0.80826968\n",
      "| epoch  38 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1053.06 | loss  0.89 | ppl     2.43 | acc     0.90 | train_ae_norm     1.00\n",
      "[38/200][1599/2499] Loss_D: 0.26500875 (Loss_D_real: 0.13974681 Loss_D_fake: 0.12526192) Loss_G: 0.09950940 Loss_Enh_Dec: -0.76406825\n",
      "| epoch  38 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1052.71 | loss  0.95 | ppl     2.58 | acc     0.89 | train_ae_norm     1.00\n",
      "[38/200][1699/2499] Loss_D: 0.32641917 (Loss_D_real: 0.17068921 Loss_D_fake: 0.15572998) Loss_G: 0.09669911 Loss_Enh_Dec: -0.84265107\n",
      "| epoch  38 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1053.50 | loss  0.90 | ppl     2.45 | acc     0.90 | train_ae_norm     1.00\n",
      "[38/200][1799/2499] Loss_D: 0.41284215 (Loss_D_real: 0.14008360 Loss_D_fake: 0.27275857) Loss_G: 0.06749411 Loss_Enh_Dec: -0.95012915\n",
      "| epoch  38 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1053.02 | loss  0.89 | ppl     2.43 | acc     0.91 | train_ae_norm     1.00\n",
      "[38/200][1899/2499] Loss_D: 0.86130416 (Loss_D_real: 0.17581318 Loss_D_fake: 0.68549097) Loss_G: 0.08896741 Loss_Enh_Dec: -0.85568398\n",
      "| epoch  38 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1053.28 | loss  0.90 | ppl     2.45 | acc     0.88 | train_ae_norm     1.00\n",
      "[38/200][1999/2499] Loss_D: 0.33956885 (Loss_D_real: 0.13516210 Loss_D_fake: 0.20440674) Loss_G: 0.09975266 Loss_Enh_Dec: -0.89030820\n",
      "| epoch  38 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1053.26 | loss  0.94 | ppl     2.56 | acc     0.91 | train_ae_norm     1.00\n",
      "[38/200][2099/2499] Loss_D: 0.46504158 (Loss_D_real: 0.24040318 Loss_D_fake: 0.22463839) Loss_G: 0.08440435 Loss_Enh_Dec: -0.87517589\n",
      "| epoch  38 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1053.60 | loss  0.90 | ppl     2.46 | acc     0.91 | train_ae_norm     1.00\n",
      "[38/200][2199/2499] Loss_D: 0.36540067 (Loss_D_real: 0.23784368 Loss_D_fake: 0.12755698) Loss_G: 0.10767765 Loss_Enh_Dec: -0.76787353\n",
      "| epoch  38 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1053.92 | loss  0.87 | ppl     2.39 | acc     0.90 | train_ae_norm     1.00\n",
      "[38/200][2299/2499] Loss_D: 0.47129190 (Loss_D_real: 0.27007493 Loss_D_fake: 0.20121698) Loss_G: 0.10036351 Loss_Enh_Dec: -0.95182532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  38 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1053.67 | loss  0.88 | ppl     2.42 | acc     0.91 | train_ae_norm     1.00\n",
      "[38/200][2399/2499] Loss_D: 0.34539288 (Loss_D_real: 0.14018202 Loss_D_fake: 0.20521085) Loss_G: 0.09693956 Loss_Enh_Dec: -0.75771564\n",
      "| epoch  38 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1052.77 | loss  0.90 | ppl     2.46 | acc     0.87 | train_ae_norm     1.00\n",
      "[38/200][2499/2499] Loss_D: 0.48101291 (Loss_D_real: 0.23992668 Loss_D_fake: 0.24108623) Loss_G: 0.07556843 Loss_Enh_Dec: -0.89775801\n",
      "| end of epoch  38 | time: 2812.23s | test loss  0.79 | test ppl  2.21 | acc 0.924\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 39 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:49.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:56.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:10.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:17.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:24.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:31.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:37.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:44.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:51.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:58.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:05.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:19.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:26.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:33.\n",
      "\n",
      "  Average training loss generetor: 0.705\n",
      "  Average training loss discriminator: 0.707\n",
      "  Training epcoh took: 0:02:40\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.500\n",
      "  Test Loss: 3.244\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  39 |     0/ 2499 batches | lr 0.000000 | ms/batch 1604.09 | loss  0.01 | ppl     1.01 | acc     0.88 | train_ae_norm     1.00\n",
      "[39/200][99/2499] Loss_D: 0.28411412 (Loss_D_real: 0.14329998 Loss_D_fake: 0.14081413) Loss_G: 0.11784579 Loss_Enh_Dec: -0.94106781\n",
      "| epoch  39 |   100/ 2499 batches | lr 0.000000 | ms/batch 1053.70 | loss  0.89 | ppl     2.43 | acc     0.86 | train_ae_norm     1.00\n",
      "[39/200][199/2499] Loss_D: 0.35846037 (Loss_D_real: 0.26219082 Loss_D_fake: 0.09626953) Loss_G: 0.12036309 Loss_Enh_Dec: -0.77951950\n",
      "| epoch  39 |   200/ 2499 batches | lr 0.000000 | ms/batch 1053.29 | loss  0.91 | ppl     2.48 | acc     0.89 | train_ae_norm     1.00\n",
      "[39/200][299/2499] Loss_D: 0.47117865 (Loss_D_real: 0.19804470 Loss_D_fake: 0.27313393) Loss_G: 0.07801773 Loss_Enh_Dec: -0.84745085\n",
      "| epoch  39 |   300/ 2499 batches | lr 0.000000 | ms/batch 1053.47 | loss  0.89 | ppl     2.44 | acc     0.88 | train_ae_norm     1.00\n",
      "[39/200][399/2499] Loss_D: 0.40288767 (Loss_D_real: 0.20893645 Loss_D_fake: 0.19395122) Loss_G: 0.07259078 Loss_Enh_Dec: -0.89637643\n",
      "| epoch  39 |   400/ 2499 batches | lr 0.000000 | ms/batch 1054.10 | loss  0.86 | ppl     2.37 | acc     0.90 | train_ae_norm     1.00\n",
      "[39/200][499/2499] Loss_D: 0.27919436 (Loss_D_real: 0.13915962 Loss_D_fake: 0.14003474) Loss_G: 0.09989994 Loss_Enh_Dec: -0.89723223\n",
      "| epoch  39 |   500/ 2499 batches | lr 0.000000 | ms/batch 1052.66 | loss  0.87 | ppl     2.39 | acc     0.89 | train_ae_norm     1.00\n",
      "[39/200][599/2499] Loss_D: 0.32763982 (Loss_D_real: 0.11960840 Loss_D_fake: 0.20803142) Loss_G: 0.09618615 Loss_Enh_Dec: -0.95600504\n",
      "| epoch  39 |   600/ 2499 batches | lr 0.000000 | ms/batch 1053.22 | loss  0.87 | ppl     2.38 | acc     0.90 | train_ae_norm     1.00\n",
      "[39/200][699/2499] Loss_D: 0.40086874 (Loss_D_real: 0.10833981 Loss_D_fake: 0.29252893) Loss_G: 0.08142529 Loss_Enh_Dec: -0.87956315\n",
      "| epoch  39 |   700/ 2499 batches | lr 0.000000 | ms/batch 1053.51 | loss  0.87 | ppl     2.38 | acc     0.89 | train_ae_norm     1.00\n",
      "[39/200][799/2499] Loss_D: 0.45467430 (Loss_D_real: 0.22146094 Loss_D_fake: 0.23321335) Loss_G: 0.07486156 Loss_Enh_Dec: -0.84510893\n",
      "| epoch  39 |   800/ 2499 batches | lr 0.000000 | ms/batch 1054.40 | loss  0.90 | ppl     2.45 | acc     0.88 | train_ae_norm     1.00\n",
      "[39/200][899/2499] Loss_D: 0.46338090 (Loss_D_real: 0.17106709 Loss_D_fake: 0.29231381) Loss_G: 0.08578541 Loss_Enh_Dec: -0.98696059\n",
      "| epoch  39 |   900/ 2499 batches | lr 0.000000 | ms/batch 1053.35 | loss  0.84 | ppl     2.32 | acc     0.93 | train_ae_norm     1.00\n",
      "[39/200][999/2499] Loss_D: 0.40511882 (Loss_D_real: 0.16247806 Loss_D_fake: 0.24264076) Loss_G: 0.07557134 Loss_Enh_Dec: -1.01653302\n",
      "| epoch  39 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1054.11 | loss  0.90 | ppl     2.46 | acc     0.91 | train_ae_norm     1.00\n",
      "[39/200][1099/2499] Loss_D: 0.29633611 (Loss_D_real: 0.17239079 Loss_D_fake: 0.12394531) Loss_G: 0.11083838 Loss_Enh_Dec: -1.00092387\n",
      "| epoch  39 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1053.58 | loss  0.88 | ppl     2.42 | acc     0.90 | train_ae_norm     1.00\n",
      "[39/200][1199/2499] Loss_D: 0.24433735 (Loss_D_real: 0.14547288 Loss_D_fake: 0.09886446) Loss_G: 0.12806538 Loss_Enh_Dec: -0.79065305\n",
      "| epoch  39 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1054.04 | loss  0.86 | ppl     2.36 | acc     0.87 | train_ae_norm     1.00\n",
      "[39/200][1299/2499] Loss_D: 0.61678910 (Loss_D_real: 0.23732130 Loss_D_fake: 0.37946782) Loss_G: 0.08176469 Loss_Enh_Dec: -0.91489470\n",
      "| epoch  39 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1054.86 | loss  0.87 | ppl     2.39 | acc     0.87 | train_ae_norm     1.00\n",
      "[39/200][1399/2499] Loss_D: 0.28782806 (Loss_D_real: 0.19203453 Loss_D_fake: 0.09579353) Loss_G: 0.13664612 Loss_Enh_Dec: -0.98426914\n",
      "| epoch  39 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1054.91 | loss  0.88 | ppl     2.40 | acc     0.90 | train_ae_norm     1.00\n",
      "[39/200][1499/2499] Loss_D: 0.29058033 (Loss_D_real: 0.18974464 Loss_D_fake: 0.10083568) Loss_G: 0.11282195 Loss_Enh_Dec: -0.91738111\n",
      "| epoch  39 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1055.27 | loss  0.87 | ppl     2.38 | acc     0.89 | train_ae_norm     1.00\n",
      "[39/200][1599/2499] Loss_D: 0.45578378 (Loss_D_real: 0.11627460 Loss_D_fake: 0.33950919) Loss_G: 0.05978324 Loss_Enh_Dec: -0.96406519\n",
      "| epoch  39 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1053.70 | loss  0.88 | ppl     2.41 | acc     0.89 | train_ae_norm     1.00\n",
      "[39/200][1699/2499] Loss_D: 0.50551331 (Loss_D_real: 0.19069278 Loss_D_fake: 0.31482053) Loss_G: 0.10720950 Loss_Enh_Dec: -1.05663669\n",
      "| epoch  39 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1053.72 | loss  0.84 | ppl     2.31 | acc     0.91 | train_ae_norm     1.00\n",
      "[39/200][1799/2499] Loss_D: 0.29158267 (Loss_D_real: 0.16145653 Loss_D_fake: 0.13012615) Loss_G: 0.11887264 Loss_Enh_Dec: -0.88963836\n",
      "| epoch  39 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1054.57 | loss  0.86 | ppl     2.36 | acc     0.92 | train_ae_norm     1.00\n",
      "[39/200][1899/2499] Loss_D: 0.32190228 (Loss_D_real: 0.20636263 Loss_D_fake: 0.11553963) Loss_G: 0.11429317 Loss_Enh_Dec: -0.82854408\n",
      "| epoch  39 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1054.21 | loss  0.88 | ppl     2.41 | acc     0.88 | train_ae_norm     1.00\n",
      "[39/200][1999/2499] Loss_D: 0.32577646 (Loss_D_real: 0.18351111 Loss_D_fake: 0.14226535) Loss_G: 0.10812040 Loss_Enh_Dec: -0.96999818\n",
      "| epoch  39 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1054.53 | loss  0.90 | ppl     2.47 | acc     0.91 | train_ae_norm     1.00\n",
      "[39/200][2099/2499] Loss_D: 0.33575332 (Loss_D_real: 0.12403266 Loss_D_fake: 0.21172068) Loss_G: 0.09970986 Loss_Enh_Dec: -0.29016978\n",
      "| epoch  39 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1055.32 | loss  0.86 | ppl     2.37 | acc     0.90 | train_ae_norm     1.00\n",
      "[39/200][2199/2499] Loss_D: 0.30393571 (Loss_D_real: 0.14269100 Loss_D_fake: 0.16124471) Loss_G: 0.12686205 Loss_Enh_Dec: -0.91320640\n",
      "| epoch  39 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1055.45 | loss  0.86 | ppl     2.35 | acc     0.89 | train_ae_norm     1.00\n",
      "[39/200][2299/2499] Loss_D: 0.22351800 (Loss_D_real: 0.14212307 Loss_D_fake: 0.08139493) Loss_G: 0.11540315 Loss_Enh_Dec: -0.80681938\n",
      "| epoch  39 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1055.87 | loss  0.94 | ppl     2.55 | acc     0.89 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39/200][2399/2499] Loss_D: 0.22344904 (Loss_D_real: 0.11670339 Loss_D_fake: 0.10674565) Loss_G: 0.11759410 Loss_Enh_Dec: -0.98958647\n",
      "| epoch  39 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1057.37 | loss  1.08 | ppl     2.95 | acc     0.86 | train_ae_norm     1.00\n",
      "[39/200][2499/2499] Loss_D: 0.14932597 (Loss_D_real: 0.07490598 Loss_D_fake: 0.07441998) Loss_G: 0.13622516 Loss_Enh_Dec: -1.00628269\n",
      "| end of epoch  39 | time: 2815.41s | test loss  0.80 | test ppl  2.22 | acc 0.923\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 40 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:49.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:56.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:10.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:17.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:24.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:31.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:38.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:45.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:52.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:59.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:06.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:13.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:20.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:27.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:34.\n",
      "\n",
      "  Average training loss generetor: 0.703\n",
      "  Average training loss discriminator: 0.707\n",
      "  Training epcoh took: 0:02:40\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.490\n",
      "  Test Loss: 3.375\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  40 |     0/ 2499 batches | lr 0.000000 | ms/batch 1606.96 | loss  0.01 | ppl     1.01 | acc     0.88 | train_ae_norm     1.00\n",
      "[40/200][99/2499] Loss_D: 0.29023913 (Loss_D_real: 0.13196552 Loss_D_fake: 0.15827361) Loss_G: 0.12183751 Loss_Enh_Dec: -0.86767751\n",
      "| epoch  40 |   100/ 2499 batches | lr 0.000000 | ms/batch 1056.61 | loss  0.91 | ppl     2.48 | acc     0.85 | train_ae_norm     1.00\n",
      "[40/200][199/2499] Loss_D: 0.53625506 (Loss_D_real: 0.24508023 Loss_D_fake: 0.29117483) Loss_G: 0.07526969 Loss_Enh_Dec: -1.02042651\n",
      "| epoch  40 |   200/ 2499 batches | lr 0.000000 | ms/batch 1055.34 | loss  0.90 | ppl     2.47 | acc     0.89 | train_ae_norm     1.00\n",
      "[40/200][299/2499] Loss_D: 0.39402279 (Loss_D_real: 0.14338204 Loss_D_fake: 0.25064075) Loss_G: 0.13392888 Loss_Enh_Dec: -0.97504312\n",
      "| epoch  40 |   300/ 2499 batches | lr 0.000000 | ms/batch 1056.14 | loss  0.92 | ppl     2.52 | acc     0.88 | train_ae_norm     1.00\n",
      "[40/200][399/2499] Loss_D: 0.21522832 (Loss_D_real: 0.08647200 Loss_D_fake: 0.12875631) Loss_G: 0.11286920 Loss_Enh_Dec: -0.58658212\n",
      "| epoch  40 |   400/ 2499 batches | lr 0.000000 | ms/batch 1055.49 | loss  0.91 | ppl     2.50 | acc     0.90 | train_ae_norm     1.00\n",
      "[40/200][499/2499] Loss_D: 0.39263028 (Loss_D_real: 0.25389740 Loss_D_fake: 0.13873288) Loss_G: 0.07955632 Loss_Enh_Dec: -0.94020098\n",
      "| epoch  40 |   500/ 2499 batches | lr 0.000000 | ms/batch 1054.17 | loss  0.90 | ppl     2.47 | acc     0.87 | train_ae_norm     1.00\n",
      "[40/200][599/2499] Loss_D: 0.26087794 (Loss_D_real: 0.13017315 Loss_D_fake: 0.13070479) Loss_G: 0.11585003 Loss_Enh_Dec: -1.15843678\n",
      "| epoch  40 |   600/ 2499 batches | lr 0.000000 | ms/batch 1052.32 | loss  0.90 | ppl     2.45 | acc     0.91 | train_ae_norm     1.00\n",
      "[40/200][699/2499] Loss_D: 0.20651115 (Loss_D_real: 0.09587069 Loss_D_fake: 0.11064047) Loss_G: 0.13397068 Loss_Enh_Dec: -1.05889833\n",
      "| epoch  40 |   700/ 2499 batches | lr 0.000000 | ms/batch 1051.52 | loss  0.88 | ppl     2.42 | acc     0.89 | train_ae_norm     1.00\n",
      "[40/200][799/2499] Loss_D: 0.24894850 (Loss_D_real: 0.16422045 Loss_D_fake: 0.08472805) Loss_G: 0.13464521 Loss_Enh_Dec: -0.94351637\n",
      "| epoch  40 |   800/ 2499 batches | lr 0.000000 | ms/batch 1051.38 | loss  0.90 | ppl     2.46 | acc     0.88 | train_ae_norm     1.00\n",
      "[40/200][899/2499] Loss_D: 0.71570694 (Loss_D_real: 0.36633432 Loss_D_fake: 0.34937263) Loss_G: 0.07353987 Loss_Enh_Dec: -1.11522329\n",
      "| epoch  40 |   900/ 2499 batches | lr 0.000000 | ms/batch 1050.06 | loss  0.85 | ppl     2.34 | acc     0.90 | train_ae_norm     1.00\n",
      "[40/200][999/2499] Loss_D: 0.59957808 (Loss_D_real: 0.24160346 Loss_D_fake: 0.35797462) Loss_G: 0.10555895 Loss_Enh_Dec: -0.97341681\n",
      "| epoch  40 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1049.49 | loss  0.88 | ppl     2.40 | acc     0.92 | train_ae_norm     1.00\n",
      "[40/200][1099/2499] Loss_D: 0.39842445 (Loss_D_real: 0.13387160 Loss_D_fake: 0.26455283) Loss_G: 0.09656435 Loss_Enh_Dec: -0.73991740\n",
      "| epoch  40 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1050.26 | loss  0.87 | ppl     2.38 | acc     0.91 | train_ae_norm     1.00\n",
      "[40/200][1199/2499] Loss_D: 0.20535529 (Loss_D_real: 0.09737901 Loss_D_fake: 0.10797628) Loss_G: 0.13681173 Loss_Enh_Dec: -1.14690781\n",
      "| epoch  40 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1048.61 | loss  0.88 | ppl     2.40 | acc     0.86 | train_ae_norm     1.00\n",
      "[40/200][1299/2499] Loss_D: 0.22039576 (Loss_D_real: 0.12192099 Loss_D_fake: 0.09847477) Loss_G: 0.13742855 Loss_Enh_Dec: -1.06416607\n",
      "| epoch  40 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1049.21 | loss  0.94 | ppl     2.56 | acc     0.85 | train_ae_norm     1.00\n",
      "[40/200][1399/2499] Loss_D: 0.36200052 (Loss_D_real: 0.12428337 Loss_D_fake: 0.23771714) Loss_G: 0.10746044 Loss_Enh_Dec: -0.66210932\n",
      "| epoch  40 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1047.19 | loss  0.92 | ppl     2.51 | acc     0.89 | train_ae_norm     1.00\n",
      "[40/200][1499/2499] Loss_D: 0.34385696 (Loss_D_real: 0.16708738 Loss_D_fake: 0.17676958) Loss_G: 0.12564458 Loss_Enh_Dec: -1.00958347\n",
      "| epoch  40 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1050.06 | loss  0.86 | ppl     2.36 | acc     0.89 | train_ae_norm     1.00\n",
      "[40/200][1599/2499] Loss_D: 0.25956762 (Loss_D_real: 0.11268063 Loss_D_fake: 0.14688699) Loss_G: 0.12014034 Loss_Enh_Dec: -1.03695965\n",
      "| epoch  40 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1048.38 | loss  0.89 | ppl     2.44 | acc     0.88 | train_ae_norm     1.00\n",
      "[40/200][1699/2499] Loss_D: 0.34473819 (Loss_D_real: 0.11800885 Loss_D_fake: 0.22672933) Loss_G: 0.10296460 Loss_Enh_Dec: -1.06370568\n",
      "| epoch  40 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1048.20 | loss  0.87 | ppl     2.38 | acc     0.91 | train_ae_norm     1.00\n",
      "[40/200][1799/2499] Loss_D: 0.29156926 (Loss_D_real: 0.15454242 Loss_D_fake: 0.13702685) Loss_G: 0.11889763 Loss_Enh_Dec: -0.96709245\n",
      "| epoch  40 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1048.89 | loss  0.87 | ppl     2.39 | acc     0.92 | train_ae_norm     1.00\n",
      "[40/200][1899/2499] Loss_D: 0.28043079 (Loss_D_real: 0.11045789 Loss_D_fake: 0.16997290) Loss_G: 0.13262069 Loss_Enh_Dec: -1.17676699\n",
      "| epoch  40 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1048.79 | loss  0.87 | ppl     2.38 | acc     0.88 | train_ae_norm     1.00\n",
      "[40/200][1999/2499] Loss_D: 0.23742864 (Loss_D_real: 0.15041921 Loss_D_fake: 0.08700942) Loss_G: 0.13127367 Loss_Enh_Dec: -1.15573275\n",
      "| epoch  40 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1047.64 | loss  0.91 | ppl     2.48 | acc     0.90 | train_ae_norm     1.00\n",
      "[40/200][2099/2499] Loss_D: 0.51132464 (Loss_D_real: 0.12096962 Loss_D_fake: 0.39035505) Loss_G: 0.06227425 Loss_Enh_Dec: -1.09401274\n",
      "| epoch  40 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1048.18 | loss  0.89 | ppl     2.43 | acc     0.92 | train_ae_norm     1.00\n",
      "[40/200][2199/2499] Loss_D: 0.43019024 (Loss_D_real: 0.16851974 Loss_D_fake: 0.26167050) Loss_G: 0.09194245 Loss_Enh_Dec: -0.18599252\n",
      "| epoch  40 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1048.39 | loss  0.87 | ppl     2.39 | acc     0.89 | train_ae_norm     1.00\n",
      "[40/200][2299/2499] Loss_D: 0.21720123 (Loss_D_real: 0.09274742 Loss_D_fake: 0.12445381) Loss_G: 0.11750069 Loss_Enh_Dec: -0.93871272\n",
      "| epoch  40 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1047.80 | loss  0.88 | ppl     2.42 | acc     0.91 | train_ae_norm     1.00\n",
      "[40/200][2399/2499] Loss_D: 0.16394535 (Loss_D_real: 0.06356795 Loss_D_fake: 0.10037740) Loss_G: 0.12769291 Loss_Enh_Dec: -1.06919599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  40 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1048.57 | loss  0.87 | ppl     2.39 | acc     0.88 | train_ae_norm     1.00\n",
      "[40/200][2499/2499] Loss_D: 0.31389058 (Loss_D_real: 0.09826317 Loss_D_fake: 0.21562740) Loss_G: 0.12121291 Loss_Enh_Dec: -1.08512485\n",
      "| end of epoch  40 | time: 2805.74s | test loss  0.77 | test ppl  2.16 | acc 0.926\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 41 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:49.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:56.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:10.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:17.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:24.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:31.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:38.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:46.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:53.\n",
      "  Batch   170  of    230.    Elapsed: 0:02:00.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:14.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:21.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:28.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:35.\n",
      "\n",
      "  Average training loss generetor: 0.705\n",
      "  Average training loss discriminator: 0.706\n",
      "  Training epcoh took: 0:02:41\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.500\n",
      "  Test Loss: 3.417\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  41 |     0/ 2499 batches | lr 0.000000 | ms/batch 1619.94 | loss  0.01 | ppl     1.01 | acc     0.88 | train_ae_norm     1.00\n",
      "[41/200][99/2499] Loss_D: 0.37968671 (Loss_D_real: 0.21690284 Loss_D_fake: 0.16278386) Loss_G: 0.06908419 Loss_Enh_Dec: -1.07732201\n",
      "| epoch  41 |   100/ 2499 batches | lr 0.000000 | ms/batch 1048.29 | loss  0.86 | ppl     2.37 | acc     0.88 | train_ae_norm     1.00\n",
      "[41/200][199/2499] Loss_D: 0.35139433 (Loss_D_real: 0.13881978 Loss_D_fake: 0.21257454) Loss_G: 0.10244960 Loss_Enh_Dec: -0.89964926\n",
      "| epoch  41 |   200/ 2499 batches | lr 0.000000 | ms/batch 1047.69 | loss  0.86 | ppl     2.36 | acc     0.91 | train_ae_norm     1.00\n",
      "[41/200][299/2499] Loss_D: 0.55861938 (Loss_D_real: 0.17804354 Loss_D_fake: 0.38057587) Loss_G: 0.08334011 Loss_Enh_Dec: -1.13602257\n",
      "| epoch  41 |   300/ 2499 batches | lr 0.000000 | ms/batch 1046.88 | loss  0.89 | ppl     2.43 | acc     0.88 | train_ae_norm     1.00\n",
      "[41/200][399/2499] Loss_D: 0.14726710 (Loss_D_real: 0.06788239 Loss_D_fake: 0.07938472) Loss_G: 0.14163215 Loss_Enh_Dec: -1.15214801\n",
      "| epoch  41 |   400/ 2499 batches | lr 0.000000 | ms/batch 1048.42 | loss  0.88 | ppl     2.40 | acc     0.89 | train_ae_norm     1.00\n",
      "[41/200][499/2499] Loss_D: 0.24856818 (Loss_D_real: 0.14223662 Loss_D_fake: 0.10633156) Loss_G: 0.11720914 Loss_Enh_Dec: -1.19352043\n",
      "| epoch  41 |   500/ 2499 batches | lr 0.000000 | ms/batch 1048.43 | loss  0.88 | ppl     2.42 | acc     0.88 | train_ae_norm     1.00\n",
      "[41/200][599/2499] Loss_D: 0.35631642 (Loss_D_real: 0.17754385 Loss_D_fake: 0.17877257) Loss_G: 0.11478195 Loss_Enh_Dec: -1.25852692\n",
      "| epoch  41 |   600/ 2499 batches | lr 0.000000 | ms/batch 1048.56 | loss  0.89 | ppl     2.44 | acc     0.90 | train_ae_norm     1.00\n",
      "[41/200][699/2499] Loss_D: 0.22913267 (Loss_D_real: 0.10023400 Loss_D_fake: 0.12889867) Loss_G: 0.12794538 Loss_Enh_Dec: -1.12244129\n",
      "| epoch  41 |   700/ 2499 batches | lr 0.000000 | ms/batch 1047.89 | loss  0.87 | ppl     2.40 | acc     0.89 | train_ae_norm     1.00\n",
      "[41/200][799/2499] Loss_D: 0.32038096 (Loss_D_real: 0.23090141 Loss_D_fake: 0.08947955) Loss_G: 0.13767803 Loss_Enh_Dec: -1.23036039\n",
      "| epoch  41 |   800/ 2499 batches | lr 0.000000 | ms/batch 1046.72 | loss  0.91 | ppl     2.49 | acc     0.89 | train_ae_norm     1.00\n",
      "[41/200][899/2499] Loss_D: 0.34294757 (Loss_D_real: 0.13960403 Loss_D_fake: 0.20334354) Loss_G: 0.11910576 Loss_Enh_Dec: -0.94483548\n",
      "| epoch  41 |   900/ 2499 batches | lr 0.000000 | ms/batch 1047.24 | loss  0.89 | ppl     2.44 | acc     0.91 | train_ae_norm     1.00\n",
      "[41/200][999/2499] Loss_D: 0.46316150 (Loss_D_real: 0.14813629 Loss_D_fake: 0.31502521) Loss_G: 0.06270470 Loss_Enh_Dec: -1.21408594\n",
      "| epoch  41 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1047.34 | loss  0.91 | ppl     2.49 | acc     0.90 | train_ae_norm     1.00\n",
      "[41/200][1099/2499] Loss_D: 0.27155378 (Loss_D_real: 0.19412407 Loss_D_fake: 0.07742971) Loss_G: 0.14740525 Loss_Enh_Dec: -0.63135254\n",
      "| epoch  41 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1046.79 | loss  0.87 | ppl     2.38 | acc     0.92 | train_ae_norm     1.00\n",
      "[41/200][1199/2499] Loss_D: 0.64428723 (Loss_D_real: 0.07547791 Loss_D_fake: 0.56880933) Loss_G: 0.05636558 Loss_Enh_Dec: -0.87974626\n",
      "| epoch  41 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1049.36 | loss  0.90 | ppl     2.47 | acc     0.87 | train_ae_norm     1.00\n",
      "[41/200][1299/2499] Loss_D: 0.28375110 (Loss_D_real: 0.12670040 Loss_D_fake: 0.15705070) Loss_G: 0.11268432 Loss_Enh_Dec: -1.27786815\n",
      "| epoch  41 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1047.24 | loss  0.89 | ppl     2.43 | acc     0.86 | train_ae_norm     1.00\n",
      "[41/200][1399/2499] Loss_D: 0.40452081 (Loss_D_real: 0.25273016 Loss_D_fake: 0.15179065) Loss_G: 0.13243836 Loss_Enh_Dec: -1.16260087\n",
      "| epoch  41 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1048.73 | loss  0.89 | ppl     2.44 | acc     0.91 | train_ae_norm     1.00\n",
      "[41/200][1499/2499] Loss_D: 0.39657503 (Loss_D_real: 0.17211106 Loss_D_fake: 0.22446398) Loss_G: 0.11094423 Loss_Enh_Dec: -1.24894595\n",
      "| epoch  41 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1048.37 | loss  0.85 | ppl     2.35 | acc     0.88 | train_ae_norm     1.00\n",
      "[41/200][1599/2499] Loss_D: 0.19187717 (Loss_D_real: 0.09769112 Loss_D_fake: 0.09418605) Loss_G: 0.13353615 Loss_Enh_Dec: -1.11137760\n",
      "| epoch  41 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1047.15 | loss  0.92 | ppl     2.52 | acc     0.89 | train_ae_norm     1.00\n",
      "[41/200][1699/2499] Loss_D: 0.36105132 (Loss_D_real: 0.13026622 Loss_D_fake: 0.23078510) Loss_G: 0.13269329 Loss_Enh_Dec: -1.07690513\n",
      "| epoch  41 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1048.07 | loss  0.90 | ppl     2.47 | acc     0.91 | train_ae_norm     1.00\n",
      "[41/200][1799/2499] Loss_D: 0.26365986 (Loss_D_real: 0.15168934 Loss_D_fake: 0.11197053) Loss_G: 0.12463277 Loss_Enh_Dec: -1.17920530\n",
      "| epoch  41 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1047.90 | loss  0.90 | ppl     2.46 | acc     0.91 | train_ae_norm     1.00\n",
      "[41/200][1899/2499] Loss_D: 0.20785184 (Loss_D_real: 0.12576066 Loss_D_fake: 0.08209118) Loss_G: 0.13809100 Loss_Enh_Dec: -1.26765120\n",
      "| epoch  41 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1047.29 | loss  0.92 | ppl     2.52 | acc     0.88 | train_ae_norm     1.00\n",
      "[41/200][1999/2499] Loss_D: 0.23558336 (Loss_D_real: 0.12776196 Loss_D_fake: 0.10782140) Loss_G: 0.12438925 Loss_Enh_Dec: -1.26760519\n",
      "| epoch  41 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1047.84 | loss  0.96 | ppl     2.61 | acc     0.89 | train_ae_norm     1.00\n",
      "[41/200][2099/2499] Loss_D: 0.50911701 (Loss_D_real: 0.32098815 Loss_D_fake: 0.18812887) Loss_G: 0.10114820 Loss_Enh_Dec: -1.26167452\n",
      "| epoch  41 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1049.61 | loss  0.90 | ppl     2.45 | acc     0.91 | train_ae_norm     1.00\n",
      "[41/200][2199/2499] Loss_D: 0.23158166 (Loss_D_real: 0.05521690 Loss_D_fake: 0.17636475) Loss_G: 0.11764012 Loss_Enh_Dec: -1.21025693\n",
      "| epoch  41 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1047.40 | loss  0.87 | ppl     2.39 | acc     0.89 | train_ae_norm     1.00\n",
      "[41/200][2299/2499] Loss_D: 0.60655677 (Loss_D_real: 0.22376156 Loss_D_fake: 0.38279518) Loss_G: 0.07309775 Loss_Enh_Dec: -1.23864305\n",
      "| epoch  41 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1048.07 | loss  0.88 | ppl     2.41 | acc     0.90 | train_ae_norm     1.00\n",
      "[41/200][2399/2499] Loss_D: 0.38676745 (Loss_D_real: 0.24137479 Loss_D_fake: 0.14539266) Loss_G: 0.09541061 Loss_Enh_Dec: -1.02879560\n",
      "| epoch  41 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1048.43 | loss  0.85 | ppl     2.34 | acc     0.88 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41/200][2499/2499] Loss_D: 0.21209860 (Loss_D_real: 0.12340522 Loss_D_fake: 0.08869337) Loss_G: 0.12648962 Loss_Enh_Dec: -1.21519196\n",
      "| end of epoch  41 | time: 2800.85s | test loss  0.78 | test ppl  2.17 | acc 0.925\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 42 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:49.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:56.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:10.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:17.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:24.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:31.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:38.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:45.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:52.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:59.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:06.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:13.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:20.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:27.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:35.\n",
      "\n",
      "  Average training loss generetor: 0.703\n",
      "  Average training loss discriminator: 0.707\n",
      "  Training epcoh took: 0:02:41\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.495\n",
      "  Test Loss: 3.463\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  42 |     0/ 2499 batches | lr 0.000000 | ms/batch 1617.11 | loss  0.01 | ppl     1.01 | acc     0.89 | train_ae_norm     1.00\n",
      "[42/200][99/2499] Loss_D: 0.20211476 (Loss_D_real: 0.10580005 Loss_D_fake: 0.09631471) Loss_G: 0.13341360 Loss_Enh_Dec: -1.30164981\n",
      "| epoch  42 |   100/ 2499 batches | lr 0.000000 | ms/batch 1049.27 | loss  0.86 | ppl     2.36 | acc     0.86 | train_ae_norm     1.00\n",
      "[42/200][199/2499] Loss_D: 0.18178847 (Loss_D_real: 0.09465259 Loss_D_fake: 0.08713590) Loss_G: 0.14663894 Loss_Enh_Dec: -1.14560926\n",
      "| epoch  42 |   200/ 2499 batches | lr 0.000000 | ms/batch 1047.91 | loss  0.88 | ppl     2.40 | acc     0.90 | train_ae_norm     1.00\n",
      "[42/200][299/2499] Loss_D: 0.26461682 (Loss_D_real: 0.13830161 Loss_D_fake: 0.12631521) Loss_G: 0.13663106 Loss_Enh_Dec: -1.00107574\n",
      "| epoch  42 |   300/ 2499 batches | lr 0.000000 | ms/batch 1048.20 | loss  0.86 | ppl     2.35 | acc     0.89 | train_ae_norm     1.00\n",
      "[42/200][399/2499] Loss_D: 0.38126406 (Loss_D_real: 0.10155954 Loss_D_fake: 0.27970451) Loss_G: 0.11163701 Loss_Enh_Dec: -1.21555746\n",
      "| epoch  42 |   400/ 2499 batches | lr 0.000000 | ms/batch 1046.10 | loss  0.84 | ppl     2.31 | acc     0.91 | train_ae_norm     1.00\n",
      "[42/200][499/2499] Loss_D: 0.25127295 (Loss_D_real: 0.17183661 Loss_D_fake: 0.07943633) Loss_G: 0.13545895 Loss_Enh_Dec: -1.30248368\n",
      "| epoch  42 |   500/ 2499 batches | lr 0.000000 | ms/batch 1047.45 | loss  0.86 | ppl     2.36 | acc     0.89 | train_ae_norm     1.00\n",
      "[42/200][599/2499] Loss_D: 0.23442133 (Loss_D_real: 0.11193168 Loss_D_fake: 0.12248965) Loss_G: 0.11570509 Loss_Enh_Dec: -1.22892106\n",
      "| epoch  42 |   600/ 2499 batches | lr 0.000000 | ms/batch 1049.18 | loss  0.88 | ppl     2.41 | acc     0.90 | train_ae_norm     1.00\n",
      "[42/200][699/2499] Loss_D: 0.12864009 (Loss_D_real: 0.05188982 Loss_D_fake: 0.07675027) Loss_G: 0.13429897 Loss_Enh_Dec: -1.16179180\n",
      "| epoch  42 |   700/ 2499 batches | lr 0.000000 | ms/batch 1047.18 | loss  0.85 | ppl     2.34 | acc     0.88 | train_ae_norm     1.00\n",
      "[42/200][799/2499] Loss_D: 0.22941090 (Loss_D_real: 0.10210256 Loss_D_fake: 0.12730834) Loss_G: 0.14409319 Loss_Enh_Dec: -1.23861253\n",
      "| epoch  42 |   800/ 2499 batches | lr 0.000000 | ms/batch 1047.84 | loss  0.90 | ppl     2.45 | acc     0.87 | train_ae_norm     1.00\n",
      "[42/200][899/2499] Loss_D: 0.36001772 (Loss_D_real: 0.16763660 Loss_D_fake: 0.19238111) Loss_G: 0.11124028 Loss_Enh_Dec: -1.09527898\n",
      "| epoch  42 |   900/ 2499 batches | lr 0.000000 | ms/batch 1047.56 | loss  0.85 | ppl     2.34 | acc     0.90 | train_ae_norm     1.00\n",
      "[42/200][999/2499] Loss_D: 0.22552872 (Loss_D_real: 0.10936694 Loss_D_fake: 0.11616178) Loss_G: 0.12911049 Loss_Enh_Dec: -1.32877123\n",
      "| epoch  42 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1048.81 | loss  0.91 | ppl     2.50 | acc     0.89 | train_ae_norm     1.00\n",
      "[42/200][1099/2499] Loss_D: 0.20287901 (Loss_D_real: 0.11922183 Loss_D_fake: 0.08365719) Loss_G: 0.13608955 Loss_Enh_Dec: -1.34134758\n",
      "| epoch  42 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1047.51 | loss  0.89 | ppl     2.43 | acc     0.89 | train_ae_norm     1.00\n",
      "[42/200][1199/2499] Loss_D: 0.47132200 (Loss_D_real: 0.30774415 Loss_D_fake: 0.16357784) Loss_G: 0.12051487 Loss_Enh_Dec: -1.31593418\n",
      "| epoch  42 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1048.46 | loss  0.90 | ppl     2.45 | acc     0.87 | train_ae_norm     1.00\n",
      "[42/200][1299/2499] Loss_D: 0.43270397 (Loss_D_real: 0.18657582 Loss_D_fake: 0.24612817) Loss_G: 0.12460460 Loss_Enh_Dec: -1.33420169\n",
      "| epoch  42 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1048.27 | loss  0.88 | ppl     2.40 | acc     0.87 | train_ae_norm     1.00\n",
      "[42/200][1399/2499] Loss_D: 0.25549755 (Loss_D_real: 0.18081793 Loss_D_fake: 0.07467961) Loss_G: 0.13903892 Loss_Enh_Dec: -1.27379632\n",
      "| epoch  42 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1049.50 | loss  0.89 | ppl     2.43 | acc     0.89 | train_ae_norm     1.00\n",
      "[42/200][1499/2499] Loss_D: 0.20907187 (Loss_D_real: 0.12569723 Loss_D_fake: 0.08337465) Loss_G: 0.16369991 Loss_Enh_Dec: -1.26972735\n",
      "| epoch  42 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1047.84 | loss  0.87 | ppl     2.38 | acc     0.89 | train_ae_norm     1.00\n",
      "[42/200][1599/2499] Loss_D: 0.35380444 (Loss_D_real: 0.26308998 Loss_D_fake: 0.09071445) Loss_G: 0.10137444 Loss_Enh_Dec: -1.31812930\n",
      "| epoch  42 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1047.98 | loss  0.88 | ppl     2.42 | acc     0.89 | train_ae_norm     1.00\n",
      "[42/200][1699/2499] Loss_D: 0.15539157 (Loss_D_real: 0.07707772 Loss_D_fake: 0.07831385) Loss_G: 0.14143108 Loss_Enh_Dec: -1.26131272\n",
      "| epoch  42 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1048.34 | loss  0.86 | ppl     2.37 | acc     0.92 | train_ae_norm     1.00\n",
      "[42/200][1799/2499] Loss_D: 0.18067232 (Loss_D_real: 0.07643084 Loss_D_fake: 0.10424148) Loss_G: 0.13025980 Loss_Enh_Dec: -1.39898968\n",
      "| epoch  42 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1047.60 | loss  0.89 | ppl     2.43 | acc     0.91 | train_ae_norm     1.00\n",
      "[42/200][1899/2499] Loss_D: 0.30919603 (Loss_D_real: 0.17200768 Loss_D_fake: 0.13718835) Loss_G: 0.14996450 Loss_Enh_Dec: -0.50706238\n",
      "| epoch  42 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1047.30 | loss  0.90 | ppl     2.45 | acc     0.88 | train_ae_norm     1.00\n",
      "[42/200][1999/2499] Loss_D: 0.43092653 (Loss_D_real: 0.09398711 Loss_D_fake: 0.33693942) Loss_G: 0.11890950 Loss_Enh_Dec: -1.34958601\n",
      "| epoch  42 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1049.24 | loss  0.89 | ppl     2.44 | acc     0.89 | train_ae_norm     1.00\n",
      "[42/200][2099/2499] Loss_D: 0.19786820 (Loss_D_real: 0.08311547 Loss_D_fake: 0.11475272) Loss_G: 0.13947301 Loss_Enh_Dec: -1.35291290\n",
      "| epoch  42 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1047.09 | loss  0.87 | ppl     2.38 | acc     0.92 | train_ae_norm     1.00\n",
      "[42/200][2199/2499] Loss_D: 0.46431875 (Loss_D_real: 0.36369920 Loss_D_fake: 0.10061955) Loss_G: 0.13214956 Loss_Enh_Dec: -1.34570205\n",
      "| epoch  42 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1049.42 | loss  0.85 | ppl     2.34 | acc     0.91 | train_ae_norm     1.00\n",
      "[42/200][2299/2499] Loss_D: 0.17155996 (Loss_D_real: 0.08004298 Loss_D_fake: 0.09151697) Loss_G: 0.14094113 Loss_Enh_Dec: -1.37442434\n",
      "| epoch  42 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1048.07 | loss  0.85 | ppl     2.34 | acc     0.89 | train_ae_norm     1.00\n",
      "[42/200][2399/2499] Loss_D: 0.21967001 (Loss_D_real: 0.10736251 Loss_D_fake: 0.11230750) Loss_G: 0.12240958 Loss_Enh_Dec: -1.33676136\n",
      "| epoch  42 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1049.46 | loss  0.99 | ppl     2.70 | acc     0.83 | train_ae_norm     1.00\n",
      "[42/200][2499/2499] Loss_D: 0.28915247 (Loss_D_real: 0.13252363 Loss_D_fake: 0.15662885) Loss_G: 0.11313085 Loss_Enh_Dec: -1.32510936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| end of epoch  42 | time: 2801.19s | test loss  0.79 | test ppl  2.20 | acc 0.924\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 43 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:49.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:56.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:10.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:17.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:24.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:31.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:38.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:45.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:52.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:59.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:06.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:14.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:21.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:28.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:35.\n",
      "\n",
      "  Average training loss generetor: 0.705\n",
      "  Average training loss discriminator: 0.705\n",
      "  Training epcoh took: 0:02:41\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.470\n",
      "  Test Loss: 3.563\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  43 |     0/ 2499 batches | lr 0.000000 | ms/batch 1617.73 | loss  0.01 | ppl     1.01 | acc     0.89 | train_ae_norm     1.00\n",
      "[43/200][99/2499] Loss_D: 0.19823053 (Loss_D_real: 0.12269190 Loss_D_fake: 0.07553864) Loss_G: 0.14913750 Loss_Enh_Dec: -1.38275588\n",
      "| epoch  43 |   100/ 2499 batches | lr 0.000000 | ms/batch 1047.99 | loss  0.90 | ppl     2.45 | acc     0.85 | train_ae_norm     1.00\n",
      "[43/200][199/2499] Loss_D: 0.35479346 (Loss_D_real: 0.12336897 Loss_D_fake: 0.23142450) Loss_G: 0.13348953 Loss_Enh_Dec: -1.33325303\n",
      "| epoch  43 |   200/ 2499 batches | lr 0.000000 | ms/batch 1047.84 | loss  0.89 | ppl     2.42 | acc     0.92 | train_ae_norm     1.00\n",
      "[43/200][299/2499] Loss_D: 0.14437956 (Loss_D_real: 0.07519819 Loss_D_fake: 0.06918137) Loss_G: 0.14229667 Loss_Enh_Dec: -0.98868591\n",
      "| epoch  43 |   300/ 2499 batches | lr 0.000000 | ms/batch 1048.83 | loss  0.89 | ppl     2.43 | acc     0.90 | train_ae_norm     1.00\n",
      "[43/200][399/2499] Loss_D: 0.25929981 (Loss_D_real: 0.08960756 Loss_D_fake: 0.16969226) Loss_G: 0.13823356 Loss_Enh_Dec: -1.18680036\n",
      "| epoch  43 |   400/ 2499 batches | lr 0.000000 | ms/batch 1047.67 | loss  0.85 | ppl     2.35 | acc     0.90 | train_ae_norm     1.00\n",
      "[43/200][499/2499] Loss_D: 0.23231238 (Loss_D_real: 0.13975637 Loss_D_fake: 0.09255602) Loss_G: 0.13093045 Loss_Enh_Dec: -1.39372289\n",
      "| epoch  43 |   500/ 2499 batches | lr 0.000000 | ms/batch 1047.87 | loss  0.86 | ppl     2.36 | acc     0.89 | train_ae_norm     1.00\n",
      "[43/200][599/2499] Loss_D: 0.52610731 (Loss_D_real: 0.21041964 Loss_D_fake: 0.31568769) Loss_G: 0.12162302 Loss_Enh_Dec: -1.07214439\n",
      "| epoch  43 |   600/ 2499 batches | lr 0.000000 | ms/batch 1047.78 | loss  0.88 | ppl     2.42 | acc     0.89 | train_ae_norm     1.00\n",
      "[43/200][699/2499] Loss_D: 0.24002574 (Loss_D_real: 0.17472720 Loss_D_fake: 0.06529854) Loss_G: 0.14145152 Loss_Enh_Dec: -1.18604267\n",
      "| epoch  43 |   700/ 2499 batches | lr 0.000000 | ms/batch 1048.44 | loss  0.87 | ppl     2.39 | acc     0.88 | train_ae_norm     1.00\n",
      "[43/200][799/2499] Loss_D: 0.20886904 (Loss_D_real: 0.09880906 Loss_D_fake: 0.11005998) Loss_G: 0.13130762 Loss_Enh_Dec: -1.36078525\n",
      "| epoch  43 |   800/ 2499 batches | lr 0.000000 | ms/batch 1048.30 | loss  0.91 | ppl     2.47 | acc     0.88 | train_ae_norm     1.00\n",
      "[43/200][899/2499] Loss_D: 0.20866755 (Loss_D_real: 0.15114680 Loss_D_fake: 0.05752074) Loss_G: 0.16274774 Loss_Enh_Dec: -1.27304530\n",
      "| epoch  43 |   900/ 2499 batches | lr 0.000000 | ms/batch 1048.14 | loss  0.87 | ppl     2.38 | acc     0.91 | train_ae_norm     1.00\n",
      "[43/200][999/2499] Loss_D: 0.18264985 (Loss_D_real: 0.08861756 Loss_D_fake: 0.09403229) Loss_G: 0.15550728 Loss_Enh_Dec: -1.08248961\n",
      "| epoch  43 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1048.43 | loss  0.93 | ppl     2.53 | acc     0.89 | train_ae_norm     1.00\n",
      "[43/200][1099/2499] Loss_D: 0.19038516 (Loss_D_real: 0.07305887 Loss_D_fake: 0.11732629) Loss_G: 0.12874669 Loss_Enh_Dec: -1.08199751\n",
      "| epoch  43 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1047.90 | loss  0.88 | ppl     2.41 | acc     0.91 | train_ae_norm     1.00\n",
      "[43/200][1199/2499] Loss_D: 0.18068480 (Loss_D_real: 0.10955600 Loss_D_fake: 0.07112880) Loss_G: 0.16475677 Loss_Enh_Dec: -1.17750919\n",
      "| epoch  43 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1049.46 | loss  0.88 | ppl     2.41 | acc     0.84 | train_ae_norm     1.00\n",
      "[43/200][1299/2499] Loss_D: 0.14536244 (Loss_D_real: 0.06864893 Loss_D_fake: 0.07671351) Loss_G: 0.14053635 Loss_Enh_Dec: -1.31494236\n",
      "| epoch  43 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1049.57 | loss  1.02 | ppl     2.79 | acc     0.84 | train_ae_norm     1.00\n",
      "[43/200][1399/2499] Loss_D: 0.13003342 (Loss_D_real: 0.04735610 Loss_D_fake: 0.08267732) Loss_G: 0.14599495 Loss_Enh_Dec: -1.35514939\n",
      "| epoch  43 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1049.07 | loss  0.95 | ppl     2.59 | acc     0.89 | train_ae_norm     1.00\n",
      "[43/200][1499/2499] Loss_D: 0.12714545 (Loss_D_real: 0.05486622 Loss_D_fake: 0.07227924) Loss_G: 0.15123808 Loss_Enh_Dec: -1.08705449\n",
      "| epoch  43 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1049.41 | loss  0.97 | ppl     2.63 | acc     0.88 | train_ae_norm     1.00\n",
      "[43/200][1599/2499] Loss_D: 0.13650563 (Loss_D_real: 0.04184984 Loss_D_fake: 0.09465578) Loss_G: 0.13711555 Loss_Enh_Dec: -1.40645087\n",
      "| epoch  43 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1048.87 | loss  0.97 | ppl     2.64 | acc     0.87 | train_ae_norm     1.00\n",
      "[43/200][1699/2499] Loss_D: 0.16840306 (Loss_D_real: 0.09176819 Loss_D_fake: 0.07663488) Loss_G: 0.13759366 Loss_Enh_Dec: -1.27603030\n",
      "| epoch  43 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1048.10 | loss  0.94 | ppl     2.55 | acc     0.90 | train_ae_norm     1.00\n",
      "[43/200][1799/2499] Loss_D: 0.12272233 (Loss_D_real: 0.04680217 Loss_D_fake: 0.07592016) Loss_G: 0.14170121 Loss_Enh_Dec: -1.42548847\n",
      "| epoch  43 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1048.22 | loss  0.96 | ppl     2.62 | acc     0.92 | train_ae_norm     1.00\n",
      "[43/200][1899/2499] Loss_D: 0.18604825 (Loss_D_real: 0.09146958 Loss_D_fake: 0.09457868) Loss_G: 0.14937122 Loss_Enh_Dec: -1.44969976\n",
      "| epoch  43 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1047.71 | loss  0.95 | ppl     2.59 | acc     0.88 | train_ae_norm     1.00\n",
      "[43/200][1999/2499] Loss_D: 0.20390050 (Loss_D_real: 0.12813714 Loss_D_fake: 0.07576336) Loss_G: 0.13943043 Loss_Enh_Dec: -1.50843048\n",
      "| epoch  43 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1048.70 | loss  0.97 | ppl     2.64 | acc     0.89 | train_ae_norm     1.00\n",
      "[43/200][2099/2499] Loss_D: 0.16899097 (Loss_D_real: 0.08785387 Loss_D_fake: 0.08113709) Loss_G: 0.13943200 Loss_Enh_Dec: -1.16770089\n",
      "| epoch  43 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1048.55 | loss  0.94 | ppl     2.57 | acc     0.91 | train_ae_norm     1.00\n",
      "[43/200][2199/2499] Loss_D: 0.14892447 (Loss_D_real: 0.07312495 Loss_D_fake: 0.07579952) Loss_G: 0.14911942 Loss_Enh_Dec: -1.16266096\n",
      "| epoch  43 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1048.59 | loss  0.91 | ppl     2.48 | acc     0.90 | train_ae_norm     1.00\n",
      "[43/200][2299/2499] Loss_D: 0.14823292 (Loss_D_real: 0.06252643 Loss_D_fake: 0.08570649) Loss_G: 0.14846791 Loss_Enh_Dec: -1.31498098\n",
      "| epoch  43 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1048.21 | loss  0.94 | ppl     2.56 | acc     0.89 | train_ae_norm     1.00\n",
      "[43/200][2399/2499] Loss_D: 0.11548538 (Loss_D_real: 0.05311472 Loss_D_fake: 0.06237066) Loss_G: 0.14744852 Loss_Enh_Dec: -1.46547914\n",
      "| epoch  43 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1049.51 | loss  0.99 | ppl     2.70 | acc     0.85 | train_ae_norm     1.00\n",
      "[43/200][2499/2499] Loss_D: 0.11501409 (Loss_D_real: 0.01818111 Loss_D_fake: 0.09683298) Loss_G: 0.15872969 Loss_Enh_Dec: -1.13048244\n",
      "| end of epoch  43 | time: 2802.22s | test loss  0.80 | test ppl  2.22 | acc 0.922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 44 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:49.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:56.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:10.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:17.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:24.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:31.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:38.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:45.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:52.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:59.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:06.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:13.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:20.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:27.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:34.\n",
      "\n",
      "  Average training loss generetor: 0.704\n",
      "  Average training loss discriminator: 0.705\n",
      "  Training epcoh took: 0:02:41\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.495\n",
      "  Test Loss: 3.607\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  44 |     0/ 2499 batches | lr 0.000000 | ms/batch 1616.20 | loss  0.01 | ppl     1.01 | acc     0.87 | train_ae_norm     1.00\n",
      "[44/200][99/2499] Loss_D: 0.16124232 (Loss_D_real: 0.09731194 Loss_D_fake: 0.06393038) Loss_G: 0.15459818 Loss_Enh_Dec: -1.25702286\n",
      "| epoch  44 |   100/ 2499 batches | lr 0.000000 | ms/batch 1050.06 | loss  1.01 | ppl     2.75 | acc     0.84 | train_ae_norm     1.00\n",
      "[44/200][199/2499] Loss_D: 0.14276545 (Loss_D_real: 0.07446402 Loss_D_fake: 0.06830142) Loss_G: 0.15727966 Loss_Enh_Dec: -0.77755541\n",
      "| epoch  44 |   200/ 2499 batches | lr 0.000000 | ms/batch 1047.50 | loss  1.05 | ppl     2.86 | acc     0.89 | train_ae_norm     1.00\n",
      "[44/200][299/2499] Loss_D: 0.19372611 (Loss_D_real: 0.12574905 Loss_D_fake: 0.06797706) Loss_G: 0.16431345 Loss_Enh_Dec: -1.38061714\n",
      "| epoch  44 |   300/ 2499 batches | lr 0.000000 | ms/batch 1048.24 | loss  1.01 | ppl     2.75 | acc     0.86 | train_ae_norm     1.00\n",
      "[44/200][399/2499] Loss_D: 0.11380257 (Loss_D_real: 0.04973398 Loss_D_fake: 0.06406859) Loss_G: 0.16938013 Loss_Enh_Dec: -1.37544620\n",
      "| epoch  44 |   400/ 2499 batches | lr 0.000000 | ms/batch 1049.33 | loss  1.04 | ppl     2.82 | acc     0.89 | train_ae_norm     1.00\n",
      "[44/200][499/2499] Loss_D: 0.08897147 (Loss_D_real: 0.03628453 Loss_D_fake: 0.05268694) Loss_G: 0.17955063 Loss_Enh_Dec: -1.33635771\n",
      "| epoch  44 |   500/ 2499 batches | lr 0.000000 | ms/batch 1049.58 | loss  1.00 | ppl     2.72 | acc     0.88 | train_ae_norm     1.00\n",
      "[44/200][599/2499] Loss_D: 0.16914947 (Loss_D_real: 0.10179963 Loss_D_fake: 0.06734984) Loss_G: 0.16674866 Loss_Enh_Dec: -1.42913628\n",
      "| epoch  44 |   600/ 2499 batches | lr 0.000000 | ms/batch 1049.30 | loss  0.97 | ppl     2.64 | acc     0.88 | train_ae_norm     1.00\n",
      "[44/200][699/2499] Loss_D: 0.26985645 (Loss_D_real: 0.11846170 Loss_D_fake: 0.15139477) Loss_G: 0.15214786 Loss_Enh_Dec: -1.33159161\n",
      "| epoch  44 |   700/ 2499 batches | lr 0.000000 | ms/batch 1049.73 | loss  0.96 | ppl     2.61 | acc     0.87 | train_ae_norm     1.00\n",
      "[44/200][799/2499] Loss_D: 0.71481425 (Loss_D_real: 0.42431507 Loss_D_fake: 0.29049918) Loss_G: 0.05507667 Loss_Enh_Dec: -0.81467009\n",
      "| epoch  44 |   800/ 2499 batches | lr 0.000000 | ms/batch 1048.70 | loss  0.95 | ppl     2.58 | acc     0.87 | train_ae_norm     1.00\n",
      "[44/200][899/2499] Loss_D: 0.37720165 (Loss_D_real: 0.19946113 Loss_D_fake: 0.17774051) Loss_G: 0.08255231 Loss_Enh_Dec: -1.10350096\n",
      "| epoch  44 |   900/ 2499 batches | lr 0.000000 | ms/batch 1049.58 | loss  0.91 | ppl     2.49 | acc     0.89 | train_ae_norm     1.00\n",
      "[44/200][999/2499] Loss_D: 0.29107863 (Loss_D_real: 0.14354515 Loss_D_fake: 0.14753349) Loss_G: 0.09243886 Loss_Enh_Dec: -1.04825425\n",
      "| epoch  44 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1048.28 | loss  0.94 | ppl     2.56 | acc     0.90 | train_ae_norm     1.00\n",
      "[44/200][1099/2499] Loss_D: 0.25036132 (Loss_D_real: 0.13137206 Loss_D_fake: 0.11898924) Loss_G: 0.10364480 Loss_Enh_Dec: -1.14643538\n",
      "| epoch  44 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1049.64 | loss  0.93 | ppl     2.52 | acc     0.90 | train_ae_norm     1.00\n",
      "[44/200][1199/2499] Loss_D: 0.36335611 (Loss_D_real: 0.11064225 Loss_D_fake: 0.25271386) Loss_G: 0.12773147 Loss_Enh_Dec: -1.29147947\n",
      "| epoch  44 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1049.36 | loss  0.90 | ppl     2.47 | acc     0.85 | train_ae_norm     1.00\n",
      "[44/200][1299/2499] Loss_D: 0.42350891 (Loss_D_real: 0.12789592 Loss_D_fake: 0.29561299) Loss_G: 0.18392725 Loss_Enh_Dec: -0.90324879\n",
      "| epoch  44 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1049.14 | loss  0.95 | ppl     2.58 | acc     0.86 | train_ae_norm     1.00\n",
      "[44/200][1399/2499] Loss_D: 0.19107428 (Loss_D_real: 0.05470201 Loss_D_fake: 0.13637227) Loss_G: 0.12902263 Loss_Enh_Dec: -1.19021487\n",
      "| epoch  44 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1049.58 | loss  0.94 | ppl     2.55 | acc     0.89 | train_ae_norm     1.00\n",
      "[44/200][1499/2499] Loss_D: 0.27388328 (Loss_D_real: 0.16526878 Loss_D_fake: 0.10861450) Loss_G: 0.13488023 Loss_Enh_Dec: -1.09025037\n",
      "| epoch  44 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1048.38 | loss  0.92 | ppl     2.52 | acc     0.87 | train_ae_norm     1.00\n",
      "[44/200][1599/2499] Loss_D: 0.16066924 (Loss_D_real: 0.10997055 Loss_D_fake: 0.05069869) Loss_G: 0.17886353 Loss_Enh_Dec: -1.26042020\n",
      "| epoch  44 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1048.97 | loss  0.96 | ppl     2.62 | acc     0.88 | train_ae_norm     1.00\n",
      "[44/200][1699/2499] Loss_D: 0.12331984 (Loss_D_real: 0.05848126 Loss_D_fake: 0.06483858) Loss_G: 0.15275358 Loss_Enh_Dec: -1.22310889\n",
      "| epoch  44 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1049.85 | loss  0.91 | ppl     2.48 | acc     0.89 | train_ae_norm     1.00\n",
      "[44/200][1799/2499] Loss_D: 0.20418164 (Loss_D_real: 0.11106452 Loss_D_fake: 0.09311712) Loss_G: 0.15366091 Loss_Enh_Dec: -1.21264637\n",
      "| epoch  44 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1048.33 | loss  0.91 | ppl     2.49 | acc     0.92 | train_ae_norm     1.00\n",
      "[44/200][1899/2499] Loss_D: 0.10904106 (Loss_D_real: 0.06076688 Loss_D_fake: 0.04827418) Loss_G: 0.16542034 Loss_Enh_Dec: -1.19344330\n",
      "| epoch  44 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1048.14 | loss  0.92 | ppl     2.52 | acc     0.88 | train_ae_norm     1.00\n",
      "[44/200][1999/2499] Loss_D: 0.09633769 (Loss_D_real: 0.03981443 Loss_D_fake: 0.05652326) Loss_G: 0.15001683 Loss_Enh_Dec: -1.26635396\n",
      "| epoch  44 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1049.89 | loss  0.94 | ppl     2.55 | acc     0.89 | train_ae_norm     1.00\n",
      "[44/200][2099/2499] Loss_D: 0.14774887 (Loss_D_real: 0.10481977 Loss_D_fake: 0.04292911) Loss_G: 0.16672781 Loss_Enh_Dec: -0.79619867\n",
      "| epoch  44 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1049.01 | loss  0.90 | ppl     2.45 | acc     0.92 | train_ae_norm     1.00\n",
      "[44/200][2199/2499] Loss_D: 0.13251035 (Loss_D_real: 0.08783305 Loss_D_fake: 0.04467730) Loss_G: 0.15291658 Loss_Enh_Dec: -1.25735533\n",
      "| epoch  44 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1050.14 | loss  0.90 | ppl     2.46 | acc     0.90 | train_ae_norm     1.00\n",
      "[44/200][2299/2499] Loss_D: 0.20784572 (Loss_D_real: 0.16144027 Loss_D_fake: 0.04640546) Loss_G: 0.16192125 Loss_Enh_Dec: -1.15138388\n",
      "| epoch  44 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1048.50 | loss  0.92 | ppl     2.51 | acc     0.90 | train_ae_norm     1.00\n",
      "[44/200][2399/2499] Loss_D: 0.16440929 (Loss_D_real: 0.10645787 Loss_D_fake: 0.05795142) Loss_G: 0.15990816 Loss_Enh_Dec: -1.44006610\n",
      "| epoch  44 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1048.63 | loss  0.91 | ppl     2.47 | acc     0.87 | train_ae_norm     1.00\n",
      "[44/200][2499/2499] Loss_D: 0.11008084 (Loss_D_real: 0.05530891 Loss_D_fake: 0.05477193) Loss_G: 0.15778635 Loss_Enh_Dec: -1.31328869\n",
      "| end of epoch  44 | time: 2803.43s | test loss  0.80 | test ppl  2.23 | acc 0.925\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 45 / 200 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:49.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:56.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:10.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:17.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:24.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:31.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:38.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:45.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:52.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:59.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:06.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:13.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:20.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:28.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:35.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.710\n",
      "  Training epcoh took: 0:02:41\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.460\n",
      "  Test Loss: 3.891\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  45 |     0/ 2499 batches | lr 0.000000 | ms/batch 1617.34 | loss  0.01 | ppl     1.01 | acc     0.89 | train_ae_norm     1.00\n",
      "[45/200][99/2499] Loss_D: 0.14203745 (Loss_D_real: 0.10522765 Loss_D_fake: 0.03680980) Loss_G: 0.18415497 Loss_Enh_Dec: -1.00787151\n",
      "| epoch  45 |   100/ 2499 batches | lr 0.000000 | ms/batch 1048.30 | loss  0.89 | ppl     2.44 | acc     0.86 | train_ae_norm     1.00\n",
      "[45/200][199/2499] Loss_D: 0.09148786 (Loss_D_real: 0.05210710 Loss_D_fake: 0.03938077) Loss_G: 0.17236443 Loss_Enh_Dec: -1.15497017\n",
      "| epoch  45 |   200/ 2499 batches | lr 0.000000 | ms/batch 1048.82 | loss  0.89 | ppl     2.43 | acc     0.91 | train_ae_norm     1.00\n",
      "[45/200][299/2499] Loss_D: 0.10829857 (Loss_D_real: 0.04777249 Loss_D_fake: 0.06052608) Loss_G: 0.16669188 Loss_Enh_Dec: -1.40292549\n",
      "| epoch  45 |   300/ 2499 batches | lr 0.000000 | ms/batch 1049.32 | loss  0.91 | ppl     2.49 | acc     0.89 | train_ae_norm     1.00\n",
      "[45/200][399/2499] Loss_D: 0.11133508 (Loss_D_real: 0.08071890 Loss_D_fake: 0.03061618) Loss_G: 0.17777382 Loss_Enh_Dec: -1.13928211\n",
      "| epoch  45 |   400/ 2499 batches | lr 0.000000 | ms/batch 1049.72 | loss  0.89 | ppl     2.45 | acc     0.90 | train_ae_norm     1.00\n",
      "[45/200][499/2499] Loss_D: 0.08912142 (Loss_D_real: 0.05375321 Loss_D_fake: 0.03536821) Loss_G: 0.16786675 Loss_Enh_Dec: -1.41135025\n",
      "| epoch  45 |   500/ 2499 batches | lr 0.000000 | ms/batch 1050.21 | loss  0.90 | ppl     2.45 | acc     0.89 | train_ae_norm     1.00\n",
      "[45/200][599/2499] Loss_D: 0.24096902 (Loss_D_real: 0.19421130 Loss_D_fake: 0.04675771) Loss_G: 0.16560546 Loss_Enh_Dec: -1.30933261\n",
      "| epoch  45 |   600/ 2499 batches | lr 0.000000 | ms/batch 1049.59 | loss  0.90 | ppl     2.45 | acc     0.91 | train_ae_norm     1.00\n",
      "[45/200][699/2499] Loss_D: 0.16899388 (Loss_D_real: 0.11134653 Loss_D_fake: 0.05764734) Loss_G: 0.16091910 Loss_Enh_Dec: -1.53285694\n",
      "| epoch  45 |   700/ 2499 batches | lr 0.000000 | ms/batch 1049.69 | loss  0.90 | ppl     2.45 | acc     0.88 | train_ae_norm     1.00\n",
      "[45/200][799/2499] Loss_D: 0.10258979 (Loss_D_real: 0.06165522 Loss_D_fake: 0.04093457) Loss_G: 0.19183658 Loss_Enh_Dec: -1.36011457\n",
      "| epoch  45 |   800/ 2499 batches | lr 0.000000 | ms/batch 1049.07 | loss  0.92 | ppl     2.50 | acc     0.88 | train_ae_norm     1.00\n",
      "[45/200][899/2499] Loss_D: 0.12286403 (Loss_D_real: 0.07169808 Loss_D_fake: 0.05116595) Loss_G: 0.16447577 Loss_Enh_Dec: -1.40636432\n",
      "| epoch  45 |   900/ 2499 batches | lr 0.000000 | ms/batch 1049.93 | loss  0.88 | ppl     2.41 | acc     0.90 | train_ae_norm     1.00\n",
      "[45/200][999/2499] Loss_D: 0.16644284 (Loss_D_real: 0.13479708 Loss_D_fake: 0.03164577) Loss_G: 0.19380927 Loss_Enh_Dec: -1.02495110\n",
      "| epoch  45 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1050.03 | loss  0.89 | ppl     2.44 | acc     0.89 | train_ae_norm     1.00\n",
      "[45/200][1099/2499] Loss_D: 0.08183268 (Loss_D_real: 0.06104603 Loss_D_fake: 0.02078665) Loss_G: 0.19882931 Loss_Enh_Dec: -0.90585041\n",
      "| epoch  45 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1050.97 | loss  0.90 | ppl     2.45 | acc     0.91 | train_ae_norm     1.00\n",
      "[45/200][1199/2499] Loss_D: 0.08371403 (Loss_D_real: 0.04297711 Loss_D_fake: 0.04073692) Loss_G: 0.18003678 Loss_Enh_Dec: -0.86728603\n",
      "| epoch  45 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1050.50 | loss  0.89 | ppl     2.44 | acc     0.86 | train_ae_norm     1.00\n",
      "[45/200][1299/2499] Loss_D: 0.17604852 (Loss_D_real: 0.06353831 Loss_D_fake: 0.11251020) Loss_G: 0.15655887 Loss_Enh_Dec: -1.31637943\n",
      "| epoch  45 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1050.00 | loss  0.96 | ppl     2.61 | acc     0.85 | train_ae_norm     1.00\n",
      "[45/200][1399/2499] Loss_D: 0.07417691 (Loss_D_real: 0.03535417 Loss_D_fake: 0.03882274) Loss_G: 0.18380557 Loss_Enh_Dec: -1.35504091\n",
      "| epoch  45 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1049.72 | loss  0.99 | ppl     2.69 | acc     0.90 | train_ae_norm     1.00\n",
      "[45/200][1499/2499] Loss_D: 0.10056013 (Loss_D_real: 0.05862639 Loss_D_fake: 0.04193374) Loss_G: 0.16902409 Loss_Enh_Dec: -1.38827169\n",
      "| epoch  45 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1050.20 | loss  0.96 | ppl     2.61 | acc     0.89 | train_ae_norm     1.00\n",
      "[45/200][1599/2499] Loss_D: 0.23022895 (Loss_D_real: 0.18184695 Loss_D_fake: 0.04838200) Loss_G: 0.19840401 Loss_Enh_Dec: -1.35980284\n",
      "| epoch  45 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1050.16 | loss  1.03 | ppl     2.80 | acc     0.88 | train_ae_norm     1.00\n",
      "[45/200][1699/2499] Loss_D: 0.13810909 (Loss_D_real: 0.11211207 Loss_D_fake: 0.02599702) Loss_G: 0.17624140 Loss_Enh_Dec: -1.30506420\n",
      "| epoch  45 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1049.28 | loss  0.93 | ppl     2.52 | acc     0.89 | train_ae_norm     1.00\n",
      "[45/200][1799/2499] Loss_D: 0.09358414 (Loss_D_real: 0.06141675 Loss_D_fake: 0.03216739) Loss_G: 0.18729873 Loss_Enh_Dec: -1.46000588\n",
      "| epoch  45 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1049.88 | loss  0.91 | ppl     2.50 | acc     0.92 | train_ae_norm     1.00\n",
      "[45/200][1899/2499] Loss_D: 0.08081408 (Loss_D_real: 0.04583941 Loss_D_fake: 0.03497467) Loss_G: 0.18164854 Loss_Enh_Dec: -1.32121921\n",
      "| epoch  45 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1048.78 | loss  0.93 | ppl     2.54 | acc     0.86 | train_ae_norm     1.00\n",
      "[45/200][1999/2499] Loss_D: 0.23059571 (Loss_D_real: 0.03326765 Loss_D_fake: 0.19732806) Loss_G: 0.15431304 Loss_Enh_Dec: -1.40482283\n",
      "| epoch  45 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1049.99 | loss  1.00 | ppl     2.71 | acc     0.89 | train_ae_norm     1.00\n",
      "[45/200][2099/2499] Loss_D: 0.09028556 (Loss_D_real: 0.06149610 Loss_D_fake: 0.02878946) Loss_G: 0.17866892 Loss_Enh_Dec: -0.80399555\n",
      "| epoch  45 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1050.07 | loss  0.98 | ppl     2.65 | acc     0.92 | train_ae_norm     1.00\n",
      "[45/200][2199/2499] Loss_D: 0.07581756 (Loss_D_real: 0.03709401 Loss_D_fake: 0.03872355) Loss_G: 0.17169356 Loss_Enh_Dec: -1.50253022\n",
      "| epoch  45 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1050.40 | loss  0.91 | ppl     2.48 | acc     0.91 | train_ae_norm     1.00\n",
      "[45/200][2299/2499] Loss_D: 0.10423410 (Loss_D_real: 0.07411688 Loss_D_fake: 0.03011722) Loss_G: 0.21368074 Loss_Enh_Dec: -1.36397099\n",
      "| epoch  45 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1049.61 | loss  0.94 | ppl     2.55 | acc     0.89 | train_ae_norm     1.00\n",
      "[45/200][2399/2499] Loss_D: 0.15413085 (Loss_D_real: 0.11588407 Loss_D_fake: 0.03824678) Loss_G: 0.17826669 Loss_Enh_Dec: -1.48407471\n",
      "| epoch  45 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1050.32 | loss  1.02 | ppl     2.77 | acc     0.85 | train_ae_norm     1.00\n",
      "[45/200][2499/2499] Loss_D: 0.08275929 (Loss_D_real: 0.04655602 Loss_D_fake: 0.03620327) Loss_G: 0.18967865 Loss_Enh_Dec: -1.47111166\n",
      "| end of epoch  45 | time: 2805.34s | test loss  1.03 | test ppl  2.79 | acc 0.899\n",
      "New saving model: epoch 045.\n",
      "Saving models to ./results/yelp_merge_results\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 46 / 200 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:49.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:56.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:10.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:17.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:24.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:31.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:38.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:45.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:52.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:59.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:06.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:14.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:21.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:28.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:35.\n",
      "\n",
      "  Average training loss generetor: 0.704\n",
      "  Average training loss discriminator: 0.705\n",
      "  Training epcoh took: 0:02:41\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.465\n",
      "  Test Loss: 3.829\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  46 |     0/ 2499 batches | lr 0.000000 | ms/batch 1618.13 | loss  0.02 | ppl     1.02 | acc     0.79 | train_ae_norm     1.00\n",
      "[46/200][99/2499] Loss_D: 0.07308860 (Loss_D_real: 0.03560483 Loss_D_fake: 0.03748378) Loss_G: 0.18554863 Loss_Enh_Dec: -1.34569693\n",
      "| epoch  46 |   100/ 2499 batches | lr 0.000000 | ms/batch 1050.57 | loss  1.77 | ppl     5.85 | acc     0.78 | train_ae_norm     1.00\n",
      "[46/200][199/2499] Loss_D: 0.06364514 (Loss_D_real: 0.03658618 Loss_D_fake: 0.02705896) Loss_G: 0.19834672 Loss_Enh_Dec: -1.44708562\n",
      "| epoch  46 |   200/ 2499 batches | lr 0.000000 | ms/batch 1049.93 | loss  1.33 | ppl     3.79 | acc     0.87 | train_ae_norm     1.00\n",
      "[46/200][299/2499] Loss_D: 0.07332946 (Loss_D_real: 0.03655932 Loss_D_fake: 0.03677014) Loss_G: 0.18274581 Loss_Enh_Dec: -1.42046261\n",
      "| epoch  46 |   300/ 2499 batches | lr 0.000000 | ms/batch 1049.73 | loss  1.12 | ppl     3.08 | acc     0.86 | train_ae_norm     1.00\n",
      "[46/200][399/2499] Loss_D: 0.10649782 (Loss_D_real: 0.07612646 Loss_D_fake: 0.03037136) Loss_G: 0.22082873 Loss_Enh_Dec: -1.39151633\n",
      "| epoch  46 |   400/ 2499 batches | lr 0.000000 | ms/batch 1050.11 | loss  1.22 | ppl     3.38 | acc     0.86 | train_ae_norm     1.00\n",
      "[46/200][499/2499] Loss_D: 0.05917388 (Loss_D_real: 0.02761954 Loss_D_fake: 0.03155434) Loss_G: 0.19747882 Loss_Enh_Dec: -1.28041875\n",
      "| epoch  46 |   500/ 2499 batches | lr 0.000000 | ms/batch 1051.04 | loss  1.15 | ppl     3.16 | acc     0.86 | train_ae_norm     1.00\n",
      "[46/200][599/2499] Loss_D: 0.23505235 (Loss_D_real: 0.11827430 Loss_D_fake: 0.11677804) Loss_G: 0.13104354 Loss_Enh_Dec: -1.12563860\n",
      "| epoch  46 |   600/ 2499 batches | lr 0.000000 | ms/batch 1050.78 | loss  1.12 | ppl     3.07 | acc     0.88 | train_ae_norm     1.00\n",
      "[46/200][699/2499] Loss_D: 0.40155154 (Loss_D_real: 0.03933606 Loss_D_fake: 0.36221549) Loss_G: 0.09757733 Loss_Enh_Dec: -0.80768341\n",
      "| epoch  46 |   700/ 2499 batches | lr 0.000000 | ms/batch 1050.68 | loss  1.13 | ppl     3.09 | acc     0.85 | train_ae_norm     1.00\n",
      "[46/200][799/2499] Loss_D: 0.06789494 (Loss_D_real: 0.03849180 Loss_D_fake: 0.02940314) Loss_G: 0.23149443 Loss_Enh_Dec: -1.19621301\n",
      "| epoch  46 |   800/ 2499 batches | lr 0.000000 | ms/batch 1050.31 | loss  1.16 | ppl     3.18 | acc     0.85 | train_ae_norm     1.00\n",
      "[46/200][899/2499] Loss_D: 0.05366288 (Loss_D_real: 0.02304871 Loss_D_fake: 0.03061417) Loss_G: 0.17983174 Loss_Enh_Dec: -1.10420215\n",
      "| epoch  46 |   900/ 2499 batches | lr 0.000000 | ms/batch 1051.07 | loss  1.11 | ppl     3.03 | acc     0.89 | train_ae_norm     1.00\n",
      "[46/200][999/2499] Loss_D: 0.05785772 (Loss_D_real: 0.02915660 Loss_D_fake: 0.02870112) Loss_G: 0.19557427 Loss_Enh_Dec: -1.38602221\n",
      "| epoch  46 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1050.67 | loss  1.12 | ppl     3.06 | acc     0.85 | train_ae_norm     1.00\n",
      "[46/200][1099/2499] Loss_D: 0.07108092 (Loss_D_real: 0.04295703 Loss_D_fake: 0.02812388) Loss_G: 0.19725190 Loss_Enh_Dec: -1.39283967\n",
      "| epoch  46 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1050.60 | loss  1.22 | ppl     3.39 | acc     0.80 | train_ae_norm     1.00\n",
      "[46/200][1199/2499] Loss_D: 0.14072374 (Loss_D_real: 0.03852926 Loss_D_fake: 0.10219447) Loss_G: 0.16899322 Loss_Enh_Dec: -1.30658555\n",
      "| epoch  46 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1051.21 | loss  1.29 | ppl     3.65 | acc     0.83 | train_ae_norm     1.00\n",
      "[46/200][1299/2499] Loss_D: 0.10462706 (Loss_D_real: 0.07146601 Loss_D_fake: 0.03316104) Loss_G: 0.20137374 Loss_Enh_Dec: -1.27608705\n",
      "| epoch  46 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1049.91 | loss  1.30 | ppl     3.66 | acc     0.81 | train_ae_norm     1.00\n",
      "[46/200][1399/2499] Loss_D: 0.08807653 (Loss_D_real: 0.02981934 Loss_D_fake: 0.05825719) Loss_G: 0.20451494 Loss_Enh_Dec: -1.23488295\n",
      "| epoch  46 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1050.21 | loss  1.24 | ppl     3.47 | acc     0.85 | train_ae_norm     1.00\n",
      "[46/200][1499/2499] Loss_D: 0.14949559 (Loss_D_real: 0.08095338 Loss_D_fake: 0.06854220) Loss_G: 0.21419488 Loss_Enh_Dec: -1.25174057\n",
      "| epoch  46 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1051.45 | loss  1.33 | ppl     3.77 | acc     0.82 | train_ae_norm     1.00\n",
      "[46/200][1599/2499] Loss_D: 0.07224762 (Loss_D_real: 0.03606404 Loss_D_fake: 0.03618358) Loss_G: 0.18535104 Loss_Enh_Dec: -1.33399427\n",
      "| epoch  46 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1051.59 | loss  1.27 | ppl     3.55 | acc     0.85 | train_ae_norm     1.00\n",
      "[46/200][1699/2499] Loss_D: 0.05119752 (Loss_D_real: 0.02650622 Loss_D_fake: 0.02469130) Loss_G: 0.19438724 Loss_Enh_Dec: -1.53791964\n",
      "| epoch  46 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1050.14 | loss  1.16 | ppl     3.18 | acc     0.88 | train_ae_norm     1.00\n",
      "[46/200][1799/2499] Loss_D: 0.06709839 (Loss_D_real: 0.04045805 Loss_D_fake: 0.02664034) Loss_G: 0.19825725 Loss_Enh_Dec: -1.34532630\n",
      "| epoch  46 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1050.29 | loss  1.14 | ppl     3.14 | acc     0.87 | train_ae_norm     1.00\n",
      "[46/200][1899/2499] Loss_D: 0.07521540 (Loss_D_real: 0.04745030 Loss_D_fake: 0.02776510) Loss_G: 0.19150908 Loss_Enh_Dec: -1.42048061\n",
      "| epoch  46 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1051.50 | loss  1.20 | ppl     3.32 | acc     0.84 | train_ae_norm     1.00\n",
      "[46/200][1999/2499] Loss_D: 0.04817565 (Loss_D_real: 0.02251859 Loss_D_fake: 0.02565706) Loss_G: 0.20347743 Loss_Enh_Dec: -1.45780444\n",
      "| epoch  46 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1051.20 | loss  1.26 | ppl     3.52 | acc     0.87 | train_ae_norm     1.00\n",
      "[46/200][2099/2499] Loss_D: 0.08405551 (Loss_D_real: 0.02995244 Loss_D_fake: 0.05410307) Loss_G: 0.18179058 Loss_Enh_Dec: -1.25984514\n",
      "| epoch  46 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1051.46 | loss  1.16 | ppl     3.20 | acc     0.90 | train_ae_norm     1.00\n",
      "[46/200][2199/2499] Loss_D: 0.08968473 (Loss_D_real: 0.06743270 Loss_D_fake: 0.02225203) Loss_G: 0.20959388 Loss_Enh_Dec: -1.33282125\n",
      "| epoch  46 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1050.74 | loss  1.12 | ppl     3.07 | acc     0.89 | train_ae_norm     1.00\n",
      "[46/200][2299/2499] Loss_D: 0.05604514 (Loss_D_real: 0.01948901 Loss_D_fake: 0.03655613) Loss_G: 0.20112815 Loss_Enh_Dec: -1.43743634\n",
      "| epoch  46 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1051.36 | loss  1.20 | ppl     3.32 | acc     0.87 | train_ae_norm     1.00\n",
      "[46/200][2399/2499] Loss_D: 0.07629912 (Loss_D_real: 0.03606445 Loss_D_fake: 0.04023467) Loss_G: 0.20186222 Loss_Enh_Dec: -1.42790508\n",
      "| epoch  46 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1052.07 | loss  1.15 | ppl     3.14 | acc     0.84 | train_ae_norm     1.00\n",
      "[46/200][2499/2499] Loss_D: 0.14972910 (Loss_D_real: 0.07331666 Loss_D_fake: 0.07641243) Loss_G: 0.17242712 Loss_Enh_Dec: -1.28441679\n",
      "| end of epoch  46 | time: 2807.76s | test loss  0.88 | test ppl  2.41 | acc 0.915\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 47 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:49.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:56.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:10.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:17.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:24.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:31.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:38.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:45.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:52.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:59.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:06.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:13.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:20.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:27.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:34.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.720\n",
      "  Training epcoh took: 0:02:41\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.495\n",
      "  Test Loss: 3.705\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  47 |     0/ 2499 batches | lr 0.000000 | ms/batch 1610.80 | loss  0.01 | ppl     1.01 | acc     0.83 | train_ae_norm     1.00\n",
      "[47/200][99/2499] Loss_D: 0.11294398 (Loss_D_real: 0.09328566 Loss_D_fake: 0.01965831) Loss_G: 0.20597325 Loss_Enh_Dec: -1.15361130\n",
      "| epoch  47 |   100/ 2499 batches | lr 0.000000 | ms/batch 1051.65 | loss  1.16 | ppl     3.20 | acc     0.81 | train_ae_norm     1.00\n",
      "[47/200][199/2499] Loss_D: 0.11669415 (Loss_D_real: 0.02922434 Loss_D_fake: 0.08746982) Loss_G: 0.15522312 Loss_Enh_Dec: -1.19379461\n",
      "| epoch  47 |   200/ 2499 batches | lr 0.000000 | ms/batch 1049.50 | loss  1.19 | ppl     3.28 | acc     0.87 | train_ae_norm     1.00\n",
      "[47/200][299/2499] Loss_D: 0.05287665 (Loss_D_real: 0.02746181 Loss_D_fake: 0.02541484) Loss_G: 0.20530775 Loss_Enh_Dec: -1.50311208\n",
      "| epoch  47 |   300/ 2499 batches | lr 0.000000 | ms/batch 1051.62 | loss  1.25 | ppl     3.50 | acc     0.84 | train_ae_norm     1.00\n",
      "[47/200][399/2499] Loss_D: 0.04939688 (Loss_D_real: 0.03409457 Loss_D_fake: 0.01530231) Loss_G: 0.24090779 Loss_Enh_Dec: -1.27683771\n",
      "| epoch  47 |   400/ 2499 batches | lr 0.000000 | ms/batch 1051.58 | loss  1.26 | ppl     3.53 | acc     0.84 | train_ae_norm     1.00\n",
      "[47/200][499/2499] Loss_D: 0.09152376 (Loss_D_real: 0.05205932 Loss_D_fake: 0.03946444) Loss_G: 0.18874292 Loss_Enh_Dec: -1.42259979\n",
      "| epoch  47 |   500/ 2499 batches | lr 0.000000 | ms/batch 1050.61 | loss  1.25 | ppl     3.48 | acc     0.86 | train_ae_norm     1.00\n",
      "[47/200][599/2499] Loss_D: 0.09847657 (Loss_D_real: 0.07156590 Loss_D_fake: 0.02691067) Loss_G: 0.17643955 Loss_Enh_Dec: -1.47127616\n",
      "| epoch  47 |   600/ 2499 batches | lr 0.000000 | ms/batch 1050.19 | loss  1.19 | ppl     3.29 | acc     0.87 | train_ae_norm     1.00\n",
      "[47/200][699/2499] Loss_D: 0.09182640 (Loss_D_real: 0.04787730 Loss_D_fake: 0.04394910) Loss_G: 0.15973572 Loss_Enh_Dec: -1.46381211\n",
      "| epoch  47 |   700/ 2499 batches | lr 0.000000 | ms/batch 1051.86 | loss  1.23 | ppl     3.41 | acc     0.80 | train_ae_norm     1.00\n",
      "[47/200][799/2499] Loss_D: 0.08301395 (Loss_D_real: 0.06519212 Loss_D_fake: 0.01782184) Loss_G: 0.21023528 Loss_Enh_Dec: -1.37471199\n",
      "| epoch  47 |   800/ 2499 batches | lr 0.000000 | ms/batch 1051.23 | loss  1.33 | ppl     3.79 | acc     0.82 | train_ae_norm     1.00\n",
      "[47/200][899/2499] Loss_D: 0.05129493 (Loss_D_real: 0.02274806 Loss_D_fake: 0.02854687) Loss_G: 0.20222878 Loss_Enh_Dec: -1.45613706\n",
      "| epoch  47 |   900/ 2499 batches | lr 0.000000 | ms/batch 1052.48 | loss  1.23 | ppl     3.43 | acc     0.86 | train_ae_norm     1.00\n",
      "[47/200][999/2499] Loss_D: 0.05819836 (Loss_D_real: 0.03752111 Loss_D_fake: 0.02067724) Loss_G: 0.21896701 Loss_Enh_Dec: -1.28075683\n",
      "| epoch  47 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1051.77 | loss  1.28 | ppl     3.60 | acc     0.86 | train_ae_norm     1.00\n",
      "[47/200][1099/2499] Loss_D: 0.04929906 (Loss_D_real: 0.02562422 Loss_D_fake: 0.02367484) Loss_G: 0.20241788 Loss_Enh_Dec: -0.21824494\n",
      "| epoch  47 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1050.83 | loss  1.24 | ppl     3.46 | acc     0.87 | train_ae_norm     1.00\n",
      "[47/200][1199/2499] Loss_D: 0.13800643 (Loss_D_real: 0.07476214 Loss_D_fake: 0.06324430) Loss_G: 0.18572257 Loss_Enh_Dec: -1.17326665\n",
      "| epoch  47 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1050.10 | loss  1.22 | ppl     3.38 | acc     0.83 | train_ae_norm     1.00\n",
      "[47/200][1299/2499] Loss_D: 0.08447480 (Loss_D_real: 0.04545499 Loss_D_fake: 0.03901981) Loss_G: 0.18301721 Loss_Enh_Dec: -1.36690748\n",
      "| epoch  47 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1050.88 | loss  1.33 | ppl     3.79 | acc     0.79 | train_ae_norm     1.00\n",
      "[47/200][1399/2499] Loss_D: 0.05575790 (Loss_D_real: 0.01745300 Loss_D_fake: 0.03830490) Loss_G: 0.18741105 Loss_Enh_Dec: -1.05972028\n",
      "| epoch  47 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1052.49 | loss  1.38 | ppl     3.98 | acc     0.87 | train_ae_norm     1.00\n",
      "[47/200][1499/2499] Loss_D: 0.06678053 (Loss_D_real: 0.02531978 Loss_D_fake: 0.04146075) Loss_G: 0.20388906 Loss_Enh_Dec: -1.49476302\n",
      "| epoch  47 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1051.18 | loss  1.27 | ppl     3.56 | acc     0.83 | train_ae_norm     1.00\n",
      "[47/200][1599/2499] Loss_D: 0.06066502 (Loss_D_real: 0.03167807 Loss_D_fake: 0.02898694) Loss_G: 0.23043656 Loss_Enh_Dec: -1.33736479\n",
      "| epoch  47 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1051.98 | loss  1.26 | ppl     3.52 | acc     0.84 | train_ae_norm     1.00\n",
      "[47/200][1699/2499] Loss_D: 0.09879903 (Loss_D_real: 0.07294130 Loss_D_fake: 0.02585773) Loss_G: 0.24199319 Loss_Enh_Dec: -1.33071649\n",
      "| epoch  47 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1051.89 | loss  1.25 | ppl     3.49 | acc     0.87 | train_ae_norm     1.00\n",
      "[47/200][1799/2499] Loss_D: 0.05308835 (Loss_D_real: 0.03582696 Loss_D_fake: 0.01726139) Loss_G: 0.22836295 Loss_Enh_Dec: -1.45273912\n",
      "| epoch  47 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1052.10 | loss  1.29 | ppl     3.63 | acc     0.87 | train_ae_norm     1.00\n",
      "[47/200][1899/2499] Loss_D: 0.09830653 (Loss_D_real: 0.02063207 Loss_D_fake: 0.07767446) Loss_G: 0.20541377 Loss_Enh_Dec: -1.49142611\n",
      "| epoch  47 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1052.62 | loss  1.39 | ppl     4.01 | acc     0.81 | train_ae_norm     1.00\n",
      "[47/200][1999/2499] Loss_D: 0.08476161 (Loss_D_real: 0.04391041 Loss_D_fake: 0.04085120) Loss_G: 0.22393253 Loss_Enh_Dec: -1.46461594\n",
      "| epoch  47 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1053.06 | loss  1.32 | ppl     3.74 | acc     0.85 | train_ae_norm     1.00\n",
      "[47/200][2099/2499] Loss_D: 0.04842664 (Loss_D_real: 0.03211433 Loss_D_fake: 0.01631231) Loss_G: 0.20985623 Loss_Enh_Dec: -0.80831528\n",
      "| epoch  47 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1054.36 | loss  1.24 | ppl     3.44 | acc     0.88 | train_ae_norm     1.00\n",
      "[47/200][2199/2499] Loss_D: 0.05229422 (Loss_D_real: 0.03259616 Loss_D_fake: 0.01969806) Loss_G: 0.21814723 Loss_Enh_Dec: -1.52398562\n",
      "| epoch  47 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1052.28 | loss  1.33 | ppl     3.78 | acc     0.85 | train_ae_norm     1.00\n",
      "[47/200][2299/2499] Loss_D: 0.05928678 (Loss_D_real: 0.04125527 Loss_D_fake: 0.01803151) Loss_G: 0.21731602 Loss_Enh_Dec: -1.41698790\n",
      "| epoch  47 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1052.71 | loss  1.52 | ppl     4.56 | acc     0.82 | train_ae_norm     1.00\n",
      "[47/200][2399/2499] Loss_D: 0.07938144 (Loss_D_real: 0.04572957 Loss_D_fake: 0.03365187) Loss_G: 0.21294852 Loss_Enh_Dec: -1.12574267\n",
      "| epoch  47 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1054.50 | loss  1.38 | ppl     3.97 | acc     0.80 | train_ae_norm     1.00\n",
      "[47/200][2499/2499] Loss_D: 0.05909886 (Loss_D_real: 0.04794201 Loss_D_fake: 0.01115686) Loss_G: 0.23356585 Loss_Enh_Dec: -1.13520038\n",
      "| end of epoch  47 | time: 2809.88s | test loss  0.92 | test ppl  2.51 | acc 0.908\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 48 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:49.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:56.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:02.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:09.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:16.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:23.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:30.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:37.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:44.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:51.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:58.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:05.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:19.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:26.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:33.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.717\n",
      "  Training epcoh took: 0:02:39\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.455\n",
      "  Test Loss: 3.931\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  48 |     0/ 2499 batches | lr 0.000000 | ms/batch 1600.00 | loss  0.01 | ppl     1.02 | acc     0.82 | train_ae_norm     1.00\n",
      "[48/200][99/2499] Loss_D: 0.06890659 (Loss_D_real: 0.04374574 Loss_D_fake: 0.02516085) Loss_G: 0.21423681 Loss_Enh_Dec: -1.37668920\n",
      "| epoch  48 |   100/ 2499 batches | lr 0.000000 | ms/batch 1053.53 | loss  1.45 | ppl     4.26 | acc     0.76 | train_ae_norm     1.00\n",
      "[48/200][199/2499] Loss_D: 0.11545555 (Loss_D_real: 0.05542754 Loss_D_fake: 0.06002801) Loss_G: 0.17075804 Loss_Enh_Dec: -1.20625556\n",
      "| epoch  48 |   200/ 2499 batches | lr 0.000000 | ms/batch 1052.33 | loss  1.50 | ppl     4.47 | acc     0.83 | train_ae_norm     1.00\n",
      "[48/200][299/2499] Loss_D: 0.06827696 (Loss_D_real: 0.02393846 Loss_D_fake: 0.04433849) Loss_G: 0.16335137 Loss_Enh_Dec: -1.23115337\n",
      "| epoch  48 |   300/ 2499 batches | lr 0.000000 | ms/batch 1051.89 | loss  1.46 | ppl     4.29 | acc     0.84 | train_ae_norm     1.00\n",
      "[48/200][399/2499] Loss_D: 0.14508665 (Loss_D_real: 0.07594776 Loss_D_fake: 0.06913888) Loss_G: 0.15306246 Loss_Enh_Dec: -1.27127492\n",
      "| epoch  48 |   400/ 2499 batches | lr 0.000000 | ms/batch 1052.42 | loss  1.32 | ppl     3.73 | acc     0.84 | train_ae_norm     1.00\n",
      "[48/200][499/2499] Loss_D: 0.07122719 (Loss_D_real: 0.03394374 Loss_D_fake: 0.03728346) Loss_G: 0.23645902 Loss_Enh_Dec: -1.08238566\n",
      "| epoch  48 |   500/ 2499 batches | lr 0.000000 | ms/batch 1052.88 | loss  1.42 | ppl     4.12 | acc     0.82 | train_ae_norm     1.00\n",
      "[48/200][599/2499] Loss_D: 0.15404819 (Loss_D_real: 0.07184489 Loss_D_fake: 0.08220330) Loss_G: 0.20029747 Loss_Enh_Dec: -1.25218201\n",
      "| epoch  48 |   600/ 2499 batches | lr 0.000000 | ms/batch 1054.01 | loss  1.40 | ppl     4.07 | acc     0.80 | train_ae_norm     1.00\n",
      "[48/200][699/2499] Loss_D: 0.59306490 (Loss_D_real: 0.03002725 Loss_D_fake: 0.56303763) Loss_G: 0.17401609 Loss_Enh_Dec: -1.04385948\n",
      "| epoch  48 |   700/ 2499 batches | lr 0.000000 | ms/batch 1053.10 | loss  1.48 | ppl     4.40 | acc     0.81 | train_ae_norm     1.00\n",
      "[48/200][799/2499] Loss_D: 0.10342567 (Loss_D_real: 0.05827719 Loss_D_fake: 0.04514848) Loss_G: 0.19162405 Loss_Enh_Dec: -0.94590753\n",
      "| epoch  48 |   800/ 2499 batches | lr 0.000000 | ms/batch 1053.66 | loss  1.44 | ppl     4.24 | acc     0.82 | train_ae_norm     1.00\n",
      "[48/200][899/2499] Loss_D: 0.06434144 (Loss_D_real: 0.03425370 Loss_D_fake: 0.03008774) Loss_G: 0.20214812 Loss_Enh_Dec: -1.26415002\n",
      "| epoch  48 |   900/ 2499 batches | lr 0.000000 | ms/batch 1054.81 | loss  1.38 | ppl     3.97 | acc     0.84 | train_ae_norm     1.00\n",
      "[48/200][999/2499] Loss_D: 0.08984234 (Loss_D_real: 0.04161472 Loss_D_fake: 0.04822762) Loss_G: 0.16936818 Loss_Enh_Dec: -1.23862040\n",
      "| epoch  48 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1053.62 | loss  1.50 | ppl     4.47 | acc     0.84 | train_ae_norm     1.00\n",
      "[48/200][1099/2499] Loss_D: 0.10248060 (Loss_D_real: 0.02457945 Loss_D_fake: 0.07790115) Loss_G: 0.15187068 Loss_Enh_Dec: -0.42899257\n",
      "| epoch  48 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1054.35 | loss  1.44 | ppl     4.23 | acc     0.84 | train_ae_norm     1.00\n",
      "[48/200][1199/2499] Loss_D: 0.15453601 (Loss_D_real: 0.08806308 Loss_D_fake: 0.06647293) Loss_G: 0.18805914 Loss_Enh_Dec: -1.28023136\n",
      "| epoch  48 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1055.60 | loss  1.40 | ppl     4.05 | acc     0.78 | train_ae_norm     1.00\n",
      "[48/200][1299/2499] Loss_D: 0.06753176 (Loss_D_real: 0.03739338 Loss_D_fake: 0.03013838) Loss_G: 0.20205937 Loss_Enh_Dec: -0.78134078\n",
      "| epoch  48 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1053.63 | loss  1.48 | ppl     4.39 | acc     0.77 | train_ae_norm     1.00\n",
      "[48/200][1399/2499] Loss_D: 0.08435670 (Loss_D_real: 0.05126498 Loss_D_fake: 0.03309171) Loss_G: 0.22115038 Loss_Enh_Dec: -1.06013167\n",
      "| epoch  48 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1053.75 | loss  1.49 | ppl     4.45 | acc     0.79 | train_ae_norm     1.00\n",
      "[48/200][1499/2499] Loss_D: 0.13042822 (Loss_D_real: 0.04215118 Loss_D_fake: 0.08827703) Loss_G: 0.17258978 Loss_Enh_Dec: -1.05179727\n",
      "| epoch  48 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1055.83 | loss  1.43 | ppl     4.17 | acc     0.82 | train_ae_norm     1.00\n",
      "[48/200][1599/2499] Loss_D: 0.09040678 (Loss_D_real: 0.04076302 Loss_D_fake: 0.04964375) Loss_G: 0.18807974 Loss_Enh_Dec: -1.00406682\n",
      "| epoch  48 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1054.30 | loss  1.36 | ppl     3.88 | acc     0.82 | train_ae_norm     1.00\n",
      "[48/200][1699/2499] Loss_D: 0.34277788 (Loss_D_real: 0.08479034 Loss_D_fake: 0.25798753) Loss_G: 0.18746452 Loss_Enh_Dec: -1.02889693\n",
      "| epoch  48 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1055.01 | loss  1.37 | ppl     3.95 | acc     0.85 | train_ae_norm     1.00\n",
      "[48/200][1799/2499] Loss_D: 0.14454424 (Loss_D_real: 0.06411243 Loss_D_fake: 0.08043180) Loss_G: 0.08361562 Loss_Enh_Dec: -1.02493083\n",
      "| epoch  48 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1056.06 | loss  1.36 | ppl     3.91 | acc     0.85 | train_ae_norm     1.00\n",
      "[48/200][1899/2499] Loss_D: 0.08790023 (Loss_D_real: 0.06180931 Loss_D_fake: 0.02609092) Loss_G: 0.20806865 Loss_Enh_Dec: -1.23898530\n",
      "| epoch  48 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1056.63 | loss  1.35 | ppl     3.84 | acc     0.83 | train_ae_norm     1.00\n",
      "[48/200][1999/2499] Loss_D: 0.08533800 (Loss_D_real: 0.04830995 Loss_D_fake: 0.03702805) Loss_G: 0.23837124 Loss_Enh_Dec: -1.19889486\n",
      "| epoch  48 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1055.11 | loss  1.36 | ppl     3.89 | acc     0.84 | train_ae_norm     1.00\n",
      "[48/200][2099/2499] Loss_D: 0.10658368 (Loss_D_real: 0.07991809 Loss_D_fake: 0.02666559) Loss_G: 0.23201433 Loss_Enh_Dec: -1.24436939\n",
      "| epoch  48 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1055.69 | loss  1.44 | ppl     4.22 | acc     0.84 | train_ae_norm     1.00\n",
      "[48/200][2199/2499] Loss_D: 0.29743624 (Loss_D_real: 0.08242629 Loss_D_fake: 0.21500993) Loss_G: 0.17643367 Loss_Enh_Dec: -1.23575711\n",
      "| epoch  48 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1056.30 | loss  1.57 | ppl     4.80 | acc     0.82 | train_ae_norm     1.00\n",
      "[48/200][2299/2499] Loss_D: 0.06988064 (Loss_D_real: 0.02148105 Loss_D_fake: 0.04839959) Loss_G: 0.19404685 Loss_Enh_Dec: -1.08845425\n",
      "| epoch  48 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1055.06 | loss  1.73 | ppl     5.64 | acc     0.82 | train_ae_norm     1.00\n",
      "[48/200][2399/2499] Loss_D: 0.12827224 (Loss_D_real: 0.05630450 Loss_D_fake: 0.07196774) Loss_G: 0.20649461 Loss_Enh_Dec: -1.17522013\n",
      "| epoch  48 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1055.90 | loss  1.49 | ppl     4.44 | acc     0.83 | train_ae_norm     1.00\n",
      "[48/200][2499/2499] Loss_D: 0.11992043 (Loss_D_real: 0.08853424 Loss_D_fake: 0.03138619) Loss_G: 0.19583009 Loss_Enh_Dec: -1.20869577\n",
      "| end of epoch  48 | time: 2815.31s | test loss  0.92 | test ppl  2.52 | acc 0.909\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 49 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    70  of    230.    Elapsed: 0:00:49.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:56.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:10.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:17.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:24.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:31.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:38.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:45.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:52.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:59.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:06.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:13.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:20.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:27.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:34.\n",
      "\n",
      "  Average training loss generetor: 0.709\n",
      "  Average training loss discriminator: 0.751\n",
      "  Training epcoh took: 0:02:41\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.480\n",
      "  Test Loss: 3.442\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  49 |     0/ 2499 batches | lr 0.000000 | ms/batch 1615.54 | loss  0.02 | ppl     1.02 | acc     0.81 | train_ae_norm     1.00\n",
      "[49/200][99/2499] Loss_D: 0.15120177 (Loss_D_real: 0.05789767 Loss_D_fake: 0.09330410) Loss_G: 0.13811487 Loss_Enh_Dec: -1.01540649\n",
      "| epoch  49 |   100/ 2499 batches | lr 0.000000 | ms/batch 1056.61 | loss  1.47 | ppl     4.37 | acc     0.81 | train_ae_norm     1.00\n",
      "[49/200][199/2499] Loss_D: 0.06747766 (Loss_D_real: 0.03826764 Loss_D_fake: 0.02921002) Loss_G: 0.20628071 Loss_Enh_Dec: -1.26299131\n",
      "| epoch  49 |   200/ 2499 batches | lr 0.000000 | ms/batch 1055.62 | loss  1.42 | ppl     4.13 | acc     0.84 | train_ae_norm     1.00\n",
      "[49/200][299/2499] Loss_D: 0.11741609 (Loss_D_real: 0.03207747 Loss_D_fake: 0.08533862) Loss_G: 0.19801031 Loss_Enh_Dec: -1.09466887\n",
      "| epoch  49 |   300/ 2499 batches | lr 0.000000 | ms/batch 1056.05 | loss  1.38 | ppl     3.98 | acc     0.79 | train_ae_norm     1.00\n",
      "[49/200][399/2499] Loss_D: 0.07058999 (Loss_D_real: 0.02673443 Loss_D_fake: 0.04385556) Loss_G: 0.24060848 Loss_Enh_Dec: -0.99682182\n",
      "| epoch  49 |   400/ 2499 batches | lr 0.000000 | ms/batch 1055.94 | loss  1.54 | ppl     4.67 | acc     0.85 | train_ae_norm     1.00\n",
      "[49/200][499/2499] Loss_D: 0.11569478 (Loss_D_real: 0.07572655 Loss_D_fake: 0.03996823) Loss_G: 0.23354042 Loss_Enh_Dec: -1.28795803\n",
      "| epoch  49 |   500/ 2499 batches | lr 0.000000 | ms/batch 1056.30 | loss  1.70 | ppl     5.47 | acc     0.80 | train_ae_norm     1.00\n",
      "[49/200][599/2499] Loss_D: 0.06671366 (Loss_D_real: 0.02974897 Loss_D_fake: 0.03696470) Loss_G: 0.18761958 Loss_Enh_Dec: -1.14679074\n",
      "| epoch  49 |   600/ 2499 batches | lr 0.000000 | ms/batch 1056.87 | loss  1.57 | ppl     4.82 | acc     0.80 | train_ae_norm     1.00\n",
      "[49/200][699/2499] Loss_D: 0.11671477 (Loss_D_real: 0.09752824 Loss_D_fake: 0.01918652) Loss_G: 0.21008043 Loss_Enh_Dec: -1.27353477\n",
      "| epoch  49 |   700/ 2499 batches | lr 0.000000 | ms/batch 1056.66 | loss  1.62 | ppl     5.07 | acc     0.76 | train_ae_norm     1.00\n",
      "[49/200][799/2499] Loss_D: 0.05182912 (Loss_D_real: 0.01714932 Loss_D_fake: 0.03467980) Loss_G: 0.22717376 Loss_Enh_Dec: -1.16824532\n",
      "| epoch  49 |   800/ 2499 batches | lr 0.000000 | ms/batch 1055.76 | loss  1.56 | ppl     4.75 | acc     0.79 | train_ae_norm     1.00\n",
      "[49/200][899/2499] Loss_D: 0.14208430 (Loss_D_real: 0.01895674 Loss_D_fake: 0.12312756) Loss_G: 0.17189899 Loss_Enh_Dec: -0.81698745\n",
      "| epoch  49 |   900/ 2499 batches | lr 0.000000 | ms/batch 1057.09 | loss  1.44 | ppl     4.22 | acc     0.83 | train_ae_norm     1.00\n",
      "[49/200][999/2499] Loss_D: 0.14782153 (Loss_D_real: 0.11265764 Loss_D_fake: 0.03516389) Loss_G: 0.23207732 Loss_Enh_Dec: -1.11806095\n",
      "| epoch  49 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1055.88 | loss  1.64 | ppl     5.16 | acc     0.83 | train_ae_norm     1.00\n",
      "[49/200][1099/2499] Loss_D: 0.12406251 (Loss_D_real: 0.08034062 Loss_D_fake: 0.04372190) Loss_G: 0.17377414 Loss_Enh_Dec: -1.04920697\n",
      "| epoch  49 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1057.23 | loss  1.50 | ppl     4.50 | acc     0.83 | train_ae_norm     1.00\n",
      "[49/200][1199/2499] Loss_D: 0.06070143 (Loss_D_real: 0.03361266 Loss_D_fake: 0.02708877) Loss_G: 0.19447975 Loss_Enh_Dec: -1.18675232\n",
      "| epoch  49 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1057.90 | loss  1.42 | ppl     4.16 | acc     0.79 | train_ae_norm     1.00\n",
      "[49/200][1299/2499] Loss_D: 0.04768458 (Loss_D_real: 0.02881918 Loss_D_fake: 0.01886540) Loss_G: 0.25151345 Loss_Enh_Dec: -0.86493939\n",
      "| epoch  49 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1056.21 | loss  1.50 | ppl     4.47 | acc     0.77 | train_ae_norm     1.00\n",
      "[49/200][1399/2499] Loss_D: 0.09381532 (Loss_D_real: 0.05515514 Loss_D_fake: 0.03866018) Loss_G: 0.22478285 Loss_Enh_Dec: -1.27453923\n",
      "| epoch  49 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1058.06 | loss  1.47 | ppl     4.36 | acc     0.83 | train_ae_norm     1.00\n",
      "[49/200][1499/2499] Loss_D: 0.10652427 (Loss_D_real: 0.06156335 Loss_D_fake: 0.04496093) Loss_G: 0.18968396 Loss_Enh_Dec: -0.96398181\n",
      "| epoch  49 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1056.47 | loss  1.43 | ppl     4.17 | acc     0.81 | train_ae_norm     1.00\n",
      "[49/200][1599/2499] Loss_D: 0.17518362 (Loss_D_real: 0.10876145 Loss_D_fake: 0.06642216) Loss_G: 0.18876563 Loss_Enh_Dec: -1.05108964\n",
      "| epoch  49 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1057.66 | loss  1.64 | ppl     5.16 | acc     0.76 | train_ae_norm     1.00\n",
      "[49/200][1699/2499] Loss_D: 0.73296928 (Loss_D_real: 0.06204650 Loss_D_fake: 0.67092276) Loss_G: 0.20370317 Loss_Enh_Dec: -1.11012340\n",
      "| epoch  49 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1054.62 | loss  2.04 | ppl     7.66 | acc     0.76 | train_ae_norm     1.00\n",
      "[49/200][1799/2499] Loss_D: 0.06970154 (Loss_D_real: 0.03613667 Loss_D_fake: 0.03356487) Loss_G: 0.21795817 Loss_Enh_Dec: -1.03724313\n",
      "| epoch  49 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1057.65 | loss  2.47 | ppl    11.83 | acc     0.71 | train_ae_norm     1.00\n",
      "[49/200][1899/2499] Loss_D: 0.05585532 (Loss_D_real: 0.02917946 Loss_D_fake: 0.02667586) Loss_G: 0.22633024 Loss_Enh_Dec: -0.87176704\n",
      "| epoch  49 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1056.10 | loss  2.34 | ppl    10.40 | acc     0.73 | train_ae_norm     1.00\n",
      "[49/200][1999/2499] Loss_D: 0.08789733 (Loss_D_real: 0.02986715 Loss_D_fake: 0.05803019) Loss_G: 0.19611526 Loss_Enh_Dec: -1.10231006\n",
      "| epoch  49 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1055.72 | loss  1.88 | ppl     6.53 | acc     0.78 | train_ae_norm     1.00\n",
      "[49/200][2099/2499] Loss_D: 0.07679365 (Loss_D_real: 0.01387514 Loss_D_fake: 0.06291851) Loss_G: 0.22820257 Loss_Enh_Dec: -1.02360570\n",
      "| epoch  49 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1056.14 | loss  1.65 | ppl     5.20 | acc     0.83 | train_ae_norm     1.00\n",
      "[49/200][2199/2499] Loss_D: 0.06144670 (Loss_D_real: 0.04277304 Loss_D_fake: 0.01867367) Loss_G: 0.23258460 Loss_Enh_Dec: -1.28778565\n",
      "| epoch  49 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1057.04 | loss  1.58 | ppl     4.85 | acc     0.83 | train_ae_norm     1.00\n",
      "[49/200][2299/2499] Loss_D: 0.26374373 (Loss_D_real: 0.17352366 Loss_D_fake: 0.09022006) Loss_G: 0.13569658 Loss_Enh_Dec: -1.13846934\n",
      "| epoch  49 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1055.20 | loss  1.58 | ppl     4.86 | acc     0.82 | train_ae_norm     1.00\n",
      "[49/200][2399/2499] Loss_D: 0.16314209 (Loss_D_real: 0.07878235 Loss_D_fake: 0.08435973) Loss_G: 0.12333658 Loss_Enh_Dec: -1.07234800\n",
      "| epoch  49 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1056.36 | loss  1.58 | ppl     4.85 | acc     0.80 | train_ae_norm     1.00\n",
      "[49/200][2499/2499] Loss_D: 0.09688706 (Loss_D_real: 0.03357873 Loss_D_fake: 0.06330833) Loss_G: 0.13960473 Loss_Enh_Dec: -1.21042740\n",
      "| end of epoch  49 | time: 2821.85s | test loss  0.96 | test ppl  2.61 | acc 0.905\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 50 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:49.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:56.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    90  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:10.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:17.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:24.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:31.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:38.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:45.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:52.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:59.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:06.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:13.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:20.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:27.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:34.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.711\n",
      "  Training epcoh took: 0:02:41\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.495\n",
      "  Test Loss: 3.427\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  50 |     0/ 2499 batches | lr 0.000000 | ms/batch 1614.89 | loss  0.02 | ppl     1.02 | acc     0.79 | train_ae_norm     1.00\n",
      "[50/200][99/2499] Loss_D: 0.10551675 (Loss_D_real: 0.04561653 Loss_D_fake: 0.05990022) Loss_G: 0.14252596 Loss_Enh_Dec: -0.99161118\n",
      "| epoch  50 |   100/ 2499 batches | lr 0.000000 | ms/batch 1056.27 | loss  1.57 | ppl     4.82 | acc     0.77 | train_ae_norm     1.00\n",
      "[50/200][199/2499] Loss_D: 0.06509868 (Loss_D_real: 0.01549793 Loss_D_fake: 0.04960075) Loss_G: 0.15199240 Loss_Enh_Dec: -1.06743503\n",
      "| epoch  50 |   200/ 2499 batches | lr 0.000000 | ms/batch 1055.88 | loss  1.63 | ppl     5.11 | acc     0.82 | train_ae_norm     1.00\n",
      "[50/200][299/2499] Loss_D: 0.04294746 (Loss_D_real: 0.01872237 Loss_D_fake: 0.02422509) Loss_G: 0.20208044 Loss_Enh_Dec: -0.97815102\n",
      "| epoch  50 |   300/ 2499 batches | lr 0.000000 | ms/batch 1056.01 | loss  1.66 | ppl     5.25 | acc     0.79 | train_ae_norm     1.00\n",
      "[50/200][399/2499] Loss_D: 0.08849073 (Loss_D_real: 0.05686396 Loss_D_fake: 0.03162678) Loss_G: 0.21273021 Loss_Enh_Dec: -1.12550867\n",
      "| epoch  50 |   400/ 2499 batches | lr 0.000000 | ms/batch 1055.96 | loss  1.61 | ppl     5.01 | acc     0.83 | train_ae_norm     1.00\n",
      "[50/200][499/2499] Loss_D: 0.09789187 (Loss_D_real: 0.05054665 Loss_D_fake: 0.04734522) Loss_G: 0.15461501 Loss_Enh_Dec: -1.34947920\n",
      "| epoch  50 |   500/ 2499 batches | lr 0.000000 | ms/batch 1056.31 | loss  1.67 | ppl     5.32 | acc     0.81 | train_ae_norm     1.00\n",
      "[50/200][599/2499] Loss_D: 0.07042915 (Loss_D_real: 0.03144682 Loss_D_fake: 0.03898233) Loss_G: 0.17166132 Loss_Enh_Dec: -1.29356313\n",
      "| epoch  50 |   600/ 2499 batches | lr 0.000000 | ms/batch 1055.17 | loss  1.87 | ppl     6.49 | acc     0.75 | train_ae_norm     1.00\n",
      "[50/200][699/2499] Loss_D: 0.08650446 (Loss_D_real: 0.03649450 Loss_D_fake: 0.05000996) Loss_G: 0.15264867 Loss_Enh_Dec: -1.34505296\n",
      "| epoch  50 |   700/ 2499 batches | lr 0.000000 | ms/batch 1056.46 | loss  2.08 | ppl     7.97 | acc     0.72 | train_ae_norm     1.00\n",
      "[50/200][799/2499] Loss_D: 0.04682579 (Loss_D_real: 0.01197489 Loss_D_fake: 0.03485090) Loss_G: 0.16693020 Loss_Enh_Dec: -0.86354792\n",
      "| epoch  50 |   800/ 2499 batches | lr 0.000000 | ms/batch 1056.94 | loss  1.97 | ppl     7.19 | acc     0.75 | train_ae_norm     1.00\n",
      "[50/200][899/2499] Loss_D: 0.04947677 (Loss_D_real: 0.01603351 Loss_D_fake: 0.03344326) Loss_G: 0.17057268 Loss_Enh_Dec: -1.42405164\n",
      "| epoch  50 |   900/ 2499 batches | lr 0.000000 | ms/batch 1056.28 | loss  1.69 | ppl     5.42 | acc     0.81 | train_ae_norm     1.00\n",
      "[50/200][999/2499] Loss_D: 0.06468019 (Loss_D_real: 0.03155027 Loss_D_fake: 0.03312992) Loss_G: 0.17280369 Loss_Enh_Dec: -1.33613002\n",
      "| epoch  50 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1055.14 | loss  1.66 | ppl     5.27 | acc     0.80 | train_ae_norm     1.00\n",
      "[50/200][1099/2499] Loss_D: 0.06623541 (Loss_D_real: 0.03625936 Loss_D_fake: 0.02997605) Loss_G: 0.17677525 Loss_Enh_Dec: -1.45843315\n",
      "| epoch  50 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1056.88 | loss  1.72 | ppl     5.60 | acc     0.81 | train_ae_norm     1.00\n",
      "[50/200][1199/2499] Loss_D: 0.30006951 (Loss_D_real: 0.11016554 Loss_D_fake: 0.18990396) Loss_G: 0.11979129 Loss_Enh_Dec: -1.26227760\n",
      "| epoch  50 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1057.13 | loss  1.70 | ppl     5.48 | acc     0.76 | train_ae_norm     1.00\n",
      "[50/200][1299/2499] Loss_D: 0.16064999 (Loss_D_real: 0.10723408 Loss_D_fake: 0.05341591) Loss_G: 0.14863782 Loss_Enh_Dec: -0.74737209\n",
      "| epoch  50 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1056.13 | loss  1.63 | ppl     5.11 | acc     0.77 | train_ae_norm     1.00\n",
      "[50/200][1399/2499] Loss_D: 0.10614722 (Loss_D_real: 0.04109193 Loss_D_fake: 0.06505529) Loss_G: 0.17398873 Loss_Enh_Dec: -0.94764072\n",
      "| epoch  50 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1056.93 | loss  1.59 | ppl     4.90 | acc     0.83 | train_ae_norm     1.00\n",
      "[50/200][1499/2499] Loss_D: 0.11729277 (Loss_D_real: 0.08987689 Loss_D_fake: 0.02741588) Loss_G: 0.22120045 Loss_Enh_Dec: -1.10708869\n",
      "| epoch  50 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1057.09 | loss  1.59 | ppl     4.90 | acc     0.77 | train_ae_norm     1.00\n",
      "[50/200][1599/2499] Loss_D: 0.05098589 (Loss_D_real: 0.01919239 Loss_D_fake: 0.03179350) Loss_G: 0.17376994 Loss_Enh_Dec: -0.91455233\n",
      "| epoch  50 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1057.30 | loss  1.60 | ppl     4.95 | acc     0.82 | train_ae_norm     1.00\n",
      "[50/200][1699/2499] Loss_D: 0.11009919 (Loss_D_real: 0.08578148 Loss_D_fake: 0.02431770) Loss_G: 0.21015413 Loss_Enh_Dec: -1.26623833\n",
      "| epoch  50 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1055.72 | loss  1.52 | ppl     4.59 | acc     0.85 | train_ae_norm     1.00\n",
      "[50/200][1799/2499] Loss_D: 0.23076095 (Loss_D_real: 0.20194542 Loss_D_fake: 0.02881553) Loss_G: 0.21886745 Loss_Enh_Dec: -1.32052267\n",
      "| epoch  50 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1056.23 | loss  1.61 | ppl     5.01 | acc     0.85 | train_ae_norm     1.00\n",
      "[50/200][1899/2499] Loss_D: 0.04605848 (Loss_D_real: 0.02814013 Loss_D_fake: 0.01791835) Loss_G: 0.22454651 Loss_Enh_Dec: -1.25589025\n",
      "| epoch  50 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1056.92 | loss  1.73 | ppl     5.63 | acc     0.75 | train_ae_norm     1.00\n",
      "[50/200][1999/2499] Loss_D: 0.07749826 (Loss_D_real: 0.01033777 Loss_D_fake: 0.06716049) Loss_G: 0.32374245 Loss_Enh_Dec: -1.07433450\n",
      "| epoch  50 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1056.11 | loss  1.97 | ppl     7.17 | acc     0.76 | train_ae_norm     1.00\n",
      "[50/200][2099/2499] Loss_D: 0.06964961 (Loss_D_real: 0.02478548 Loss_D_fake: 0.04486413) Loss_G: 0.18118204 Loss_Enh_Dec: -1.13904870\n",
      "| epoch  50 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1057.01 | loss  1.94 | ppl     6.97 | acc     0.79 | train_ae_norm     1.00\n",
      "[50/200][2199/2499] Loss_D: 0.03505329 (Loss_D_real: 0.01507650 Loss_D_fake: 0.01997679) Loss_G: 0.21358867 Loss_Enh_Dec: -1.30339313\n",
      "| epoch  50 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1055.97 | loss  1.72 | ppl     5.57 | acc     0.79 | train_ae_norm     1.00\n",
      "[50/200][2299/2499] Loss_D: 0.21921133 (Loss_D_real: 0.12867309 Loss_D_fake: 0.09053823) Loss_G: 0.19789648 Loss_Enh_Dec: -1.20957792\n",
      "| epoch  50 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1056.15 | loss  1.88 | ppl     6.55 | acc     0.75 | train_ae_norm     1.00\n",
      "[50/200][2399/2499] Loss_D: 0.04753532 (Loss_D_real: 0.03403608 Loss_D_fake: 0.01349923) Loss_G: 0.25045374 Loss_Enh_Dec: -1.15203786\n",
      "| epoch  50 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1057.77 | loss  2.09 | ppl     8.09 | acc     0.68 | train_ae_norm     1.00\n",
      "[50/200][2499/2499] Loss_D: 0.04097120 (Loss_D_real: 0.02711641 Loss_D_fake: 0.01385479) Loss_G: 0.24592228 Loss_Enh_Dec: -1.17627060\n",
      "| end of epoch  50 | time: 2821.66s | test loss  1.12 | test ppl  3.07 | acc 0.883\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 51 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:49.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:56.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   110  of    230.    Elapsed: 0:01:17.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:24.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:31.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:38.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:45.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:52.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:59.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:06.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:13.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:21.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:28.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:35.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.708\n",
      "  Training epcoh took: 0:02:41\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.490\n",
      "  Test Loss: 3.514\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  51 |     0/ 2499 batches | lr 0.000000 | ms/batch 1617.20 | loss  0.02 | ppl     1.02 | acc     0.78 | train_ae_norm     1.00\n",
      "[51/200][99/2499] Loss_D: 0.04252191 (Loss_D_real: 0.02796833 Loss_D_fake: 0.01455358) Loss_G: 0.24351583 Loss_Enh_Dec: -1.33083987\n",
      "| epoch  51 |   100/ 2499 batches | lr 0.000000 | ms/batch 1057.36 | loss  1.79 | ppl     5.98 | acc     0.73 | train_ae_norm     1.00\n",
      "[51/200][199/2499] Loss_D: 0.03051644 (Loss_D_real: 0.00946968 Loss_D_fake: 0.02104676) Loss_G: 0.25103387 Loss_Enh_Dec: -1.05330086\n",
      "| epoch  51 |   200/ 2499 batches | lr 0.000000 | ms/batch 1056.50 | loss  1.60 | ppl     4.98 | acc     0.82 | train_ae_norm     1.00\n",
      "[51/200][299/2499] Loss_D: 0.08006826 (Loss_D_real: 0.03728959 Loss_D_fake: 0.04277867) Loss_G: 0.15660390 Loss_Enh_Dec: -1.17522192\n",
      "| epoch  51 |   300/ 2499 batches | lr 0.000000 | ms/batch 1057.50 | loss  1.58 | ppl     4.86 | acc     0.78 | train_ae_norm     1.00\n",
      "[51/200][399/2499] Loss_D: 0.03585659 (Loss_D_real: 0.02745305 Loss_D_fake: 0.00840354) Loss_G: 0.25571892 Loss_Enh_Dec: -1.37330425\n",
      "| epoch  51 |   400/ 2499 batches | lr 0.000000 | ms/batch 1056.45 | loss  1.56 | ppl     4.78 | acc     0.82 | train_ae_norm     1.00\n",
      "[51/200][499/2499] Loss_D: 0.05935571 (Loss_D_real: 0.01603906 Loss_D_fake: 0.04331664) Loss_G: 0.15275095 Loss_Enh_Dec: -1.25960720\n",
      "| epoch  51 |   500/ 2499 batches | lr 0.000000 | ms/batch 1057.82 | loss  1.63 | ppl     5.12 | acc     0.77 | train_ae_norm     1.00\n",
      "[51/200][599/2499] Loss_D: 0.05251178 (Loss_D_real: 0.02347083 Loss_D_fake: 0.02904095) Loss_G: 0.21507131 Loss_Enh_Dec: -1.47215581\n",
      "| epoch  51 |   600/ 2499 batches | lr 0.000000 | ms/batch 1056.05 | loss  1.67 | ppl     5.32 | acc     0.81 | train_ae_norm     1.00\n",
      "[51/200][699/2499] Loss_D: 0.04357169 (Loss_D_real: 0.02018122 Loss_D_fake: 0.02339047) Loss_G: 0.23116243 Loss_Enh_Dec: -1.38446915\n",
      "| epoch  51 |   700/ 2499 batches | lr 0.000000 | ms/batch 1056.60 | loss  1.67 | ppl     5.32 | acc     0.76 | train_ae_norm     1.00\n",
      "[51/200][799/2499] Loss_D: 0.04193836 (Loss_D_real: 0.01522847 Loss_D_fake: 0.02670988) Loss_G: 0.21295667 Loss_Enh_Dec: -1.30475497\n",
      "| epoch  51 |   800/ 2499 batches | lr 0.000000 | ms/batch 1055.92 | loss  1.68 | ppl     5.39 | acc     0.77 | train_ae_norm     1.00\n",
      "[51/200][899/2499] Loss_D: 1.65712404 (Loss_D_real: 0.03199840 Loss_D_fake: 1.62512565) Loss_G: 0.14941996 Loss_Enh_Dec: -1.25606084\n",
      "| epoch  51 |   900/ 2499 batches | lr 0.000000 | ms/batch 1057.26 | loss  1.63 | ppl     5.09 | acc     0.83 | train_ae_norm     1.00\n",
      "[51/200][999/2499] Loss_D: 0.05304728 (Loss_D_real: 0.04564562 Loss_D_fake: 0.00740166) Loss_G: 0.25613254 Loss_Enh_Dec: -1.25836170\n",
      "| epoch  51 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1056.81 | loss  1.58 | ppl     4.83 | acc     0.83 | train_ae_norm     1.00\n",
      "[51/200][1099/2499] Loss_D: 0.14613554 (Loss_D_real: 0.06175468 Loss_D_fake: 0.08438085) Loss_G: 0.16443320 Loss_Enh_Dec: -1.26778901\n",
      "| epoch  51 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1056.85 | loss  1.64 | ppl     5.13 | acc     0.82 | train_ae_norm     1.00\n",
      "[51/200][1199/2499] Loss_D: 0.11965194 (Loss_D_real: 0.09267643 Loss_D_fake: 0.02697551) Loss_G: 0.17353837 Loss_Enh_Dec: -1.16087627\n",
      "| epoch  51 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1057.82 | loss  1.70 | ppl     5.48 | acc     0.73 | train_ae_norm     1.00\n",
      "[51/200][1299/2499] Loss_D: 0.03242234 (Loss_D_real: 0.02091880 Loss_D_fake: 0.01150354) Loss_G: 0.22683160 Loss_Enh_Dec: -1.21111131\n",
      "| epoch  51 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1057.95 | loss  1.68 | ppl     5.36 | acc     0.77 | train_ae_norm     1.00\n",
      "[51/200][1399/2499] Loss_D: 0.03973802 (Loss_D_real: 0.02994260 Loss_D_fake: 0.00979542) Loss_G: 0.24643055 Loss_Enh_Dec: -1.29494810\n",
      "| epoch  51 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1056.98 | loss  1.59 | ppl     4.89 | acc     0.83 | train_ae_norm     1.00\n",
      "[51/200][1499/2499] Loss_D: 0.09889257 (Loss_D_real: 0.08197770 Loss_D_fake: 0.01691486) Loss_G: 0.21608721 Loss_Enh_Dec: -1.11304116\n",
      "| epoch  51 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1057.49 | loss  1.66 | ppl     5.24 | acc     0.75 | train_ae_norm     1.00\n",
      "[51/200][1599/2499] Loss_D: 0.04675525 (Loss_D_real: 0.03474538 Loss_D_fake: 0.01200988) Loss_G: 0.22530524 Loss_Enh_Dec: -1.09182990\n",
      "| epoch  51 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1057.11 | loss  1.81 | ppl     6.11 | acc     0.77 | train_ae_norm     1.00\n",
      "[51/200][1699/2499] Loss_D: 0.06636201 (Loss_D_real: 0.05197443 Loss_D_fake: 0.01438758) Loss_G: 0.22568405 Loss_Enh_Dec: -1.32818449\n",
      "| epoch  51 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1056.32 | loss  1.77 | ppl     5.88 | acc     0.77 | train_ae_norm     1.00\n",
      "[51/200][1799/2499] Loss_D: 0.13886541 (Loss_D_real: 0.08568566 Loss_D_fake: 0.05317976) Loss_G: 0.16582969 Loss_Enh_Dec: -1.06594765\n",
      "| epoch  51 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1058.05 | loss  1.79 | ppl     6.00 | acc     0.81 | train_ae_norm     1.00\n",
      "[51/200][1899/2499] Loss_D: 0.05722414 (Loss_D_real: 0.04519824 Loss_D_fake: 0.01202590) Loss_G: 0.25424355 Loss_Enh_Dec: -1.18957615\n",
      "| epoch  51 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1057.00 | loss  1.67 | ppl     5.33 | acc     0.76 | train_ae_norm     1.00\n",
      "[51/200][1999/2499] Loss_D: 0.08933130 (Loss_D_real: 0.05972188 Loss_D_fake: 0.02960942) Loss_G: 0.18134058 Loss_Enh_Dec: -0.77254027\n",
      "| epoch  51 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1057.36 | loss  1.63 | ppl     5.11 | acc     0.81 | train_ae_norm     1.00\n",
      "[51/200][2099/2499] Loss_D: 0.06934239 (Loss_D_real: 0.04296420 Loss_D_fake: 0.02637819) Loss_G: 0.20238064 Loss_Enh_Dec: -1.16428220\n",
      "| epoch  51 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1057.77 | loss  1.66 | ppl     5.24 | acc     0.82 | train_ae_norm     1.00\n",
      "[51/200][2199/2499] Loss_D: 0.07471639 (Loss_D_real: 0.04458775 Loss_D_fake: 0.03012863) Loss_G: 0.17850764 Loss_Enh_Dec: -1.03224838\n",
      "| epoch  51 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1058.28 | loss  1.78 | ppl     5.90 | acc     0.81 | train_ae_norm     1.00\n",
      "[51/200][2299/2499] Loss_D: 0.04761310 (Loss_D_real: 0.02973196 Loss_D_fake: 0.01788115) Loss_G: 0.22390547 Loss_Enh_Dec: -1.10543096\n",
      "| epoch  51 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1058.72 | loss  1.79 | ppl     6.00 | acc     0.77 | train_ae_norm     1.00\n",
      "[51/200][2399/2499] Loss_D: 0.09654506 (Loss_D_real: 0.07802864 Loss_D_fake: 0.01851642) Loss_G: 0.20235364 Loss_Enh_Dec: -0.46384069\n",
      "| epoch  51 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1059.15 | loss  1.82 | ppl     6.20 | acc     0.75 | train_ae_norm     1.00\n",
      "[51/200][2499/2499] Loss_D: 0.12070891 (Loss_D_real: 0.08715318 Loss_D_fake: 0.03355573) Loss_G: 0.22170115 Loss_Enh_Dec: -1.05584860\n",
      "| end of epoch  51 | time: 2824.05s | test loss  1.05 | test ppl  2.86 | acc 0.893\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 52 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:49.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:56.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:10.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:17.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:24.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   130  of    230.    Elapsed: 0:01:31.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:38.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:45.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:52.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:59.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:06.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:13.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:20.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:27.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:35.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.708\n",
      "  Training epcoh took: 0:02:41\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.485\n",
      "  Test Loss: 3.622\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  52 |     0/ 2499 batches | lr 0.000000 | ms/batch 1616.91 | loss  0.02 | ppl     1.02 | acc     0.76 | train_ae_norm     1.00\n",
      "[52/200][99/2499] Loss_D: 0.03820732 (Loss_D_real: 0.02584602 Loss_D_fake: 0.01236130) Loss_G: 0.23339820 Loss_Enh_Dec: -1.07461226\n",
      "| epoch  52 |   100/ 2499 batches | lr 0.000000 | ms/batch 1057.51 | loss  2.25 | ppl     9.46 | acc     0.69 | train_ae_norm     1.00\n",
      "[52/200][199/2499] Loss_D: 0.08317813 (Loss_D_real: 0.01724961 Loss_D_fake: 0.06592851) Loss_G: 0.19383949 Loss_Enh_Dec: -0.99227542\n",
      "| epoch  52 |   200/ 2499 batches | lr 0.000000 | ms/batch 1056.77 | loss  2.40 | ppl    11.04 | acc     0.69 | train_ae_norm     1.00\n",
      "[52/200][299/2499] Loss_D: 0.08006813 (Loss_D_real: 0.02692184 Loss_D_fake: 0.05314628) Loss_G: 0.18532841 Loss_Enh_Dec: -1.34341204\n",
      "| epoch  52 |   300/ 2499 batches | lr 0.000000 | ms/batch 1057.00 | loss  2.19 | ppl     8.89 | acc     0.72 | train_ae_norm     1.00\n",
      "[52/200][399/2499] Loss_D: 0.13191560 (Loss_D_real: 0.08963433 Loss_D_fake: 0.04228126) Loss_G: 0.15838499 Loss_Enh_Dec: -1.18527484\n",
      "| epoch  52 |   400/ 2499 batches | lr 0.000000 | ms/batch 1057.73 | loss  1.86 | ppl     6.45 | acc     0.79 | train_ae_norm     1.00\n",
      "[52/200][499/2499] Loss_D: 0.12364238 (Loss_D_real: 0.08523913 Loss_D_fake: 0.03840324) Loss_G: 0.16311975 Loss_Enh_Dec: -1.27964818\n",
      "| epoch  52 |   500/ 2499 batches | lr 0.000000 | ms/batch 1057.23 | loss  1.67 | ppl     5.34 | acc     0.80 | train_ae_norm     1.00\n",
      "[52/200][599/2499] Loss_D: 0.05748922 (Loss_D_real: 0.02542168 Loss_D_fake: 0.03206754) Loss_G: 0.16817196 Loss_Enh_Dec: -0.99003261\n",
      "| epoch  52 |   600/ 2499 batches | lr 0.000000 | ms/batch 1056.33 | loss  1.68 | ppl     5.37 | acc     0.78 | train_ae_norm     1.00\n",
      "[52/200][699/2499] Loss_D: 0.04739745 (Loss_D_real: 0.02281718 Loss_D_fake: 0.02458027) Loss_G: 0.17018166 Loss_Enh_Dec: -1.29766309\n",
      "| epoch  52 |   700/ 2499 batches | lr 0.000000 | ms/batch 1057.93 | loss  1.76 | ppl     5.83 | acc     0.73 | train_ae_norm     1.00\n",
      "[52/200][799/2499] Loss_D: 0.08879365 (Loss_D_real: 0.05888480 Loss_D_fake: 0.02990885) Loss_G: 0.17509922 Loss_Enh_Dec: -1.24299133\n",
      "| epoch  52 |   800/ 2499 batches | lr 0.000000 | ms/batch 1056.84 | loss  1.83 | ppl     6.25 | acc     0.78 | train_ae_norm     1.00\n",
      "[52/200][899/2499] Loss_D: 0.04731159 (Loss_D_real: 0.01652672 Loss_D_fake: 0.03078487) Loss_G: 0.17700857 Loss_Enh_Dec: -1.27120888\n",
      "| epoch  52 |   900/ 2499 batches | lr 0.000000 | ms/batch 1056.79 | loss  1.67 | ppl     5.32 | acc     0.80 | train_ae_norm     1.00\n",
      "[52/200][999/2499] Loss_D: 0.04361383 (Loss_D_real: 0.01857295 Loss_D_fake: 0.02504088) Loss_G: 0.18392657 Loss_Enh_Dec: -0.98852426\n",
      "| epoch  52 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1058.15 | loss  1.75 | ppl     5.75 | acc     0.80 | train_ae_norm     1.00\n",
      "[52/200][1099/2499] Loss_D: 0.03991904 (Loss_D_real: 0.00960775 Loss_D_fake: 0.03031129) Loss_G: 0.20479731 Loss_Enh_Dec: -1.41642129\n",
      "| epoch  52 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1056.49 | loss  1.69 | ppl     5.44 | acc     0.81 | train_ae_norm     1.00\n",
      "[52/200][1199/2499] Loss_D: 0.03855076 (Loss_D_real: 0.01670860 Loss_D_fake: 0.02184215) Loss_G: 0.18901132 Loss_Enh_Dec: -1.42707622\n",
      "| epoch  52 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1057.44 | loss  1.64 | ppl     5.15 | acc     0.76 | train_ae_norm     1.00\n",
      "[52/200][1299/2499] Loss_D: 0.09692428 (Loss_D_real: 0.07584087 Loss_D_fake: 0.02108341) Loss_G: 0.19531950 Loss_Enh_Dec: -1.36871815\n",
      "| epoch  52 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1056.61 | loss  1.61 | ppl     4.99 | acc     0.78 | train_ae_norm     1.00\n",
      "[52/200][1399/2499] Loss_D: 0.02783382 (Loss_D_real: 0.00902722 Loss_D_fake: 0.01880660) Loss_G: 0.21668175 Loss_Enh_Dec: -1.44356537\n",
      "| epoch  52 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1056.51 | loss  1.72 | ppl     5.57 | acc     0.80 | train_ae_norm     1.00\n",
      "[52/200][1499/2499] Loss_D: 0.12859038 (Loss_D_real: 0.09333919 Loss_D_fake: 0.03525119) Loss_G: 0.21020043 Loss_Enh_Dec: -1.18515420\n",
      "| epoch  52 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1058.33 | loss  1.76 | ppl     5.81 | acc     0.78 | train_ae_norm     1.00\n",
      "[52/200][1599/2499] Loss_D: 0.04263828 (Loss_D_real: 0.02881408 Loss_D_fake: 0.01382420) Loss_G: 0.22307964 Loss_Enh_Dec: -1.42280900\n",
      "| epoch  52 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1058.06 | loss  1.73 | ppl     5.63 | acc     0.78 | train_ae_norm     1.00\n",
      "[52/200][1699/2499] Loss_D: 0.11545183 (Loss_D_real: 0.05155520 Loss_D_fake: 0.06389663) Loss_G: 0.17512457 Loss_Enh_Dec: -0.84615725\n",
      "| epoch  52 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1056.62 | loss  1.63 | ppl     5.13 | acc     0.80 | train_ae_norm     1.00\n",
      "[52/200][1799/2499] Loss_D: 0.12524992 (Loss_D_real: 0.09761536 Loss_D_fake: 0.02763456) Loss_G: 0.20071898 Loss_Enh_Dec: -1.10631716\n",
      "| epoch  52 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1058.25 | loss  1.73 | ppl     5.67 | acc     0.81 | train_ae_norm     1.00\n",
      "[52/200][1899/2499] Loss_D: 0.04384520 (Loss_D_real: 0.01458940 Loss_D_fake: 0.02925579) Loss_G: 0.24380676 Loss_Enh_Dec: -0.62031347\n",
      "| epoch  52 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1056.99 | loss  1.69 | ppl     5.44 | acc     0.73 | train_ae_norm     1.00\n",
      "[52/200][1999/2499] Loss_D: 0.04984764 (Loss_D_real: 0.03684786 Loss_D_fake: 0.01299978) Loss_G: 0.24007832 Loss_Enh_Dec: -0.89165348\n",
      "| epoch  52 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1057.82 | loss  1.90 | ppl     6.71 | acc     0.80 | train_ae_norm     1.00\n",
      "[52/200][2099/2499] Loss_D: 0.12362969 (Loss_D_real: 0.06615174 Loss_D_fake: 0.05747795) Loss_G: 0.21384285 Loss_Enh_Dec: -0.73024637\n",
      "| epoch  52 |  2100/ 2499 batches | lr 0.000000 | ms/batch 1058.00 | loss  2.07 | ppl     7.90 | acc     0.74 | train_ae_norm     1.00\n",
      "[52/200][2199/2499] Loss_D: 0.06680395 (Loss_D_real: 0.03971078 Loss_D_fake: 0.02709317) Loss_G: 0.22469668 Loss_Enh_Dec: -0.71188867\n",
      "| epoch  52 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1058.42 | loss  2.01 | ppl     7.43 | acc     0.77 | train_ae_norm     1.00\n",
      "[52/200][2299/2499] Loss_D: 0.04392584 (Loss_D_real: 0.02250307 Loss_D_fake: 0.02142276) Loss_G: 0.20207749 Loss_Enh_Dec: -0.84853840\n",
      "| epoch  52 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1057.14 | loss  1.91 | ppl     6.76 | acc     0.76 | train_ae_norm     1.00\n",
      "[52/200][2399/2499] Loss_D: 0.05765924 (Loss_D_real: 0.03686808 Loss_D_fake: 0.02079116) Loss_G: 0.20747867 Loss_Enh_Dec: -0.54773217\n",
      "| epoch  52 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1058.56 | loss  2.21 | ppl     9.10 | acc     0.73 | train_ae_norm     1.00\n",
      "[52/200][2499/2499] Loss_D: 0.03127995 (Loss_D_real: 0.01902147 Loss_D_fake: 0.01225847) Loss_G: 0.24716102 Loss_Enh_Dec: -0.85708773\n",
      "| end of epoch  52 | time: 2824.17s | test loss  1.12 | test ppl  3.07 | acc 0.881\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 53 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:49.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:56.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:10.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:17.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:24.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:31.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:38.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   150  of    230.    Elapsed: 0:01:45.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:52.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:59.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:06.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:13.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:20.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:27.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:34.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.706\n",
      "  Training epcoh took: 0:02:41\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.480\n",
      "  Test Loss: 3.669\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  53 |     0/ 2499 batches | lr 0.000000 | ms/batch 1612.98 | loss  0.02 | ppl     1.02 | acc     0.79 | train_ae_norm     1.00\n",
      "[53/200][99/2499] Loss_D: 0.07472822 (Loss_D_real: 0.05710062 Loss_D_fake: 0.01762759) Loss_G: 0.19569004 Loss_Enh_Dec: -0.93542117\n",
      "| epoch  53 |   100/ 2499 batches | lr 0.000000 | ms/batch 1057.07 | loss  1.81 | ppl     6.14 | acc     0.77 | train_ae_norm     1.00\n",
      "[53/200][199/2499] Loss_D: 0.11384767 (Loss_D_real: 0.07962738 Loss_D_fake: 0.03422029) Loss_G: 0.15266471 Loss_Enh_Dec: -0.38658175\n",
      "| epoch  53 |   200/ 2499 batches | lr 0.000000 | ms/batch 1056.69 | loss  1.91 | ppl     6.76 | acc     0.77 | train_ae_norm     1.00\n",
      "[53/200][299/2499] Loss_D: 0.84401059 (Loss_D_real: 0.03764156 Loss_D_fake: 0.80636901) Loss_G: 0.17751805 Loss_Enh_Dec: -1.00235844\n",
      "| epoch  53 |   300/ 2499 batches | lr 0.000000 | ms/batch 1057.85 | loss  2.18 | ppl     8.88 | acc     0.67 | train_ae_norm     1.00\n",
      "[53/200][399/2499] Loss_D: 0.05466665 (Loss_D_real: 0.03991565 Loss_D_fake: 0.01475100) Loss_G: 0.23961969 Loss_Enh_Dec: -0.92131275\n",
      "| epoch  53 |   400/ 2499 batches | lr 0.000000 | ms/batch 1056.96 | loss  2.37 | ppl    10.65 | acc     0.75 | train_ae_norm     1.00\n",
      "[53/200][499/2499] Loss_D: 0.11365095 (Loss_D_real: 0.04351352 Loss_D_fake: 0.07013743) Loss_G: 0.17199692 Loss_Enh_Dec: -0.91176480\n",
      "| epoch  53 |   500/ 2499 batches | lr 0.000000 | ms/batch 1057.93 | loss  2.28 | ppl     9.79 | acc     0.61 | train_ae_norm     1.00\n",
      "[53/200][599/2499] Loss_D: 0.05553393 (Loss_D_real: 0.02759389 Loss_D_fake: 0.02794005) Loss_G: 0.21572705 Loss_Enh_Dec: -0.90201229\n",
      "| epoch  53 |   600/ 2499 batches | lr 0.000000 | ms/batch 1058.46 | loss  2.55 | ppl    12.83 | acc     0.63 | train_ae_norm     1.00\n",
      "[53/200][699/2499] Loss_D: 0.05526116 (Loss_D_real: 0.03271957 Loss_D_fake: 0.02254159) Loss_G: 0.18867353 Loss_Enh_Dec: -0.58215028\n",
      "| epoch  53 |   700/ 2499 batches | lr 0.000000 | ms/batch 1056.53 | loss  2.71 | ppl    14.95 | acc     0.61 | train_ae_norm     1.00\n",
      "[53/200][799/2499] Loss_D: 0.08050492 (Loss_D_real: 0.05095714 Loss_D_fake: 0.02954778) Loss_G: 0.20823431 Loss_Enh_Dec: -0.52168047\n",
      "| epoch  53 |   800/ 2499 batches | lr 0.000000 | ms/batch 1057.19 | loss  3.15 | ppl    23.30 | acc     0.49 | train_ae_norm     1.00\n",
      "[53/200][899/2499] Loss_D: 0.05897257 (Loss_D_real: 0.04390551 Loss_D_fake: 0.01506707) Loss_G: 0.22341786 Loss_Enh_Dec: -0.63728136\n",
      "| epoch  53 |   900/ 2499 batches | lr 0.000000 | ms/batch 1057.06 | loss  3.52 | ppl    33.75 | acc     0.53 | train_ae_norm     1.00\n",
      "[53/200][999/2499] Loss_D: 0.18072674 (Loss_D_real: 0.15565722 Loss_D_fake: 0.02506951) Loss_G: 0.20935719 Loss_Enh_Dec: -0.27072752\n",
      "| epoch  53 |  1000/ 2499 batches | lr 0.000000 | ms/batch 1057.12 | loss  3.63 | ppl    37.68 | acc     0.52 | train_ae_norm     1.00\n",
      "[53/200][1099/2499] Loss_D: 0.16817316 (Loss_D_real: 0.09820934 Loss_D_fake: 0.06996381) Loss_G: 0.19777469 Loss_Enh_Dec: -0.42788935\n",
      "| epoch  53 |  1100/ 2499 batches | lr 0.000000 | ms/batch 1056.59 | loss  3.79 | ppl    44.14 | acc     0.48 | train_ae_norm     1.00\n",
      "[53/200][1199/2499] Loss_D: 0.05882454 (Loss_D_real: 0.01915881 Loss_D_fake: 0.03966573) Loss_G: 0.21326745 Loss_Enh_Dec: -0.51529151\n",
      "| epoch  53 |  1200/ 2499 batches | lr 0.000000 | ms/batch 1056.80 | loss  3.70 | ppl    40.51 | acc     0.49 | train_ae_norm     1.00\n",
      "[53/200][1299/2499] Loss_D: 0.05844180 (Loss_D_real: 0.03825087 Loss_D_fake: 0.02019093) Loss_G: 0.24238634 Loss_Enh_Dec: -0.85533220\n",
      "| epoch  53 |  1300/ 2499 batches | lr 0.000000 | ms/batch 1056.70 | loss  3.21 | ppl    24.87 | acc     0.58 | train_ae_norm     1.00\n",
      "[53/200][1399/2499] Loss_D: 0.11210907 (Loss_D_real: 0.08994072 Loss_D_fake: 0.02216835) Loss_G: 0.22615896 Loss_Enh_Dec: -0.95751792\n",
      "| epoch  53 |  1400/ 2499 batches | lr 0.000000 | ms/batch 1057.84 | loss  2.64 | ppl    13.98 | acc     0.72 | train_ae_norm     1.00\n",
      "[53/200][1499/2499] Loss_D: 0.10075781 (Loss_D_real: 0.04732823 Loss_D_fake: 0.05342957) Loss_G: 0.19492924 Loss_Enh_Dec: -0.79915804\n",
      "| epoch  53 |  1500/ 2499 batches | lr 0.000000 | ms/batch 1057.38 | loss  2.33 | ppl    10.31 | acc     0.67 | train_ae_norm     1.00\n",
      "[53/200][1599/2499] Loss_D: 0.03178724 (Loss_D_real: 0.01969830 Loss_D_fake: 0.01208894) Loss_G: 0.23730163 Loss_Enh_Dec: -0.78410387\n",
      "| epoch  53 |  1600/ 2499 batches | lr 0.000000 | ms/batch 1058.13 | loss  2.25 | ppl     9.45 | acc     0.69 | train_ae_norm     1.00\n",
      "[53/200][1699/2499] Loss_D: 0.40135643 (Loss_D_real: 0.24875009 Loss_D_fake: 0.15260634) Loss_G: 0.11754888 Loss_Enh_Dec: -0.87649769\n",
      "| epoch  53 |  1700/ 2499 batches | lr 0.000000 | ms/batch 1057.48 | loss  2.05 | ppl     7.79 | acc     0.75 | train_ae_norm     1.00\n",
      "[53/200][1799/2499] Loss_D: 0.03120930 (Loss_D_real: 0.01400238 Loss_D_fake: 0.01720692) Loss_G: 0.21170144 Loss_Enh_Dec: -0.64962643\n",
      "| epoch  53 |  1800/ 2499 batches | lr 0.000000 | ms/batch 1058.72 | loss  1.98 | ppl     7.22 | acc     0.78 | train_ae_norm     1.00\n",
      "[53/200][1899/2499] Loss_D: 0.04565131 (Loss_D_real: 0.03154551 Loss_D_fake: 0.01410580) Loss_G: 0.22342694 Loss_Enh_Dec: -0.62486309\n",
      "| epoch  53 |  1900/ 2499 batches | lr 0.000000 | ms/batch 1058.26 | loss  2.01 | ppl     7.44 | acc     0.74 | train_ae_norm     1.00\n",
      "[53/200][1999/2499] Loss_D: 0.16256261 (Loss_D_real: 0.09507167 Loss_D_fake: 0.06749094) Loss_G: 0.19260667 Loss_Enh_Dec: -0.48919210\n",
      "| epoch  53 |  2000/ 2499 batches | lr 0.000000 | ms/batch 1057.79 | loss  1.99 | ppl     7.31 | acc     0.74 | train_ae_norm     1.00\n",
      "[53/200][2199/2499] Loss_D: 0.06035388 (Loss_D_real: 0.03003863 Loss_D_fake: 0.03031525) Loss_G: 0.21805167 Loss_Enh_Dec: -0.85276949\n",
      "| epoch  53 |  2200/ 2499 batches | lr 0.000000 | ms/batch 1057.91 | loss  1.91 | ppl     6.74 | acc     0.82 | train_ae_norm     1.00\n",
      "[53/200][2299/2499] Loss_D: 0.02033283 (Loss_D_real: 0.00866379 Loss_D_fake: 0.01166903) Loss_G: 0.24662553 Loss_Enh_Dec: -1.05242002\n",
      "| epoch  53 |  2300/ 2499 batches | lr 0.000000 | ms/batch 1056.04 | loss  1.93 | ppl     6.89 | acc     0.75 | train_ae_norm     1.00\n",
      "[53/200][2399/2499] Loss_D: 0.03149456 (Loss_D_real: 0.01601988 Loss_D_fake: 0.01547468) Loss_G: 0.22453068 Loss_Enh_Dec: -0.95895666\n",
      "| epoch  53 |  2400/ 2499 batches | lr 0.000000 | ms/batch 1056.83 | loss  1.98 | ppl     7.27 | acc     0.72 | train_ae_norm     1.00\n",
      "[53/200][2499/2499] Loss_D: 0.06045505 (Loss_D_real: 0.03690311 Loss_D_fake: 0.02355193) Loss_G: 0.23070660 Loss_Enh_Dec: -0.54915160\n",
      "| end of epoch  53 | time: 2823.52s | test loss  1.28 | test ppl  3.61 | acc 0.865\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 54 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:14.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:21.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:28.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:35.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:42.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:49.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:56.\n",
      "  Batch    90  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   100  of    230.    Elapsed: 0:01:10.\n",
      "  Batch   110  of    230.    Elapsed: 0:01:18.\n",
      "  Batch   120  of    230.    Elapsed: 0:01:25.\n",
      "  Batch   130  of    230.    Elapsed: 0:01:32.\n",
      "  Batch   140  of    230.    Elapsed: 0:01:39.\n",
      "  Batch   150  of    230.    Elapsed: 0:01:46.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:53.\n",
      "  Batch   170  of    230.    Elapsed: 0:02:00.\n",
      "  Batch   180  of    230.    Elapsed: 0:02:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:02:14.\n",
      "  Batch   200  of    230.    Elapsed: 0:02:21.\n",
      "  Batch   210  of    230.    Elapsed: 0:02:28.\n",
      "  Batch   220  of    230.    Elapsed: 0:02:35.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.706\n",
      "  Training epcoh took: 0:02:42\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.485\n",
      "  Test Loss: 3.737\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  54 |     0/ 2499 batches | lr 0.000000 | ms/batch 1621.46 | loss  0.02 | ppl     1.02 | acc     0.72 | train_ae_norm     1.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-c2292324a930>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    267\u001b[0m                     \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# end of epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 total_loss_ae, start_time = train_ae(epoch, train_data[niter],\n\u001b[0;32m--> 269\u001b[0;31m                                 total_loss_ae, start_time, niter)\n\u001b[0m\u001b[1;32m    270\u001b[0m                 \u001b[0mniter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0;31m# train gan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-1e06c93669b5>\u001b[0m in \u001b[0;36mtrain_ae\u001b[0;34m(epoch, batch, total_loss_ae, start_time, i)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmasked_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtrain_ae_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcal_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d879385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f112a591a20>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accuracy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "91448c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.54\n"
     ]
    }
   ],
   "source": [
    "print(max(accuracy_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "54d6680d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.485\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_array[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7fdef8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_save = pd.DataFrame(accuracy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d4e624b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_save.to_csv('accuracy_array_harrygan_E.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa94da89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
