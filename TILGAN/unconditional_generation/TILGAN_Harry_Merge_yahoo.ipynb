{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "673b1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils import to_gpu, train_ngram_lm, get_ppl, create_exp_dir, Dictionary, length_sort\n",
    "from models import Seq2Seq, MLP_D, MLP_D_local, MLP_G\n",
    "from bleu_self import *\n",
    "from bleu_test import *\n",
    "import datetime\n",
    "now_time = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f208b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='TILGAN for unconditional generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c204c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import *\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "520324a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=4\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "228c8e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path=\"data/MS_COCO\"\n",
    "data_path=\"../../yahoo/unlabelled_small\"\n",
    "save=\"./results/yahoo_merge_assigned_results\"\n",
    "maxlen=16\n",
    "batch_size=48\n",
    "eval_batch_size = 16\n",
    "noise_seq_length = 15\n",
    "add_noise=True #what does this do? - question applies to most parameters\n",
    "emsize=512\n",
    "nhidden=512\n",
    "nlayers=2\n",
    "nheads=4\n",
    "nff=1024\n",
    "aehidden=56\n",
    "noise_r=0.05\n",
    "hidden_init=True\n",
    "dropout=0.3\n",
    "gpu=True\n",
    "z_size=100\n",
    "arch_g='300-300'\n",
    "gan_g_activation=True\n",
    "arch_d='300-300'\n",
    "gan_d_local=False\n",
    "gan_d_local_windowsize=3\n",
    "arch_d_local='300-300'\n",
    "lr_ae=0.12\n",
    "lr_gan_e=1e-04\n",
    "beta1=0.5\n",
    "lr_gan_g=4e-04\n",
    "lr_gan_d=1e-04\n",
    "epochs=200\n",
    "sample=True\n",
    "clip=1\n",
    "log_interval=100\n",
    "gan_lambda=0.1\n",
    "niters_gan_d=1\n",
    "niters_gan_g=1\n",
    "niters_gan_ae=1\n",
    "niters_gan_dec=1\n",
    "niters_gan_schedule=''\n",
    "niters_ae=1\n",
    "gan_type='kl'\n",
    "enhance_dec=True\n",
    "gan_gp_lambda=1\n",
    "vocab_size=0\n",
    "lowercase=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23c42b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=\"../../yahoo/yahoo_everything.csv\"\n",
    "\n",
    "label_list = [\"UNK\",1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "# df_yahoo = pd.read_csv(data)\n",
    "# #df_yahoo=df_yahoo.rename(columns = {\"Unnamed: 0\":'label'})\n",
    "# df_yahoo=df_yahoo.set_index(\"Unnamed: 0\")\n",
    "# df = df_yahoo.sample(frac=1)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71192455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbers=list(df['label'].unique())\n",
    "# list_zeros = [0]*len(numbers)\n",
    "# count_dictionary = dict(zip(numbers, list_zeros))\n",
    "\n",
    "# values_array_train_labelled=[]\n",
    "# values_array_test_labelled=[]\n",
    "# values_array_test_unlabelled=[]\n",
    "# values_array_train_unlabelled=[]\n",
    "# values_array_unlabelled=[]\n",
    "# data_all=[]\n",
    "# for index, row in df.iterrows():\n",
    "#     if count_dictionary[row['label']]<20:\n",
    "#         count_dictionary[row['label']]=count_dictionary[row['label']]+1\n",
    "#         values_array_train_labelled.append((row['question_1'],row['label']))\n",
    "#     elif count_dictionary[row['label']]<60:\n",
    "#         count_dictionary[row['label']]=count_dictionary[row['label']]+1\n",
    "#         values_array_test_labelled.append((row['question_1'],row['label']))\n",
    "#     elif count_dictionary[row['label']]<600:\n",
    "#         count_dictionary[row['label']]=count_dictionary[row['label']]+1\n",
    "#         values_array_unlabelled.append((row['question_1'],'UNK'))\n",
    "#     elif count_dictionary[row['label']]<1600:\n",
    "#         count_dictionary[row['label']]=count_dictionary[row['label']]+1\n",
    "#         values_array_test_unlabelled.append((row['question_1'],'UNK'))\n",
    "#     elif count_dictionary[row['label']]<7600:\n",
    "#         count_dictionary[row['label']]=count_dictionary[row['label']]+1\n",
    "#         values_array_train_unlabelled.append((row['question_1'],'UNK'))\n",
    "#     data_all.append(row['question_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0f8d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_l =  values_array_train_labelled\n",
    "# test_l = values_array_test_labelled\n",
    "# test_u = values_array_test_unlabelled\n",
    "# train_u = values_array_train_unlabelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaa6fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_l=pd.read_csv(\"../../yahoo/assigned/train_l.csv\", index_col=\"Unnamed: 0\")\n",
    "df_test_l=pd.read_csv(\"../../yahoo/assigned/test_l.csv\", index_col=\"Unnamed: 0\")\n",
    "df_u=pd.read_csv(\"../../yahoo/assigned/u.csv\", index_col=\"Unnamed: 0\")\n",
    "df_train_u=pd.read_csv(\"../../yahoo/assigned/train_u.csv\", index_col=\"Unnamed: 0\")#.head(30000)\n",
    "df_test_u=pd.read_csv(\"../../yahoo/assigned/test_u.csv\", index_col=\"Unnamed: 0\")#.head(5000)\n",
    "df_all = pd.concat([df_train_l, df_test_l, df_u, df_train_u, df_test_u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f42a99ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_l =  list(df_train_l.to_records(index=False))\n",
    "test_l = list(df_test_l.to_records(index=False))\n",
    "u_list = list(df_u.to_records(index=False))\n",
    "test_u = list(df_test_u.to_records(index=False))\n",
    "train_u = list(df_train_u.to_records(index=False))\n",
    "data_all = list(df_all[\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71d1175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, data_all, train_l, test_l, train_u, test_u, maxlen, vocab_size=11000, lowercase=False):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.maxlen = maxlen\n",
    "        self.lowercase = lowercase\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.data_all = data_all\n",
    "        self.train_l = train_l\n",
    "        self.test_l = test_l\n",
    "        self.train_u = train_u\n",
    "        self.test_u = test_u\n",
    "\n",
    "        # make the vocabulary from training set\n",
    "        self.make_vocab()\n",
    "        \n",
    "        self.train_l_tok = self.tokenize(self.train_l)\n",
    "        self.test_l_tok = self.tokenize(self.test_l)\n",
    "        self.train_u_tok = self.tokenize(self.train_u)\n",
    "        self.test_u_tok = self.tokenize(self.test_u)\n",
    "\n",
    "    def make_vocab(self):\n",
    "        # Add words to the dictionary\n",
    "        print(len(self.data_all))\n",
    "        print(self.data_all[0])\n",
    "        for line in self.data_all:\n",
    "            if self.lowercase:\n",
    "                # -1 to get rid of \\n character\n",
    "                words = line[:-1].lower().split(\" \")\n",
    "            else:\n",
    "                words = line[:-1].split(\" \")\n",
    "            for word in words:\n",
    "                self.dictionary.add_word(word)\n",
    "\n",
    "        # prune the vocabulary\n",
    "        self.dictionary.prune_vocab(k=self.vocab_size, cnt=True)\n",
    "\n",
    "    def tokenize(self, data):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        dropped = 0\n",
    "        #with open(path, 'r') as f:\n",
    "        linecount = 0\n",
    "        lines = []\n",
    "        for line, label in data:\n",
    "            linecount += 1\n",
    "            if self.lowercase:\n",
    "                words = line[:-1].lower().strip().split(\" \")\n",
    "            else:\n",
    "                words = line[:-1].strip().split(\" \")\n",
    "            if len(words) > self.maxlen:\n",
    "                dropped += 1\n",
    "                continue\n",
    "            words = ['<sos>'] + words\n",
    "            words += ['<eos>']\n",
    "            # vectorize\n",
    "            vocab = self.dictionary.word2idx\n",
    "            unk_idx = vocab['<oov>']\n",
    "            indices = [vocab[w] if w in vocab else unk_idx for w in words]\n",
    "            lines.append(indices)\n",
    "\n",
    "        print(\"Number of sentences dropped: {} out of {} total\".\n",
    "              format(dropped, linecount))\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24e5f2df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240001\n",
      "how has religion affected war?\n",
      "original vocab 93259; pruned to 93263\n",
      "Number of sentences dropped: 0 out of 200 total\n",
      "Number of sentences dropped: 1 out of 400 total\n",
      "Number of sentences dropped: 39 out of 209401 total\n",
      "Number of sentences dropped: 1 out of 20000 total\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(data_all = data_all,\n",
    "                train_l=train_l,\n",
    "                test_l=test_l,\n",
    "                train_u=train_u,\n",
    "                test_u=test_u,\n",
    "                maxlen=maxlen,\n",
    "                vocab_size=vocab_size,\n",
    "                lowercase=lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daa4ee98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 93263\n"
     ]
    }
   ],
   "source": [
    "# save arguments\n",
    "ntokens = len(corpus.dictionary.word2idx)\n",
    "print(\"Vocabulary Size: {}\".format(ntokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f66f2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : ./results/yahoo_merge_assigned_results\n"
     ]
    }
   ],
   "source": [
    "# exp dir\n",
    "create_exp_dir(os.path.join(save), ['train.py', 'models.py', 'utils.py'],\n",
    "        dict=corpus.dictionary.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a05112cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging(str, to_stdout=True):\n",
    "    with open(os.path.join(save, 'log.txt'), 'a') as f:\n",
    "        f.write(str + '\\n')\n",
    "    if to_stdout:\n",
    "        print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "275c6976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz, max_len, shuffle=False, gpu=False):\n",
    "    if shuffle:\n",
    "        random.shuffle(data)\n",
    "    nbatch = len(data) // bsz\n",
    "    batches = []\n",
    "\n",
    "    for i in range(nbatch):\n",
    "        maxlen = max_len+1\n",
    "        # Pad batches to maximum sequence length in batch\n",
    "        batch = data[i*bsz:(i+1)*bsz]\n",
    "        # subtract 1 from lengths b/c includes BOTH starts & end symbols\n",
    "        lengths = [len(x)-1 for x in batch]\n",
    "\n",
    "        # sort items by length (decreasing)\n",
    "        batch, lengths = length_sort(batch, lengths)\n",
    "\n",
    "        # source has no end symbol\n",
    "        source = [x[:-1] for x in batch]\n",
    "        # target has no start symbol\n",
    "        target = [x[1:] for x in batch]\n",
    "\n",
    "\n",
    "        for x, y in zip(source, target):\n",
    "            zeros = (maxlen-len(x))*[0]\n",
    "            x += zeros\n",
    "            y += zeros\n",
    "        source = torch.LongTensor(np.array(source))\n",
    "        target = torch.LongTensor(np.array(target)).view(-1)\n",
    "\n",
    "        if gpu:\n",
    "            source = source.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "        batches.append((source, target, lengths))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4042220b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data!\n"
     ]
    }
   ],
   "source": [
    "test_data = batchify(corpus.test_u_tok, eval_batch_size, maxlen, shuffle=False)\n",
    "train_data = batchify(corpus.train_u_tok, batch_size, maxlen,  shuffle=True)\n",
    "\n",
    "print(\"Loaded data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04be3a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1249\n",
      "4361\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51cc7067",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Build the models\n",
    "###############################################################################\n",
    "autoencoder = Seq2Seq(add_noise=add_noise,\n",
    "                      emsize=emsize,\n",
    "                      nhidden=nhidden,\n",
    "                      ntokens=ntokens,\n",
    "                      nlayers=nlayers,\n",
    "                      nheads=nheads,\n",
    "                      nff=nff,\n",
    "                      aehidden=aehidden,\n",
    "                      noise_r=noise_r,\n",
    "                      hidden_init=hidden_init,\n",
    "                      dropout=dropout,\n",
    "                      gpu=True)\n",
    "nlatent = aehidden * (maxlen+1)\n",
    "gan_gen = MLP_G(ninput=z_size, noutput=nlatent, layers=arch_g, gan_g_activation=gan_g_activation)\n",
    "gan_disc = MLP_D(ninput=nlatent, noutput=1, layers=arch_d)\n",
    "gan_disc_local = MLP_D_local(ninput=gan_d_local_windowsize * aehidden, noutput=1, layers=arch_d_local)\n",
    "\n",
    "optimizer_ae = optim.SGD(autoencoder.parameters(), lr=lr_ae)\n",
    "\n",
    "\n",
    "optimizer_gan_e = optim.Adam(autoencoder.encoder.parameters(),\n",
    "                             lr=lr_gan_e,\n",
    "                             betas=(beta1, 0.999))\n",
    "optimizer_gan_g = optim.Adam(gan_gen.parameters(),\n",
    "                             lr=lr_gan_g,\n",
    "                             betas=(beta1, 0.999))\n",
    "optimizer_gan_d = optim.Adam(gan_disc.parameters(),\n",
    "                             lr=lr_gan_d,\n",
    "                             betas=(beta1, 0.999))\n",
    "optimizer_gan_d_local = optim.Adam(gan_disc_local.parameters(),\n",
    "                             lr=lr_gan_d,\n",
    "                             betas=(beta1, 0.999))\n",
    "optimizer_gan_dec = optim.Adam(autoencoder.decoder.parameters(),\n",
    "                             lr=lr_gan_e,\n",
    "                             betas=(beta1, 0.999))\n",
    "\n",
    "autoencoder = autoencoder.to(device)\n",
    "gan_gen = gan_gen.to(device)\n",
    "gan_disc = gan_disc.to(device)\n",
    "gan_disc_local = gan_disc_local.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42a2dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    print(\"Saving models to {}\".format(save))\n",
    "    torch.save({\n",
    "        \"ae\": autoencoder.state_dict(),\n",
    "        \"gan_g\": gan_gen.state_dict(),\n",
    "        \"gan_d\": gan_disc.state_dict(),\n",
    "        \"gan_d_local\": gan_disc_local.state_dict()\n",
    "\n",
    "        },\n",
    "        os.path.join(save, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b528fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        param_norm = p.grad.data.norm(2)\n",
    "        total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** (1. / 2)\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7a9fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    model_args = json.load(open(os.path.join(save, 'options.json'), 'r'))\n",
    "    word2idx = json.load(open(os.path.join(save, 'vocab.json'), 'r'))\n",
    "    idx2word = {v: k for k, v in word2idx.items()}\n",
    "\n",
    "    print('Loading models from {}'.format(save))\n",
    "    loaded = torch.load(os.path.join(save, \"model.pt\"))\n",
    "    autoencoder.load_state_dict(loaded.get('ae'))\n",
    "    gan_gen.load_state_dict(loaded.get('gan_g'))\n",
    "    gan_disc.load_state_dict(loaded.get('gan_d'))\n",
    "    gan_disc_local.load_state_dict(loaded.get('gan_d_local'))\n",
    "    return model_args, idx2word, autoencoder, gan_gen, gan_disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03edc3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_autoencoder(data_source, epoch):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    autoencoder.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary.word2idx)\n",
    "    all_accuracies = 0\n",
    "    bcnt = 0\n",
    "    for i, batch in enumerate(data_source):\n",
    "        source, target, lengths = batch\n",
    "        with torch.no_grad():\n",
    "            source = Variable(source.to(device))\n",
    "            target = Variable(target.to(device))\n",
    "            mask = target.gt(0)\n",
    "            masked_target = target.masked_select(mask)\n",
    "            # examples x ntokens\n",
    "            output_mask = mask.unsqueeze(1).expand(mask.size(0), ntokens)\n",
    "\n",
    "            # output: batch x seq_len x ntokens\n",
    "            output = autoencoder(source, lengths, source, add_noise=add_noise, soft=False)\n",
    "            flattened_output = output.view(-1, ntokens)\n",
    "\n",
    "            masked_output = \\\n",
    "                flattened_output.masked_select(output_mask).view(-1, ntokens)\n",
    "            total_loss += F.cross_entropy(masked_output, masked_target)\n",
    "\n",
    "            # accuracy\n",
    "            max_vals, max_indices = torch.max(masked_output, 1)\n",
    "            accuracy = torch.mean(max_indices.eq(masked_target).float()).data.item()\n",
    "            all_accuracies += accuracy\n",
    "            bcnt += 1\n",
    "\n",
    "        aeoutf = os.path.join(save, \"autoencoder.txt\")\n",
    "        with open(aeoutf, \"w\") as f:\n",
    "            max_values, max_indices = torch.max(output, 2)\n",
    "            max_indices = \\\n",
    "                max_indices.view(output.size(0), -1).data.cpu().numpy()\n",
    "            target = target.view(output.size(0), -1).data.cpu().numpy()\n",
    "            for t, idx in zip(target, max_indices):\n",
    "                # real sentence\n",
    "                chars = \" \".join([corpus.dictionary.idx2word[x] for x in t])\n",
    "                f.write(chars + '\\n')\n",
    "                # autoencoder output sentence\n",
    "                chars = \" \".join([corpus.dictionary.idx2word[x] for x in idx])\n",
    "                f.write(chars + '\\n'*2)\n",
    "\n",
    "    return total_loss.item() / len(data_source), all_accuracies/bcnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b838544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fixed_noise(noise, to_save):\n",
    "    gan_gen.eval()\n",
    "    autoencoder.eval()\n",
    "\n",
    "    fake_hidden = gan_gen(noise)\n",
    "    max_indices = autoencoder.generate(fake_hidden, maxlen, sample=sample)\n",
    "\n",
    "    with open(to_save, \"w\") as f:\n",
    "        max_indices = max_indices.data.cpu().numpy()\n",
    "        for idx in max_indices:\n",
    "            # generated sentence\n",
    "            words = [corpus.dictionary.idx2word[x] for x in idx]\n",
    "            # truncate sentences to first occurrence of <eos>\n",
    "            truncated_sent = []\n",
    "            for w in words:\n",
    "                if w != '<eos>':\n",
    "                    truncated_sent.append(w)\n",
    "                else:\n",
    "                    break\n",
    "            chars = \" \".join(truncated_sent)\n",
    "            f.write(chars + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36124e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fixed_noise_new(noise):\n",
    "    gan_gen.eval()\n",
    "    autoencoder.eval()\n",
    "\n",
    "    fake_hidden = gan_gen(noise)\n",
    "    max_indices = autoencoder.generate(fake_hidden, maxlen, sample=sample)\n",
    "    \n",
    "    sent_list = []\n",
    "    \n",
    "    #with open(to_save, \"w\") as f:\n",
    "    max_indices = max_indices.data.cpu().numpy()\n",
    "    for idx in max_indices:\n",
    "        # generated sentence\n",
    "        words = [corpus.dictionary.idx2word[x] for x in idx]\n",
    "        # truncate sentences to first occurrence of <eos>\n",
    "        truncated_sent = []\n",
    "        for w in words:\n",
    "            if w != '<eos>':\n",
    "                truncated_sent.append(w)\n",
    "            else:\n",
    "                break\n",
    "        chars = \" \".join(truncated_sent)\n",
    "        #f.write(chars + '\\n')\n",
    "        sent_list.append(chars)\n",
    "    #print(sent_list)\n",
    "    return sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58dced27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(gen_text_savepath):\n",
    "    selfbleu = bleu_self(gen_text_savepath)\n",
    "    real_text = os.path.join(data_path, \"test.txt\")\n",
    "    testbleu = bleu_test(real_text, gen_text_savepath)\n",
    "    return selfbleu, testbleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "124dcc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae(epoch, batch, total_loss_ae, start_time, i):\n",
    "    '''Train AE with the negative log-likelihood loss'''\n",
    "    autoencoder.train()\n",
    "    optimizer_ae.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = Variable(source.to(device))\n",
    "    target = Variable(target.to(device))\n",
    "    output = autoencoder(source, lengths, source, add_noise=add_noise, soft=False)\n",
    "\n",
    "    mask = target.gt(0)\n",
    "    masked_target = target.masked_select(mask)\n",
    "    output_mask = mask.unsqueeze(1).expand(mask.size(0), ntokens)\n",
    "    flat_output = output.view(-1, ntokens)\n",
    "    masked_output = flat_output.masked_select(output_mask).view(-1, ntokens)\n",
    "    loss = F.cross_entropy(masked_output, masked_target)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(autoencoder.parameters(), clip)\n",
    "    train_ae_norm = cal_norm(autoencoder)\n",
    "    optimizer_ae.step()\n",
    "\n",
    "    total_loss_ae += loss.data.item()\n",
    "    if i % log_interval == 0:\n",
    "        probs = F.softmax(masked_output, dim=-1)\n",
    "        max_vals, max_indices = torch.max(probs, 1)\n",
    "        accuracy = torch.mean(max_indices.eq(masked_target).float()).data.item()\n",
    "        cur_loss = total_loss_ae / log_interval\n",
    "        elapsed = time.time() - start_time\n",
    "        logging('| epoch {:3d} | {:5d}/{:5d} batches | lr {:08.6f} | ms/batch {:5.2f} | '\n",
    "                'loss {:5.2f} | ppl {:8.2f} | acc {:8.2f} | train_ae_norm {:8.2f}'.format(\n",
    "                epoch, i, len(train_data), 0,\n",
    "                elapsed * 1000 / log_interval,\n",
    "                cur_loss, math.exp(cur_loss), accuracy, train_ae_norm))\n",
    "\n",
    "        total_loss_ae = 0\n",
    "        start_time = time.time()\n",
    "    return total_loss_ae, start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64870e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_g(gan_type='kl'):\n",
    "    gan_gen.train()\n",
    "    optimizer_gan_g.zero_grad()\n",
    "\n",
    "    z = Variable(torch.Tensor(batch_size, z_size).normal_(0, 1).to(device))\n",
    "    fake_hidden = gan_gen(z)\n",
    "    fake_score = gan_disc(fake_hidden)\n",
    "\n",
    "    if gan_d_local:\n",
    "        idx = random.randint(0, maxlen - gan_d_local_windowsize)\n",
    "        fake_hidden_local = fake_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        fake_score_local = gan_disc_local(fake_hidden_local)\n",
    "\n",
    "        if gan_type == 'kl':\n",
    "            errG = -(torch.exp(fake_score.detach()).clamp(0.5, 2) * fake_score).mean() -(torch.exp(fake_score_local.detach()).clamp(0.5, 2) * fake_score_local).mean()\n",
    "        else:\n",
    "            errG = -fake_score.mean() -fake_score_local.mean()\n",
    "    else:\n",
    "        if gan_type == 'kl':\n",
    "            errG = -(torch.exp(fake_score.detach()).clamp(0.5, 2) * fake_score).mean()\n",
    "        else:\n",
    "            errG = -fake_score.mean()\n",
    "\n",
    "\n",
    "    errG *= gan_lambda\n",
    "    errG.backward()\n",
    "    optimizer_gan_g.step()\n",
    "\n",
    "    return errG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33244527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_dec(gan_type='kl'):\n",
    "    autoencoder.decoder.train()\n",
    "    optimizer_gan_dec.zero_grad()\n",
    "\n",
    "    z = Variable(torch.Tensor(batch_size, z_size).normal_(0, 1).to(device))\n",
    "    fake_hidden = gan_gen(z)\n",
    "\n",
    "    # 1. decoder  - soft distribution\n",
    "    enhance_source, max_indices= autoencoder.generate_enh_dec(fake_hidden, maxlen, sample=sample)\n",
    "    # 2. soft distribution - > encoder  -> fake_hidden\n",
    "    enhance_hidden = autoencoder(enhance_source, None, max_indices, add_noise=add_noise, soft=True, encode_only=True)\n",
    "    fake_score = gan_disc(enhance_hidden)\n",
    "\n",
    "    if gan_d_local:\n",
    "        idx = random.randint(0, maxlen - gan_d_local_windowsize)\n",
    "        fake_hidden_local = fake_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        fake_score_local = gan_disc_local(fake_hidden_local)\n",
    "\n",
    "        if gan_type == 'kl':\n",
    "            errG = -(torch.exp(fake_score.detach()).clamp(0.5, 2) * fake_score).mean() -(torch.exp(fake_score_local.detach()).clamp(0.5, 2) * fake_score_local).mean()\n",
    "        else:\n",
    "            errG = -fake_score.mean() -fake_score_local.mean()\n",
    "    else:\n",
    "        if gan_type == 'kl':\n",
    "            errG = -(torch.exp(fake_score.detach()).clamp(0.5, 2) * fake_score).mean()\n",
    "        else:\n",
    "            errG = -fake_score.mean()\n",
    "\n",
    "\n",
    "    errG *= gan_lambda\n",
    "    errG.backward()\n",
    "    optimizer_gan_dec.step()\n",
    "\n",
    "    return errG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce1e7869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_hook(grad):\n",
    "    return grad * gan_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5cf668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Steal from https://github.com/caogang/wgan-gp/blob/master/gan_cifar10.py '''\n",
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    bsz = real_data.size(0)\n",
    "    alpha = torch.rand(bsz, 1)\n",
    "    alpha = alpha.expand(bsz, real_data.size(1))  # only works for 2D XXX\n",
    "    alpha = alpha.to(device)\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                                    grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
    "                                    create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * gan_gp_lambda\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e65e6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_d(batch, gan_type='kl'):\n",
    "    gan_disc.train()\n",
    "    gan_disc_local.train()\n",
    "    optimizer_gan_d.zero_grad()\n",
    "    optimizer_gan_d_local.zero_grad()\n",
    "\n",
    "    # + samples\n",
    "    source, target, lengths = batch\n",
    "    source = Variable(source.to(device))\n",
    "    target = Variable(target.to(device))\n",
    "    real_hidden = autoencoder(source, lengths, source, add_noise=add_noise, soft=False, encode_only=True)\n",
    "    real_score = gan_disc(real_hidden.detach())\n",
    "\n",
    "    idx = random.randint(0, maxlen - gan_d_local_windowsize)\n",
    "    if gan_d_local:\n",
    "        real_hidden_local = real_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        real_score_local = gan_disc_local(real_hidden_local)\n",
    "        real_score += real_score_local\n",
    "\n",
    "\n",
    "    if gan_type == 'wgan':\n",
    "        errD_real = -real_score.mean()\n",
    "    else: # kl or all\n",
    "        errD_real = F.softplus(-real_score).mean()\n",
    "    errD_real.backward()\n",
    "\n",
    "    # - samples\n",
    "    z = Variable(torch.Tensor(batch_size, z_size).normal_(0, 1).to(device))\n",
    "    fake_hidden = gan_gen(z)\n",
    "    fake_score = gan_disc(fake_hidden.detach())\n",
    "\n",
    "    if gan_d_local:\n",
    "        fake_hidden_local = fake_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        fake_score_local = gan_disc_local(fake_hidden_local)\n",
    "        fake_score += fake_score_local\n",
    "\n",
    "    if gan_type == 'wgan':\n",
    "        errD_fake = fake_score.mean()\n",
    "    else:  # kl or all\n",
    "        errD_fake = F.softplus(fake_score).mean()\n",
    "    errD_fake.backward()\n",
    "\n",
    "    # gradient penalty\n",
    "    if gan_type == 'wgan':\n",
    "        gradient_penalty = calc_gradient_penalty(gan_disc, real_hidden.data, fake_hidden.data)\n",
    "        gradient_penalty.backward()\n",
    "\n",
    "    optimizer_gan_d.step()\n",
    "    optimizer_gan_d_local.step()\n",
    "    return errD_real + errD_fake, errD_real, errD_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f12ead25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_d_into_ae(batch):\n",
    "    autoencoder.train()\n",
    "    optimizer_gan_e.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = Variable(source.to(device))\n",
    "    target = Variable(target.to(device))\n",
    "    real_hidden = autoencoder(source, lengths, source, add_noise=add_noise, soft=False, encode_only=True)\n",
    "\n",
    "    if gan_d_local:\n",
    "        idx = random.randint(0, maxlen - gan_d_local_windowsize)\n",
    "        real_hidden_local = real_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        real_score_local = gan_disc_local(real_hidden_local)\n",
    "        errD_real = gan_disc(real_hidden).mean() + real_score_local.mean()\n",
    "    else:\n",
    "        errD_real = gan_disc(real_hidden).mean()\n",
    "\n",
    "    errD_real *= gan_lambda\n",
    "    errD_real.backward()\n",
    "    torch.nn.utils.clip_grad_norm(autoencoder.parameters(), clip)\n",
    "\n",
    "    optimizer_gan_e.step()\n",
    "    return errD_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b7fb974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "#  Transformer parameters\n",
    "#--------------------------------\n",
    "max_seq_length = 20\n",
    "batch_size_d = 48\n",
    "\n",
    "#--------------------------------\n",
    "#  GAN-BERT specific parameters\n",
    "#--------------------------------\n",
    "# number of hidden layers in the generator, \n",
    "# each of the size of the output space\n",
    "#num_hidden_layers_g = 1; \n",
    "# number of hidden layers in the discriminator, \n",
    "# each of the size of the input space\n",
    "num_hidden_layers_d = 1; \n",
    "# size of the generator's input noisy vectors\n",
    "noise_size = 100\n",
    "# dropout to be applied to discriminator's input vectors\n",
    "out_dropout_rate = 0.2\n",
    "\n",
    "# Replicate labeled data to balance poorly represented datasets, \n",
    "# e.g., less than 1% of labeled material\n",
    "apply_balance = True\n",
    "\n",
    "#--------------------------------\n",
    "#  Optimization parameters\n",
    "#--------------------------------\n",
    "learning_rate_discriminator = 5e-6 #5e-6?\n",
    "#learning_rate_generator = 5e-5\n",
    "epsilon = 1e-8\n",
    "num_train_epochs = 50\n",
    "multi_gpu = True\n",
    "# Scheduler\n",
    "apply_scheduler = False\n",
    "warmup_proportion = 0.1\n",
    "# Print\n",
    "print_each_n_step = 10\n",
    "\n",
    "#--------------------------------\n",
    "#  Adopted Tranformer model\n",
    "#--------------------------------\n",
    "# Since this version is compatible with Huggingface transformers, you can uncomment\n",
    "# (or add) transformer models compatible with GAN\n",
    "\n",
    "model_name = \"bert-base-cased\"\n",
    "#model_name = \"bert-base-uncased\"\n",
    "#model_name = \"roberta-base\"\n",
    "#model_name = \"albert-base-v2\"\n",
    "#model_name = \"xlm-roberta-base\"\n",
    "#model_name = \"amazon/bort\"\n",
    "#model_name=\"google/electra-large-discriminator\"\n",
    "#model_name=\"google/electra-small-discriminator\"\n",
    "#model_name=\"microsoft/deberta-v2-xxlarge\"\n",
    "#model_name=\"microsoft/deberta-v3-base\"\n",
    "#model_name = \"google/electra-base-discriminator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "852f7b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/harry/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /home/harry/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/harry/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /home/harry/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /home/harry/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /home/harry/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/harry/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e199402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_loader(input_examples, label_masks, label_map, do_shuffle = False, balance_label_examples = False):\n",
    "  '''\n",
    "  Generate a Dataloader given the input examples, eventually masked if they are \n",
    "  to be considered NOT labeled.\n",
    "  '''\n",
    "  examples = []\n",
    "\n",
    "  # Count the percentage of labeled examples  \n",
    "  num_labeled_examples = 0\n",
    "  for label_mask in label_masks:\n",
    "    if label_mask: \n",
    "      num_labeled_examples += 1\n",
    "  label_mask_rate = num_labeled_examples/len(input_examples)\n",
    "\n",
    "  # if required it applies the balance\n",
    "  for index, ex in enumerate(input_examples): \n",
    "    if label_mask_rate == 1 or not balance_label_examples:\n",
    "      examples.append((ex, label_masks[index]))\n",
    "    else:\n",
    "      # IT SIMULATE A LABELED EXAMPLE\n",
    "      if label_masks[index]:\n",
    "        balance = int(1/label_mask_rate)\n",
    "        balance = int(math.log(balance,2))\n",
    "        if balance < 1:\n",
    "          balance = 1\n",
    "        for b in range(0, int(balance)):\n",
    "          examples.append((ex, label_masks[index]))\n",
    "      else:\n",
    "        examples.append((ex, label_masks[index]))\n",
    "  \n",
    "  #-----------------------------------------------\n",
    "  # Generate input examples to the Transformer\n",
    "  #-----------------------------------------------\n",
    "  input_ids = []\n",
    "  input_mask_array = []\n",
    "  label_mask_array = []\n",
    "  label_id_array = []\n",
    "\n",
    "  # Tokenization \n",
    "  for (text, label_mask) in examples:\n",
    "    encoded_sent = tokenizer.encode(text[0], add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
    "    input_ids.append(encoded_sent)\n",
    "    label_id_array.append(label_map[text[1]])\n",
    "    label_mask_array.append(label_mask)\n",
    "  \n",
    "  # Attention to token (to ignore padded input wordpieces)\n",
    "  for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]                          \n",
    "    input_mask_array.append(att_mask)\n",
    "  # Convertion to Tensor\n",
    "  input_ids = torch.tensor(input_ids) \n",
    "  input_mask_array = torch.tensor(input_mask_array)\n",
    "  label_id_array = torch.tensor(label_id_array, dtype=torch.long)\n",
    "  label_mask_array = torch.tensor(label_mask_array)\n",
    "\n",
    "  # Building the TensorDataset\n",
    "  dataset = TensorDataset(input_ids, input_mask_array, label_id_array, label_mask_array)\n",
    "\n",
    "  if do_shuffle:\n",
    "    sampler = RandomSampler\n",
    "  else:\n",
    "    sampler = SequentialSampler\n",
    "\n",
    "  # Building the DataLoader\n",
    "  return DataLoader(\n",
    "              dataset,  # The training samples.\n",
    "              sampler = sampler(dataset), \n",
    "              batch_size = batch_size) # Trains with this batch size.\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8dc39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_fake(input_examples):\n",
    "  '''\n",
    "  Generate a Dataloader given the input examples, eventually masked if they are \n",
    "  to be considered NOT labeled.\n",
    "  '''\n",
    "  \n",
    "  #-----------------------------------------------\n",
    "  # Generate input examples to the Transformer\n",
    "  #-----------------------------------------------\n",
    "  input_ids = []\n",
    "  input_mask_array = []\n",
    "\n",
    "  # Tokenization \n",
    "  for text in input_examples:\n",
    "    encoded_sent = tokenizer.encode(text, add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
    "    input_ids.append(encoded_sent)\n",
    "  \n",
    "  # Attention to token (to ignore padded input wordpieces)\n",
    "  for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]                          \n",
    "    input_mask_array.append(att_mask)\n",
    "  # Convertion to Tensor\n",
    "  input_ids = torch.tensor(input_ids) \n",
    "  input_mask_array = torch.tensor(input_mask_array)\n",
    "\n",
    "  # Building the DataLoader\n",
    "  return input_ids, input_mask_array # Trains with this batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35a3c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the examples\n",
    "labeled_examples = train_l\n",
    "unlabeled_examples = u_list\n",
    "test_examples = test_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9826b424",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {}\n",
    "for (i, label) in enumerate(label_list):\n",
    "  label_map[label] = i\n",
    "#------------------------------\n",
    "#   Load the train dataset\n",
    "#------------------------------\n",
    "train_examples = labeled_examples\n",
    "#The labeled (train) dataset is assigned with a mask set to True\n",
    "train_label_masks = np.ones(len(labeled_examples), dtype=bool)\n",
    "#If unlabel examples are available\n",
    "if unlabeled_examples:\n",
    "  train_examples = train_examples + unlabeled_examples\n",
    "  #The unlabeled (train) dataset is assigned with a mask set to False\n",
    "  tmp_masks = np.zeros(len(unlabeled_examples), dtype=bool)\n",
    "  train_label_masks = np.concatenate([train_label_masks,tmp_masks])\n",
    "\n",
    "train_dataloader = generate_data_loader(train_examples, train_label_masks, label_map, do_shuffle = True, balance_label_examples = apply_balance)\n",
    "\n",
    "#------------------------------\n",
    "#   Load the test dataset\n",
    "#------------------------------\n",
    "#The labeled (test) dataset is assigned with a mask set to True\n",
    "test_label_masks = np.ones(len(test_examples), dtype=bool)\n",
    "\n",
    "test_dataloader = generate_data_loader(test_examples, test_label_masks, label_map, do_shuffle = False, balance_label_examples = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f4df8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------\n",
    "#   The Discriminator\n",
    "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
    "#   https://github.com/crux82/ganbert\n",
    "#------------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
    "        layers = []\n",
    "        hidden_sizes = [input_size] + hidden_sizes\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
    "\n",
    "        self.layers = nn.Sequential(*layers) #per il flatten\n",
    "        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1) # +1 for the probability of this sample being fake/real.\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_rep):\n",
    "        input_rep = self.input_dropout(input_rep)\n",
    "        last_rep = self.layers(input_rep)\n",
    "        logits = self.logit(last_rep)\n",
    "        probs = self.softmax(logits)\n",
    "        return last_rep, logits, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1b267c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/harry/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The config file is required to get the dimension of the vector produced by \n",
    "# the underlying transformer\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "hidden_size = int(config.hidden_size)\n",
    "# Define the number and width of hidden layers\n",
    "#hidden_levels_g = [hidden_size for i in range(0, num_hidden_layers_g)]\n",
    "hidden_levels_d = [hidden_size for i in range(0, num_hidden_layers_d)]\n",
    "\n",
    "#-------------------------------------------------\n",
    "#   Instantiate the Generator and Discriminator\n",
    "#-------------------------------------------------\n",
    "#generator = Generator(noise_size=noise_size, output_size=hidden_size, hidden_sizes=hidden_levels_g, dropout_rate=out_dropout_rate)\n",
    "discriminator = Discriminator(input_size=hidden_size, hidden_sizes=hidden_levels_d, num_labels=len(label_list), dropout_rate=out_dropout_rate)\n",
    "\n",
    "# Put everything in the GPU if available\n",
    "if torch.cuda.is_available():    \n",
    "  #generator.cuda()\n",
    "  discriminator.cuda()\n",
    "  transformer.cuda()\n",
    "  if multi_gpu:\n",
    "    transformer = torch.nn.DataParallel(transformer)\n",
    "\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5f21295",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {}\n",
    "for (i, label) in enumerate(label_list):\n",
    "  label_map[label] = i\n",
    "#------------------------------\n",
    "#   Load the train dataset\n",
    "#------------------------------\n",
    "train_examples = labeled_examples\n",
    "#The labeled (train) dataset is assigned with a mask set to True\n",
    "train_label_masks = np.ones(len(labeled_examples), dtype=bool)\n",
    "#If unlabel examples are available\n",
    "if unlabeled_examples:\n",
    "  train_examples = train_examples + unlabeled_examples\n",
    "  #The unlabeled (train) dataset is assigned with a mask set to False\n",
    "  tmp_masks = np.zeros(len(unlabeled_examples), dtype=bool)\n",
    "  train_label_masks = np.concatenate([train_label_masks,tmp_masks])\n",
    "\n",
    "train_dataloader = generate_data_loader(train_examples, train_label_masks, label_map, do_shuffle = True, balance_label_examples = apply_balance)\n",
    "\n",
    "#------------------------------\n",
    "#   Load the test dataset\n",
    "#------------------------------\n",
    "#The labeled (test) dataset is assigned with a mask set to True\n",
    "test_label_masks = np.ones(len(test_examples), dtype=bool)\n",
    "\n",
    "test_dataloader = generate_data_loader(test_examples, test_label_masks, label_map, do_shuffle = False, balance_label_examples = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be0cd3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stats = []\n",
    "\n",
    "accuracy_array=[]\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "#models parameters\n",
    "transformer_vars = [i for i in transformer.parameters()]\n",
    "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
    "#g_vars = [v for v in generator.parameters()]\n",
    "\n",
    "#optimizer\n",
    "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
    "#gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator) \n",
    "\n",
    "#scheduler\n",
    "if apply_scheduler:\n",
    "  num_train_examples = len(train_examples)\n",
    "  num_train_steps = int(num_train_examples / batch_size * num_train_epochs)\n",
    "  num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
    "\n",
    "  scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n",
    "                                           num_warmup_steps = num_warmup_steps)\n",
    "  scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n",
    "                                           num_warmup_steps = num_warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44e7895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    logging(\"Training\")\n",
    "    train_data = batchify(corpus.train_u_tok, batch_size, maxlen, shuffle=True)\n",
    "\n",
    "    # gan: preparation\n",
    "    if niters_gan_schedule != \"\":\n",
    "        gan_schedule = [int(x) for x in niters_gan_schedule.split(\"-\")]\n",
    "    else:\n",
    "        gan_schedule = []\n",
    "    niter_gan = 1\n",
    "    fixed_noise = Variable(torch.ones(eval_batch_size, z_size).normal_(0, 1).to(device))\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # update gan training schedule\n",
    "        if epoch in gan_schedule:\n",
    "            niter_gan += 1\n",
    "            logging(\"GAN training loop schedule: {}\".format(niter_gan))\n",
    "\n",
    "        total_loss_ae = 0\n",
    "        epoch_start_time = time.time()\n",
    "        start_time = time.time()\n",
    "        niter = 0\n",
    "        niter_g = 1\n",
    "        print(\"Train classification discriminator\")\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        # Perform one full pass over the training set.\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        tr_g_loss = 0\n",
    "        tr_d_loss = 0\n",
    "\n",
    "        # Put the model into training mode.\n",
    "        transformer.train() \n",
    "        #generator.train()\n",
    "        discriminator.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Progress update every print_each_n_step batches.\n",
    "            if step % print_each_n_step == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            b_label_mask = batch[3].to(device)\n",
    "\n",
    "            real_batch_size = b_input_ids.shape[0]\n",
    "\n",
    "            # Encode real data in the Transformer\n",
    "            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
    "            hidden_states = model_outputs.last_hidden_state[:,0,:] \n",
    "            #hidden_states = model_outputs[-1]\n",
    "            #print(\"  Number of real sentences (labelled and unlabelled): {}\".format(len(hidden_states)))\n",
    "            \n",
    "            # Generate fake data that should have the same distribution of the ones\n",
    "            # encoded by the transformer. \n",
    "            # First noisy input are used in input to the Generator\n",
    "            fixed_noise = Variable(torch.ones(real_batch_size, 100).normal_(0, 1).to(device))\n",
    "            fake_sentences = gen_fixed_noise_new(fixed_noise)\n",
    "            #print(\"  Number of generated sentences: {}\".format(len(fake_sentences)))\n",
    "\n",
    "            b_input_ids_fake, b_input_mask_fake = generate_data_fake(fake_sentences)\n",
    "            model_outputs_fake = transformer(b_input_ids_fake, attention_mask=b_input_mask_fake)\n",
    "            hidden_states_fake = model_outputs_fake.last_hidden_state[:,0,:] \n",
    "            #hidden_states_fake = model_outputs_fake[-1]\n",
    "\n",
    "            #noise = torch.zeros(real_batch_size, noise_size, device=device).uniform_(0, 1)\n",
    "            # Gnerate Fake data\n",
    "            #gen_rep = generator(noise)\n",
    "            #print(\"Length of generator output {}\".format(len(gen_rep)))\n",
    "            #print(\"Length of single generator output {}\".format(len(gen_rep[0])))\n",
    "\n",
    "            # Generate the output of the Discriminator for real and fake data.\n",
    "            # First, we put together the output of the tranformer and the generator\n",
    "            disciminator_input = torch.cat([hidden_states, hidden_states_fake], dim=0)\n",
    "            # Then, we select the output of the disciminator\n",
    "            features, logits, probs = discriminator(disciminator_input)\n",
    "\n",
    "            # Finally, we separate the discriminator's output for the real and fake\n",
    "            # data\n",
    "            features_list = torch.split(features, real_batch_size)\n",
    "            D_real_features = features_list[0]\n",
    "            D_fake_features = features_list[1]\n",
    "\n",
    "            logits_list = torch.split(logits, real_batch_size)\n",
    "            D_real_logits = logits_list[0]\n",
    "            D_fake_logits = logits_list[1]\n",
    "\n",
    "            probs_list = torch.split(probs, real_batch_size)\n",
    "            D_real_probs = probs_list[0]\n",
    "            D_fake_probs = probs_list[1]\n",
    "\n",
    "            #---------------------------------\n",
    "            #  LOSS evaluation\n",
    "            #---------------------------------\n",
    "            # Generator's LOSS estimation\n",
    "            g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + epsilon))\n",
    "            g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n",
    "            g_loss = g_loss_d + g_feat_reg\n",
    "\n",
    "            # Disciminator's LOSS estimation\n",
    "            logits = D_real_logits[:,0:-1]\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            # The discriminator provides an output for labeled and unlabeled real data\n",
    "            # so the loss evaluated for unlabeled data is ignored (masked)\n",
    "            label2one_hot = torch.nn.functional.one_hot(b_labels, len(label_list))\n",
    "            per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n",
    "            per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n",
    "            labeled_example_count = per_example_loss.type(torch.float32).numel()\n",
    "\n",
    "            # It may be the case that a batch does not contain labeled examples, \n",
    "            # so the \"supervised loss\" in this case is not evaluated\n",
    "            if labeled_example_count == 0:\n",
    "              D_L_Supervised = 0\n",
    "            else:\n",
    "              D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n",
    "\n",
    "            D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n",
    "            D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n",
    "            d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n",
    "\n",
    "            #---------------------------------\n",
    "            #  OPTIMIZATION\n",
    "            #---------------------------------\n",
    "            # Avoid gradient accumulation\n",
    "            #gen_optimizer.zero_grad()\n",
    "            dis_optimizer.zero_grad()\n",
    "\n",
    "            # Calculate weigth updates\n",
    "            # retain_graph=True is required since the underlying graph will be deleted after backward\n",
    "            g_loss.backward(retain_graph=True)\n",
    "            d_loss.backward() \n",
    "\n",
    "            # Apply modifications\n",
    "            #gen_optimizer.step()\n",
    "            dis_optimizer.step()\n",
    "\n",
    "            # A detail log of the individual losses\n",
    "            #print(\"{0:.4f}\\t{1:.4f}\\t{2:.4f}\\t{3:.4f}\\t{4:.4f}\".\n",
    "            #      format(D_L_Supervised, D_L_unsupervised1U, D_L_unsupervised2U,\n",
    "            #             g_loss_d, g_feat_reg))\n",
    "\n",
    "            # Save the losses to print them later\n",
    "            tr_g_loss += g_loss.item()\n",
    "            tr_d_loss += d_loss.item()\n",
    "\n",
    "            # Update the learning rate with the scheduler\n",
    "            if apply_scheduler:\n",
    "              scheduler_d.step()\n",
    "              #scheduler_g.step()\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss_g = tr_g_loss / len(train_dataloader)\n",
    "        avg_train_loss_d = tr_d_loss / len(train_dataloader)             \n",
    "\n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss generetor: {0:.3f}\".format(avg_train_loss_g))\n",
    "        print(\"  Average training loss discriminator: {0:.3f}\".format(avg_train_loss_d))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "        # ========================================\n",
    "        #     TEST ON THE EVALUATION DATASET\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our test set.\n",
    "        print(\"\")\n",
    "        print(\"Running Test...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        transformer.eval() #maybe redundant\n",
    "        discriminator.eval()\n",
    "        #generator.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_test_accuracy = 0\n",
    "\n",
    "        total_test_loss = 0\n",
    "        nb_test_steps = 0\n",
    "\n",
    "        all_preds = []\n",
    "        all_labels_ids = []\n",
    "\n",
    "        #loss\n",
    "        nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in test_dataloader:\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "                model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
    "                hidden_states = model_outputs.last_hidden_state[:,0,:] \n",
    "                #hidden_states = model_outputs[-1]\n",
    "                _, logits, probs = discriminator(hidden_states)\n",
    "                ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n",
    "                filtered_logits = logits[:,0:-1]\n",
    "                # Accumulate the test loss.\n",
    "                total_test_loss += nll_loss(filtered_logits, b_labels)\n",
    "\n",
    "            # Accumulate the predictions and the input labels\n",
    "            _, preds = torch.max(filtered_logits, 1)\n",
    "            all_preds += preds.detach().cpu()\n",
    "            all_labels_ids += b_labels.detach().cpu()\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        all_preds = torch.stack(all_preds).numpy()\n",
    "        all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
    "        test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
    "        print(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "        avg_test_loss = avg_test_loss.item()\n",
    "\n",
    "        # Measure how long the validation run took.\n",
    "        test_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"  Test Loss: {0:.3f}\".format(avg_test_loss))\n",
    "        print(\"  Test took: {:}\".format(test_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch + 1,\n",
    "                'Training Loss generator': avg_train_loss_g,\n",
    "                'Training Loss discriminator': avg_train_loss_d,\n",
    "                'Valid. Loss': avg_test_loss,\n",
    "                'Valid. Accur.': test_accuracy,\n",
    "                'Training Time': training_time,\n",
    "                'Test Time': test_time\n",
    "            }\n",
    "        )\n",
    "\n",
    "        accuracy_array.append(test_accuracy)\n",
    "        print(\"Train other shit\")\n",
    "        while niter < len(train_data):\n",
    "            # train ae\n",
    "            for i in range(niters_ae):\n",
    "                if niter >= len(train_data):\n",
    "                    break  # end of epoch\n",
    "                total_loss_ae, start_time = train_ae(epoch, train_data[niter],\n",
    "                                total_loss_ae, start_time, niter)\n",
    "                niter += 1\n",
    "            # train gan\n",
    "            for k in range(niter_gan):\n",
    "                for i in range(niters_gan_d):\n",
    "                    errD, errD_real, errD_fake = train_gan_d(\n",
    "                            train_data[random.randint(0, len(train_data)-1)], gan_type)\n",
    "                for i in range(niters_gan_ae):\n",
    "                    train_gan_d_into_ae(train_data[random.randint(0, len(train_data)-1)])\n",
    "                for i in range(niters_gan_g):\n",
    "                    errG = train_gan_g(gan_type)\n",
    "                if enhance_dec:\n",
    "                    for i in range(niters_gan_dec):\n",
    "                        errG_enh_dec = train_gan_dec()\n",
    "                else:\n",
    "                    errG_enh_dec = torch.Tensor([0])\n",
    "\n",
    "            niter_g += 1\n",
    "            if niter_g % log_interval == 0:\n",
    "                logging('[{}/{}][{}/{}] Loss_D: {:.8f} (Loss_D_real: {:.8f} '\n",
    "                        'Loss_D_fake: {:.8f}) Loss_G: {:.8f} Loss_Enh_Dec: {:.8f}'.format(\n",
    "                         epoch, epochs, niter, len(train_data),\n",
    "                         errD.data.item(), errD_real.data.item(),\n",
    "                         errD_fake.data.item(), errG.data.item(), errG_enh_dec.data.item()))\n",
    "        # eval\n",
    "        test_loss, accuracy = evaluate_autoencoder(test_data, epoch)\n",
    "        logging('| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} | '\n",
    "                'test ppl {:5.2f} | acc {:3.3f}'.format(epoch,\n",
    "                (time.time() - epoch_start_time), test_loss,\n",
    "                math.exp(test_loss), accuracy))\n",
    "\n",
    "        gen_text_savepath = os.path.join(save, \"{:03d}_examplar_gen\".format(epoch))\n",
    "        gen_fixed_noise(fixed_noise, gen_text_savepath)\n",
    "        if epoch % 5 == 0 or epoch % 4 == 0 or (epochs - epoch) <=2:\n",
    "            selfbleu, testbleu = eval_bleu(gen_text_savepath)\n",
    "            logging('bleu_self: [{:.8f},{:.8f},{:.8f},{:.8f},{:.8f}]'.format(selfbleu[0], selfbleu[1], selfbleu[2], selfbleu[3], selfbleu[4]))\n",
    "            logging('bleu_test: [{:.8f},{:.8f},{:.8f},{:.8f},{:.8f}]'.format(testbleu[0], testbleu[1], testbleu[2], testbleu[3], testbleu[4]))\n",
    "\n",
    "        if epoch % 15 == 0 or epoch == epochs-1:\n",
    "            logging(\"New saving model: epoch {:03d}.\".format(epoch))\n",
    "            save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24898f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 1 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:27.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:46.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:57.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:01.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:05.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:09.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:16.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:20.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:24.\n",
      "\n",
      "  Average training loss generetor: 0.589\n",
      "  Average training loss discriminator: 3.361\n",
      "  Training epcoh took: 0:01:27\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.225\n",
      "  Test Loss: 2.251\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   1 |     0/ 4361 batches | lr 0.000000 | ms/batch 877.45 | loss  0.12 | ppl     1.13 | acc     0.00 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harry/venv/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/harry/venv/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/200][99/4361] Loss_D: 1.38524914 (Loss_D_real: 0.69120073 Loss_D_fake: 0.69404840) Loss_G: -0.00016868 Loss_Enh_Dec: -0.00040080\n",
      "| epoch   1 |   100/ 4361 batches | lr 0.000000 | ms/batch 399.08 | loss  8.57 | ppl  5260.61 | acc     0.19 | train_ae_norm     1.00\n",
      "[1/200][199/4361] Loss_D: 1.38535583 (Loss_D_real: 0.69144821 Loss_D_fake: 0.69390762) Loss_G: -0.00013183 Loss_Enh_Dec: -0.00032994\n",
      "| epoch   1 |   200/ 4361 batches | lr 0.000000 | ms/batch 398.60 | loss  7.58 | ppl  1949.98 | acc     0.23 | train_ae_norm     1.00\n",
      "[1/200][299/4361] Loss_D: 1.38564086 (Loss_D_real: 0.69203019 Loss_D_fake: 0.69361067) Loss_G: -0.00004620 Loss_Enh_Dec: -0.00020663\n",
      "| epoch   1 |   300/ 4361 batches | lr 0.000000 | ms/batch 398.52 | loss  7.21 | ppl  1350.46 | acc     0.22 | train_ae_norm     1.00\n",
      "[1/200][399/4361] Loss_D: 1.38577974 (Loss_D_real: 0.69268084 Loss_D_fake: 0.69309890) Loss_G: -0.00000916 Loss_Enh_Dec: -0.00005714\n",
      "| epoch   1 |   400/ 4361 batches | lr 0.000000 | ms/batch 397.57 | loss  6.93 | ppl  1020.68 | acc     0.24 | train_ae_norm     1.00\n",
      "[1/200][499/4361] Loss_D: 1.38609850 (Loss_D_real: 0.69302106 Loss_D_fake: 0.69307745) Loss_G: 0.00002991 Loss_Enh_Dec: -0.00003621\n",
      "| epoch   1 |   500/ 4361 batches | lr 0.000000 | ms/batch 397.89 | loss  6.85 | ppl   944.75 | acc     0.28 | train_ae_norm     1.00\n",
      "[1/200][599/4361] Loss_D: 1.38617539 (Loss_D_real: 0.69329357 Loss_D_fake: 0.69288176) Loss_G: 0.00007342 Loss_Enh_Dec: 0.00003915\n",
      "| epoch   1 |   600/ 4361 batches | lr 0.000000 | ms/batch 397.96 | loss  6.62 | ppl   746.93 | acc     0.23 | train_ae_norm     1.00\n",
      "[1/200][699/4361] Loss_D: 1.38594735 (Loss_D_real: 0.69342816 Loss_D_fake: 0.69251919) Loss_G: 0.00009003 Loss_Enh_Dec: 0.00004783\n",
      "| epoch   1 |   700/ 4361 batches | lr 0.000000 | ms/batch 398.11 | loss  6.55 | ppl   701.54 | acc     0.28 | train_ae_norm     1.00\n",
      "[1/200][799/4361] Loss_D: 1.38612461 (Loss_D_real: 0.69344491 Loss_D_fake: 0.69267976) Loss_G: 0.00008991 Loss_Enh_Dec: 0.00008583\n",
      "| epoch   1 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.43 | loss  6.47 | ppl   644.84 | acc     0.28 | train_ae_norm     1.00\n",
      "[1/200][899/4361] Loss_D: 1.38592839 (Loss_D_real: 0.69316578 Loss_D_fake: 0.69276255) Loss_G: 0.00006526 Loss_Enh_Dec: 0.00001675\n",
      "| epoch   1 |   900/ 4361 batches | lr 0.000000 | ms/batch 396.54 | loss  6.39 | ppl   598.20 | acc     0.32 | train_ae_norm     1.00\n",
      "[1/200][999/4361] Loss_D: 1.38629448 (Loss_D_real: 0.69328493 Loss_D_fake: 0.69300956) Loss_G: 0.00002459 Loss_Enh_Dec: 0.00003062\n",
      "| epoch   1 |  1000/ 4361 batches | lr 0.000000 | ms/batch 397.05 | loss  6.30 | ppl   544.46 | acc     0.32 | train_ae_norm     1.00\n",
      "[1/200][1099/4361] Loss_D: 1.38610339 (Loss_D_real: 0.69293880 Loss_D_fake: 0.69316453) Loss_G: 0.00000160 Loss_Enh_Dec: -0.00002400\n",
      "| epoch   1 |  1100/ 4361 batches | lr 0.000000 | ms/batch 397.27 | loss  6.20 | ppl   494.70 | acc     0.29 | train_ae_norm     1.00\n",
      "[1/200][1199/4361] Loss_D: 1.38634026 (Loss_D_real: 0.69341677 Loss_D_fake: 0.69292349) Loss_G: 0.00007221 Loss_Enh_Dec: 0.00006658\n",
      "| epoch   1 |  1200/ 4361 batches | lr 0.000000 | ms/batch 397.12 | loss  6.18 | ppl   483.66 | acc     0.34 | train_ae_norm     1.00\n",
      "[1/200][1299/4361] Loss_D: 1.38637066 (Loss_D_real: 0.69308013 Loss_D_fake: 0.69329059) Loss_G: -0.00001708 Loss_Enh_Dec: 0.00000535\n",
      "| epoch   1 |  1300/ 4361 batches | lr 0.000000 | ms/batch 396.39 | loss  6.08 | ppl   437.61 | acc     0.32 | train_ae_norm     1.00\n",
      "[1/200][1399/4361] Loss_D: 1.38641560 (Loss_D_real: 0.69337177 Loss_D_fake: 0.69304383) Loss_G: 0.00007867 Loss_Enh_Dec: 0.00008424\n",
      "| epoch   1 |  1400/ 4361 batches | lr 0.000000 | ms/batch 397.62 | loss  6.04 | ppl   419.95 | acc     0.28 | train_ae_norm     1.00\n",
      "[1/200][1499/4361] Loss_D: 1.38639998 (Loss_D_real: 0.69318092 Loss_D_fake: 0.69321907) Loss_G: 0.00000283 Loss_Enh_Dec: -0.00002271\n",
      "| epoch   1 |  1500/ 4361 batches | lr 0.000000 | ms/batch 397.07 | loss  5.97 | ppl   392.27 | acc     0.32 | train_ae_norm     1.00\n",
      "[1/200][1599/4361] Loss_D: 1.38629889 (Loss_D_real: 0.69318026 Loss_D_fake: 0.69311869) Loss_G: 0.00001193 Loss_Enh_Dec: 0.00001173\n",
      "| epoch   1 |  1600/ 4361 batches | lr 0.000000 | ms/batch 396.70 | loss  5.89 | ppl   359.84 | acc     0.33 | train_ae_norm     1.00\n",
      "[1/200][1699/4361] Loss_D: 1.38644075 (Loss_D_real: 0.69315767 Loss_D_fake: 0.69328308) Loss_G: -0.00000540 Loss_Enh_Dec: 0.00002054\n",
      "| epoch   1 |  1700/ 4361 batches | lr 0.000000 | ms/batch 396.63 | loss  5.82 | ppl   338.32 | acc     0.34 | train_ae_norm     1.00\n",
      "[1/200][1799/4361] Loss_D: 1.38658297 (Loss_D_real: 0.69315308 Loss_D_fake: 0.69342989) Loss_G: -0.00000277 Loss_Enh_Dec: 0.00002127\n",
      "| epoch   1 |  1800/ 4361 batches | lr 0.000000 | ms/batch 396.95 | loss  5.76 | ppl   316.95 | acc     0.36 | train_ae_norm     1.00\n",
      "[1/200][1899/4361] Loss_D: 1.38647008 (Loss_D_real: 0.69305098 Loss_D_fake: 0.69341916) Loss_G: -0.00003071 Loss_Enh_Dec: 0.00002635\n",
      "| epoch   1 |  1900/ 4361 batches | lr 0.000000 | ms/batch 396.78 | loss  5.76 | ppl   317.06 | acc     0.38 | train_ae_norm     1.00\n",
      "[1/200][1999/4361] Loss_D: 1.38634419 (Loss_D_real: 0.69320935 Loss_D_fake: 0.69313478) Loss_G: -0.00000078 Loss_Enh_Dec: 0.00001865\n",
      "| epoch   1 |  2000/ 4361 batches | lr 0.000000 | ms/batch 396.43 | loss  5.67 | ppl   288.72 | acc     0.36 | train_ae_norm     1.00\n",
      "[1/200][2099/4361] Loss_D: 1.38653111 (Loss_D_real: 0.69317770 Loss_D_fake: 0.69335335) Loss_G: -0.00002937 Loss_Enh_Dec: 0.00001863\n",
      "| epoch   1 |  2100/ 4361 batches | lr 0.000000 | ms/batch 396.71 | loss  5.62 | ppl   276.84 | acc     0.41 | train_ae_norm     1.00\n",
      "[1/200][2199/4361] Loss_D: 1.38642550 (Loss_D_real: 0.69320482 Loss_D_fake: 0.69322073) Loss_G: -0.00002444 Loss_Enh_Dec: 0.00003027\n",
      "| epoch   1 |  2200/ 4361 batches | lr 0.000000 | ms/batch 396.54 | loss  5.57 | ppl   262.23 | acc     0.36 | train_ae_norm     1.00\n",
      "[1/200][2299/4361] Loss_D: 1.38646483 (Loss_D_real: 0.69328320 Loss_D_fake: 0.69318169) Loss_G: -0.00000762 Loss_Enh_Dec: 0.00000563\n",
      "| epoch   1 |  2300/ 4361 batches | lr 0.000000 | ms/batch 397.29 | loss  5.53 | ppl   251.01 | acc     0.40 | train_ae_norm     1.00\n",
      "[1/200][2399/4361] Loss_D: 1.38632321 (Loss_D_real: 0.69313776 Loss_D_fake: 0.69318551) Loss_G: 0.00001061 Loss_Enh_Dec: 0.00003443\n",
      "| epoch   1 |  2400/ 4361 batches | lr 0.000000 | ms/batch 397.48 | loss  5.45 | ppl   233.79 | acc     0.37 | train_ae_norm     1.00\n",
      "[1/200][2499/4361] Loss_D: 1.38627636 (Loss_D_real: 0.69311929 Loss_D_fake: 0.69315708) Loss_G: -0.00002016 Loss_Enh_Dec: 0.00001442\n",
      "| epoch   1 |  2500/ 4361 batches | lr 0.000000 | ms/batch 396.73 | loss  5.47 | ppl   237.89 | acc     0.41 | train_ae_norm     1.00\n",
      "[1/200][2599/4361] Loss_D: 1.38638425 (Loss_D_real: 0.69322652 Loss_D_fake: 0.69315767) Loss_G: -0.00000983 Loss_Enh_Dec: 0.00002175\n",
      "| epoch   1 |  2600/ 4361 batches | lr 0.000000 | ms/batch 396.40 | loss  5.36 | ppl   212.49 | acc     0.41 | train_ae_norm     1.00\n",
      "[1/200][2699/4361] Loss_D: 1.38616705 (Loss_D_real: 0.69318581 Loss_D_fake: 0.69298118) Loss_G: 0.00000123 Loss_Enh_Dec: 0.00004342\n",
      "| epoch   1 |  2700/ 4361 batches | lr 0.000000 | ms/batch 397.20 | loss  5.36 | ppl   212.16 | acc     0.40 | train_ae_norm     1.00\n",
      "[1/200][2799/4361] Loss_D: 1.38632011 (Loss_D_real: 0.69308722 Loss_D_fake: 0.69323295) Loss_G: -0.00000852 Loss_Enh_Dec: 0.00002122\n",
      "| epoch   1 |  2800/ 4361 batches | lr 0.000000 | ms/batch 396.59 | loss  5.24 | ppl   189.13 | acc     0.43 | train_ae_norm     1.00\n",
      "[1/200][2899/4361] Loss_D: 1.38652515 (Loss_D_real: 0.69321084 Loss_D_fake: 0.69331425) Loss_G: -0.00001711 Loss_Enh_Dec: 0.00000278\n",
      "| epoch   1 |  2900/ 4361 batches | lr 0.000000 | ms/batch 395.94 | loss  5.25 | ppl   190.69 | acc     0.47 | train_ae_norm     1.00\n",
      "[1/200][2999/4361] Loss_D: 1.38643575 (Loss_D_real: 0.69326735 Loss_D_fake: 0.69316834) Loss_G: 0.00000379 Loss_Enh_Dec: 0.00001364\n",
      "| epoch   1 |  3000/ 4361 batches | lr 0.000000 | ms/batch 397.68 | loss  5.21 | ppl   182.45 | acc     0.42 | train_ae_norm     1.00\n",
      "[1/200][3099/4361] Loss_D: 1.38638604 (Loss_D_real: 0.69314015 Loss_D_fake: 0.69324589) Loss_G: -0.00001503 Loss_Enh_Dec: 0.00001604\n",
      "| epoch   1 |  3100/ 4361 batches | lr 0.000000 | ms/batch 396.15 | loss  5.17 | ppl   176.16 | acc     0.39 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/200][3199/4361] Loss_D: 1.38636816 (Loss_D_real: 0.69314611 Loss_D_fake: 0.69322205) Loss_G: 0.00000650 Loss_Enh_Dec: -0.00000008\n",
      "| epoch   1 |  3200/ 4361 batches | lr 0.000000 | ms/batch 396.04 | loss  5.15 | ppl   172.96 | acc     0.42 | train_ae_norm     1.00\n",
      "[1/200][3299/4361] Loss_D: 1.38643909 (Loss_D_real: 0.69333905 Loss_D_fake: 0.69310009) Loss_G: 0.00003178 Loss_Enh_Dec: 0.00002790\n",
      "| epoch   1 |  3300/ 4361 batches | lr 0.000000 | ms/batch 396.07 | loss  5.13 | ppl   169.13 | acc     0.44 | train_ae_norm     1.00\n",
      "[1/200][3399/4361] Loss_D: 1.38636720 (Loss_D_real: 0.69319361 Loss_D_fake: 0.69317359) Loss_G: 0.00001398 Loss_Enh_Dec: 0.00000927\n",
      "| epoch   1 |  3400/ 4361 batches | lr 0.000000 | ms/batch 396.37 | loss  5.08 | ppl   161.00 | acc     0.45 | train_ae_norm     1.00\n",
      "[1/200][3499/4361] Loss_D: 1.38639891 (Loss_D_real: 0.69322628 Loss_D_fake: 0.69317263) Loss_G: -0.00001093 Loss_Enh_Dec: 0.00000599\n",
      "| epoch   1 |  3500/ 4361 batches | lr 0.000000 | ms/batch 396.75 | loss  5.00 | ppl   148.53 | acc     0.47 | train_ae_norm     1.00\n",
      "[1/200][3599/4361] Loss_D: 1.38638163 (Loss_D_real: 0.69319332 Loss_D_fake: 0.69318837) Loss_G: 0.00000421 Loss_Enh_Dec: 0.00002676\n",
      "| epoch   1 |  3600/ 4361 batches | lr 0.000000 | ms/batch 396.96 | loss  4.97 | ppl   144.37 | acc     0.47 | train_ae_norm     1.00\n",
      "[1/200][3699/4361] Loss_D: 1.38626063 (Loss_D_real: 0.69312781 Loss_D_fake: 0.69313282) Loss_G: -0.00000241 Loss_Enh_Dec: 0.00000731\n",
      "| epoch   1 |  3700/ 4361 batches | lr 0.000000 | ms/batch 396.11 | loss  4.92 | ppl   137.36 | acc     0.46 | train_ae_norm     1.00\n",
      "[1/200][3799/4361] Loss_D: 1.38626933 (Loss_D_real: 0.69312567 Loss_D_fake: 0.69314373) Loss_G: -0.00001050 Loss_Enh_Dec: -0.00003945\n",
      "| epoch   1 |  3800/ 4361 batches | lr 0.000000 | ms/batch 395.71 | loss  4.91 | ppl   135.58 | acc     0.51 | train_ae_norm     1.00\n",
      "[1/200][3899/4361] Loss_D: 1.38630223 (Loss_D_real: 0.69313014 Loss_D_fake: 0.69317216) Loss_G: 0.00000404 Loss_Enh_Dec: -0.00004038\n",
      "| epoch   1 |  3900/ 4361 batches | lr 0.000000 | ms/batch 395.50 | loss  4.87 | ppl   130.38 | acc     0.45 | train_ae_norm     1.00\n",
      "[1/200][3999/4361] Loss_D: 1.38624728 (Loss_D_real: 0.69315612 Loss_D_fake: 0.69309115) Loss_G: 0.00000214 Loss_Enh_Dec: -0.00004800\n",
      "| epoch   1 |  4000/ 4361 batches | lr 0.000000 | ms/batch 395.68 | loss  4.84 | ppl   125.98 | acc     0.47 | train_ae_norm     1.00\n",
      "[1/200][4099/4361] Loss_D: 1.38632667 (Loss_D_real: 0.69320202 Loss_D_fake: 0.69312465) Loss_G: 0.00000270 Loss_Enh_Dec: -0.00005140\n",
      "| epoch   1 |  4100/ 4361 batches | lr 0.000000 | ms/batch 395.67 | loss  4.79 | ppl   120.19 | acc     0.47 | train_ae_norm     1.00\n",
      "[1/200][4199/4361] Loss_D: 1.38635325 (Loss_D_real: 0.69313860 Loss_D_fake: 0.69321460) Loss_G: -0.00000805 Loss_Enh_Dec: -0.00008259\n",
      "| epoch   1 |  4200/ 4361 batches | lr 0.000000 | ms/batch 394.54 | loss  4.79 | ppl   120.77 | acc     0.48 | train_ae_norm     1.00\n",
      "[1/200][4299/4361] Loss_D: 1.38634539 (Loss_D_real: 0.69313204 Loss_D_fake: 0.69321334) Loss_G: -0.00000376 Loss_Enh_Dec: -0.00006540\n",
      "| epoch   1 |  4300/ 4361 batches | lr 0.000000 | ms/batch 395.90 | loss  4.70 | ppl   109.72 | acc     0.49 | train_ae_norm     1.00\n",
      "| end of epoch   1 | time: 1834.21s | test loss  4.40 | test ppl 81.74 | acc 0.531\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 2 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:33.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:48.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:00:59.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:14.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:18.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.692\n",
      "  Average training loss discriminator: 2.677\n",
      "  Training epcoh took: 0:01:25\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.388\n",
      "  Test Loss: 2.009\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   2 |     0/ 4361 batches | lr 0.000000 | ms/batch 855.90 | loss  0.04 | ppl     1.04 | acc     0.49 | train_ae_norm     1.00\n",
      "[2/200][99/4361] Loss_D: 1.38625431 (Loss_D_real: 0.69318688 Loss_D_fake: 0.69306743) Loss_G: 0.00001590 Loss_Enh_Dec: -0.00005095\n",
      "| epoch   2 |   100/ 4361 batches | lr 0.000000 | ms/batch 395.99 | loss  4.65 | ppl   105.10 | acc     0.50 | train_ae_norm     1.00\n",
      "[2/200][199/4361] Loss_D: 1.38632369 (Loss_D_real: 0.69314212 Loss_D_fake: 0.69318163) Loss_G: 0.00000095 Loss_Enh_Dec: -0.00007626\n",
      "| epoch   2 |   200/ 4361 batches | lr 0.000000 | ms/batch 395.92 | loss  4.66 | ppl   105.32 | acc     0.52 | train_ae_norm     1.00\n",
      "[2/200][299/4361] Loss_D: 1.38626873 (Loss_D_real: 0.69313657 Loss_D_fake: 0.69313216) Loss_G: 0.00000293 Loss_Enh_Dec: -0.00002950\n",
      "| epoch   2 |   300/ 4361 batches | lr 0.000000 | ms/batch 395.66 | loss  4.60 | ppl    99.55 | acc     0.51 | train_ae_norm     1.00\n",
      "[2/200][399/4361] Loss_D: 1.38630700 (Loss_D_real: 0.69317484 Loss_D_fake: 0.69313210) Loss_G: -0.00000188 Loss_Enh_Dec: -0.00003775\n",
      "| epoch   2 |   400/ 4361 batches | lr 0.000000 | ms/batch 396.28 | loss  4.50 | ppl    90.00 | acc     0.51 | train_ae_norm     1.00\n",
      "[2/200][499/4361] Loss_D: 1.38631296 (Loss_D_real: 0.69317472 Loss_D_fake: 0.69313830) Loss_G: -0.00000365 Loss_Enh_Dec: -0.00003576\n",
      "| epoch   2 |   500/ 4361 batches | lr 0.000000 | ms/batch 396.82 | loss  4.54 | ppl    93.28 | acc     0.53 | train_ae_norm     1.00\n",
      "[2/200][599/4361] Loss_D: 1.38630629 (Loss_D_real: 0.69315314 Loss_D_fake: 0.69315308) Loss_G: -0.00000781 Loss_Enh_Dec: -0.00003886\n",
      "| epoch   2 |   600/ 4361 batches | lr 0.000000 | ms/batch 395.95 | loss  4.47 | ppl    87.50 | acc     0.48 | train_ae_norm     1.00\n",
      "[2/200][699/4361] Loss_D: 1.38626289 (Loss_D_real: 0.69313306 Loss_D_fake: 0.69312990) Loss_G: 0.00000108 Loss_Enh_Dec: -0.00007027\n",
      "| epoch   2 |   700/ 4361 batches | lr 0.000000 | ms/batch 395.75 | loss  4.50 | ppl    90.43 | acc     0.53 | train_ae_norm     1.00\n",
      "[2/200][799/4361] Loss_D: 1.38628078 (Loss_D_real: 0.69314718 Loss_D_fake: 0.69313353) Loss_G: 0.00000774 Loss_Enh_Dec: -0.00004567\n",
      "| epoch   2 |   800/ 4361 batches | lr 0.000000 | ms/batch 396.08 | loss  4.45 | ppl    85.50 | acc     0.54 | train_ae_norm     1.00\n",
      "[2/200][899/4361] Loss_D: 1.38627148 (Loss_D_real: 0.69314849 Loss_D_fake: 0.69312298) Loss_G: 0.00000259 Loss_Enh_Dec: -0.00006440\n",
      "| epoch   2 |   900/ 4361 batches | lr 0.000000 | ms/batch 395.67 | loss  4.46 | ppl    86.26 | acc     0.56 | train_ae_norm     1.00\n",
      "[2/200][999/4361] Loss_D: 1.38633835 (Loss_D_real: 0.69318312 Loss_D_fake: 0.69315523) Loss_G: 0.00000260 Loss_Enh_Dec: -0.00002934\n",
      "| epoch   2 |  1000/ 4361 batches | lr 0.000000 | ms/batch 396.93 | loss  4.41 | ppl    82.53 | acc     0.52 | train_ae_norm     1.00\n",
      "[2/200][1099/4361] Loss_D: 1.38629735 (Loss_D_real: 0.69316214 Loss_D_fake: 0.69313520) Loss_G: 0.00000347 Loss_Enh_Dec: -0.00002709\n",
      "| epoch   2 |  1100/ 4361 batches | lr 0.000000 | ms/batch 396.27 | loss  4.39 | ppl    80.98 | acc     0.50 | train_ae_norm     1.00\n",
      "[2/200][1199/4361] Loss_D: 1.38628674 (Loss_D_real: 0.69315481 Loss_D_fake: 0.69313192) Loss_G: -0.00000182 Loss_Enh_Dec: 0.00000640\n",
      "| epoch   2 |  1200/ 4361 batches | lr 0.000000 | ms/batch 397.19 | loss  4.38 | ppl    80.12 | acc     0.53 | train_ae_norm     1.00\n",
      "[2/200][1299/4361] Loss_D: 1.38633180 (Loss_D_real: 0.69319600 Loss_D_fake: 0.69313574) Loss_G: 0.00000319 Loss_Enh_Dec: -0.00001865\n",
      "| epoch   2 |  1300/ 4361 batches | lr 0.000000 | ms/batch 397.39 | loss  4.37 | ppl    79.12 | acc     0.51 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/200][1399/4361] Loss_D: 1.38631761 (Loss_D_real: 0.69314563 Loss_D_fake: 0.69317198) Loss_G: 0.00000090 Loss_Enh_Dec: -0.00000929\n",
      "| epoch   2 |  1400/ 4361 batches | lr 0.000000 | ms/batch 397.12 | loss  4.36 | ppl    78.03 | acc     0.44 | train_ae_norm     1.00\n",
      "[2/200][1499/4361] Loss_D: 1.38631582 (Loss_D_real: 0.69315672 Loss_D_fake: 0.69315904) Loss_G: 0.00000143 Loss_Enh_Dec: -0.00000251\n",
      "| epoch   2 |  1500/ 4361 batches | lr 0.000000 | ms/batch 397.41 | loss  4.37 | ppl    79.33 | acc     0.49 | train_ae_norm     1.00\n",
      "[2/200][1599/4361] Loss_D: 1.38631392 (Loss_D_real: 0.69317311 Loss_D_fake: 0.69314086) Loss_G: 0.00000308 Loss_Enh_Dec: -0.00000895\n",
      "| epoch   2 |  1600/ 4361 batches | lr 0.000000 | ms/batch 396.61 | loss  4.33 | ppl    76.05 | acc     0.55 | train_ae_norm     1.00\n",
      "[2/200][1699/4361] Loss_D: 1.38628292 (Loss_D_real: 0.69313288 Loss_D_fake: 0.69314998) Loss_G: -0.00000669 Loss_Enh_Dec: -0.00002164\n",
      "| epoch   2 |  1700/ 4361 batches | lr 0.000000 | ms/batch 396.29 | loss  4.30 | ppl    73.42 | acc     0.52 | train_ae_norm     1.00\n",
      "[2/200][1799/4361] Loss_D: 1.38621008 (Loss_D_real: 0.69309181 Loss_D_fake: 0.69311827) Loss_G: 0.00000750 Loss_Enh_Dec: -0.00000836\n",
      "| epoch   2 |  1800/ 4361 batches | lr 0.000000 | ms/batch 396.87 | loss  4.23 | ppl    68.38 | acc     0.56 | train_ae_norm     1.00\n",
      "[2/200][1899/4361] Loss_D: 1.38629043 (Loss_D_real: 0.69312686 Loss_D_fake: 0.69316357) Loss_G: 0.00001035 Loss_Enh_Dec: 0.00000519\n",
      "| epoch   2 |  1900/ 4361 batches | lr 0.000000 | ms/batch 397.05 | loss  4.25 | ppl    70.18 | acc     0.54 | train_ae_norm     1.00\n",
      "[2/200][1999/4361] Loss_D: 1.38631439 (Loss_D_real: 0.69316721 Loss_D_fake: 0.69314718) Loss_G: 0.00001295 Loss_Enh_Dec: 0.00000973\n",
      "| epoch   2 |  2000/ 4361 batches | lr 0.000000 | ms/batch 396.32 | loss  4.18 | ppl    65.45 | acc     0.53 | train_ae_norm     1.00\n",
      "[2/200][2099/4361] Loss_D: 1.38626266 (Loss_D_real: 0.69313276 Loss_D_fake: 0.69312996) Loss_G: 0.00000983 Loss_Enh_Dec: -0.00004578\n",
      "| epoch   2 |  2100/ 4361 batches | lr 0.000000 | ms/batch 396.49 | loss  4.23 | ppl    68.58 | acc     0.56 | train_ae_norm     1.00\n",
      "[2/200][2199/4361] Loss_D: 1.38631463 (Loss_D_real: 0.69315434 Loss_D_fake: 0.69316024) Loss_G: 0.00000749 Loss_Enh_Dec: -0.00001637\n",
      "| epoch   2 |  2200/ 4361 batches | lr 0.000000 | ms/batch 396.08 | loss  4.22 | ppl    68.16 | acc     0.54 | train_ae_norm     1.00\n",
      "[2/200][2299/4361] Loss_D: 1.38622189 (Loss_D_real: 0.69315171 Loss_D_fake: 0.69307011) Loss_G: 0.00000608 Loss_Enh_Dec: -0.00003672\n",
      "| epoch   2 |  2300/ 4361 batches | lr 0.000000 | ms/batch 396.19 | loss  4.20 | ppl    67.00 | acc     0.56 | train_ae_norm     1.00\n",
      "[2/200][2399/4361] Loss_D: 1.38627720 (Loss_D_real: 0.69315743 Loss_D_fake: 0.69311976) Loss_G: 0.00000043 Loss_Enh_Dec: -0.00003133\n",
      "| epoch   2 |  2400/ 4361 batches | lr 0.000000 | ms/batch 396.00 | loss  4.19 | ppl    66.05 | acc     0.50 | train_ae_norm     1.00\n",
      "[2/200][2499/4361] Loss_D: 1.38631058 (Loss_D_real: 0.69314694 Loss_D_fake: 0.69316357) Loss_G: -0.00000128 Loss_Enh_Dec: -0.00000003\n",
      "| epoch   2 |  2500/ 4361 batches | lr 0.000000 | ms/batch 396.56 | loss  4.20 | ppl    66.39 | acc     0.56 | train_ae_norm     1.00\n",
      "[2/200][2599/4361] Loss_D: 1.38626480 (Loss_D_real: 0.69313616 Loss_D_fake: 0.69312859) Loss_G: 0.00000312 Loss_Enh_Dec: -0.00000783\n",
      "| epoch   2 |  2600/ 4361 batches | lr 0.000000 | ms/batch 396.14 | loss  4.15 | ppl    63.28 | acc     0.52 | train_ae_norm     1.00\n",
      "[2/200][2699/4361] Loss_D: 1.38628292 (Loss_D_real: 0.69313848 Loss_D_fake: 0.69314450) Loss_G: -0.00000262 Loss_Enh_Dec: -0.00001283\n",
      "| epoch   2 |  2700/ 4361 batches | lr 0.000000 | ms/batch 396.80 | loss  4.14 | ppl    62.81 | acc     0.55 | train_ae_norm     1.00\n",
      "[2/200][2799/4361] Loss_D: 1.38633776 (Loss_D_real: 0.69317406 Loss_D_fake: 0.69316363) Loss_G: 0.00001300 Loss_Enh_Dec: -0.00008295\n",
      "| epoch   2 |  2800/ 4361 batches | lr 0.000000 | ms/batch 397.13 | loss  4.08 | ppl    58.99 | acc     0.50 | train_ae_norm     1.00\n",
      "[2/200][2899/4361] Loss_D: 1.38629925 (Loss_D_real: 0.69316792 Loss_D_fake: 0.69313133) Loss_G: 0.00000856 Loss_Enh_Dec: 0.00001449\n",
      "| epoch   2 |  2900/ 4361 batches | lr 0.000000 | ms/batch 396.60 | loss  4.20 | ppl    66.87 | acc     0.55 | train_ae_norm     1.00\n",
      "[2/200][2999/4361] Loss_D: 1.38625443 (Loss_D_real: 0.69311875 Loss_D_fake: 0.69313568) Loss_G: 0.00001048 Loss_Enh_Dec: -0.00001799\n",
      "| epoch   2 |  3000/ 4361 batches | lr 0.000000 | ms/batch 396.61 | loss  4.20 | ppl    66.79 | acc     0.54 | train_ae_norm     1.00\n",
      "[2/200][3099/4361] Loss_D: 1.38631022 (Loss_D_real: 0.69321656 Loss_D_fake: 0.69309366) Loss_G: -0.00000802 Loss_Enh_Dec: -0.00002965\n",
      "| epoch   2 |  3100/ 4361 batches | lr 0.000000 | ms/batch 398.04 | loss  4.18 | ppl    65.54 | acc     0.54 | train_ae_norm     1.00\n",
      "[2/200][3199/4361] Loss_D: 1.38631594 (Loss_D_real: 0.69316709 Loss_D_fake: 0.69314885) Loss_G: 0.00003310 Loss_Enh_Dec: -0.00001789\n",
      "| epoch   2 |  3200/ 4361 batches | lr 0.000000 | ms/batch 396.69 | loss  4.25 | ppl    70.10 | acc     0.52 | train_ae_norm     1.00\n",
      "[2/200][3299/4361] Loss_D: 1.38630843 (Loss_D_real: 0.69318551 Loss_D_fake: 0.69312298) Loss_G: 0.00000699 Loss_Enh_Dec: 0.00002208\n",
      "| epoch   2 |  3300/ 4361 batches | lr 0.000000 | ms/batch 397.02 | loss  4.28 | ppl    72.44 | acc     0.50 | train_ae_norm     1.00\n",
      "[2/200][3399/4361] Loss_D: 1.38629055 (Loss_D_real: 0.69310558 Loss_D_fake: 0.69318503) Loss_G: 0.00001233 Loss_Enh_Dec: 0.00000616\n",
      "| epoch   2 |  3400/ 4361 batches | lr 0.000000 | ms/batch 398.31 | loss  4.28 | ppl    72.21 | acc     0.53 | train_ae_norm     1.00\n",
      "[2/200][3499/4361] Loss_D: 1.38638806 (Loss_D_real: 0.69316763 Loss_D_fake: 0.69322050) Loss_G: -0.00000471 Loss_Enh_Dec: 0.00002068\n",
      "| epoch   2 |  3500/ 4361 batches | lr 0.000000 | ms/batch 399.01 | loss  4.24 | ppl    69.41 | acc     0.55 | train_ae_norm     1.00\n",
      "[2/200][3599/4361] Loss_D: 1.38626814 (Loss_D_real: 0.69311470 Loss_D_fake: 0.69315350) Loss_G: 0.00000797 Loss_Enh_Dec: -0.00007052\n",
      "| epoch   2 |  3600/ 4361 batches | lr 0.000000 | ms/batch 397.78 | loss  4.26 | ppl    70.94 | acc     0.52 | train_ae_norm     1.00\n",
      "[2/200][3699/4361] Loss_D: 1.38637507 (Loss_D_real: 0.69308853 Loss_D_fake: 0.69328654) Loss_G: 0.00000997 Loss_Enh_Dec: -0.00002759\n",
      "| epoch   2 |  3700/ 4361 batches | lr 0.000000 | ms/batch 395.95 | loss  4.31 | ppl    74.60 | acc     0.51 | train_ae_norm     1.00\n",
      "[2/200][3799/4361] Loss_D: 1.38624859 (Loss_D_real: 0.69308603 Loss_D_fake: 0.69316256) Loss_G: 0.00001003 Loss_Enh_Dec: -0.00001783\n",
      "| epoch   2 |  3800/ 4361 batches | lr 0.000000 | ms/batch 397.16 | loss  4.40 | ppl    81.52 | acc     0.55 | train_ae_norm     1.00\n",
      "[2/200][3899/4361] Loss_D: 1.38635683 (Loss_D_real: 0.69312066 Loss_D_fake: 0.69323623) Loss_G: 0.00003102 Loss_Enh_Dec: -0.00004899\n",
      "| epoch   2 |  3900/ 4361 batches | lr 0.000000 | ms/batch 397.06 | loss  4.38 | ppl    79.73 | acc     0.48 | train_ae_norm     1.00\n",
      "[2/200][3999/4361] Loss_D: 1.38631189 (Loss_D_real: 0.69308996 Loss_D_fake: 0.69322193) Loss_G: 0.00000938 Loss_Enh_Dec: -0.00006600\n",
      "| epoch   2 |  4000/ 4361 batches | lr 0.000000 | ms/batch 397.50 | loss  4.42 | ppl    82.73 | acc     0.50 | train_ae_norm     1.00\n",
      "[2/200][4099/4361] Loss_D: 1.38637686 (Loss_D_real: 0.69324529 Loss_D_fake: 0.69313163) Loss_G: 0.00000717 Loss_Enh_Dec: -0.00006727\n",
      "| epoch   2 |  4100/ 4361 batches | lr 0.000000 | ms/batch 398.23 | loss  4.39 | ppl    81.01 | acc     0.51 | train_ae_norm     1.00\n",
      "[2/200][4199/4361] Loss_D: 1.38604438 (Loss_D_real: 0.69290847 Loss_D_fake: 0.69313592) Loss_G: 0.00003085 Loss_Enh_Dec: 0.00000180\n",
      "| epoch   2 |  4200/ 4361 batches | lr 0.000000 | ms/batch 398.49 | loss  4.35 | ppl    77.24 | acc     0.53 | train_ae_norm     1.00\n",
      "[2/200][4299/4361] Loss_D: 1.38633561 (Loss_D_real: 0.69315434 Loss_D_fake: 0.69318122) Loss_G: 0.00002048 Loss_Enh_Dec: -0.00005596\n",
      "| epoch   2 |  4300/ 4361 batches | lr 0.000000 | ms/batch 396.43 | loss  4.31 | ppl    74.33 | acc     0.53 | train_ae_norm     1.00\n",
      "| end of epoch   2 | time: 1831.68s | test loss  4.05 | test ppl 57.32 | acc 0.556\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 3 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.712\n",
      "  Average training loss discriminator: 1.920\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.398\n",
      "  Test Loss: 1.827\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   3 |     0/ 4361 batches | lr 0.000000 | ms/batch 864.66 | loss  0.04 | ppl     1.04 | acc     0.52 | train_ae_norm     1.00\n",
      "[3/200][99/4361] Loss_D: 1.38634336 (Loss_D_real: 0.69320011 Loss_D_fake: 0.69314325) Loss_G: 0.00000664 Loss_Enh_Dec: -0.00008779\n",
      "| epoch   3 |   100/ 4361 batches | lr 0.000000 | ms/batch 397.51 | loss  4.40 | ppl    81.33 | acc     0.49 | train_ae_norm     1.00\n",
      "[3/200][199/4361] Loss_D: 1.38608778 (Loss_D_real: 0.69311494 Loss_D_fake: 0.69297284) Loss_G: 0.00002738 Loss_Enh_Dec: -0.00004224\n",
      "| epoch   3 |   200/ 4361 batches | lr 0.000000 | ms/batch 397.34 | loss  4.49 | ppl    88.91 | acc     0.51 | train_ae_norm     1.00\n",
      "[3/200][299/4361] Loss_D: 1.38609874 (Loss_D_real: 0.69294137 Loss_D_fake: 0.69315737) Loss_G: 0.00002663 Loss_Enh_Dec: 0.00002207\n",
      "| epoch   3 |   300/ 4361 batches | lr 0.000000 | ms/batch 398.33 | loss  4.49 | ppl    88.71 | acc     0.50 | train_ae_norm     1.00\n",
      "[3/200][399/4361] Loss_D: 1.38613319 (Loss_D_real: 0.69313806 Loss_D_fake: 0.69299507) Loss_G: 0.00003913 Loss_Enh_Dec: -0.00007231\n",
      "| epoch   3 |   400/ 4361 batches | lr 0.000000 | ms/batch 397.05 | loss  4.37 | ppl    78.99 | acc     0.49 | train_ae_norm     1.00\n",
      "[3/200][499/4361] Loss_D: 1.38596153 (Loss_D_real: 0.69294137 Loss_D_fake: 0.69302011) Loss_G: 0.00005190 Loss_Enh_Dec: -0.00020429\n",
      "| epoch   3 |   500/ 4361 batches | lr 0.000000 | ms/batch 396.79 | loss  4.57 | ppl    96.28 | acc     0.52 | train_ae_norm     1.00\n",
      "[3/200][599/4361] Loss_D: 1.38580024 (Loss_D_real: 0.69273967 Loss_D_fake: 0.69306058) Loss_G: 0.00003918 Loss_Enh_Dec: -0.00007164\n",
      "| epoch   3 |   600/ 4361 batches | lr 0.000000 | ms/batch 397.63 | loss  4.46 | ppl    86.80 | acc     0.44 | train_ae_norm     1.00\n",
      "[3/200][699/4361] Loss_D: 1.38599479 (Loss_D_real: 0.69301260 Loss_D_fake: 0.69298220) Loss_G: -0.00000353 Loss_Enh_Dec: -0.00020211\n",
      "| epoch   3 |   700/ 4361 batches | lr 0.000000 | ms/batch 397.72 | loss  4.51 | ppl    90.54 | acc     0.49 | train_ae_norm     1.00\n",
      "[3/200][799/4361] Loss_D: 1.38614726 (Loss_D_real: 0.69301248 Loss_D_fake: 0.69313478) Loss_G: 0.00008315 Loss_Enh_Dec: 0.00008489\n",
      "| epoch   3 |   800/ 4361 batches | lr 0.000000 | ms/batch 397.08 | loss  4.47 | ppl    87.00 | acc     0.51 | train_ae_norm     1.00\n",
      "[3/200][899/4361] Loss_D: 1.38553941 (Loss_D_real: 0.69257832 Loss_D_fake: 0.69296110) Loss_G: 0.00007873 Loss_Enh_Dec: -0.00013085\n",
      "| epoch   3 |   900/ 4361 batches | lr 0.000000 | ms/batch 397.73 | loss  4.53 | ppl    92.32 | acc     0.52 | train_ae_norm     1.00\n",
      "[3/200][999/4361] Loss_D: 1.38621187 (Loss_D_real: 0.69343150 Loss_D_fake: 0.69278038) Loss_G: 0.00007274 Loss_Enh_Dec: -0.00003853\n",
      "| epoch   3 |  1000/ 4361 batches | lr 0.000000 | ms/batch 397.61 | loss  4.63 | ppl   102.05 | acc     0.47 | train_ae_norm     1.00\n",
      "[3/200][1099/4361] Loss_D: 1.38582647 (Loss_D_real: 0.69294733 Loss_D_fake: 0.69287914) Loss_G: 0.00010302 Loss_Enh_Dec: -0.00015906\n",
      "| epoch   3 |  1100/ 4361 batches | lr 0.000000 | ms/batch 399.42 | loss  4.66 | ppl   106.11 | acc     0.47 | train_ae_norm     1.00\n",
      "[3/200][1199/4361] Loss_D: 1.38543093 (Loss_D_real: 0.69262850 Loss_D_fake: 0.69280243) Loss_G: 0.00007215 Loss_Enh_Dec: -0.00004737\n",
      "| epoch   3 |  1200/ 4361 batches | lr 0.000000 | ms/batch 398.43 | loss  4.61 | ppl   100.91 | acc     0.48 | train_ae_norm     1.00\n",
      "[3/200][1299/4361] Loss_D: 1.38639164 (Loss_D_real: 0.69311941 Loss_D_fake: 0.69327229) Loss_G: -0.00000099 Loss_Enh_Dec: -0.00004582\n",
      "| epoch   3 |  1300/ 4361 batches | lr 0.000000 | ms/batch 397.91 | loss  4.74 | ppl   114.24 | acc     0.44 | train_ae_norm     1.00\n",
      "[3/200][1399/4361] Loss_D: 1.38520956 (Loss_D_real: 0.69301021 Loss_D_fake: 0.69219941) Loss_G: 0.00006598 Loss_Enh_Dec: -0.00008862\n",
      "| epoch   3 |  1400/ 4361 batches | lr 0.000000 | ms/batch 397.50 | loss  4.65 | ppl   104.80 | acc     0.39 | train_ae_norm     1.00\n",
      "[3/200][1499/4361] Loss_D: 1.38612521 (Loss_D_real: 0.69298947 Loss_D_fake: 0.69313574) Loss_G: 0.00010079 Loss_Enh_Dec: -0.00034967\n",
      "| epoch   3 |  1500/ 4361 batches | lr 0.000000 | ms/batch 397.44 | loss  4.83 | ppl   125.31 | acc     0.41 | train_ae_norm     1.00\n",
      "[3/200][1599/4361] Loss_D: 1.38594508 (Loss_D_real: 0.69299448 Loss_D_fake: 0.69295061) Loss_G: 0.00005403 Loss_Enh_Dec: 0.00000701\n",
      "| epoch   3 |  1600/ 4361 batches | lr 0.000000 | ms/batch 398.47 | loss  5.03 | ppl   152.55 | acc     0.44 | train_ae_norm     1.00\n",
      "[3/200][1699/4361] Loss_D: 1.38595772 (Loss_D_real: 0.69284528 Loss_D_fake: 0.69311249) Loss_G: 0.00002624 Loss_Enh_Dec: -0.00005012\n",
      "| epoch   3 |  1700/ 4361 batches | lr 0.000000 | ms/batch 397.19 | loss  4.87 | ppl   130.22 | acc     0.43 | train_ae_norm     1.00\n",
      "[3/200][1799/4361] Loss_D: 1.38554776 (Loss_D_real: 0.69253248 Loss_D_fake: 0.69301528) Loss_G: 0.00011459 Loss_Enh_Dec: -0.00013564\n",
      "| epoch   3 |  1800/ 4361 batches | lr 0.000000 | ms/batch 398.37 | loss  4.70 | ppl   109.47 | acc     0.46 | train_ae_norm     1.00\n",
      "[3/200][1899/4361] Loss_D: 1.38481498 (Loss_D_real: 0.69220769 Loss_D_fake: 0.69260734) Loss_G: 0.00022814 Loss_Enh_Dec: -0.00007192\n",
      "| epoch   3 |  1900/ 4361 batches | lr 0.000000 | ms/batch 398.39 | loss  4.80 | ppl   121.03 | acc     0.50 | train_ae_norm     1.00\n",
      "[3/200][1999/4361] Loss_D: 1.38565516 (Loss_D_real: 0.69294131 Loss_D_fake: 0.69271386) Loss_G: 0.00002036 Loss_Enh_Dec: -0.00036248\n",
      "| epoch   3 |  2000/ 4361 batches | lr 0.000000 | ms/batch 397.84 | loss  4.76 | ppl   116.84 | acc     0.46 | train_ae_norm     1.00\n",
      "[3/200][2099/4361] Loss_D: 1.38588881 (Loss_D_real: 0.69223881 Loss_D_fake: 0.69365001) Loss_G: 0.00014179 Loss_Enh_Dec: 0.00012647\n",
      "| epoch   3 |  2100/ 4361 batches | lr 0.000000 | ms/batch 397.13 | loss  4.64 | ppl   103.79 | acc     0.50 | train_ae_norm     1.00\n",
      "[3/200][2199/4361] Loss_D: 1.38577652 (Loss_D_real: 0.69291133 Loss_D_fake: 0.69286525) Loss_G: 0.00015288 Loss_Enh_Dec: 0.00001988\n",
      "| epoch   3 |  2200/ 4361 batches | lr 0.000000 | ms/batch 398.51 | loss  4.67 | ppl   106.37 | acc     0.49 | train_ae_norm     1.00\n",
      "[3/200][2299/4361] Loss_D: 1.38465893 (Loss_D_real: 0.69230890 Loss_D_fake: 0.69235003) Loss_G: 0.00019683 Loss_Enh_Dec: 0.00007576\n",
      "| epoch   3 |  2300/ 4361 batches | lr 0.000000 | ms/batch 398.82 | loss  4.64 | ppl   103.74 | acc     0.49 | train_ae_norm     1.00\n",
      "[3/200][2399/4361] Loss_D: 1.38523352 (Loss_D_real: 0.69256818 Loss_D_fake: 0.69266534) Loss_G: 0.00020903 Loss_Enh_Dec: 0.00010353\n",
      "| epoch   3 |  2400/ 4361 batches | lr 0.000000 | ms/batch 400.04 | loss  4.69 | ppl   109.07 | acc     0.43 | train_ae_norm     1.00\n",
      "[3/200][2499/4361] Loss_D: 1.38494003 (Loss_D_real: 0.69245476 Loss_D_fake: 0.69248527) Loss_G: 0.00020672 Loss_Enh_Dec: 0.00005033\n",
      "| epoch   3 |  2500/ 4361 batches | lr 0.000000 | ms/batch 399.78 | loss  4.86 | ppl   128.70 | acc     0.47 | train_ae_norm     1.00\n",
      "[3/200][2599/4361] Loss_D: 1.38504469 (Loss_D_real: 0.69206858 Loss_D_fake: 0.69297612) Loss_G: 0.00005609 Loss_Enh_Dec: -0.00005515\n",
      "| epoch   3 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.08 | loss  4.78 | ppl   119.19 | acc     0.44 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/200][2899/4361] Loss_D: 1.38290119 (Loss_D_real: 0.69103277 Loss_D_fake: 0.69186842) Loss_G: 0.00021061 Loss_Enh_Dec: -0.00009812\n",
      "| epoch   3 |  2900/ 4361 batches | lr 0.000000 | ms/batch 399.24 | loss  4.89 | ppl   132.89 | acc     0.49 | train_ae_norm     1.00\n",
      "[3/200][2999/4361] Loss_D: 1.38509440 (Loss_D_real: 0.69205928 Loss_D_fake: 0.69303513) Loss_G: 0.00025547 Loss_Enh_Dec: -0.00004536\n",
      "| epoch   3 |  3000/ 4361 batches | lr 0.000000 | ms/batch 399.86 | loss  4.81 | ppl   122.52 | acc     0.43 | train_ae_norm     1.00\n",
      "[3/200][3099/4361] Loss_D: 1.38382077 (Loss_D_real: 0.69211906 Loss_D_fake: 0.69170177) Loss_G: 0.00027694 Loss_Enh_Dec: -0.00013886\n",
      "| epoch   3 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.23 | loss  4.89 | ppl   132.77 | acc     0.42 | train_ae_norm     1.00\n",
      "[3/200][3199/4361] Loss_D: 1.38478470 (Loss_D_real: 0.69254416 Loss_D_fake: 0.69224060) Loss_G: 0.00031622 Loss_Enh_Dec: 0.00004376\n",
      "| epoch   3 |  3200/ 4361 batches | lr 0.000000 | ms/batch 399.64 | loss  4.88 | ppl   132.27 | acc     0.43 | train_ae_norm     1.00\n",
      "[3/200][3299/4361] Loss_D: 1.38443363 (Loss_D_real: 0.69281387 Loss_D_fake: 0.69161975) Loss_G: 0.00035558 Loss_Enh_Dec: 0.00019859\n",
      "| epoch   3 |  3300/ 4361 batches | lr 0.000000 | ms/batch 398.47 | loss  4.81 | ppl   122.27 | acc     0.44 | train_ae_norm     1.00\n",
      "[3/200][3399/4361] Loss_D: 1.38438892 (Loss_D_real: 0.69207740 Loss_D_fake: 0.69231147) Loss_G: -0.00013022 Loss_Enh_Dec: 0.00012847\n",
      "| epoch   3 |  3400/ 4361 batches | lr 0.000000 | ms/batch 399.04 | loss  4.77 | ppl   117.34 | acc     0.43 | train_ae_norm     1.00\n",
      "[3/200][3499/4361] Loss_D: 1.38476229 (Loss_D_real: 0.69251478 Loss_D_fake: 0.69224757) Loss_G: 0.00001875 Loss_Enh_Dec: -0.00007611\n",
      "| epoch   3 |  3500/ 4361 batches | lr 0.000000 | ms/batch 398.70 | loss  4.69 | ppl   109.39 | acc     0.49 | train_ae_norm     1.00\n",
      "[3/200][3599/4361] Loss_D: 1.38310862 (Loss_D_real: 0.69196033 Loss_D_fake: 0.69114828) Loss_G: 0.00022940 Loss_Enh_Dec: 0.00012226\n",
      "| epoch   3 |  3600/ 4361 batches | lr 0.000000 | ms/batch 399.25 | loss  4.72 | ppl   112.24 | acc     0.46 | train_ae_norm     1.00\n",
      "[3/200][3699/4361] Loss_D: 1.38515472 (Loss_D_real: 0.69273210 Loss_D_fake: 0.69242257) Loss_G: 0.00017284 Loss_Enh_Dec: 0.00015507\n",
      "| epoch   3 |  3700/ 4361 batches | lr 0.000000 | ms/batch 399.10 | loss  4.71 | ppl   111.03 | acc     0.45 | train_ae_norm     1.00\n",
      "[3/200][3799/4361] Loss_D: 1.38331056 (Loss_D_real: 0.69187486 Loss_D_fake: 0.69143569) Loss_G: 0.00013784 Loss_Enh_Dec: -0.00005343\n",
      "| epoch   3 |  3800/ 4361 batches | lr 0.000000 | ms/batch 399.19 | loss  4.70 | ppl   109.72 | acc     0.51 | train_ae_norm     1.00\n",
      "[3/200][3899/4361] Loss_D: 1.38312030 (Loss_D_real: 0.69173479 Loss_D_fake: 0.69138551) Loss_G: 0.00036202 Loss_Enh_Dec: 0.00000518\n",
      "| epoch   3 |  3900/ 4361 batches | lr 0.000000 | ms/batch 398.45 | loss  4.77 | ppl   118.09 | acc     0.41 | train_ae_norm     1.00\n",
      "[3/200][3999/4361] Loss_D: 1.38269031 (Loss_D_real: 0.69172716 Loss_D_fake: 0.69096315) Loss_G: 0.00040687 Loss_Enh_Dec: 0.00003638\n",
      "| epoch   3 |  4000/ 4361 batches | lr 0.000000 | ms/batch 399.12 | loss  4.79 | ppl   119.87 | acc     0.48 | train_ae_norm     1.00\n",
      "[3/200][4099/4361] Loss_D: 1.38327324 (Loss_D_real: 0.69142699 Loss_D_fake: 0.69184625) Loss_G: 0.00011777 Loss_Enh_Dec: 0.00025451\n",
      "| epoch   3 |  4100/ 4361 batches | lr 0.000000 | ms/batch 398.35 | loss  4.73 | ppl   113.55 | acc     0.47 | train_ae_norm     1.00\n",
      "[3/200][4199/4361] Loss_D: 1.38290906 (Loss_D_real: 0.69111770 Loss_D_fake: 0.69179142) Loss_G: 0.00033860 Loss_Enh_Dec: 0.00019774\n",
      "| epoch   3 |  4200/ 4361 batches | lr 0.000000 | ms/batch 399.49 | loss  4.80 | ppl   121.57 | acc     0.48 | train_ae_norm     1.00\n",
      "[3/200][4299/4361] Loss_D: 1.38187289 (Loss_D_real: 0.69025934 Loss_D_fake: 0.69161355) Loss_G: 0.00031960 Loss_Enh_Dec: 0.00013783\n",
      "| epoch   3 |  4300/ 4361 batches | lr 0.000000 | ms/batch 398.88 | loss  4.65 | ppl   105.08 | acc     0.46 | train_ae_norm     1.00\n",
      "| end of epoch   3 | time: 1840.17s | test loss  4.50 | test ppl 89.93 | acc 0.505\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 4 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.706\n",
      "  Average training loss discriminator: 1.309\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.440\n",
      "  Test Loss: 1.790\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   4 |     0/ 4361 batches | lr 0.000000 | ms/batch 862.10 | loss  0.04 | ppl     1.04 | acc     0.48 | train_ae_norm     1.00\n",
      "[4/200][99/4361] Loss_D: 1.38336778 (Loss_D_real: 0.69054168 Loss_D_fake: 0.69282603) Loss_G: 0.00026954 Loss_Enh_Dec: -0.00034847\n",
      "| epoch   4 |   100/ 4361 batches | lr 0.000000 | ms/batch 399.67 | loss  4.69 | ppl   108.72 | acc     0.47 | train_ae_norm     1.00\n",
      "[4/200][199/4361] Loss_D: 1.38033605 (Loss_D_real: 0.68896711 Loss_D_fake: 0.69136894) Loss_G: 0.00009563 Loss_Enh_Dec: -0.00007725\n",
      "| epoch   4 |   200/ 4361 batches | lr 0.000000 | ms/batch 399.96 | loss  4.72 | ppl   111.67 | acc     0.47 | train_ae_norm     1.00\n",
      "[4/200][299/4361] Loss_D: 1.38457572 (Loss_D_real: 0.69219130 Loss_D_fake: 0.69238442) Loss_G: 0.00000738 Loss_Enh_Dec: 0.00022262\n",
      "| epoch   4 |   300/ 4361 batches | lr 0.000000 | ms/batch 399.62 | loss  4.70 | ppl   109.78 | acc     0.48 | train_ae_norm     1.00\n",
      "[4/200][399/4361] Loss_D: 1.38292789 (Loss_D_real: 0.69107360 Loss_D_fake: 0.69185436) Loss_G: 0.00048805 Loss_Enh_Dec: 0.00011772\n",
      "| epoch   4 |   400/ 4361 batches | lr 0.000000 | ms/batch 399.45 | loss  4.58 | ppl    97.89 | acc     0.48 | train_ae_norm     1.00\n",
      "[4/200][499/4361] Loss_D: 1.38328946 (Loss_D_real: 0.69187772 Loss_D_fake: 0.69141173) Loss_G: 0.00018319 Loss_Enh_Dec: -0.00020625\n",
      "| epoch   4 |   500/ 4361 batches | lr 0.000000 | ms/batch 399.40 | loss  4.64 | ppl   104.06 | acc     0.52 | train_ae_norm     1.00\n",
      "[4/200][599/4361] Loss_D: 1.38523006 (Loss_D_real: 0.69373357 Loss_D_fake: 0.69149655) Loss_G: 0.00044944 Loss_Enh_Dec: 0.00037185\n",
      "| epoch   4 |   600/ 4361 batches | lr 0.000000 | ms/batch 399.50 | loss  4.57 | ppl    96.20 | acc     0.45 | train_ae_norm     1.00\n",
      "[4/200][699/4361] Loss_D: 1.38186944 (Loss_D_real: 0.69043821 Loss_D_fake: 0.69143122) Loss_G: 0.00050383 Loss_Enh_Dec: 0.00000366\n",
      "| epoch   4 |   700/ 4361 batches | lr 0.000000 | ms/batch 399.30 | loss  4.62 | ppl   101.64 | acc     0.50 | train_ae_norm     1.00\n",
      "[4/200][799/4361] Loss_D: 1.37754416 (Loss_D_real: 0.68755949 Loss_D_fake: 0.68998468) Loss_G: 0.00049475 Loss_Enh_Dec: -0.00030102\n",
      "| epoch   4 |   800/ 4361 batches | lr 0.000000 | ms/batch 399.62 | loss  4.57 | ppl    96.47 | acc     0.51 | train_ae_norm     1.00\n",
      "[4/200][899/4361] Loss_D: 1.38236904 (Loss_D_real: 0.69103003 Loss_D_fake: 0.69133896) Loss_G: 0.00054305 Loss_Enh_Dec: 0.00006165\n",
      "| epoch   4 |   900/ 4361 batches | lr 0.000000 | ms/batch 399.72 | loss  4.60 | ppl    99.31 | acc     0.53 | train_ae_norm     1.00\n",
      "[4/200][999/4361] Loss_D: 1.37975287 (Loss_D_real: 0.68984628 Loss_D_fake: 0.68990654) Loss_G: 0.00039966 Loss_Enh_Dec: -0.00024607\n",
      "| epoch   4 |  1000/ 4361 batches | lr 0.000000 | ms/batch 399.83 | loss  4.63 | ppl   102.13 | acc     0.48 | train_ae_norm     1.00\n",
      "[4/200][1099/4361] Loss_D: 1.37931681 (Loss_D_real: 0.69027376 Loss_D_fake: 0.68904305) Loss_G: 0.00068085 Loss_Enh_Dec: -0.00037768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  1100/ 4361 batches | lr 0.000000 | ms/batch 399.76 | loss  4.67 | ppl   106.24 | acc     0.45 | train_ae_norm     1.00\n",
      "[4/200][1199/4361] Loss_D: 1.37860334 (Loss_D_real: 0.68801737 Loss_D_fake: 0.69058597) Loss_G: 0.00090298 Loss_Enh_Dec: -0.00054578\n",
      "| epoch   4 |  1200/ 4361 batches | lr 0.000000 | ms/batch 399.81 | loss  4.68 | ppl   107.45 | acc     0.47 | train_ae_norm     1.00\n",
      "[4/200][1299/4361] Loss_D: 1.38117766 (Loss_D_real: 0.69123399 Loss_D_fake: 0.68994367) Loss_G: 0.00082703 Loss_Enh_Dec: -0.00142455\n",
      "| epoch   4 |  1300/ 4361 batches | lr 0.000000 | ms/batch 399.49 | loss  4.71 | ppl   110.64 | acc     0.46 | train_ae_norm     1.00\n",
      "[4/200][1399/4361] Loss_D: 1.38234091 (Loss_D_real: 0.69137335 Loss_D_fake: 0.69096756) Loss_G: 0.00031389 Loss_Enh_Dec: -0.00046136\n",
      "| epoch   4 |  1400/ 4361 batches | lr 0.000000 | ms/batch 399.51 | loss  4.78 | ppl   119.32 | acc     0.39 | train_ae_norm     1.00\n",
      "[4/200][1499/4361] Loss_D: 1.37893939 (Loss_D_real: 0.68911934 Loss_D_fake: 0.68981999) Loss_G: 0.00057224 Loss_Enh_Dec: -0.00102456\n",
      "| epoch   4 |  1500/ 4361 batches | lr 0.000000 | ms/batch 399.63 | loss  4.80 | ppl   120.95 | acc     0.44 | train_ae_norm     1.00\n",
      "[4/200][1599/4361] Loss_D: 1.37723804 (Loss_D_real: 0.68895912 Loss_D_fake: 0.68827891) Loss_G: 0.00008981 Loss_Enh_Dec: -0.00070913\n",
      "| epoch   4 |  1600/ 4361 batches | lr 0.000000 | ms/batch 399.34 | loss  4.77 | ppl   118.42 | acc     0.45 | train_ae_norm     1.00\n",
      "[4/200][1699/4361] Loss_D: 1.38023019 (Loss_D_real: 0.69217175 Loss_D_fake: 0.68805838) Loss_G: 0.00097502 Loss_Enh_Dec: -0.00076728\n",
      "| epoch   4 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.18 | loss  4.79 | ppl   119.82 | acc     0.45 | train_ae_norm     1.00\n",
      "[4/200][1799/4361] Loss_D: 1.37338102 (Loss_D_real: 0.68623447 Loss_D_fake: 0.68714654) Loss_G: 0.00090852 Loss_Enh_Dec: -0.00157552\n",
      "| epoch   4 |  1800/ 4361 batches | lr 0.000000 | ms/batch 399.39 | loss  4.78 | ppl   119.01 | acc     0.45 | train_ae_norm     1.00\n",
      "[4/200][1899/4361] Loss_D: 1.37914681 (Loss_D_real: 0.69003236 Loss_D_fake: 0.68911445) Loss_G: 0.00016042 Loss_Enh_Dec: -0.00047154\n",
      "| epoch   4 |  1900/ 4361 batches | lr 0.000000 | ms/batch 399.53 | loss  4.83 | ppl   125.10 | acc     0.46 | train_ae_norm     1.00\n",
      "[4/200][1999/4361] Loss_D: 1.38140941 (Loss_D_real: 0.69227982 Loss_D_fake: 0.68912953) Loss_G: 0.00043036 Loss_Enh_Dec: -0.00097823\n",
      "| epoch   4 |  2000/ 4361 batches | lr 0.000000 | ms/batch 399.82 | loss  4.78 | ppl   118.72 | acc     0.45 | train_ae_norm     1.00\n",
      "[4/200][2099/4361] Loss_D: 1.37693465 (Loss_D_real: 0.68894303 Loss_D_fake: 0.68799162) Loss_G: 0.00132916 Loss_Enh_Dec: 0.00011261\n",
      "| epoch   4 |  2100/ 4361 batches | lr 0.000000 | ms/batch 399.81 | loss  4.71 | ppl   111.07 | acc     0.49 | train_ae_norm     1.00\n",
      "[4/200][2199/4361] Loss_D: 1.38129056 (Loss_D_real: 0.69061559 Loss_D_fake: 0.69067496) Loss_G: 0.00049517 Loss_Enh_Dec: -0.00120436\n",
      "| epoch   4 |  2200/ 4361 batches | lr 0.000000 | ms/batch 399.56 | loss  4.81 | ppl   123.00 | acc     0.45 | train_ae_norm     1.00\n",
      "[4/200][2299/4361] Loss_D: 1.38016069 (Loss_D_real: 0.68826616 Loss_D_fake: 0.69189453) Loss_G: 0.00067210 Loss_Enh_Dec: -0.00239688\n",
      "| epoch   4 |  2300/ 4361 batches | lr 0.000000 | ms/batch 399.51 | loss  4.82 | ppl   123.38 | acc     0.48 | train_ae_norm     1.00\n",
      "[4/200][2399/4361] Loss_D: 1.37931609 (Loss_D_real: 0.69001997 Loss_D_fake: 0.68929613) Loss_G: 0.00148595 Loss_Enh_Dec: -0.00033846\n",
      "| epoch   4 |  2400/ 4361 batches | lr 0.000000 | ms/batch 399.05 | loss  4.73 | ppl   113.40 | acc     0.42 | train_ae_norm     1.00\n",
      "[4/200][2499/4361] Loss_D: 1.38184059 (Loss_D_real: 0.68814135 Loss_D_fake: 0.69369924) Loss_G: 0.00047490 Loss_Enh_Dec: -0.00186286\n",
      "| epoch   4 |  2500/ 4361 batches | lr 0.000000 | ms/batch 399.82 | loss  4.77 | ppl   118.01 | acc     0.50 | train_ae_norm     1.00\n",
      "[4/200][2599/4361] Loss_D: 1.38202596 (Loss_D_real: 0.69286013 Loss_D_fake: 0.68916577) Loss_G: 0.00035742 Loss_Enh_Dec: -0.00010651\n",
      "| epoch   4 |  2600/ 4361 batches | lr 0.000000 | ms/batch 399.93 | loss  4.81 | ppl   122.16 | acc     0.43 | train_ae_norm     1.00\n",
      "[4/200][2699/4361] Loss_D: 1.37823224 (Loss_D_real: 0.68933696 Loss_D_fake: 0.68889523) Loss_G: 0.00032089 Loss_Enh_Dec: -0.00096270\n",
      "| epoch   4 |  2700/ 4361 batches | lr 0.000000 | ms/batch 399.01 | loss  4.78 | ppl   118.97 | acc     0.49 | train_ae_norm     1.00\n",
      "[4/200][2799/4361] Loss_D: 1.37717867 (Loss_D_real: 0.69026917 Loss_D_fake: 0.68690956) Loss_G: 0.00040307 Loss_Enh_Dec: -0.00019767\n",
      "| epoch   4 |  2800/ 4361 batches | lr 0.000000 | ms/batch 399.00 | loss  4.68 | ppl   107.38 | acc     0.43 | train_ae_norm     1.00\n",
      "[4/200][2899/4361] Loss_D: 1.37620473 (Loss_D_real: 0.68735802 Loss_D_fake: 0.68884671) Loss_G: 0.00053479 Loss_Enh_Dec: -0.00011979\n",
      "| epoch   4 |  2900/ 4361 batches | lr 0.000000 | ms/batch 398.92 | loss  4.68 | ppl   107.35 | acc     0.48 | train_ae_norm     1.00\n",
      "[4/200][2999/4361] Loss_D: 1.37880683 (Loss_D_real: 0.69012010 Loss_D_fake: 0.68868673) Loss_G: 0.00121214 Loss_Enh_Dec: -0.00075358\n",
      "| epoch   4 |  3000/ 4361 batches | lr 0.000000 | ms/batch 399.77 | loss  4.67 | ppl   106.97 | acc     0.48 | train_ae_norm     1.00\n",
      "[4/200][3099/4361] Loss_D: 1.37972188 (Loss_D_real: 0.68847936 Loss_D_fake: 0.69124258) Loss_G: 0.00028960 Loss_Enh_Dec: -0.00066176\n",
      "| epoch   4 |  3100/ 4361 batches | lr 0.000000 | ms/batch 399.35 | loss  4.68 | ppl   107.62 | acc     0.46 | train_ae_norm     1.00\n",
      "[4/200][3199/4361] Loss_D: 1.38242602 (Loss_D_real: 0.69045180 Loss_D_fake: 0.69197428) Loss_G: 0.00042657 Loss_Enh_Dec: -0.00090291\n",
      "| epoch   4 |  3200/ 4361 batches | lr 0.000000 | ms/batch 399.95 | loss  4.74 | ppl   114.46 | acc     0.48 | train_ae_norm     1.00\n",
      "[4/200][3299/4361] Loss_D: 1.38529170 (Loss_D_real: 0.69314927 Loss_D_fake: 0.69214243) Loss_G: 0.00076768 Loss_Enh_Dec: 0.00020468\n",
      "| epoch   4 |  3300/ 4361 batches | lr 0.000000 | ms/batch 399.18 | loss  4.75 | ppl   115.86 | acc     0.47 | train_ae_norm     1.00\n",
      "[4/200][3399/4361] Loss_D: 1.37992477 (Loss_D_real: 0.68844509 Loss_D_fake: 0.69147968) Loss_G: 0.00107178 Loss_Enh_Dec: -0.00048110\n",
      "| epoch   4 |  3400/ 4361 batches | lr 0.000000 | ms/batch 398.87 | loss  4.69 | ppl   108.95 | acc     0.49 | train_ae_norm     1.00\n",
      "[4/200][3499/4361] Loss_D: 1.38557029 (Loss_D_real: 0.69329369 Loss_D_fake: 0.69227666) Loss_G: 0.00057144 Loss_Enh_Dec: -0.00055251\n",
      "| epoch   4 |  3500/ 4361 batches | lr 0.000000 | ms/batch 398.55 | loss  4.65 | ppl   104.23 | acc     0.49 | train_ae_norm     1.00\n",
      "[4/200][3599/4361] Loss_D: 1.37396383 (Loss_D_real: 0.68382871 Loss_D_fake: 0.69013518) Loss_G: 0.00091213 Loss_Enh_Dec: -0.00029368\n",
      "| epoch   4 |  3600/ 4361 batches | lr 0.000000 | ms/batch 399.38 | loss  4.68 | ppl   107.45 | acc     0.47 | train_ae_norm     1.00\n",
      "[4/200][3699/4361] Loss_D: 1.37993097 (Loss_D_real: 0.68751597 Loss_D_fake: 0.69241494) Loss_G: 0.00059695 Loss_Enh_Dec: -0.00195761\n",
      "| epoch   4 |  3700/ 4361 batches | lr 0.000000 | ms/batch 399.95 | loss  4.69 | ppl   108.58 | acc     0.47 | train_ae_norm     1.00\n",
      "[4/200][3799/4361] Loss_D: 1.37343860 (Loss_D_real: 0.68558061 Loss_D_fake: 0.68785799) Loss_G: 0.00188524 Loss_Enh_Dec: -0.00188918\n",
      "| epoch   4 |  3800/ 4361 batches | lr 0.000000 | ms/batch 399.94 | loss  4.70 | ppl   109.71 | acc     0.52 | train_ae_norm     1.00\n",
      "[4/200][3899/4361] Loss_D: 1.37832117 (Loss_D_real: 0.68794286 Loss_D_fake: 0.69037837) Loss_G: 0.00145954 Loss_Enh_Dec: -0.00233052\n",
      "| epoch   4 |  3900/ 4361 batches | lr 0.000000 | ms/batch 399.33 | loss  4.67 | ppl   106.39 | acc     0.43 | train_ae_norm     1.00\n",
      "[4/200][3999/4361] Loss_D: 1.37820911 (Loss_D_real: 0.68900573 Loss_D_fake: 0.68920344) Loss_G: 0.00057010 Loss_Enh_Dec: -0.00019868\n",
      "| epoch   4 |  4000/ 4361 batches | lr 0.000000 | ms/batch 398.96 | loss  4.70 | ppl   110.05 | acc     0.46 | train_ae_norm     1.00\n",
      "[4/200][4099/4361] Loss_D: 1.38255906 (Loss_D_real: 0.69095373 Loss_D_fake: 0.69160527) Loss_G: 0.00088407 Loss_Enh_Dec: -0.00033643\n",
      "| epoch   4 |  4100/ 4361 batches | lr 0.000000 | ms/batch 399.24 | loss  4.69 | ppl   108.71 | acc     0.49 | train_ae_norm     1.00\n",
      "[4/200][4199/4361] Loss_D: 1.37433863 (Loss_D_real: 0.68891704 Loss_D_fake: 0.68542165) Loss_G: 0.00204880 Loss_Enh_Dec: -0.00112258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  4200/ 4361 batches | lr 0.000000 | ms/batch 399.57 | loss  4.68 | ppl   107.61 | acc     0.48 | train_ae_norm     1.00\n",
      "[4/200][4299/4361] Loss_D: 1.37417293 (Loss_D_real: 0.68536615 Loss_D_fake: 0.68880671) Loss_G: 0.00118500 Loss_Enh_Dec: -0.00326756\n",
      "| epoch   4 |  4300/ 4361 batches | lr 0.000000 | ms/batch 398.72 | loss  4.65 | ppl   104.30 | acc     0.47 | train_ae_norm     1.00\n",
      "| end of epoch   4 | time: 1843.99s | test loss  4.14 | test ppl 62.49 | acc 0.572\n",
      "bleu_self:  [3.51377609e-01 2.17325488e-01 1.95000379e-06 6.54209428e-09\n",
      " 5.22970001e-09]\n",
      "bleu_test:  [8.17708333e-01 4.68120800e-01 7.87482272e-02 1.25407537e-05\n",
      " 5.05060968e-07]\n",
      "bleu_self: [0.35137761,0.21732549,0.00000195,0.00000001,0.00000001]\n",
      "bleu_test: [0.81770833,0.46812080,0.07874823,0.00001254,0.00000051]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 5 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:18.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.711\n",
      "  Average training loss discriminator: 1.027\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.435\n",
      "  Test Loss: 1.862\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   5 |     0/ 4361 batches | lr 0.000000 | ms/batch 860.80 | loss  0.04 | ppl     1.04 | acc     0.49 | train_ae_norm     1.00\n",
      "[5/200][99/4361] Loss_D: 1.37195325 (Loss_D_real: 0.68560529 Loss_D_fake: 0.68634796) Loss_G: 0.00083480 Loss_Enh_Dec: -0.00215904\n",
      "| epoch   5 |   100/ 4361 batches | lr 0.000000 | ms/batch 398.35 | loss  4.67 | ppl   106.98 | acc     0.45 | train_ae_norm     1.00\n",
      "[5/200][199/4361] Loss_D: 1.36777282 (Loss_D_real: 0.68439329 Loss_D_fake: 0.68337953) Loss_G: 0.00091452 Loss_Enh_Dec: -0.00084768\n",
      "| epoch   5 |   200/ 4361 batches | lr 0.000000 | ms/batch 398.80 | loss  4.68 | ppl   107.51 | acc     0.50 | train_ae_norm     1.00\n",
      "[5/200][299/4361] Loss_D: 1.37088299 (Loss_D_real: 0.68421447 Loss_D_fake: 0.68666852) Loss_G: 0.00107324 Loss_Enh_Dec: -0.00019149\n",
      "| epoch   5 |   300/ 4361 batches | lr 0.000000 | ms/batch 398.83 | loss  4.64 | ppl   104.04 | acc     0.48 | train_ae_norm     1.00\n",
      "[5/200][399/4361] Loss_D: 1.37445962 (Loss_D_real: 0.68422937 Loss_D_fake: 0.69023025) Loss_G: 0.00218360 Loss_Enh_Dec: -0.00213388\n",
      "| epoch   5 |   400/ 4361 batches | lr 0.000000 | ms/batch 399.33 | loss  4.58 | ppl    97.52 | acc     0.47 | train_ae_norm     1.00\n",
      "[5/200][499/4361] Loss_D: 1.36682093 (Loss_D_real: 0.68347776 Loss_D_fake: 0.68334317) Loss_G: 0.00194426 Loss_Enh_Dec: -0.00173646\n",
      "| epoch   5 |   500/ 4361 batches | lr 0.000000 | ms/batch 399.42 | loss  4.64 | ppl   103.77 | acc     0.47 | train_ae_norm     1.00\n",
      "[5/200][599/4361] Loss_D: 1.37802696 (Loss_D_real: 0.68453091 Loss_D_fake: 0.69349611) Loss_G: 0.00079260 Loss_Enh_Dec: -0.00103790\n",
      "| epoch   5 |   600/ 4361 batches | lr 0.000000 | ms/batch 398.93 | loss  4.63 | ppl   102.57 | acc     0.43 | train_ae_norm     1.00\n",
      "[5/200][699/4361] Loss_D: 1.37038875 (Loss_D_real: 0.68735683 Loss_D_fake: 0.68303192) Loss_G: 0.00200307 Loss_Enh_Dec: -0.00054630\n",
      "| epoch   5 |   700/ 4361 batches | lr 0.000000 | ms/batch 398.88 | loss  4.59 | ppl    98.83 | acc     0.51 | train_ae_norm     1.00\n",
      "[5/200][799/4361] Loss_D: 1.37360358 (Loss_D_real: 0.68764114 Loss_D_fake: 0.68596238) Loss_G: 0.00180983 Loss_Enh_Dec: -0.00432620\n",
      "| epoch   5 |   800/ 4361 batches | lr 0.000000 | ms/batch 399.45 | loss  4.59 | ppl    98.27 | acc     0.48 | train_ae_norm     1.00\n",
      "[5/200][899/4361] Loss_D: 1.37044001 (Loss_D_real: 0.68559647 Loss_D_fake: 0.68484354) Loss_G: 0.00187383 Loss_Enh_Dec: 0.00005842\n",
      "| epoch   5 |   900/ 4361 batches | lr 0.000000 | ms/batch 399.43 | loss  4.68 | ppl   107.40 | acc     0.51 | train_ae_norm     1.00\n",
      "[5/200][999/4361] Loss_D: 1.37325084 (Loss_D_real: 0.68814510 Loss_D_fake: 0.68510574) Loss_G: 0.00086028 Loss_Enh_Dec: -0.00179047\n",
      "| epoch   5 |  1000/ 4361 batches | lr 0.000000 | ms/batch 398.73 | loss  4.64 | ppl   103.83 | acc     0.46 | train_ae_norm     1.00\n",
      "[5/200][1099/4361] Loss_D: 1.37853074 (Loss_D_real: 0.69320887 Loss_D_fake: 0.68532181) Loss_G: 0.00052241 Loss_Enh_Dec: -0.00180767\n",
      "| epoch   5 |  1100/ 4361 batches | lr 0.000000 | ms/batch 399.32 | loss  4.64 | ppl   103.66 | acc     0.46 | train_ae_norm     1.00\n",
      "[5/200][1199/4361] Loss_D: 1.36821151 (Loss_D_real: 0.68733376 Loss_D_fake: 0.68087780) Loss_G: 0.00117605 Loss_Enh_Dec: 0.00011743\n",
      "| epoch   5 |  1200/ 4361 batches | lr 0.000000 | ms/batch 399.28 | loss  4.67 | ppl   107.16 | acc     0.49 | train_ae_norm     1.00\n",
      "[5/200][1299/4361] Loss_D: 1.36970782 (Loss_D_real: 0.68332672 Loss_D_fake: 0.68638110) Loss_G: 0.00223763 Loss_Enh_Dec: -0.00194068\n",
      "| epoch   5 |  1300/ 4361 batches | lr 0.000000 | ms/batch 399.47 | loss  4.71 | ppl   111.20 | acc     0.48 | train_ae_norm     1.00\n",
      "[5/200][1399/4361] Loss_D: 1.37807620 (Loss_D_real: 0.69247335 Loss_D_fake: 0.68560284) Loss_G: 0.00110926 Loss_Enh_Dec: -0.00273744\n",
      "| epoch   5 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.01 | loss  4.75 | ppl   115.01 | acc     0.42 | train_ae_norm     1.00\n",
      "[5/200][1499/4361] Loss_D: 1.36539960 (Loss_D_real: 0.68739074 Loss_D_fake: 0.67800891) Loss_G: 0.00214744 Loss_Enh_Dec: -0.00104904\n",
      "| epoch   5 |  1500/ 4361 batches | lr 0.000000 | ms/batch 399.08 | loss  4.75 | ppl   115.21 | acc     0.45 | train_ae_norm     1.00\n",
      "[5/200][1599/4361] Loss_D: 1.35051358 (Loss_D_real: 0.67641324 Loss_D_fake: 0.67410034) Loss_G: 0.00354516 Loss_Enh_Dec: -0.00466012\n",
      "| epoch   5 |  1600/ 4361 batches | lr 0.000000 | ms/batch 399.65 | loss  4.72 | ppl   111.93 | acc     0.50 | train_ae_norm     1.00\n",
      "[5/200][1699/4361] Loss_D: 1.36691952 (Loss_D_real: 0.68032491 Loss_D_fake: 0.68659467) Loss_G: 0.00127689 Loss_Enh_Dec: -0.00243091\n",
      "| epoch   5 |  1700/ 4361 batches | lr 0.000000 | ms/batch 399.41 | loss  4.80 | ppl   121.87 | acc     0.40 | train_ae_norm     1.00\n",
      "[5/200][1799/4361] Loss_D: 1.36966276 (Loss_D_real: 0.68061996 Loss_D_fake: 0.68904275) Loss_G: -0.00104375 Loss_Enh_Dec: -0.00718486\n",
      "| epoch   5 |  1800/ 4361 batches | lr 0.000000 | ms/batch 399.71 | loss  4.94 | ppl   140.32 | acc     0.43 | train_ae_norm     1.00\n",
      "[5/200][1899/4361] Loss_D: 1.36989915 (Loss_D_real: 0.68575561 Loss_D_fake: 0.68414354) Loss_G: 0.00249000 Loss_Enh_Dec: -0.00714696\n",
      "| epoch   5 |  1900/ 4361 batches | lr 0.000000 | ms/batch 398.95 | loss  4.99 | ppl   146.70 | acc     0.41 | train_ae_norm     1.00\n",
      "[5/200][1999/4361] Loss_D: 1.36597085 (Loss_D_real: 0.68529934 Loss_D_fake: 0.68067157) Loss_G: 0.00165566 Loss_Enh_Dec: -0.00969819\n",
      "| epoch   5 |  2000/ 4361 batches | lr 0.000000 | ms/batch 399.58 | loss  5.01 | ppl   150.01 | acc     0.39 | train_ae_norm     1.00\n",
      "[5/200][2099/4361] Loss_D: 1.36046231 (Loss_D_real: 0.68354046 Loss_D_fake: 0.67692184) Loss_G: 0.00214128 Loss_Enh_Dec: -0.00606943\n",
      "| epoch   5 |  2100/ 4361 batches | lr 0.000000 | ms/batch 399.37 | loss  5.05 | ppl   156.75 | acc     0.43 | train_ae_norm     1.00\n",
      "[5/200][2199/4361] Loss_D: 1.35343790 (Loss_D_real: 0.68039006 Loss_D_fake: 0.67304790) Loss_G: 0.00060236 Loss_Enh_Dec: -0.01122056\n",
      "| epoch   5 |  2200/ 4361 batches | lr 0.000000 | ms/batch 399.09 | loss  5.05 | ppl   155.80 | acc     0.42 | train_ae_norm     1.00\n",
      "[5/200][2299/4361] Loss_D: 1.35808063 (Loss_D_real: 0.67955703 Loss_D_fake: 0.67852354) Loss_G: 0.00056368 Loss_Enh_Dec: -0.00560974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  2300/ 4361 batches | lr 0.000000 | ms/batch 399.24 | loss  5.01 | ppl   150.01 | acc     0.42 | train_ae_norm     1.00\n",
      "[5/200][2399/4361] Loss_D: 1.35194910 (Loss_D_real: 0.68435478 Loss_D_fake: 0.66759431) Loss_G: 0.00273768 Loss_Enh_Dec: -0.00711892\n",
      "| epoch   5 |  2400/ 4361 batches | lr 0.000000 | ms/batch 398.97 | loss  5.01 | ppl   149.49 | acc     0.39 | train_ae_norm     1.00\n",
      "[5/200][2499/4361] Loss_D: 1.36469972 (Loss_D_real: 0.68392104 Loss_D_fake: 0.68077868) Loss_G: 0.00080840 Loss_Enh_Dec: -0.00262409\n",
      "| epoch   5 |  2500/ 4361 batches | lr 0.000000 | ms/batch 399.16 | loss  5.15 | ppl   172.79 | acc     0.40 | train_ae_norm     1.00\n",
      "[5/200][2599/4361] Loss_D: 1.36778021 (Loss_D_real: 0.68448496 Loss_D_fake: 0.68329519) Loss_G: 0.00151070 Loss_Enh_Dec: -0.00600428\n",
      "| epoch   5 |  2600/ 4361 batches | lr 0.000000 | ms/batch 399.23 | loss  5.07 | ppl   159.22 | acc     0.40 | train_ae_norm     1.00\n",
      "[5/200][2699/4361] Loss_D: 1.37031984 (Loss_D_real: 0.68631268 Loss_D_fake: 0.68400717) Loss_G: 0.00126503 Loss_Enh_Dec: -0.00513118\n",
      "| epoch   5 |  2700/ 4361 batches | lr 0.000000 | ms/batch 397.84 | loss  5.03 | ppl   152.57 | acc     0.38 | train_ae_norm     1.00\n",
      "[5/200][2799/4361] Loss_D: 1.35061586 (Loss_D_real: 0.67012185 Loss_D_fake: 0.68049401) Loss_G: 0.00312782 Loss_Enh_Dec: -0.00706513\n",
      "| epoch   5 |  2800/ 4361 batches | lr 0.000000 | ms/batch 398.82 | loss  4.98 | ppl   146.14 | acc     0.39 | train_ae_norm     1.00\n",
      "[5/200][2899/4361] Loss_D: 1.35768974 (Loss_D_real: 0.68074524 Loss_D_fake: 0.67694449) Loss_G: 0.00196303 Loss_Enh_Dec: -0.00487066\n",
      "| epoch   5 |  2900/ 4361 batches | lr 0.000000 | ms/batch 399.79 | loss  5.03 | ppl   152.28 | acc     0.47 | train_ae_norm     1.00\n",
      "[5/200][2999/4361] Loss_D: 1.33983397 (Loss_D_real: 0.66737241 Loss_D_fake: 0.67246151) Loss_G: 0.00235665 Loss_Enh_Dec: -0.00297535\n",
      "| epoch   5 |  3000/ 4361 batches | lr 0.000000 | ms/batch 398.90 | loss  5.02 | ppl   151.91 | acc     0.39 | train_ae_norm     1.00\n",
      "[5/200][3099/4361] Loss_D: 1.34668350 (Loss_D_real: 0.67203844 Loss_D_fake: 0.67464507) Loss_G: 0.00238110 Loss_Enh_Dec: -0.00297636\n",
      "| epoch   5 |  3100/ 4361 batches | lr 0.000000 | ms/batch 398.94 | loss  5.06 | ppl   157.07 | acc     0.39 | train_ae_norm     1.00\n",
      "[5/200][3199/4361] Loss_D: 1.34435236 (Loss_D_real: 0.66914093 Loss_D_fake: 0.67521143) Loss_G: 0.00223322 Loss_Enh_Dec: -0.00230507\n",
      "| epoch   5 |  3200/ 4361 batches | lr 0.000000 | ms/batch 399.40 | loss  5.16 | ppl   173.77 | acc     0.34 | train_ae_norm     1.00\n",
      "[5/200][3299/4361] Loss_D: 1.34603190 (Loss_D_real: 0.68011397 Loss_D_fake: 0.66591799) Loss_G: 0.00458339 Loss_Enh_Dec: -0.00329031\n",
      "| epoch   5 |  3300/ 4361 batches | lr 0.000000 | ms/batch 399.61 | loss  5.28 | ppl   195.73 | acc     0.38 | train_ae_norm     1.00\n",
      "[5/200][3399/4361] Loss_D: 1.32557845 (Loss_D_real: 0.65293276 Loss_D_fake: 0.67264575) Loss_G: 0.00456370 Loss_Enh_Dec: -0.00769213\n",
      "| epoch   5 |  3400/ 4361 batches | lr 0.000000 | ms/batch 399.31 | loss  5.18 | ppl   177.20 | acc     0.40 | train_ae_norm     1.00\n",
      "[5/200][3499/4361] Loss_D: 1.34507775 (Loss_D_real: 0.66796565 Loss_D_fake: 0.67711204) Loss_G: 0.00244413 Loss_Enh_Dec: -0.00563011\n",
      "| epoch   5 |  3500/ 4361 batches | lr 0.000000 | ms/batch 399.43 | loss  5.12 | ppl   167.89 | acc     0.42 | train_ae_norm     1.00\n",
      "[5/200][3599/4361] Loss_D: 1.34324884 (Loss_D_real: 0.67791772 Loss_D_fake: 0.66533107) Loss_G: 0.00402953 Loss_Enh_Dec: -0.00461103\n",
      "| epoch   5 |  3600/ 4361 batches | lr 0.000000 | ms/batch 399.13 | loss  5.09 | ppl   162.41 | acc     0.42 | train_ae_norm     1.00\n",
      "[5/200][3699/4361] Loss_D: 1.33024001 (Loss_D_real: 0.67077202 Loss_D_fake: 0.65946794) Loss_G: 0.00515680 Loss_Enh_Dec: -0.01062487\n",
      "| epoch   5 |  3700/ 4361 batches | lr 0.000000 | ms/batch 399.05 | loss  5.10 | ppl   163.47 | acc     0.41 | train_ae_norm     1.00\n",
      "[5/200][3799/4361] Loss_D: 1.32937038 (Loss_D_real: 0.66299027 Loss_D_fake: 0.66638011) Loss_G: 0.00432203 Loss_Enh_Dec: -0.01224595\n",
      "| epoch   5 |  3800/ 4361 batches | lr 0.000000 | ms/batch 398.36 | loss  5.19 | ppl   179.85 | acc     0.39 | train_ae_norm     1.00\n",
      "[5/200][3899/4361] Loss_D: 1.30893612 (Loss_D_real: 0.64699996 Loss_D_fake: 0.66193616) Loss_G: 0.00599852 Loss_Enh_Dec: -0.01808614\n",
      "| epoch   5 |  3900/ 4361 batches | lr 0.000000 | ms/batch 399.57 | loss  5.10 | ppl   164.03 | acc     0.39 | train_ae_norm     1.00\n",
      "[5/200][3999/4361] Loss_D: 1.28377652 (Loss_D_real: 0.63799679 Loss_D_fake: 0.64577973) Loss_G: 0.00805042 Loss_Enh_Dec: -0.01893536\n",
      "| epoch   5 |  4000/ 4361 batches | lr 0.000000 | ms/batch 399.40 | loss  5.08 | ppl   160.44 | acc     0.41 | train_ae_norm     1.00\n",
      "[5/200][4099/4361] Loss_D: 1.28711665 (Loss_D_real: 0.65196240 Loss_D_fake: 0.63515425) Loss_G: 0.00963104 Loss_Enh_Dec: -0.02775219\n",
      "| epoch   5 |  4100/ 4361 batches | lr 0.000000 | ms/batch 399.10 | loss  5.05 | ppl   156.02 | acc     0.40 | train_ae_norm     1.00\n",
      "[5/200][4199/4361] Loss_D: 1.29241490 (Loss_D_real: 0.65166986 Loss_D_fake: 0.64074510) Loss_G: 0.00656179 Loss_Enh_Dec: -0.00785373\n",
      "| epoch   5 |  4200/ 4361 batches | lr 0.000000 | ms/batch 398.95 | loss  5.15 | ppl   173.09 | acc     0.40 | train_ae_norm     1.00\n",
      "[5/200][4299/4361] Loss_D: 1.27292514 (Loss_D_real: 0.63652050 Loss_D_fake: 0.63640463) Loss_G: 0.01262055 Loss_Enh_Dec: -0.01523140\n",
      "| epoch   5 |  4300/ 4361 batches | lr 0.000000 | ms/batch 398.99 | loss  5.11 | ppl   165.84 | acc     0.39 | train_ae_norm     1.00\n",
      "| end of epoch   5 | time: 1842.54s | test loss  4.78 | test ppl 118.52 | acc 0.466\n",
      "bleu_self:  [2.20322454e-01 4.59922928e-05 4.61834229e-06 1.49156200e-06\n",
      " 7.83538099e-07]\n",
      "bleu_test:  [9.37499999e-01 4.23828606e-01 1.87145948e-01 1.61626039e-04\n",
      " 3.98958324e-05]\n",
      "bleu_self: [0.22032245,0.00004599,0.00000462,0.00000149,0.00000078]\n",
      "bleu_test: [0.93750000,0.42382861,0.18714595,0.00016163,0.00003990]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 6 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:48.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:14.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:18.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.713\n",
      "  Average training loss discriminator: 0.896\n",
      "  Training epcoh took: 0:01:25\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.455\n",
      "  Test Loss: 1.953\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   6 |     0/ 4361 batches | lr 0.000000 | ms/batch 858.33 | loss  0.05 | ppl     1.05 | acc     0.41 | train_ae_norm     1.00\n",
      "[6/200][99/4361] Loss_D: 1.28999591 (Loss_D_real: 0.65673262 Loss_D_fake: 0.63326335) Loss_G: 0.00944723 Loss_Enh_Dec: -0.02869051\n",
      "| epoch   6 |   100/ 4361 batches | lr 0.000000 | ms/batch 399.29 | loss  5.19 | ppl   178.62 | acc     0.40 | train_ae_norm     1.00\n",
      "[6/200][199/4361] Loss_D: 1.24015999 (Loss_D_real: 0.61277169 Loss_D_fake: 0.62738824) Loss_G: 0.01089746 Loss_Enh_Dec: -0.02196250\n",
      "| epoch   6 |   200/ 4361 batches | lr 0.000000 | ms/batch 399.40 | loss  5.23 | ppl   186.55 | acc     0.38 | train_ae_norm     1.00\n",
      "[6/200][299/4361] Loss_D: 1.32882929 (Loss_D_real: 0.66924572 Loss_D_fake: 0.65958363) Loss_G: 0.00214755 Loss_Enh_Dec: -0.01207471\n",
      "| epoch   6 |   300/ 4361 batches | lr 0.000000 | ms/batch 399.39 | loss  5.29 | ppl   199.20 | acc     0.35 | train_ae_norm     1.00\n",
      "[6/200][399/4361] Loss_D: 1.29313040 (Loss_D_real: 0.65207684 Loss_D_fake: 0.64105362) Loss_G: 0.00762502 Loss_Enh_Dec: -0.01752446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   6 |   400/ 4361 batches | lr 0.000000 | ms/batch 398.88 | loss  5.14 | ppl   170.17 | acc     0.40 | train_ae_norm     1.00\n",
      "[6/200][499/4361] Loss_D: 1.26427722 (Loss_D_real: 0.63366890 Loss_D_fake: 0.63060826) Loss_G: 0.00980524 Loss_Enh_Dec: -0.01898212\n",
      "| epoch   6 |   500/ 4361 batches | lr 0.000000 | ms/batch 398.87 | loss  5.16 | ppl   174.32 | acc     0.38 | train_ae_norm     1.00\n",
      "[6/200][599/4361] Loss_D: 1.28044415 (Loss_D_real: 0.64185023 Loss_D_fake: 0.63859397) Loss_G: 0.00904784 Loss_Enh_Dec: -0.00834971\n",
      "| epoch   6 |   600/ 4361 batches | lr 0.000000 | ms/batch 399.31 | loss  5.14 | ppl   171.21 | acc     0.33 | train_ae_norm     1.00\n",
      "[6/200][699/4361] Loss_D: 1.25440288 (Loss_D_real: 0.63271916 Loss_D_fake: 0.62168378) Loss_G: 0.01051683 Loss_Enh_Dec: -0.02744132\n",
      "| epoch   6 |   700/ 4361 batches | lr 0.000000 | ms/batch 398.62 | loss  5.18 | ppl   177.64 | acc     0.40 | train_ae_norm     1.00\n",
      "[6/200][799/4361] Loss_D: 1.21689820 (Loss_D_real: 0.60143614 Loss_D_fake: 0.61546206) Loss_G: 0.01253337 Loss_Enh_Dec: -0.03521898\n",
      "| epoch   6 |   800/ 4361 batches | lr 0.000000 | ms/batch 398.21 | loss  5.16 | ppl   174.81 | acc     0.38 | train_ae_norm     1.00\n",
      "[6/200][899/4361] Loss_D: 1.17970145 (Loss_D_real: 0.60456431 Loss_D_fake: 0.57513714) Loss_G: 0.01521485 Loss_Enh_Dec: -0.04049115\n",
      "| epoch   6 |   900/ 4361 batches | lr 0.000000 | ms/batch 398.98 | loss  5.20 | ppl   182.13 | acc     0.41 | train_ae_norm     1.00\n",
      "[6/200][999/4361] Loss_D: 1.17708302 (Loss_D_real: 0.58696711 Loss_D_fake: 0.59011585) Loss_G: 0.01428562 Loss_Enh_Dec: -0.09385922\n",
      "| epoch   6 |  1000/ 4361 batches | lr 0.000000 | ms/batch 398.14 | loss  5.26 | ppl   192.14 | acc     0.32 | train_ae_norm     1.00\n",
      "[6/200][1099/4361] Loss_D: 1.29103148 (Loss_D_real: 0.64639986 Loss_D_fake: 0.64463162) Loss_G: 0.00593591 Loss_Enh_Dec: -0.03172961\n",
      "| epoch   6 |  1100/ 4361 batches | lr 0.000000 | ms/batch 398.26 | loss  5.28 | ppl   196.11 | acc     0.35 | train_ae_norm     1.00\n",
      "[6/200][1199/4361] Loss_D: 1.27361429 (Loss_D_real: 0.63730729 Loss_D_fake: 0.63630700) Loss_G: 0.00628804 Loss_Enh_Dec: -0.02020489\n",
      "| epoch   6 |  1200/ 4361 batches | lr 0.000000 | ms/batch 398.44 | loss  5.25 | ppl   190.89 | acc     0.38 | train_ae_norm     1.00\n",
      "[6/200][1299/4361] Loss_D: 1.24722338 (Loss_D_real: 0.63797212 Loss_D_fake: 0.60925126) Loss_G: 0.01160991 Loss_Enh_Dec: -0.01686276\n",
      "| epoch   6 |  1300/ 4361 batches | lr 0.000000 | ms/batch 398.58 | loss  5.25 | ppl   190.54 | acc     0.36 | train_ae_norm     1.00\n",
      "[6/200][1399/4361] Loss_D: 1.20880675 (Loss_D_real: 0.60854125 Loss_D_fake: 0.60026550) Loss_G: 0.01064896 Loss_Enh_Dec: -0.03414362\n",
      "| epoch   6 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.42 | loss  5.25 | ppl   191.33 | acc     0.32 | train_ae_norm     1.00\n",
      "[6/200][1499/4361] Loss_D: 1.13577986 (Loss_D_real: 0.55435139 Loss_D_fake: 0.58142841) Loss_G: 0.01531654 Loss_Enh_Dec: -0.02989854\n",
      "| epoch   6 |  1500/ 4361 batches | lr 0.000000 | ms/batch 399.70 | loss  5.27 | ppl   193.99 | acc     0.35 | train_ae_norm     1.00\n",
      "[6/200][1599/4361] Loss_D: 1.10017562 (Loss_D_real: 0.57246929 Loss_D_fake: 0.52770627) Loss_G: 0.02103956 Loss_Enh_Dec: -0.03329284\n",
      "| epoch   6 |  1600/ 4361 batches | lr 0.000000 | ms/batch 400.20 | loss  5.25 | ppl   189.73 | acc     0.37 | train_ae_norm     1.00\n",
      "[6/200][1699/4361] Loss_D: 1.07326770 (Loss_D_real: 0.54202622 Loss_D_fake: 0.53124142) Loss_G: 0.02344660 Loss_Enh_Dec: -0.02878749\n",
      "| epoch   6 |  1700/ 4361 batches | lr 0.000000 | ms/batch 399.00 | loss  5.21 | ppl   183.56 | acc     0.35 | train_ae_norm     1.00\n",
      "[6/200][1799/4361] Loss_D: 1.07585299 (Loss_D_real: 0.55184573 Loss_D_fake: 0.52400726) Loss_G: 0.02492925 Loss_Enh_Dec: -0.04091255\n",
      "| epoch   6 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.09 | loss  5.22 | ppl   184.13 | acc     0.38 | train_ae_norm     1.00\n",
      "[6/200][1899/4361] Loss_D: 0.99468142 (Loss_D_real: 0.49398744 Loss_D_fake: 0.50069398) Loss_G: 0.02555507 Loss_Enh_Dec: -0.08110320\n",
      "| epoch   6 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.38 | loss  5.26 | ppl   192.29 | acc     0.36 | train_ae_norm     1.00\n",
      "[6/200][1999/4361] Loss_D: 1.16415834 (Loss_D_real: 0.58862066 Loss_D_fake: 0.57553768) Loss_G: 0.01226383 Loss_Enh_Dec: -0.07469783\n",
      "| epoch   6 |  2000/ 4361 batches | lr 0.000000 | ms/batch 399.86 | loss  5.26 | ppl   192.02 | acc     0.36 | train_ae_norm     1.00\n",
      "[6/200][2099/4361] Loss_D: 1.07289863 (Loss_D_real: 0.51328468 Loss_D_fake: 0.55961400) Loss_G: 0.01941721 Loss_Enh_Dec: -0.05293762\n",
      "| epoch   6 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.10 | loss  5.28 | ppl   196.92 | acc     0.40 | train_ae_norm     1.00\n",
      "[6/200][2199/4361] Loss_D: 1.26227093 (Loss_D_real: 0.64729863 Loss_D_fake: 0.61497235) Loss_G: 0.00327446 Loss_Enh_Dec: -0.05631954\n",
      "| epoch   6 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.16 | loss  5.26 | ppl   192.13 | acc     0.35 | train_ae_norm     1.00\n",
      "[6/200][2299/4361] Loss_D: 1.12827516 (Loss_D_real: 0.56008875 Loss_D_fake: 0.56818646) Loss_G: 0.01346021 Loss_Enh_Dec: -0.04009027\n",
      "| epoch   6 |  2300/ 4361 batches | lr 0.000000 | ms/batch 399.44 | loss  5.29 | ppl   198.13 | acc     0.35 | train_ae_norm     1.00\n",
      "[6/200][2399/4361] Loss_D: 1.04731989 (Loss_D_real: 0.53515065 Loss_D_fake: 0.51216930) Loss_G: 0.02254409 Loss_Enh_Dec: -0.07139372\n",
      "| epoch   6 |  2400/ 4361 batches | lr 0.000000 | ms/batch 399.71 | loss  5.20 | ppl   181.54 | acc     0.35 | train_ae_norm     1.00\n",
      "[6/200][2499/4361] Loss_D: 1.08013248 (Loss_D_real: 0.52677035 Loss_D_fake: 0.55336213) Loss_G: 0.02192189 Loss_Enh_Dec: -0.06882817\n",
      "| epoch   6 |  2500/ 4361 batches | lr 0.000000 | ms/batch 399.40 | loss  5.21 | ppl   183.77 | acc     0.41 | train_ae_norm     1.00\n",
      "[6/200][2599/4361] Loss_D: 1.19342518 (Loss_D_real: 0.55567944 Loss_D_fake: 0.63774580) Loss_G: 0.01568540 Loss_Enh_Dec: -0.02751477\n",
      "| epoch   6 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.19 | loss  5.22 | ppl   184.42 | acc     0.33 | train_ae_norm     1.00\n",
      "[6/200][2699/4361] Loss_D: 1.01543319 (Loss_D_real: 0.48318577 Loss_D_fake: 0.53224742) Loss_G: 0.01366698 Loss_Enh_Dec: -0.09428431\n",
      "| epoch   6 |  2700/ 4361 batches | lr 0.000000 | ms/batch 399.81 | loss  5.31 | ppl   201.53 | acc     0.35 | train_ae_norm     1.00\n",
      "[6/200][2799/4361] Loss_D: 1.08990121 (Loss_D_real: 0.55442882 Loss_D_fake: 0.53547239) Loss_G: 0.02051461 Loss_Enh_Dec: -0.06158515\n",
      "| epoch   6 |  2800/ 4361 batches | lr 0.000000 | ms/batch 398.87 | loss  5.24 | ppl   188.17 | acc     0.35 | train_ae_norm     1.00\n",
      "[6/200][2899/4361] Loss_D: 1.06968498 (Loss_D_real: 0.56743789 Loss_D_fake: 0.50224704) Loss_G: 0.02470087 Loss_Enh_Dec: -0.11940686\n",
      "| epoch   6 |  2900/ 4361 batches | lr 0.000000 | ms/batch 398.60 | loss  5.35 | ppl   210.60 | acc     0.33 | train_ae_norm     1.00\n",
      "[6/200][2999/4361] Loss_D: 0.88436681 (Loss_D_real: 0.40373170 Loss_D_fake: 0.48063511) Loss_G: 0.02711378 Loss_Enh_Dec: -0.09291708\n",
      "| epoch   6 |  3000/ 4361 batches | lr 0.000000 | ms/batch 399.54 | loss  5.44 | ppl   230.52 | acc     0.32 | train_ae_norm     1.00\n",
      "[6/200][3099/4361] Loss_D: 0.83212960 (Loss_D_real: 0.40591526 Loss_D_fake: 0.42621431) Loss_G: 0.03699476 Loss_Enh_Dec: -0.12266157\n",
      "| epoch   6 |  3100/ 4361 batches | lr 0.000000 | ms/batch 399.88 | loss  5.37 | ppl   215.77 | acc     0.30 | train_ae_norm     1.00\n",
      "[6/200][3199/4361] Loss_D: 1.01605177 (Loss_D_real: 0.50166631 Loss_D_fake: 0.51438546) Loss_G: 0.02722893 Loss_Enh_Dec: -0.07998540\n",
      "| epoch   6 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.65 | loss  5.28 | ppl   195.78 | acc     0.33 | train_ae_norm     1.00\n",
      "[6/200][3299/4361] Loss_D: 1.04900599 (Loss_D_real: 0.51271623 Loss_D_fake: 0.53628969) Loss_G: 0.02344812 Loss_Enh_Dec: -0.11775956\n",
      "| epoch   6 |  3300/ 4361 batches | lr 0.000000 | ms/batch 400.01 | loss  5.22 | ppl   185.21 | acc     0.32 | train_ae_norm     1.00\n",
      "[6/200][3399/4361] Loss_D: 0.92022187 (Loss_D_real: 0.47605944 Loss_D_fake: 0.44416243) Loss_G: 0.02657954 Loss_Enh_Dec: -0.11972757\n",
      "| epoch   6 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.32 | loss  5.18 | ppl   177.31 | acc     0.39 | train_ae_norm     1.00\n",
      "[6/200][3499/4361] Loss_D: 0.86865979 (Loss_D_real: 0.42773324 Loss_D_fake: 0.44092655) Loss_G: 0.03666216 Loss_Enh_Dec: -0.10610988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   6 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.20 | loss  5.08 | ppl   160.90 | acc     0.41 | train_ae_norm     1.00\n",
      "[6/200][3599/4361] Loss_D: 0.87626243 (Loss_D_real: 0.45039782 Loss_D_fake: 0.42586464) Loss_G: 0.03964215 Loss_Enh_Dec: -0.11943912\n",
      "| epoch   6 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  5.06 | ppl   157.13 | acc     0.43 | train_ae_norm     1.00\n",
      "[6/200][3699/4361] Loss_D: 0.79220510 (Loss_D_real: 0.40514016 Loss_D_fake: 0.38706493) Loss_G: 0.04118444 Loss_Enh_Dec: -0.09333160\n",
      "| epoch   6 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.53 | loss  5.06 | ppl   157.92 | acc     0.40 | train_ae_norm     1.00\n",
      "[6/200][3799/4361] Loss_D: 0.82662231 (Loss_D_real: 0.42923647 Loss_D_fake: 0.39738584) Loss_G: 0.04020854 Loss_Enh_Dec: -0.12629218\n",
      "| epoch   6 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  5.05 | ppl   156.27 | acc     0.42 | train_ae_norm     1.00\n",
      "[6/200][3899/4361] Loss_D: 0.94717169 (Loss_D_real: 0.52507174 Loss_D_fake: 0.42209992) Loss_G: 0.03914130 Loss_Enh_Dec: -0.07212094\n",
      "| epoch   6 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.73 | loss  5.03 | ppl   152.92 | acc     0.38 | train_ae_norm     1.00\n",
      "[6/200][3999/4361] Loss_D: 0.85151237 (Loss_D_real: 0.42786211 Loss_D_fake: 0.42365026) Loss_G: 0.04023667 Loss_Enh_Dec: -0.07038717\n",
      "| epoch   6 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  5.06 | ppl   157.64 | acc     0.41 | train_ae_norm     1.00\n",
      "[6/200][4099/4361] Loss_D: 0.81398100 (Loss_D_real: 0.39302039 Loss_D_fake: 0.42096061) Loss_G: 0.04187850 Loss_Enh_Dec: -0.13060960\n",
      "| epoch   6 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.92 | loss  5.04 | ppl   154.18 | acc     0.39 | train_ae_norm     1.00\n",
      "[6/200][4199/4361] Loss_D: 0.83126593 (Loss_D_real: 0.43768978 Loss_D_fake: 0.39357615) Loss_G: 0.03952419 Loss_Enh_Dec: -0.13757753\n",
      "| epoch   6 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  5.09 | ppl   163.04 | acc     0.41 | train_ae_norm     1.00\n",
      "[6/200][4299/4361] Loss_D: 0.83517700 (Loss_D_real: 0.42716810 Loss_D_fake: 0.40800890) Loss_G: 0.04293497 Loss_Enh_Dec: -0.16331966\n",
      "| epoch   6 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.58 | loss  5.11 | ppl   164.86 | acc     0.40 | train_ae_norm     1.00\n",
      "| end of epoch   6 | time: 1844.72s | test loss  4.80 | test ppl 121.25 | acc 0.462\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 7 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.712\n",
      "  Average training loss discriminator: 0.851\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.453\n",
      "  Test Loss: 2.024\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   7 |     0/ 4361 batches | lr 0.000000 | ms/batch 863.04 | loss  0.05 | ppl     1.05 | acc     0.40 | train_ae_norm     1.00\n",
      "[7/200][99/4361] Loss_D: 0.83392549 (Loss_D_real: 0.42477846 Loss_D_fake: 0.40914702) Loss_G: 0.04564336 Loss_Enh_Dec: -0.14780062\n",
      "| epoch   7 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  5.12 | ppl   167.84 | acc     0.36 | train_ae_norm     1.00\n",
      "[7/200][199/4361] Loss_D: 0.90518326 (Loss_D_real: 0.45170233 Loss_D_fake: 0.45348093) Loss_G: 0.03410577 Loss_Enh_Dec: -0.13258600\n",
      "| epoch   7 |   200/ 4361 batches | lr 0.000000 | ms/batch 400.01 | loss  5.09 | ppl   162.96 | acc     0.42 | train_ae_norm     1.00\n",
      "[7/200][299/4361] Loss_D: 0.94235623 (Loss_D_real: 0.48176026 Loss_D_fake: 0.46059594) Loss_G: 0.03249378 Loss_Enh_Dec: -0.11888459\n",
      "| epoch   7 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.62 | loss  5.04 | ppl   155.02 | acc     0.37 | train_ae_norm     1.00\n",
      "[7/200][399/4361] Loss_D: 0.81194258 (Loss_D_real: 0.39820665 Loss_D_fake: 0.41373596) Loss_G: 0.03820798 Loss_Enh_Dec: -0.17180002\n",
      "| epoch   7 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  4.98 | ppl   145.54 | acc     0.39 | train_ae_norm     1.00\n",
      "[7/200][499/4361] Loss_D: 0.82920736 (Loss_D_real: 0.41204080 Loss_D_fake: 0.41716656) Loss_G: 0.03950933 Loss_Enh_Dec: -0.19673832\n",
      "| epoch   7 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  5.07 | ppl   159.78 | acc     0.42 | train_ae_norm     1.00\n",
      "[7/200][599/4361] Loss_D: 0.94911271 (Loss_D_real: 0.54957521 Loss_D_fake: 0.39953750) Loss_G: 0.04080997 Loss_Enh_Dec: -0.19250096\n",
      "| epoch   7 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.43 | loss  5.04 | ppl   153.71 | acc     0.36 | train_ae_norm     1.00\n",
      "[7/200][699/4361] Loss_D: 0.69957083 (Loss_D_real: 0.31758699 Loss_D_fake: 0.38198385) Loss_G: 0.04548090 Loss_Enh_Dec: -0.19604735\n",
      "| epoch   7 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  5.09 | ppl   161.63 | acc     0.40 | train_ae_norm     1.00\n",
      "[7/200][799/4361] Loss_D: 0.78110021 (Loss_D_real: 0.41378111 Loss_D_fake: 0.36731911) Loss_G: 0.04055439 Loss_Enh_Dec: -0.17621449\n",
      "| epoch   7 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.52 | loss  5.01 | ppl   149.34 | acc     0.42 | train_ae_norm     1.00\n",
      "[7/200][899/4361] Loss_D: 0.76960135 (Loss_D_real: 0.38964605 Loss_D_fake: 0.37995526) Loss_G: 0.04583232 Loss_Enh_Dec: -0.18407746\n",
      "| epoch   7 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.69 | loss  4.97 | ppl   144.31 | acc     0.43 | train_ae_norm     1.00\n",
      "[7/200][999/4361] Loss_D: 0.77894771 (Loss_D_real: 0.41826248 Loss_D_fake: 0.36068520) Loss_G: 0.04303937 Loss_Enh_Dec: -0.16036072\n",
      "| epoch   7 |  1000/ 4361 batches | lr 0.000000 | ms/batch 400.81 | loss  5.02 | ppl   151.53 | acc     0.40 | train_ae_norm     1.00\n",
      "[7/200][1099/4361] Loss_D: 0.82603693 (Loss_D_real: 0.39143336 Loss_D_fake: 0.43460357) Loss_G: 0.04338784 Loss_Enh_Dec: -0.14764395\n",
      "| epoch   7 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.79 | loss  5.04 | ppl   153.71 | acc     0.38 | train_ae_norm     1.00\n",
      "[7/200][1199/4361] Loss_D: 0.66709745 (Loss_D_real: 0.31140611 Loss_D_fake: 0.35569131) Loss_G: 0.05353662 Loss_Enh_Dec: -0.19013980\n",
      "| epoch   7 |  1200/ 4361 batches | lr 0.000000 | ms/batch 399.96 | loss  5.03 | ppl   153.67 | acc     0.43 | train_ae_norm     1.00\n",
      "[7/200][1299/4361] Loss_D: 0.69980443 (Loss_D_real: 0.35908818 Loss_D_fake: 0.34071621) Loss_G: 0.04557237 Loss_Enh_Dec: -0.22048235\n",
      "| epoch   7 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.58 | loss  5.01 | ppl   149.90 | acc     0.39 | train_ae_norm     1.00\n",
      "[7/200][1399/4361] Loss_D: 0.69589996 (Loss_D_real: 0.36293805 Loss_D_fake: 0.33296189) Loss_G: 0.04846518 Loss_Enh_Dec: -0.18647507\n",
      "| epoch   7 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.51 | loss  5.01 | ppl   149.33 | acc     0.38 | train_ae_norm     1.00\n",
      "[7/200][1499/4361] Loss_D: 0.64847445 (Loss_D_real: 0.35895884 Loss_D_fake: 0.28951558) Loss_G: 0.05878375 Loss_Enh_Dec: -0.17653136\n",
      "| epoch   7 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  4.98 | ppl   145.58 | acc     0.41 | train_ae_norm     1.00\n",
      "[7/200][1599/4361] Loss_D: 0.63408989 (Loss_D_real: 0.28225240 Loss_D_fake: 0.35183749) Loss_G: 0.05388980 Loss_Enh_Dec: -0.30189642\n",
      "| epoch   7 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  4.96 | ppl   142.40 | acc     0.40 | train_ae_norm     1.00\n",
      "[7/200][1699/4361] Loss_D: 0.80798745 (Loss_D_real: 0.43833256 Loss_D_fake: 0.36965489) Loss_G: 0.04454328 Loss_Enh_Dec: -0.25553057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   7 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.88 | loss  5.05 | ppl   155.90 | acc     0.37 | train_ae_norm     1.00\n",
      "[7/200][1799/4361] Loss_D: 0.58561182 (Loss_D_real: 0.29022726 Loss_D_fake: 0.29538456) Loss_G: 0.06255469 Loss_Enh_Dec: -0.29149818\n",
      "| epoch   7 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.83 | loss  5.04 | ppl   154.92 | acc     0.38 | train_ae_norm     1.00\n",
      "[7/200][1899/4361] Loss_D: 0.56981874 (Loss_D_real: 0.25752592 Loss_D_fake: 0.31229281) Loss_G: 0.05477221 Loss_Enh_Dec: -0.16432695\n",
      "| epoch   7 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  5.06 | ppl   156.91 | acc     0.39 | train_ae_norm     1.00\n",
      "[7/200][1999/4361] Loss_D: 0.78163743 (Loss_D_real: 0.43602118 Loss_D_fake: 0.34561628) Loss_G: 0.05282926 Loss_Enh_Dec: -0.19783631\n",
      "| epoch   7 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.71 | loss  5.02 | ppl   151.55 | acc     0.43 | train_ae_norm     1.00\n",
      "[7/200][2099/4361] Loss_D: 0.83000255 (Loss_D_real: 0.37027463 Loss_D_fake: 0.45972794) Loss_G: 0.05218854 Loss_Enh_Dec: -0.09092144\n",
      "| epoch   7 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  4.95 | ppl   141.85 | acc     0.44 | train_ae_norm     1.00\n",
      "[7/200][2199/4361] Loss_D: 0.62135440 (Loss_D_real: 0.29201597 Loss_D_fake: 0.32933843) Loss_G: 0.05802888 Loss_Enh_Dec: -0.14009686\n",
      "| epoch   7 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.91 | loss  4.86 | ppl   128.55 | acc     0.43 | train_ae_norm     1.00\n",
      "[7/200][2299/4361] Loss_D: 0.66030568 (Loss_D_real: 0.36947030 Loss_D_fake: 0.29083538) Loss_G: 0.06319191 Loss_Enh_Dec: -0.13201919\n",
      "| epoch   7 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  4.85 | ppl   127.59 | acc     0.43 | train_ae_norm     1.00\n",
      "[7/200][2399/4361] Loss_D: 0.62938023 (Loss_D_real: 0.31752831 Loss_D_fake: 0.31185192) Loss_G: 0.04780807 Loss_Enh_Dec: -0.16407752\n",
      "| epoch   7 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  4.91 | ppl   135.47 | acc     0.37 | train_ae_norm     1.00\n",
      "[7/200][2499/4361] Loss_D: 0.52334267 (Loss_D_real: 0.28537801 Loss_D_fake: 0.23796466) Loss_G: 0.05827084 Loss_Enh_Dec: -0.27538717\n",
      "| epoch   7 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  4.95 | ppl   140.83 | acc     0.44 | train_ae_norm     1.00\n",
      "[7/200][2599/4361] Loss_D: 0.74897623 (Loss_D_real: 0.37414759 Loss_D_fake: 0.37482867) Loss_G: 0.03436674 Loss_Enh_Dec: -0.17894964\n",
      "| epoch   7 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  4.97 | ppl   144.68 | acc     0.39 | train_ae_norm     1.00\n",
      "[7/200][2699/4361] Loss_D: 0.79623318 (Loss_D_real: 0.43100381 Loss_D_fake: 0.36522937) Loss_G: 0.04948812 Loss_Enh_Dec: -0.12646396\n",
      "| epoch   7 |  2700/ 4361 batches | lr 0.000000 | ms/batch 400.67 | loss  4.99 | ppl   146.97 | acc     0.41 | train_ae_norm     1.00\n",
      "[7/200][2799/4361] Loss_D: 0.67183518 (Loss_D_real: 0.34277630 Loss_D_fake: 0.32905886) Loss_G: 0.06107553 Loss_Enh_Dec: -0.14896975\n",
      "| epoch   7 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.60 | loss  4.90 | ppl   133.92 | acc     0.38 | train_ae_norm     1.00\n",
      "[7/200][2899/4361] Loss_D: 0.68026221 (Loss_D_real: 0.36659417 Loss_D_fake: 0.31366801) Loss_G: 0.05349078 Loss_Enh_Dec: -0.21202154\n",
      "| epoch   7 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.67 | loss  4.92 | ppl   136.57 | acc     0.47 | train_ae_norm     1.00\n",
      "[7/200][2999/4361] Loss_D: 0.90466058 (Loss_D_real: 0.44926012 Loss_D_fake: 0.45540047) Loss_G: 0.02406521 Loss_Enh_Dec: -0.14662442\n",
      "| epoch   7 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  4.92 | ppl   137.05 | acc     0.42 | train_ae_norm     1.00\n",
      "[7/200][3099/4361] Loss_D: 0.66823471 (Loss_D_real: 0.33198580 Loss_D_fake: 0.33624893) Loss_G: 0.05446639 Loss_Enh_Dec: -0.13221739\n",
      "| epoch   7 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.76 | loss  4.91 | ppl   136.26 | acc     0.39 | train_ae_norm     1.00\n",
      "[7/200][3199/4361] Loss_D: 0.89216125 (Loss_D_real: 0.45105442 Loss_D_fake: 0.44110680) Loss_G: 0.02190655 Loss_Enh_Dec: -0.07237425\n",
      "| epoch   7 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  4.93 | ppl   137.98 | acc     0.38 | train_ae_norm     1.00\n",
      "[7/200][3299/4361] Loss_D: 0.84293658 (Loss_D_real: 0.43014503 Loss_D_fake: 0.41279155) Loss_G: 0.02763835 Loss_Enh_Dec: -0.16712059\n",
      "| epoch   7 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  4.96 | ppl   142.30 | acc     0.40 | train_ae_norm     1.00\n",
      "[7/200][3399/4361] Loss_D: 0.80599207 (Loss_D_real: 0.41755694 Loss_D_fake: 0.38843513) Loss_G: 0.02651344 Loss_Enh_Dec: -0.13964844\n",
      "| epoch   7 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.29 | loss  4.95 | ppl   140.58 | acc     0.43 | train_ae_norm     1.00\n",
      "[7/200][3499/4361] Loss_D: 0.84855318 (Loss_D_real: 0.39245126 Loss_D_fake: 0.45610189) Loss_G: 0.01810955 Loss_Enh_Dec: -0.12641668\n",
      "| epoch   7 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.90 | loss  4.84 | ppl   126.71 | acc     0.45 | train_ae_norm     1.00\n",
      "[7/200][3599/4361] Loss_D: 1.05084777 (Loss_D_real: 0.52064604 Loss_D_fake: 0.53020179) Loss_G: 0.01011378 Loss_Enh_Dec: -0.08557443\n",
      "| epoch   7 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  4.86 | ppl   129.17 | acc     0.42 | train_ae_norm     1.00\n",
      "[7/200][3699/4361] Loss_D: 0.81576610 (Loss_D_real: 0.39054000 Loss_D_fake: 0.42522606) Loss_G: 0.03533576 Loss_Enh_Dec: -0.06021432\n",
      "| epoch   7 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  4.89 | ppl   132.65 | acc     0.41 | train_ae_norm     1.00\n",
      "[7/200][3799/4361] Loss_D: 0.98058885 (Loss_D_real: 0.51500106 Loss_D_fake: 0.46558779) Loss_G: 0.02431679 Loss_Enh_Dec: -0.11981615\n",
      "| epoch   7 |  3800/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  4.84 | ppl   126.40 | acc     0.49 | train_ae_norm     1.00\n",
      "[7/200][3899/4361] Loss_D: 1.08366394 (Loss_D_real: 0.46584880 Loss_D_fake: 0.61781514) Loss_G: 0.00195466 Loss_Enh_Dec: -0.07478601\n",
      "| epoch   7 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.77 | loss  4.81 | ppl   122.93 | acc     0.42 | train_ae_norm     1.00\n",
      "[7/200][3999/4361] Loss_D: 1.11563647 (Loss_D_real: 0.54572570 Loss_D_fake: 0.56991076) Loss_G: 0.02140918 Loss_Enh_Dec: -0.07541081\n",
      "| epoch   7 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  4.79 | ppl   120.72 | acc     0.44 | train_ae_norm     1.00\n",
      "[7/200][4099/4361] Loss_D: 1.00631857 (Loss_D_real: 0.55466986 Loss_D_fake: 0.45164871) Loss_G: 0.02520766 Loss_Enh_Dec: -0.03232868\n",
      "| epoch   7 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.80 | loss  4.73 | ppl   112.81 | acc     0.43 | train_ae_norm     1.00\n",
      "[7/200][4199/4361] Loss_D: 1.04239774 (Loss_D_real: 0.55267036 Loss_D_fake: 0.48972732) Loss_G: 0.02587399 Loss_Enh_Dec: -0.05982889\n",
      "| epoch   7 |  4200/ 4361 batches | lr 0.000000 | ms/batch 400.72 | loss  4.75 | ppl   115.76 | acc     0.46 | train_ae_norm     1.00\n",
      "[7/200][4299/4361] Loss_D: 0.91236246 (Loss_D_real: 0.50236988 Loss_D_fake: 0.40999255) Loss_G: 0.03797495 Loss_Enh_Dec: -0.13361414\n",
      "| epoch   7 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.27 | loss  4.73 | ppl   113.39 | acc     0.45 | train_ae_norm     1.00\n",
      "| end of epoch   7 | time: 1850.07s | test loss  4.38 | test ppl 79.69 | acc 0.533\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 8 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:18.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.708\n",
      "  Average training loss discriminator: 0.825\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.453\n",
      "  Test Loss: 2.090\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   8 |     0/ 4361 batches | lr 0.000000 | ms/batch 859.73 | loss  0.04 | ppl     1.04 | acc     0.48 | train_ae_norm     1.00\n",
      "[8/200][99/4361] Loss_D: 1.10688496 (Loss_D_real: 0.55506200 Loss_D_fake: 0.55182302) Loss_G: -0.01377871 Loss_Enh_Dec: -0.09172004\n",
      "| epoch   8 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  4.71 | ppl   110.97 | acc     0.45 | train_ae_norm     1.00\n",
      "[8/200][199/4361] Loss_D: 0.98205340 (Loss_D_real: 0.48050237 Loss_D_fake: 0.50155103) Loss_G: 0.02098194 Loss_Enh_Dec: -0.13646187\n",
      "| epoch   8 |   200/ 4361 batches | lr 0.000000 | ms/batch 400.50 | loss  4.77 | ppl   117.37 | acc     0.47 | train_ae_norm     1.00\n",
      "[8/200][299/4361] Loss_D: 1.07516670 (Loss_D_real: 0.52335018 Loss_D_fake: 0.55181652) Loss_G: 0.02006643 Loss_Enh_Dec: -0.11080855\n",
      "| epoch   8 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.73 | loss  4.76 | ppl   116.99 | acc     0.43 | train_ae_norm     1.00\n",
      "[8/200][399/4361] Loss_D: 0.99872321 (Loss_D_real: 0.54961038 Loss_D_fake: 0.44911283) Loss_G: 0.03437701 Loss_Enh_Dec: -0.07456758\n",
      "| epoch   8 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.32 | loss  4.68 | ppl   107.28 | acc     0.48 | train_ae_norm     1.00\n",
      "[8/200][499/4361] Loss_D: 1.06880963 (Loss_D_real: 0.57080638 Loss_D_fake: 0.49800324) Loss_G: 0.02444044 Loss_Enh_Dec: -0.07820817\n",
      "| epoch   8 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.74 | loss  4.71 | ppl   110.56 | acc     0.48 | train_ae_norm     1.00\n",
      "[8/200][599/4361] Loss_D: 1.04427528 (Loss_D_real: 0.53838909 Loss_D_fake: 0.50588620) Loss_G: 0.00739487 Loss_Enh_Dec: -0.05386051\n",
      "| epoch   8 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.65 | loss  4.68 | ppl   107.69 | acc     0.41 | train_ae_norm     1.00\n",
      "[8/200][699/4361] Loss_D: 1.03548217 (Loss_D_real: 0.54514885 Loss_D_fake: 0.49033326) Loss_G: 0.02008571 Loss_Enh_Dec: -0.17883959\n",
      "| epoch   8 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.51 | loss  4.74 | ppl   114.05 | acc     0.48 | train_ae_norm     1.00\n",
      "[8/200][799/4361] Loss_D: 0.90409762 (Loss_D_real: 0.48396677 Loss_D_fake: 0.42013085) Loss_G: 0.01938558 Loss_Enh_Dec: -0.09615435\n",
      "| epoch   8 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.51 | loss  4.74 | ppl   114.02 | acc     0.45 | train_ae_norm     1.00\n",
      "[8/200][899/4361] Loss_D: 0.98206353 (Loss_D_real: 0.49626860 Loss_D_fake: 0.48579496) Loss_G: 0.02916541 Loss_Enh_Dec: -0.10950830\n",
      "| epoch   8 |   900/ 4361 batches | lr 0.000000 | ms/batch 399.21 | loss  4.79 | ppl   120.86 | acc     0.46 | train_ae_norm     1.00\n",
      "[8/200][999/4361] Loss_D: 0.84935933 (Loss_D_real: 0.44856715 Loss_D_fake: 0.40079218) Loss_G: 0.03535340 Loss_Enh_Dec: -0.30142170\n",
      "| epoch   8 |  1000/ 4361 batches | lr 0.000000 | ms/batch 399.84 | loss  4.79 | ppl   120.21 | acc     0.46 | train_ae_norm     1.00\n",
      "[8/200][1099/4361] Loss_D: 0.93079066 (Loss_D_real: 0.49263772 Loss_D_fake: 0.43815297) Loss_G: 0.02710704 Loss_Enh_Dec: -0.15941803\n",
      "| epoch   8 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  4.80 | ppl   122.06 | acc     0.42 | train_ae_norm     1.00\n",
      "[8/200][1199/4361] Loss_D: 0.78430140 (Loss_D_real: 0.40343139 Loss_D_fake: 0.38086998) Loss_G: 0.03494777 Loss_Enh_Dec: -0.20855646\n",
      "| epoch   8 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.41 | loss  4.82 | ppl   124.30 | acc     0.49 | train_ae_norm     1.00\n",
      "[8/200][1299/4361] Loss_D: 0.75089908 (Loss_D_real: 0.34977981 Loss_D_fake: 0.40111923) Loss_G: 0.03419628 Loss_Enh_Dec: -0.25644061\n",
      "| epoch   8 |  1300/ 4361 batches | lr 0.000000 | ms/batch 399.52 | loss  4.79 | ppl   120.56 | acc     0.44 | train_ae_norm     1.00\n",
      "[8/200][1399/4361] Loss_D: 0.82098848 (Loss_D_real: 0.37172315 Loss_D_fake: 0.44926533) Loss_G: 0.03867481 Loss_Enh_Dec: -0.16213545\n",
      "| epoch   8 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  4.81 | ppl   123.06 | acc     0.39 | train_ae_norm     1.00\n",
      "[8/200][1499/4361] Loss_D: 0.88955677 (Loss_D_real: 0.45704383 Loss_D_fake: 0.43251297) Loss_G: 0.05148213 Loss_Enh_Dec: -0.13656762\n",
      "| epoch   8 |  1500/ 4361 batches | lr 0.000000 | ms/batch 400.93 | loss  4.80 | ppl   121.64 | acc     0.44 | train_ae_norm     1.00\n",
      "[8/200][1599/4361] Loss_D: 1.13536215 (Loss_D_real: 0.56178671 Loss_D_fake: 0.57357538) Loss_G: 0.01846153 Loss_Enh_Dec: -0.08646294\n",
      "| epoch   8 |  1600/ 4361 batches | lr 0.000000 | ms/batch 399.75 | loss  4.77 | ppl   117.84 | acc     0.46 | train_ae_norm     1.00\n",
      "[8/200][1699/4361] Loss_D: 1.06510448 (Loss_D_real: 0.53367031 Loss_D_fake: 0.53143424) Loss_G: 0.02305737 Loss_Enh_Dec: -0.10821586\n",
      "| epoch   8 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.10 | loss  4.73 | ppl   113.58 | acc     0.42 | train_ae_norm     1.00\n",
      "[8/200][1799/4361] Loss_D: 0.88516247 (Loss_D_real: 0.45752916 Loss_D_fake: 0.42763329) Loss_G: 0.03454142 Loss_Enh_Dec: -0.10585896\n",
      "| epoch   8 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.74 | loss  4.70 | ppl   110.41 | acc     0.46 | train_ae_norm     1.00\n",
      "[8/200][1899/4361] Loss_D: 0.84105819 (Loss_D_real: 0.42367232 Loss_D_fake: 0.41738588) Loss_G: 0.03326220 Loss_Enh_Dec: -0.03287729\n",
      "| epoch   8 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.13 | loss  4.71 | ppl   110.56 | acc     0.46 | train_ae_norm     1.00\n",
      "[8/200][1999/4361] Loss_D: 0.95622081 (Loss_D_real: 0.51610416 Loss_D_fake: 0.44011664) Loss_G: 0.02298358 Loss_Enh_Dec: -0.09036269\n",
      "| epoch   8 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.02 | loss  4.68 | ppl   107.48 | acc     0.49 | train_ae_norm     1.00\n",
      "[8/200][2099/4361] Loss_D: 0.87790298 (Loss_D_real: 0.47171682 Loss_D_fake: 0.40618616) Loss_G: 0.03080800 Loss_Enh_Dec: -0.07226304\n",
      "| epoch   8 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  4.66 | ppl   105.48 | acc     0.48 | train_ae_norm     1.00\n",
      "[8/200][2199/4361] Loss_D: 0.90740770 (Loss_D_real: 0.44955525 Loss_D_fake: 0.45785245) Loss_G: 0.03343828 Loss_Enh_Dec: -0.07271802\n",
      "| epoch   8 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.90 | loss  4.68 | ppl   107.33 | acc     0.47 | train_ae_norm     1.00\n",
      "[8/200][2299/4361] Loss_D: 0.94808233 (Loss_D_real: 0.47709376 Loss_D_fake: 0.47098860) Loss_G: 0.02851436 Loss_Enh_Dec: -0.08942083\n",
      "| epoch   8 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.43 | loss  4.65 | ppl   104.57 | acc     0.46 | train_ae_norm     1.00\n",
      "[8/200][2399/4361] Loss_D: 0.91279149 (Loss_D_real: 0.46197033 Loss_D_fake: 0.45082116) Loss_G: 0.02878833 Loss_Enh_Dec: -0.14897716\n",
      "| epoch   8 |  2400/ 4361 batches | lr 0.000000 | ms/batch 399.49 | loss  4.66 | ppl   105.72 | acc     0.43 | train_ae_norm     1.00\n",
      "[8/200][2499/4361] Loss_D: 0.90629852 (Loss_D_real: 0.39808029 Loss_D_fake: 0.50821823) Loss_G: 0.04501752 Loss_Enh_Dec: -0.15375441\n",
      "| epoch   8 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  4.71 | ppl   111.01 | acc     0.51 | train_ae_norm     1.00\n",
      "[8/200][2599/4361] Loss_D: 1.18508172 (Loss_D_real: 0.70478863 Loss_D_fake: 0.48029312) Loss_G: 0.02692038 Loss_Enh_Dec: -0.10726052\n",
      "| epoch   8 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.62 | loss  4.69 | ppl   108.64 | acc     0.46 | train_ae_norm     1.00\n",
      "[8/200][2699/4361] Loss_D: 0.96337497 (Loss_D_real: 0.56477642 Loss_D_fake: 0.39859852) Loss_G: 0.03575429 Loss_Enh_Dec: -0.10078941\n",
      "| epoch   8 |  2700/ 4361 batches | lr 0.000000 | ms/batch 399.80 | loss  4.72 | ppl   111.72 | acc     0.45 | train_ae_norm     1.00\n",
      "[8/200][2799/4361] Loss_D: 1.17465210 (Loss_D_real: 0.53254098 Loss_D_fake: 0.64211106) Loss_G: -0.00121190 Loss_Enh_Dec: -0.08229981\n",
      "| epoch   8 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.52 | loss  4.62 | ppl   101.26 | acc     0.46 | train_ae_norm     1.00\n",
      "[8/200][2899/4361] Loss_D: 1.02425826 (Loss_D_real: 0.48989031 Loss_D_fake: 0.53436792) Loss_G: 0.03939487 Loss_Enh_Dec: -0.09431631\n",
      "| epoch   8 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.76 | loss  4.65 | ppl   104.24 | acc     0.50 | train_ae_norm     1.00\n",
      "[8/200][2999/4361] Loss_D: 1.01513815 (Loss_D_real: 0.46330810 Loss_D_fake: 0.55183005) Loss_G: 0.01525396 Loss_Enh_Dec: -0.07391414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   8 |  3000/ 4361 batches | lr 0.000000 | ms/batch 399.95 | loss  4.65 | ppl   104.61 | acc     0.46 | train_ae_norm     1.00\n",
      "[8/200][3099/4361] Loss_D: 0.96318138 (Loss_D_real: 0.39700669 Loss_D_fake: 0.56617469) Loss_G: 0.02801378 Loss_Enh_Dec: -0.09518508\n",
      "| epoch   8 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.20 | loss  4.66 | ppl   105.76 | acc     0.44 | train_ae_norm     1.00\n",
      "[8/200][3199/4361] Loss_D: 0.87862903 (Loss_D_real: 0.42013109 Loss_D_fake: 0.45849794) Loss_G: 0.02900676 Loss_Enh_Dec: -0.10450589\n",
      "| epoch   8 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.24 | loss  4.67 | ppl   106.83 | acc     0.46 | train_ae_norm     1.00\n",
      "[8/200][3299/4361] Loss_D: 0.99634153 (Loss_D_real: 0.43515652 Loss_D_fake: 0.56118500) Loss_G: 0.01838928 Loss_Enh_Dec: -0.12413012\n",
      "| epoch   8 |  3300/ 4361 batches | lr 0.000000 | ms/batch 400.72 | loss  4.66 | ppl   106.16 | acc     0.46 | train_ae_norm     1.00\n",
      "[8/200][3399/4361] Loss_D: 0.85905617 (Loss_D_real: 0.43155226 Loss_D_fake: 0.42750391) Loss_G: 0.03784872 Loss_Enh_Dec: -0.24847284\n",
      "| epoch   8 |  3400/ 4361 batches | lr 0.000000 | ms/batch 399.69 | loss  4.64 | ppl   103.91 | acc     0.45 | train_ae_norm     1.00\n",
      "[8/200][3499/4361] Loss_D: 0.90868872 (Loss_D_real: 0.49430439 Loss_D_fake: 0.41438434) Loss_G: 0.02942871 Loss_Enh_Dec: -0.22086063\n",
      "| epoch   8 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.91 | loss  4.59 | ppl    98.39 | acc     0.49 | train_ae_norm     1.00\n",
      "[8/200][3599/4361] Loss_D: 0.71107543 (Loss_D_real: 0.31492823 Loss_D_fake: 0.39614719) Loss_G: 0.03783526 Loss_Enh_Dec: -0.16812141\n",
      "| epoch   8 |  3600/ 4361 batches | lr 0.000000 | ms/batch 400.31 | loss  4.57 | ppl    96.67 | acc     0.49 | train_ae_norm     1.00\n",
      "[8/200][3699/4361] Loss_D: 0.80502892 (Loss_D_real: 0.43031999 Loss_D_fake: 0.37470889) Loss_G: 0.05458452 Loss_Enh_Dec: -0.18989171\n",
      "| epoch   8 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.47 | loss  4.58 | ppl    97.48 | acc     0.47 | train_ae_norm     1.00\n",
      "[8/200][3799/4361] Loss_D: 0.78086209 (Loss_D_real: 0.35231894 Loss_D_fake: 0.42854315) Loss_G: 0.04057670 Loss_Enh_Dec: -0.17011201\n",
      "| epoch   8 |  3800/ 4361 batches | lr 0.000000 | ms/batch 400.78 | loss  4.63 | ppl   102.34 | acc     0.48 | train_ae_norm     1.00\n",
      "[8/200][3899/4361] Loss_D: 0.99764812 (Loss_D_real: 0.58425730 Loss_D_fake: 0.41339085) Loss_G: 0.04230615 Loss_Enh_Dec: -0.15158318\n",
      "| epoch   8 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.25 | loss  4.64 | ppl   103.08 | acc     0.45 | train_ae_norm     1.00\n",
      "[8/200][3999/4361] Loss_D: 0.91610229 (Loss_D_real: 0.51492846 Loss_D_fake: 0.40117383) Loss_G: 0.03115987 Loss_Enh_Dec: -0.17628317\n",
      "| epoch   8 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.46 | loss  4.63 | ppl   102.07 | acc     0.48 | train_ae_norm     1.00\n",
      "[8/200][4099/4361] Loss_D: 0.96552444 (Loss_D_real: 0.54214305 Loss_D_fake: 0.42338142) Loss_G: 0.03428868 Loss_Enh_Dec: -0.14278995\n",
      "| epoch   8 |  4100/ 4361 batches | lr 0.000000 | ms/batch 399.88 | loss  4.62 | ppl   101.89 | acc     0.47 | train_ae_norm     1.00\n",
      "[8/200][4199/4361] Loss_D: 0.83475614 (Loss_D_real: 0.43352291 Loss_D_fake: 0.40123320) Loss_G: 0.03281547 Loss_Enh_Dec: -0.12728074\n",
      "| epoch   8 |  4200/ 4361 batches | lr 0.000000 | ms/batch 400.51 | loss  4.67 | ppl   106.23 | acc     0.48 | train_ae_norm     1.00\n",
      "[8/200][4299/4361] Loss_D: 0.94037980 (Loss_D_real: 0.45430788 Loss_D_fake: 0.48607191) Loss_G: 0.00603829 Loss_Enh_Dec: -0.09734613\n",
      "| epoch   8 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.27 | loss  4.59 | ppl    98.69 | acc     0.47 | train_ae_norm     1.00\n",
      "| end of epoch   8 | time: 1847.88s | test loss  4.30 | test ppl 73.64 | acc 0.552\n",
      "bleu_self:  [2.73915873e-01 6.68057120e-02 6.69474862e-07 6.46499973e-09\n",
      " 6.18431762e-09]\n",
      "bleu_test:  [7.74553571e-01 5.18117121e-01 1.14822179e-01 1.05237058e-01\n",
      " 1.26131092e-04]\n",
      "bleu_self: [0.27391587,0.06680571,0.00000067,0.00000001,0.00000001]\n",
      "bleu_test: [0.77455357,0.51811712,0.11482218,0.10523706,0.00012613]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 9 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:18.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.707\n",
      "  Average training loss discriminator: 0.807\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.458\n",
      "  Test Loss: 2.164\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   9 |     0/ 4361 batches | lr 0.000000 | ms/batch 859.94 | loss  0.04 | ppl     1.04 | acc     0.48 | train_ae_norm     1.00\n",
      "[9/200][99/4361] Loss_D: 0.95083714 (Loss_D_real: 0.46804032 Loss_D_fake: 0.48279685) Loss_G: 0.01790196 Loss_Enh_Dec: -0.24033082\n",
      "| epoch   9 |   100/ 4361 batches | lr 0.000000 | ms/batch 400.58 | loss  4.63 | ppl   102.05 | acc     0.47 | train_ae_norm     1.00\n",
      "[9/200][199/4361] Loss_D: 0.96347243 (Loss_D_real: 0.47735453 Loss_D_fake: 0.48611790) Loss_G: 0.03400330 Loss_Enh_Dec: -0.15450640\n",
      "| epoch   9 |   200/ 4361 batches | lr 0.000000 | ms/batch 400.58 | loss  4.61 | ppl   100.58 | acc     0.49 | train_ae_norm     1.00\n",
      "[9/200][299/4361] Loss_D: 0.76671743 (Loss_D_real: 0.35587066 Loss_D_fake: 0.41084680) Loss_G: 0.04299268 Loss_Enh_Dec: -0.08886082\n",
      "| epoch   9 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.92 | loss  4.57 | ppl    96.60 | acc     0.46 | train_ae_norm     1.00\n",
      "[9/200][399/4361] Loss_D: 1.06900430 (Loss_D_real: 0.54998171 Loss_D_fake: 0.51902258) Loss_G: 0.03780549 Loss_Enh_Dec: -0.14809798\n",
      "| epoch   9 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.31 | loss  4.51 | ppl    91.35 | acc     0.51 | train_ae_norm     1.00\n",
      "[9/200][499/4361] Loss_D: 0.96145523 (Loss_D_real: 0.45924088 Loss_D_fake: 0.50221431) Loss_G: 0.03169382 Loss_Enh_Dec: -0.13007759\n",
      "| epoch   9 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.81 | loss  4.59 | ppl    98.50 | acc     0.47 | train_ae_norm     1.00\n",
      "[9/200][599/4361] Loss_D: 0.94984859 (Loss_D_real: 0.44303274 Loss_D_fake: 0.50681585) Loss_G: 0.02050862 Loss_Enh_Dec: -0.11750250\n",
      "| epoch   9 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.79 | loss  4.55 | ppl    94.85 | acc     0.44 | train_ae_norm     1.00\n",
      "[9/200][699/4361] Loss_D: 1.07018769 (Loss_D_real: 0.55704522 Loss_D_fake: 0.51314247) Loss_G: 0.03506342 Loss_Enh_Dec: -0.13244003\n",
      "| epoch   9 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.08 | loss  4.59 | ppl    98.95 | acc     0.51 | train_ae_norm     1.00\n",
      "[9/200][799/4361] Loss_D: 0.95896840 (Loss_D_real: 0.43594179 Loss_D_fake: 0.52302665) Loss_G: 0.00981343 Loss_Enh_Dec: -0.13858353\n",
      "| epoch   9 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.71 | loss  4.58 | ppl    97.17 | acc     0.50 | train_ae_norm     1.00\n",
      "[9/200][899/4361] Loss_D: 1.19579577 (Loss_D_real: 0.65241688 Loss_D_fake: 0.54337895) Loss_G: 0.02530333 Loss_Enh_Dec: -0.10553473\n",
      "| epoch   9 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.81 | loss  4.57 | ppl    96.22 | acc     0.53 | train_ae_norm     1.00\n",
      "[9/200][999/4361] Loss_D: 0.99529231 (Loss_D_real: 0.56175220 Loss_D_fake: 0.43354008) Loss_G: 0.03739559 Loss_Enh_Dec: -0.10225473\n",
      "| epoch   9 |  1000/ 4361 batches | lr 0.000000 | ms/batch 400.61 | loss  4.54 | ppl    93.92 | acc     0.46 | train_ae_norm     1.00\n",
      "[9/200][1099/4361] Loss_D: 0.86904728 (Loss_D_real: 0.44932398 Loss_D_fake: 0.41972327) Loss_G: 0.01997516 Loss_Enh_Dec: -0.12106190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   9 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.80 | loss  4.52 | ppl    91.94 | acc     0.48 | train_ae_norm     1.00\n",
      "[9/200][1199/4361] Loss_D: 0.95281482 (Loss_D_real: 0.49139977 Loss_D_fake: 0.46141505) Loss_G: 0.02603627 Loss_Enh_Dec: -0.12924515\n",
      "| epoch   9 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  4.54 | ppl    93.43 | acc     0.51 | train_ae_norm     1.00\n",
      "[9/200][1299/4361] Loss_D: 0.78395474 (Loss_D_real: 0.38308454 Loss_D_fake: 0.40087017) Loss_G: 0.03144911 Loss_Enh_Dec: -0.13457690\n",
      "| epoch   9 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.67 | loss  4.50 | ppl    90.26 | acc     0.49 | train_ae_norm     1.00\n",
      "[9/200][1399/4361] Loss_D: 0.83318412 (Loss_D_real: 0.41624007 Loss_D_fake: 0.41694409) Loss_G: 0.05064386 Loss_Enh_Dec: -0.26178440\n",
      "| epoch   9 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.60 | loss  4.50 | ppl    89.91 | acc     0.44 | train_ae_norm     1.00\n",
      "[9/200][1499/4361] Loss_D: 0.86734396 (Loss_D_real: 0.48535424 Loss_D_fake: 0.38198972) Loss_G: 0.02766383 Loss_Enh_Dec: -0.11995535\n",
      "| epoch   9 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.05 | loss  4.54 | ppl    93.40 | acc     0.47 | train_ae_norm     1.00\n",
      "[9/200][1599/4361] Loss_D: 0.98379666 (Loss_D_real: 0.49316940 Loss_D_fake: 0.49062726) Loss_G: 0.03544335 Loss_Enh_Dec: -0.16790254\n",
      "| epoch   9 |  1600/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  4.50 | ppl    90.02 | acc     0.51 | train_ae_norm     1.00\n",
      "[9/200][1699/4361] Loss_D: 0.79052806 (Loss_D_real: 0.38305339 Loss_D_fake: 0.40747470) Loss_G: 0.03672655 Loss_Enh_Dec: -0.16557659\n",
      "| epoch   9 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.37 | loss  4.49 | ppl    89.34 | acc     0.50 | train_ae_norm     1.00\n",
      "[9/200][1799/4361] Loss_D: 0.84347183 (Loss_D_real: 0.40226573 Loss_D_fake: 0.44120610) Loss_G: 0.05203170 Loss_Enh_Dec: -0.18940409\n",
      "| epoch   9 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.38 | loss  4.44 | ppl    84.66 | acc     0.50 | train_ae_norm     1.00\n",
      "[9/200][1899/4361] Loss_D: 0.73422003 (Loss_D_real: 0.36060572 Loss_D_fake: 0.37361431) Loss_G: 0.05601326 Loss_Enh_Dec: -0.24690019\n",
      "| epoch   9 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.25 | loss  4.51 | ppl    90.73 | acc     0.50 | train_ae_norm     1.00\n",
      "[9/200][1999/4361] Loss_D: 0.94909704 (Loss_D_real: 0.46748203 Loss_D_fake: 0.48161501) Loss_G: 0.01830497 Loss_Enh_Dec: -0.20167382\n",
      "| epoch   9 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.56 | loss  4.47 | ppl    86.99 | acc     0.51 | train_ae_norm     1.00\n",
      "[9/200][2099/4361] Loss_D: 0.86199117 (Loss_D_real: 0.43751571 Loss_D_fake: 0.42447549) Loss_G: 0.03931355 Loss_Enh_Dec: -0.17829695\n",
      "| epoch   9 |  2100/ 4361 batches | lr 0.000000 | ms/batch 402.18 | loss  4.46 | ppl    86.60 | acc     0.52 | train_ae_norm     1.00\n",
      "[9/200][2199/4361] Loss_D: 0.88943088 (Loss_D_real: 0.52274060 Loss_D_fake: 0.36669025) Loss_G: 0.05104599 Loss_Enh_Dec: -0.15103221\n",
      "| epoch   9 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.66 | loss  4.44 | ppl    85.08 | acc     0.51 | train_ae_norm     1.00\n",
      "[9/200][2299/4361] Loss_D: 0.88125110 (Loss_D_real: 0.40522879 Loss_D_fake: 0.47602233) Loss_G: 0.02135130 Loss_Enh_Dec: -0.16682188\n",
      "| epoch   9 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.35 | loss  4.42 | ppl    82.94 | acc     0.53 | train_ae_norm     1.00\n",
      "[9/200][2399/4361] Loss_D: 0.71969283 (Loss_D_real: 0.36248940 Loss_D_fake: 0.35720345) Loss_G: 0.05504055 Loss_Enh_Dec: -0.22117472\n",
      "| epoch   9 |  2400/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  4.42 | ppl    83.02 | acc     0.48 | train_ae_norm     1.00\n",
      "[9/200][2499/4361] Loss_D: 0.93829232 (Loss_D_real: 0.44501024 Loss_D_fake: 0.49328208) Loss_G: 0.01461332 Loss_Enh_Dec: -0.13203518\n",
      "| epoch   9 |  2500/ 4361 batches | lr 0.000000 | ms/batch 399.98 | loss  4.48 | ppl    88.30 | acc     0.51 | train_ae_norm     1.00\n",
      "[9/200][2599/4361] Loss_D: 0.81200355 (Loss_D_real: 0.41639456 Loss_D_fake: 0.39560899) Loss_G: 0.05296070 Loss_Enh_Dec: -0.18790525\n",
      "| epoch   9 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.29 | loss  4.46 | ppl    86.73 | acc     0.49 | train_ae_norm     1.00\n",
      "[9/200][2699/4361] Loss_D: 1.01531863 (Loss_D_real: 0.51940119 Loss_D_fake: 0.49591741) Loss_G: 0.03519553 Loss_Enh_Dec: -0.13499731\n",
      "| epoch   9 |  2700/ 4361 batches | lr 0.000000 | ms/batch 400.69 | loss  4.48 | ppl    88.41 | acc     0.46 | train_ae_norm     1.00\n",
      "[9/200][2799/4361] Loss_D: 0.92143637 (Loss_D_real: 0.47310528 Loss_D_fake: 0.44833109) Loss_G: 0.02163409 Loss_Enh_Dec: -0.17393418\n",
      "| epoch   9 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.61 | loss  4.41 | ppl    82.31 | acc     0.47 | train_ae_norm     1.00\n",
      "[9/200][2899/4361] Loss_D: 1.10953248 (Loss_D_real: 0.56842136 Loss_D_fake: 0.54111111) Loss_G: 0.01051155 Loss_Enh_Dec: -0.20440924\n",
      "| epoch   9 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.37 | loss  4.44 | ppl    84.96 | acc     0.51 | train_ae_norm     1.00\n",
      "[9/200][2999/4361] Loss_D: 0.88869405 (Loss_D_real: 0.46280071 Loss_D_fake: 0.42589331) Loss_G: 0.02523807 Loss_Enh_Dec: -0.10608237\n",
      "| epoch   9 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.05 | loss  4.44 | ppl    84.95 | acc     0.49 | train_ae_norm     1.00\n",
      "[9/200][3099/4361] Loss_D: 0.95584041 (Loss_D_real: 0.56424850 Loss_D_fake: 0.39159191) Loss_G: 0.05846812 Loss_Enh_Dec: -0.15218401\n",
      "| epoch   9 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.37 | loss  4.45 | ppl    85.34 | acc     0.43 | train_ae_norm     1.00\n",
      "[9/200][3199/4361] Loss_D: 0.95863360 (Loss_D_real: 0.50415313 Loss_D_fake: 0.45448047) Loss_G: 0.03888598 Loss_Enh_Dec: -0.18367724\n",
      "| epoch   9 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.54 | loss  4.50 | ppl    90.04 | acc     0.47 | train_ae_norm     1.00\n",
      "[9/200][3299/4361] Loss_D: 0.84827483 (Loss_D_real: 0.47181511 Loss_D_fake: 0.37645972) Loss_G: 0.03913068 Loss_Enh_Dec: -0.17381349\n",
      "| epoch   9 |  3300/ 4361 batches | lr 0.000000 | ms/batch 400.51 | loss  4.50 | ppl    90.24 | acc     0.48 | train_ae_norm     1.00\n",
      "[9/200][3399/4361] Loss_D: 0.87985861 (Loss_D_real: 0.40282544 Loss_D_fake: 0.47703320) Loss_G: 0.03433046 Loss_Enh_Dec: -0.12684828\n",
      "| epoch   9 |  3400/ 4361 batches | lr 0.000000 | ms/batch 399.72 | loss  4.46 | ppl    86.90 | acc     0.51 | train_ae_norm     1.00\n",
      "[9/200][3499/4361] Loss_D: 0.91136646 (Loss_D_real: 0.44303083 Loss_D_fake: 0.46833563) Loss_G: 0.03417895 Loss_Enh_Dec: -0.21891312\n",
      "| epoch   9 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.46 | loss  4.40 | ppl    81.67 | acc     0.51 | train_ae_norm     1.00\n",
      "[9/200][3599/4361] Loss_D: 0.93847114 (Loss_D_real: 0.47403562 Loss_D_fake: 0.46443552) Loss_G: 0.01845038 Loss_Enh_Dec: -0.21116363\n",
      "| epoch   9 |  3600/ 4361 batches | lr 0.000000 | ms/batch 400.91 | loss  4.41 | ppl    82.29 | acc     0.50 | train_ae_norm     1.00\n",
      "[9/200][3699/4361] Loss_D: 0.94055891 (Loss_D_real: 0.52724910 Loss_D_fake: 0.41330981) Loss_G: 0.04834376 Loss_Enh_Dec: -0.21811764\n",
      "| epoch   9 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.45 | loss  4.40 | ppl    81.07 | acc     0.47 | train_ae_norm     1.00\n",
      "[9/200][3799/4361] Loss_D: 0.91342175 (Loss_D_real: 0.43169850 Loss_D_fake: 0.48172325) Loss_G: 0.03278637 Loss_Enh_Dec: -0.17754720\n",
      "| epoch   9 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  4.43 | ppl    83.66 | acc     0.54 | train_ae_norm     1.00\n",
      "[9/200][3899/4361] Loss_D: 1.02666879 (Loss_D_real: 0.53459728 Loss_D_fake: 0.49207148) Loss_G: 0.03808637 Loss_Enh_Dec: -0.13786744\n",
      "| epoch   9 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.77 | loss  4.39 | ppl    80.81 | acc     0.51 | train_ae_norm     1.00\n",
      "[9/200][3999/4361] Loss_D: 0.94733739 (Loss_D_real: 0.53286308 Loss_D_fake: 0.41447431) Loss_G: 0.03280015 Loss_Enh_Dec: -0.14339851\n",
      "| epoch   9 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.47 | loss  4.39 | ppl    80.64 | acc     0.53 | train_ae_norm     1.00\n",
      "[9/200][4099/4361] Loss_D: 0.77915323 (Loss_D_real: 0.42332709 Loss_D_fake: 0.35582614) Loss_G: 0.04043802 Loss_Enh_Dec: -0.14022344\n",
      "| epoch   9 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.24 | loss  4.34 | ppl    76.80 | acc     0.50 | train_ae_norm     1.00\n",
      "[9/200][4199/4361] Loss_D: 0.90088832 (Loss_D_real: 0.49218363 Loss_D_fake: 0.40870473) Loss_G: 0.01810968 Loss_Enh_Dec: -0.16057152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   9 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  4.37 | ppl    79.41 | acc     0.54 | train_ae_norm     1.00\n",
      "[9/200][4299/4361] Loss_D: 0.87830770 (Loss_D_real: 0.42352575 Loss_D_fake: 0.45478198) Loss_G: 0.00942458 Loss_Enh_Dec: -0.18022668\n",
      "| epoch   9 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.77 | loss  4.31 | ppl    74.34 | acc     0.55 | train_ae_norm     1.00\n",
      "| end of epoch   9 | time: 1848.80s | test loss  4.05 | test ppl 57.31 | acc 0.584\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 10 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.706\n",
      "  Average training loss discriminator: 0.796\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.472\n",
      "  Test Loss: 2.225\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  10 |     0/ 4361 batches | lr 0.000000 | ms/batch 862.01 | loss  0.04 | ppl     1.04 | acc     0.52 | train_ae_norm     1.00\n",
      "[10/200][99/4361] Loss_D: 0.93656611 (Loss_D_real: 0.50189942 Loss_D_fake: 0.43466672) Loss_G: 0.04907402 Loss_Enh_Dec: -0.09316742\n",
      "| epoch  10 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  4.33 | ppl    75.62 | acc     0.48 | train_ae_norm     1.00\n",
      "[10/200][199/4361] Loss_D: 0.96940792 (Loss_D_real: 0.51205742 Loss_D_fake: 0.45735049) Loss_G: 0.02739054 Loss_Enh_Dec: -0.13546407\n",
      "| epoch  10 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  4.35 | ppl    77.21 | acc     0.55 | train_ae_norm     1.00\n",
      "[10/200][299/4361] Loss_D: 0.81473160 (Loss_D_real: 0.34173423 Loss_D_fake: 0.47299740) Loss_G: 0.04028337 Loss_Enh_Dec: -0.16114557\n",
      "| epoch  10 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  4.34 | ppl    76.43 | acc     0.50 | train_ae_norm     1.00\n",
      "[10/200][399/4361] Loss_D: 0.77992642 (Loss_D_real: 0.37506950 Loss_D_fake: 0.40485692) Loss_G: 0.02784416 Loss_Enh_Dec: -0.10273745\n",
      "| epoch  10 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  4.28 | ppl    72.13 | acc     0.53 | train_ae_norm     1.00\n",
      "[10/200][499/4361] Loss_D: 0.91424239 (Loss_D_real: 0.48547620 Loss_D_fake: 0.42876619) Loss_G: 0.03744866 Loss_Enh_Dec: -0.13403611\n",
      "| epoch  10 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  4.33 | ppl    75.90 | acc     0.52 | train_ae_norm     1.00\n",
      "[10/200][599/4361] Loss_D: 0.84974468 (Loss_D_real: 0.38880405 Loss_D_fake: 0.46094066) Loss_G: 0.03845325 Loss_Enh_Dec: -0.15127495\n",
      "| epoch  10 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  4.27 | ppl    71.48 | acc     0.47 | train_ae_norm     1.00\n",
      "[10/200][699/4361] Loss_D: 0.94514346 (Loss_D_real: 0.41039246 Loss_D_fake: 0.53475100) Loss_G: 0.02906910 Loss_Enh_Dec: -0.08973875\n",
      "| epoch  10 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  4.31 | ppl    74.37 | acc     0.53 | train_ae_norm     1.00\n",
      "[10/200][799/4361] Loss_D: 0.81280291 (Loss_D_real: 0.43350562 Loss_D_fake: 0.37929726) Loss_G: 0.05325186 Loss_Enh_Dec: -0.11528680\n",
      "| epoch  10 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.60 | loss  4.28 | ppl    72.28 | acc     0.52 | train_ae_norm     1.00\n",
      "[10/200][899/4361] Loss_D: 0.87768376 (Loss_D_real: 0.46867639 Loss_D_fake: 0.40900740) Loss_G: 0.03790454 Loss_Enh_Dec: -0.22003880\n",
      "| epoch  10 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.78 | loss  4.31 | ppl    74.51 | acc     0.55 | train_ae_norm     1.00\n",
      "[10/200][999/4361] Loss_D: 0.77211702 (Loss_D_real: 0.40049297 Loss_D_fake: 0.37162408) Loss_G: 0.05678188 Loss_Enh_Dec: -0.07528321\n",
      "| epoch  10 |  1000/ 4361 batches | lr 0.000000 | ms/batch 400.41 | loss  4.30 | ppl    73.86 | acc     0.52 | train_ae_norm     1.00\n",
      "[10/200][1099/4361] Loss_D: 0.82507575 (Loss_D_real: 0.42738387 Loss_D_fake: 0.39769191) Loss_G: 0.04186497 Loss_Enh_Dec: -0.09547468\n",
      "| epoch  10 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.46 | loss  4.28 | ppl    72.08 | acc     0.50 | train_ae_norm     1.00\n",
      "[10/200][1199/4361] Loss_D: 0.74554777 (Loss_D_real: 0.40932608 Loss_D_fake: 0.33622169) Loss_G: 0.01057346 Loss_Enh_Dec: -0.12149494\n",
      "| epoch  10 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  4.33 | ppl    75.94 | acc     0.55 | train_ae_norm     1.00\n",
      "[10/200][1299/4361] Loss_D: 0.77255809 (Loss_D_real: 0.38208565 Loss_D_fake: 0.39047241) Loss_G: 0.03889191 Loss_Enh_Dec: -0.24288712\n",
      "| epoch  10 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.63 | loss  4.32 | ppl    75.11 | acc     0.49 | train_ae_norm     1.00\n",
      "[10/200][1399/4361] Loss_D: 0.91740960 (Loss_D_real: 0.42265722 Loss_D_fake: 0.49475238) Loss_G: 0.05072662 Loss_Enh_Dec: -0.10576021\n",
      "| epoch  10 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  4.33 | ppl    75.77 | acc     0.47 | train_ae_norm     1.00\n",
      "[10/200][1499/4361] Loss_D: 0.77840614 (Loss_D_real: 0.36599845 Loss_D_fake: 0.41240770) Loss_G: 0.04782274 Loss_Enh_Dec: -0.17686282\n",
      "| epoch  10 |  1500/ 4361 batches | lr 0.000000 | ms/batch 409.93 | loss  4.36 | ppl    77.96 | acc     0.48 | train_ae_norm     1.00\n",
      "[10/200][1599/4361] Loss_D: 1.12062931 (Loss_D_real: 0.57071561 Loss_D_fake: 0.54991376) Loss_G: 0.03872704 Loss_Enh_Dec: -0.12493729\n",
      "| epoch  10 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  4.30 | ppl    73.96 | acc     0.56 | train_ae_norm     1.00\n",
      "[10/200][1699/4361] Loss_D: 1.07881641 (Loss_D_real: 0.55087399 Loss_D_fake: 0.52794236) Loss_G: 0.04016388 Loss_Enh_Dec: -0.18063207\n",
      "| epoch  10 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.54 | loss  4.30 | ppl    73.98 | acc     0.49 | train_ae_norm     1.00\n",
      "[10/200][1799/4361] Loss_D: 0.92274880 (Loss_D_real: 0.50553656 Loss_D_fake: 0.41721222) Loss_G: 0.04670759 Loss_Enh_Dec: -0.16984119\n",
      "| epoch  10 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.84 | loss  4.26 | ppl    71.07 | acc     0.54 | train_ae_norm     1.00\n",
      "[10/200][1899/4361] Loss_D: 0.93762785 (Loss_D_real: 0.47859007 Loss_D_fake: 0.45903778) Loss_G: 0.01486550 Loss_Enh_Dec: -0.14874588\n",
      "| epoch  10 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.58 | loss  4.31 | ppl    74.79 | acc     0.54 | train_ae_norm     1.00\n",
      "[10/200][1999/4361] Loss_D: 0.70943433 (Loss_D_real: 0.35116065 Loss_D_fake: 0.35827368) Loss_G: 0.03023232 Loss_Enh_Dec: -0.14606588\n",
      "| epoch  10 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.50 | loss  4.27 | ppl    71.80 | acc     0.54 | train_ae_norm     1.00\n",
      "[10/200][2099/4361] Loss_D: 0.90303051 (Loss_D_real: 0.39764303 Loss_D_fake: 0.50538749) Loss_G: 0.04662268 Loss_Enh_Dec: -0.12615506\n",
      "| epoch  10 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.52 | loss  4.26 | ppl    70.95 | acc     0.57 | train_ae_norm     1.00\n",
      "[10/200][2199/4361] Loss_D: 0.68591911 (Loss_D_real: 0.32157451 Loss_D_fake: 0.36434460) Loss_G: 0.04692208 Loss_Enh_Dec: -0.09797995\n",
      "| epoch  10 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.76 | loss  4.29 | ppl    72.85 | acc     0.55 | train_ae_norm     1.00\n",
      "[10/200][2299/4361] Loss_D: 0.84718394 (Loss_D_real: 0.37734637 Loss_D_fake: 0.46983761) Loss_G: 0.05172202 Loss_Enh_Dec: -0.12746105\n",
      "| epoch  10 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.86 | loss  4.26 | ppl    70.94 | acc     0.54 | train_ae_norm     1.00\n",
      "[10/200][2399/4361] Loss_D: 0.88184261 (Loss_D_real: 0.47647509 Loss_D_fake: 0.40536755) Loss_G: 0.04110157 Loss_Enh_Dec: -0.15347636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  10 |  2400/ 4361 batches | lr 0.000000 | ms/batch 400.61 | loss  4.26 | ppl    70.81 | acc     0.51 | train_ae_norm     1.00\n",
      "[10/200][2499/4361] Loss_D: 0.88159353 (Loss_D_real: 0.41445860 Loss_D_fake: 0.46713492) Loss_G: 0.05639929 Loss_Enh_Dec: -0.14430088\n",
      "| epoch  10 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  4.30 | ppl    73.92 | acc     0.54 | train_ae_norm     1.00\n",
      "[10/200][2599/4361] Loss_D: 0.79461032 (Loss_D_real: 0.39315850 Loss_D_fake: 0.40145183) Loss_G: 0.04704618 Loss_Enh_Dec: -0.11444175\n",
      "| epoch  10 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  4.25 | ppl    70.29 | acc     0.51 | train_ae_norm     1.00\n",
      "[10/200][2699/4361] Loss_D: 0.81175971 (Loss_D_real: 0.43435448 Loss_D_fake: 0.37740526) Loss_G: 0.05008814 Loss_Enh_Dec: -0.16995715\n",
      "| epoch  10 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  4.28 | ppl    72.56 | acc     0.54 | train_ae_norm     1.00\n",
      "[10/200][2799/4361] Loss_D: 0.78502005 (Loss_D_real: 0.30132115 Loss_D_fake: 0.48369890) Loss_G: 0.04888381 Loss_Enh_Dec: -0.22075854\n",
      "| epoch  10 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.82 | loss  4.25 | ppl    70.01 | acc     0.52 | train_ae_norm     1.00\n",
      "[10/200][2899/4361] Loss_D: 1.11388063 (Loss_D_real: 0.51823032 Loss_D_fake: 0.59565026) Loss_G: 0.04402297 Loss_Enh_Dec: -0.13676207\n",
      "| epoch  10 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.65 | loss  4.28 | ppl    72.00 | acc     0.57 | train_ae_norm     1.00\n",
      "[10/200][2999/4361] Loss_D: 0.97271067 (Loss_D_real: 0.57034051 Loss_D_fake: 0.40237015) Loss_G: 0.03661422 Loss_Enh_Dec: -0.16956206\n",
      "| epoch  10 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  4.26 | ppl    70.82 | acc     0.52 | train_ae_norm     1.00\n",
      "[10/200][3099/4361] Loss_D: 0.72158849 (Loss_D_real: 0.29748863 Loss_D_fake: 0.42409986) Loss_G: 0.04345679 Loss_Enh_Dec: -0.17424814\n",
      "| epoch  10 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  4.32 | ppl    75.43 | acc     0.49 | train_ae_norm     1.00\n",
      "[10/200][3199/4361] Loss_D: 0.87011397 (Loss_D_real: 0.49022532 Loss_D_fake: 0.37988862) Loss_G: 0.03302513 Loss_Enh_Dec: -0.22524686\n",
      "| epoch  10 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  4.32 | ppl    75.42 | acc     0.50 | train_ae_norm     1.00\n",
      "[10/200][3299/4361] Loss_D: 0.96234155 (Loss_D_real: 0.50221050 Loss_D_fake: 0.46013105) Loss_G: 0.03597667 Loss_Enh_Dec: -0.25522515\n",
      "| epoch  10 |  3300/ 4361 batches | lr 0.000000 | ms/batch 400.93 | loss  4.36 | ppl    78.27 | acc     0.50 | train_ae_norm     1.00\n",
      "[10/200][3399/4361] Loss_D: 0.78504229 (Loss_D_real: 0.35894510 Loss_D_fake: 0.42609715) Loss_G: 0.05155531 Loss_Enh_Dec: -0.16681437\n",
      "| epoch  10 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.47 | loss  4.34 | ppl    76.56 | acc     0.52 | train_ae_norm     1.00\n",
      "[10/200][3499/4361] Loss_D: 0.94767487 (Loss_D_real: 0.51295316 Loss_D_fake: 0.43472168) Loss_G: 0.02594060 Loss_Enh_Dec: -0.15404582\n",
      "| epoch  10 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  4.30 | ppl    73.73 | acc     0.52 | train_ae_norm     1.00\n",
      "[10/200][3599/4361] Loss_D: 1.03068662 (Loss_D_real: 0.50193387 Loss_D_fake: 0.52875274) Loss_G: 0.03478089 Loss_Enh_Dec: -0.17671357\n",
      "| epoch  10 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.06 | loss  4.33 | ppl    76.27 | acc     0.51 | train_ae_norm     1.00\n",
      "[10/200][3699/4361] Loss_D: 0.90248626 (Loss_D_real: 0.51746368 Loss_D_fake: 0.38502258) Loss_G: -0.00562848 Loss_Enh_Dec: -0.16113403\n",
      "| epoch  10 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  4.34 | ppl    76.39 | acc     0.49 | train_ae_norm     1.00\n",
      "[10/200][3799/4361] Loss_D: 0.94427240 (Loss_D_real: 0.45483133 Loss_D_fake: 0.48944110) Loss_G: 0.04423684 Loss_Enh_Dec: -0.14279182\n",
      "| epoch  10 |  3800/ 4361 batches | lr 0.000000 | ms/batch 400.27 | loss  4.36 | ppl    78.63 | acc     0.54 | train_ae_norm     1.00\n",
      "[10/200][3899/4361] Loss_D: 0.97956812 (Loss_D_real: 0.50233608 Loss_D_fake: 0.47723207) Loss_G: 0.04121296 Loss_Enh_Dec: -0.22611551\n",
      "| epoch  10 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  4.39 | ppl    80.46 | acc     0.51 | train_ae_norm     1.00\n",
      "[10/200][3999/4361] Loss_D: 0.83154434 (Loss_D_real: 0.45709395 Loss_D_fake: 0.37445039) Loss_G: 0.02989928 Loss_Enh_Dec: -0.15435980\n",
      "| epoch  10 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.55 | loss  4.36 | ppl    78.64 | acc     0.53 | train_ae_norm     1.00\n",
      "[10/200][4099/4361] Loss_D: 0.87021816 (Loss_D_real: 0.43373084 Loss_D_fake: 0.43648729) Loss_G: 0.03792074 Loss_Enh_Dec: -0.21614292\n",
      "| epoch  10 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.08 | loss  4.32 | ppl    75.23 | acc     0.52 | train_ae_norm     1.00\n",
      "[10/200][4199/4361] Loss_D: 0.76416117 (Loss_D_real: 0.37426627 Loss_D_fake: 0.38989490) Loss_G: 0.05041757 Loss_Enh_Dec: -0.23741646\n",
      "| epoch  10 |  4200/ 4361 batches | lr 0.000000 | ms/batch 400.79 | loss  4.36 | ppl    78.19 | acc     0.52 | train_ae_norm     1.00\n",
      "[10/200][4299/4361] Loss_D: 0.86263990 (Loss_D_real: 0.45258236 Loss_D_fake: 0.41005754) Loss_G: 0.05817866 Loss_Enh_Dec: -0.23281212\n",
      "| epoch  10 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.08 | loss  4.29 | ppl    73.19 | acc     0.52 | train_ae_norm     1.00\n",
      "| end of epoch  10 | time: 1851.12s | test loss  4.07 | test ppl 58.30 | acc 0.583\n",
      "bleu_self:  [2.77333564e-01 7.19834961e-02 6.37142431e-07 2.00529860e-09\n",
      " 6.70671875e-11]\n",
      "bleu_test:  [8.63636363e-01 5.62163738e-01 1.43493497e-01 5.19301678e-02\n",
      " 4.49223935e-05]\n",
      "bleu_self: [0.27733356,0.07198350,0.00000064,0.00000000,0.00000000]\n",
      "bleu_test: [0.86363636,0.56216374,0.14349350,0.05193017,0.00004492]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 11 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.705\n",
      "  Average training loss discriminator: 0.785\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.477\n",
      "  Test Loss: 2.273\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  11 |     0/ 4361 batches | lr 0.000000 | ms/batch 863.01 | loss  0.04 | ppl     1.04 | acc     0.54 | train_ae_norm     1.00\n",
      "[11/200][99/4361] Loss_D: 0.86869895 (Loss_D_real: 0.45234740 Loss_D_fake: 0.41635156) Loss_G: 0.02807878 Loss_Enh_Dec: -0.15027378\n",
      "| epoch  11 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  4.35 | ppl    77.33 | acc     0.50 | train_ae_norm     1.00\n",
      "[11/200][199/4361] Loss_D: 0.85241807 (Loss_D_real: 0.37999749 Loss_D_fake: 0.47242057) Loss_G: 0.02877280 Loss_Enh_Dec: -0.18029608\n",
      "| epoch  11 |   200/ 4361 batches | lr 0.000000 | ms/batch 400.93 | loss  4.35 | ppl    77.64 | acc     0.53 | train_ae_norm     1.00\n",
      "[11/200][299/4361] Loss_D: 0.73661608 (Loss_D_real: 0.33504772 Loss_D_fake: 0.40156835) Loss_G: 0.05991952 Loss_Enh_Dec: -0.22932211\n",
      "| epoch  11 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.18 | loss  4.33 | ppl    75.90 | acc     0.49 | train_ae_norm     1.00\n",
      "[11/200][399/4361] Loss_D: 0.84115249 (Loss_D_real: 0.43788147 Loss_D_fake: 0.40327102) Loss_G: 0.03667816 Loss_Enh_Dec: -0.16491427\n",
      "| epoch  11 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.60 | loss  4.25 | ppl    70.44 | acc     0.53 | train_ae_norm     1.00\n",
      "[11/200][499/4361] Loss_D: 0.84584975 (Loss_D_real: 0.46423906 Loss_D_fake: 0.38161069) Loss_G: 0.03478937 Loss_Enh_Dec: -0.23742035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  11 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.84 | loss  4.36 | ppl    78.26 | acc     0.55 | train_ae_norm     1.00\n",
      "[11/200][599/4361] Loss_D: 0.88315666 (Loss_D_real: 0.46825320 Loss_D_fake: 0.41490349) Loss_G: 0.03740079 Loss_Enh_Dec: -0.14397679\n",
      "| epoch  11 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  4.29 | ppl    73.04 | acc     0.49 | train_ae_norm     1.00\n",
      "[11/200][699/4361] Loss_D: 1.01715302 (Loss_D_real: 0.59164453 Loss_D_fake: 0.42550847) Loss_G: 0.01957024 Loss_Enh_Dec: -0.16427353\n",
      "| epoch  11 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  4.33 | ppl    76.27 | acc     0.53 | train_ae_norm     1.00\n",
      "[11/200][799/4361] Loss_D: 0.94139647 (Loss_D_real: 0.48482651 Loss_D_fake: 0.45656997) Loss_G: 0.03302510 Loss_Enh_Dec: -0.15721522\n",
      "| epoch  11 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  4.36 | ppl    78.55 | acc     0.52 | train_ae_norm     1.00\n",
      "[11/200][899/4361] Loss_D: 0.99412429 (Loss_D_real: 0.46027812 Loss_D_fake: 0.53384614) Loss_G: 0.02357920 Loss_Enh_Dec: -0.20591684\n",
      "| epoch  11 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  4.38 | ppl    79.81 | acc     0.55 | train_ae_norm     1.00\n",
      "[11/200][999/4361] Loss_D: 0.71849823 (Loss_D_real: 0.36907837 Loss_D_fake: 0.34941983) Loss_G: 0.04389294 Loss_Enh_Dec: -0.16183622\n",
      "| epoch  11 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.05 | loss  4.38 | ppl    80.20 | acc     0.51 | train_ae_norm     1.00\n",
      "[11/200][1099/4361] Loss_D: 0.82731688 (Loss_D_real: 0.39950424 Loss_D_fake: 0.42781264) Loss_G: 0.04492690 Loss_Enh_Dec: -0.13993582\n",
      "| epoch  11 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  4.36 | ppl    78.22 | acc     0.49 | train_ae_norm     1.00\n",
      "[11/200][1199/4361] Loss_D: 0.79039621 (Loss_D_real: 0.36835295 Loss_D_fake: 0.42204326) Loss_G: 0.03611613 Loss_Enh_Dec: -0.21088271\n",
      "| epoch  11 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  4.43 | ppl    83.99 | acc     0.51 | train_ae_norm     1.00\n",
      "[11/200][1299/4361] Loss_D: 0.68280900 (Loss_D_real: 0.30523503 Loss_D_fake: 0.37757394) Loss_G: 0.03984093 Loss_Enh_Dec: -0.20563555\n",
      "| epoch  11 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  4.40 | ppl    81.62 | acc     0.53 | train_ae_norm     1.00\n",
      "[11/200][1399/4361] Loss_D: 0.84245360 (Loss_D_real: 0.48007974 Loss_D_fake: 0.36237383) Loss_G: 0.05256381 Loss_Enh_Dec: -0.27685508\n",
      "| epoch  11 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.67 | loss  4.41 | ppl    82.47 | acc     0.41 | train_ae_norm     1.00\n",
      "[11/200][1499/4361] Loss_D: 0.77735996 (Loss_D_real: 0.35920295 Loss_D_fake: 0.41815698) Loss_G: 0.05208528 Loss_Enh_Dec: -0.14674191\n",
      "| epoch  11 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  4.46 | ppl    86.10 | acc     0.45 | train_ae_norm     1.00\n",
      "[11/200][1599/4361] Loss_D: 0.82487559 (Loss_D_real: 0.37285703 Loss_D_fake: 0.45201859) Loss_G: 0.02979580 Loss_Enh_Dec: -0.12330969\n",
      "| epoch  11 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  4.38 | ppl    79.97 | acc     0.53 | train_ae_norm     1.00\n",
      "[11/200][1699/4361] Loss_D: 0.82351983 (Loss_D_real: 0.42251360 Loss_D_fake: 0.40100622) Loss_G: 0.04109645 Loss_Enh_Dec: -0.17640541\n",
      "| epoch  11 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.76 | loss  4.33 | ppl    76.09 | acc     0.50 | train_ae_norm     1.00\n",
      "[11/200][1799/4361] Loss_D: 0.91710079 (Loss_D_real: 0.53839815 Loss_D_fake: 0.37870261) Loss_G: 0.02134542 Loss_Enh_Dec: -0.25456712\n",
      "| epoch  11 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  4.32 | ppl    75.12 | acc     0.52 | train_ae_norm     1.00\n",
      "[11/200][1899/4361] Loss_D: 0.87275147 (Loss_D_real: 0.52714717 Loss_D_fake: 0.34560427) Loss_G: 0.04962722 Loss_Enh_Dec: -0.18869960\n",
      "| epoch  11 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.81 | loss  4.38 | ppl    79.52 | acc     0.51 | train_ae_norm     1.00\n",
      "[11/200][1999/4361] Loss_D: 0.84913129 (Loss_D_real: 0.47099456 Loss_D_fake: 0.37813672) Loss_G: 0.05223012 Loss_Enh_Dec: -0.11305547\n",
      "| epoch  11 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  4.31 | ppl    74.52 | acc     0.52 | train_ae_norm     1.00\n",
      "[11/200][2099/4361] Loss_D: 0.83939409 (Loss_D_real: 0.49310955 Loss_D_fake: 0.34628457) Loss_G: 0.05904573 Loss_Enh_Dec: -0.22888859\n",
      "| epoch  11 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  4.33 | ppl    76.17 | acc     0.53 | train_ae_norm     1.00\n",
      "[11/200][2199/4361] Loss_D: 0.85944647 (Loss_D_real: 0.42177224 Loss_D_fake: 0.43767422) Loss_G: 0.06499281 Loss_Enh_Dec: -0.15523705\n",
      "| epoch  11 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  4.33 | ppl    75.88 | acc     0.51 | train_ae_norm     1.00\n",
      "[11/200][2299/4361] Loss_D: 0.66907549 (Loss_D_real: 0.32425818 Loss_D_fake: 0.34481734) Loss_G: 0.01663901 Loss_Enh_Dec: -0.21975175\n",
      "| epoch  11 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  4.31 | ppl    74.38 | acc     0.56 | train_ae_norm     1.00\n",
      "[11/200][2599/4361] Loss_D: 0.86735141 (Loss_D_real: 0.47175533 Loss_D_fake: 0.39559612) Loss_G: 0.04192384 Loss_Enh_Dec: -0.21885252\n",
      "| epoch  11 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  4.27 | ppl    71.37 | acc     0.52 | train_ae_norm     1.00\n",
      "[11/200][2699/4361] Loss_D: 0.69483650 (Loss_D_real: 0.30541015 Loss_D_fake: 0.38942635) Loss_G: 0.04825515 Loss_Enh_Dec: -0.26668009\n",
      "| epoch  11 |  2700/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  4.28 | ppl    72.11 | acc     0.52 | train_ae_norm     1.00\n",
      "[11/200][2799/4361] Loss_D: 1.11464131 (Loss_D_real: 0.62497753 Loss_D_fake: 0.48966378) Loss_G: 0.03350750 Loss_Enh_Dec: -0.15574208\n",
      "| epoch  11 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.72 | loss  4.21 | ppl    67.15 | acc     0.51 | train_ae_norm     1.00\n",
      "[11/200][2899/4361] Loss_D: 0.78822702 (Loss_D_real: 0.35047770 Loss_D_fake: 0.43774933) Loss_G: 0.02474643 Loss_Enh_Dec: -0.18217368\n",
      "| epoch  11 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  4.25 | ppl    69.96 | acc     0.57 | train_ae_norm     1.00\n",
      "[11/200][2999/4361] Loss_D: 0.72552162 (Loss_D_real: 0.32708472 Loss_D_fake: 0.39843690) Loss_G: 0.04498054 Loss_Enh_Dec: -0.20388794\n",
      "| epoch  11 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.77 | loss  4.26 | ppl    70.80 | acc     0.54 | train_ae_norm     1.00\n",
      "[11/200][3099/4361] Loss_D: 0.89128119 (Loss_D_real: 0.56561202 Loss_D_fake: 0.32566917) Loss_G: 0.07129714 Loss_Enh_Dec: -0.17702997\n",
      "| epoch  11 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  4.26 | ppl    70.97 | acc     0.49 | train_ae_norm     1.00\n",
      "[11/200][3199/4361] Loss_D: 0.78652984 (Loss_D_real: 0.43815899 Loss_D_fake: 0.34837085) Loss_G: 0.03328360 Loss_Enh_Dec: -0.18065143\n",
      "| epoch  11 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  4.30 | ppl    73.96 | acc     0.51 | train_ae_norm     1.00\n",
      "[11/200][3299/4361] Loss_D: 0.97644103 (Loss_D_real: 0.49732837 Loss_D_fake: 0.47911263) Loss_G: 0.04812610 Loss_Enh_Dec: -0.14172025\n",
      "| epoch  11 |  3300/ 4361 batches | lr 0.000000 | ms/batch 400.33 | loss  4.28 | ppl    72.60 | acc     0.48 | train_ae_norm     1.00\n",
      "[11/200][3399/4361] Loss_D: 0.84489393 (Loss_D_real: 0.45174250 Loss_D_fake: 0.39315140) Loss_G: 0.05055833 Loss_Enh_Dec: -0.13442008\n",
      "| epoch  11 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.75 | loss  4.27 | ppl    71.59 | acc     0.54 | train_ae_norm     1.00\n",
      "[11/200][3499/4361] Loss_D: 0.78536880 (Loss_D_real: 0.40101147 Loss_D_fake: 0.38435733) Loss_G: 0.03692621 Loss_Enh_Dec: -0.28711811\n",
      "| epoch  11 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.08 | loss  4.20 | ppl    66.39 | acc     0.56 | train_ae_norm     1.00\n",
      "[11/200][3599/4361] Loss_D: 0.98932558 (Loss_D_real: 0.49914187 Loss_D_fake: 0.49018371) Loss_G: 0.03981612 Loss_Enh_Dec: -0.19397663\n",
      "| epoch  11 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  4.21 | ppl    67.26 | acc     0.54 | train_ae_norm     1.00\n",
      "[11/200][3699/4361] Loss_D: 0.98122048 (Loss_D_real: 0.42936301 Loss_D_fake: 0.55185747) Loss_G: 0.01975184 Loss_Enh_Dec: -0.17162548\n",
      "| epoch  11 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  4.22 | ppl    67.72 | acc     0.52 | train_ae_norm     1.00\n",
      "[11/200][3799/4361] Loss_D: 0.81151927 (Loss_D_real: 0.44933972 Loss_D_fake: 0.36217952) Loss_G: 0.04669392 Loss_Enh_Dec: -0.12039515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  11 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  4.24 | ppl    69.11 | acc     0.56 | train_ae_norm     1.00\n",
      "[11/200][3899/4361] Loss_D: 0.84387302 (Loss_D_real: 0.34365475 Loss_D_fake: 0.50021827) Loss_G: 0.02601921 Loss_Enh_Dec: -0.19253121\n",
      "| epoch  11 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.78 | loss  4.22 | ppl    68.19 | acc     0.51 | train_ae_norm     1.00\n",
      "[11/200][3999/4361] Loss_D: 0.86420333 (Loss_D_real: 0.44707236 Loss_D_fake: 0.41713101) Loss_G: 0.05684992 Loss_Enh_Dec: -0.12679562\n",
      "| epoch  11 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  4.21 | ppl    67.62 | acc     0.55 | train_ae_norm     1.00\n",
      "[11/200][4099/4361] Loss_D: 0.97465277 (Loss_D_real: 0.57159972 Loss_D_fake: 0.40305305) Loss_G: 0.02511944 Loss_Enh_Dec: -0.16945656\n",
      "| epoch  11 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  4.19 | ppl    66.14 | acc     0.53 | train_ae_norm     1.00\n",
      "[11/200][4199/4361] Loss_D: 0.94467866 (Loss_D_real: 0.52028322 Loss_D_fake: 0.42439541) Loss_G: 0.05096432 Loss_Enh_Dec: -0.14533758\n",
      "| epoch  11 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  4.22 | ppl    67.84 | acc     0.54 | train_ae_norm     1.00\n",
      "[11/200][4299/4361] Loss_D: 0.80059457 (Loss_D_real: 0.41781807 Loss_D_fake: 0.38277650) Loss_G: 0.05020098 Loss_Enh_Dec: -0.18038702\n",
      "| epoch  11 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  4.16 | ppl    63.91 | acc     0.57 | train_ae_norm     1.00\n",
      "| end of epoch  11 | time: 1851.28s | test loss  3.94 | test ppl 51.16 | acc 0.603\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 12 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.705\n",
      "  Average training loss discriminator: 0.777\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.472\n",
      "  Test Loss: 2.348\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  12 |     0/ 4361 batches | lr 0.000000 | ms/batch 863.23 | loss  0.04 | ppl     1.04 | acc     0.57 | train_ae_norm     1.00\n",
      "[12/200][99/4361] Loss_D: 0.79138112 (Loss_D_real: 0.35887021 Loss_D_fake: 0.43251091) Loss_G: 0.03997753 Loss_Enh_Dec: -0.19572686\n",
      "| epoch  12 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  4.19 | ppl    65.87 | acc     0.52 | train_ae_norm     1.00\n",
      "[12/200][199/4361] Loss_D: 0.80937892 (Loss_D_real: 0.33937693 Loss_D_fake: 0.47000200) Loss_G: 0.05488963 Loss_Enh_Dec: -0.16030736\n",
      "| epoch  12 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  4.21 | ppl    67.50 | acc     0.53 | train_ae_norm     1.00\n",
      "[12/200][299/4361] Loss_D: 0.84282291 (Loss_D_real: 0.38770542 Loss_D_fake: 0.45511746) Loss_G: 0.05853974 Loss_Enh_Dec: -0.18649073\n",
      "| epoch  12 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  4.22 | ppl    68.03 | acc     0.52 | train_ae_norm     1.00\n",
      "[12/200][399/4361] Loss_D: 0.79698133 (Loss_D_real: 0.40576315 Loss_D_fake: 0.39121819) Loss_G: 0.02468015 Loss_Enh_Dec: -0.17089823\n",
      "| epoch  12 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  4.12 | ppl    61.69 | acc     0.56 | train_ae_norm     1.00\n",
      "[12/200][499/4361] Loss_D: 0.70908165 (Loss_D_real: 0.35452336 Loss_D_fake: 0.35455832) Loss_G: 0.05349643 Loss_Enh_Dec: -0.23213172\n",
      "| epoch  12 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  4.16 | ppl    64.11 | acc     0.57 | train_ae_norm     1.00\n",
      "[12/200][599/4361] Loss_D: 0.77064955 (Loss_D_real: 0.32414019 Loss_D_fake: 0.44650933) Loss_G: 0.05919277 Loss_Enh_Dec: -0.19167876\n",
      "| epoch  12 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  4.12 | ppl    61.65 | acc     0.52 | train_ae_norm     1.00\n",
      "[12/200][699/4361] Loss_D: 0.73343670 (Loss_D_real: 0.39880919 Loss_D_fake: 0.33462751) Loss_G: 0.05058544 Loss_Enh_Dec: -0.25867352\n",
      "| epoch  12 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  4.19 | ppl    65.80 | acc     0.57 | train_ae_norm     1.00\n",
      "[12/200][799/4361] Loss_D: 0.75525540 (Loss_D_real: 0.35239673 Loss_D_fake: 0.40285867) Loss_G: 0.04015909 Loss_Enh_Dec: -0.18424810\n",
      "| epoch  12 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.72 | loss  4.15 | ppl    63.30 | acc     0.53 | train_ae_norm     1.00\n",
      "[12/200][899/4361] Loss_D: 0.92283940 (Loss_D_real: 0.54368544 Loss_D_fake: 0.37915394) Loss_G: 0.06558861 Loss_Enh_Dec: -0.24047776\n",
      "| epoch  12 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  4.17 | ppl    64.74 | acc     0.58 | train_ae_norm     1.00\n",
      "[12/200][999/4361] Loss_D: 0.71402133 (Loss_D_real: 0.34117883 Loss_D_fake: 0.37284249) Loss_G: 0.05412887 Loss_Enh_Dec: -0.19293937\n",
      "| epoch  12 |  1000/ 4361 batches | lr 0.000000 | ms/batch 400.85 | loss  4.16 | ppl    63.90 | acc     0.56 | train_ae_norm     1.00\n",
      "[12/200][1099/4361] Loss_D: 0.98134851 (Loss_D_real: 0.59896874 Loss_D_fake: 0.38237977) Loss_G: 0.05447158 Loss_Enh_Dec: -0.20037214\n",
      "| epoch  12 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  4.15 | ppl    63.59 | acc     0.51 | train_ae_norm     1.00\n",
      "[12/200][1199/4361] Loss_D: 0.89005655 (Loss_D_real: 0.45162219 Loss_D_fake: 0.43843436) Loss_G: 0.04562399 Loss_Enh_Dec: -0.16237430\n",
      "| epoch  12 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  4.17 | ppl    65.00 | acc     0.56 | train_ae_norm     1.00\n",
      "[12/200][1299/4361] Loss_D: 0.76182145 (Loss_D_real: 0.38079685 Loss_D_fake: 0.38102460) Loss_G: 0.01986249 Loss_Enh_Dec: -0.13882478\n",
      "| epoch  12 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.41 | loss  4.15 | ppl    63.25 | acc     0.56 | train_ae_norm     1.00\n",
      "[12/200][1399/4361] Loss_D: 0.88619113 (Loss_D_real: 0.45470867 Loss_D_fake: 0.43148249) Loss_G: 0.02655880 Loss_Enh_Dec: -0.17703460\n",
      "| epoch  12 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  4.15 | ppl    63.47 | acc     0.48 | train_ae_norm     1.00\n",
      "[12/200][1499/4361] Loss_D: 0.74149817 (Loss_D_real: 0.37807721 Loss_D_fake: 0.36342096) Loss_G: 0.05088396 Loss_Enh_Dec: -0.12561058\n",
      "| epoch  12 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.05 | loss  4.19 | ppl    66.10 | acc     0.52 | train_ae_norm     1.00\n",
      "[12/200][1599/4361] Loss_D: 1.01387596 (Loss_D_real: 0.58138633 Loss_D_fake: 0.43248963) Loss_G: 0.02349781 Loss_Enh_Dec: -0.17368595\n",
      "| epoch  12 |  1600/ 4361 batches | lr 0.000000 | ms/batch 400.61 | loss  4.16 | ppl    64.24 | acc     0.55 | train_ae_norm     1.00\n",
      "[12/200][1699/4361] Loss_D: 0.77585435 (Loss_D_real: 0.39589208 Loss_D_fake: 0.37996230) Loss_G: 0.04367308 Loss_Enh_Dec: -0.23989189\n",
      "| epoch  12 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  4.14 | ppl    63.07 | acc     0.51 | train_ae_norm     1.00\n",
      "[12/200][1799/4361] Loss_D: 0.89920080 (Loss_D_real: 0.41176891 Loss_D_fake: 0.48743188) Loss_G: 0.06069454 Loss_Enh_Dec: -0.11905023\n",
      "| epoch  12 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  4.12 | ppl    61.41 | acc     0.56 | train_ae_norm     1.00\n",
      "[12/200][1899/4361] Loss_D: 0.73433822 (Loss_D_real: 0.33231100 Loss_D_fake: 0.40202722) Loss_G: 0.03739775 Loss_Enh_Dec: -0.17210129\n",
      "| epoch  12 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.66 | loss  4.18 | ppl    65.47 | acc     0.56 | train_ae_norm     1.00\n",
      "[12/200][1999/4361] Loss_D: 0.78395486 (Loss_D_real: 0.39943919 Loss_D_fake: 0.38451564) Loss_G: 0.05046675 Loss_Enh_Dec: -0.12421781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  12 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.71 | loss  4.12 | ppl    61.76 | acc     0.55 | train_ae_norm     1.00\n",
      "[12/200][2099/4361] Loss_D: 0.90172458 (Loss_D_real: 0.42287391 Loss_D_fake: 0.47885069) Loss_G: 0.03328840 Loss_Enh_Dec: -0.24904649\n",
      "| epoch  12 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  4.13 | ppl    62.20 | acc     0.57 | train_ae_norm     1.00\n",
      "[12/200][2199/4361] Loss_D: 0.84783083 (Loss_D_real: 0.43230638 Loss_D_fake: 0.41552445) Loss_G: 0.03060243 Loss_Enh_Dec: -0.17660984\n",
      "| epoch  12 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.88 | loss  4.12 | ppl    61.42 | acc     0.56 | train_ae_norm     1.00\n",
      "[12/200][2299/4361] Loss_D: 0.74670964 (Loss_D_real: 0.41728517 Loss_D_fake: 0.32942447) Loss_G: 0.03350262 Loss_Enh_Dec: -0.23734656\n",
      "| epoch  12 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.60 | loss  4.10 | ppl    60.64 | acc     0.58 | train_ae_norm     1.00\n",
      "[12/200][2399/4361] Loss_D: 0.83984601 (Loss_D_real: 0.37052363 Loss_D_fake: 0.46932238) Loss_G: 0.05764671 Loss_Enh_Dec: -0.19894636\n",
      "| epoch  12 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  4.12 | ppl    61.40 | acc     0.51 | train_ae_norm     1.00\n",
      "[12/200][2499/4361] Loss_D: 0.93630099 (Loss_D_real: 0.53864229 Loss_D_fake: 0.39765868) Loss_G: 0.02195203 Loss_Enh_Dec: -0.20167351\n",
      "| epoch  12 |  2500/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  4.13 | ppl    62.44 | acc     0.55 | train_ae_norm     1.00\n",
      "[12/200][2599/4361] Loss_D: 0.78709120 (Loss_D_real: 0.38149381 Loss_D_fake: 0.40559739) Loss_G: 0.04942117 Loss_Enh_Dec: -0.23238324\n",
      "| epoch  12 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.84 | loss  4.13 | ppl    61.87 | acc     0.52 | train_ae_norm     1.00\n",
      "[12/200][2699/4361] Loss_D: 0.77976841 (Loss_D_real: 0.38487566 Loss_D_fake: 0.39489275) Loss_G: 0.03837826 Loss_Enh_Dec: -0.21693793\n",
      "| epoch  12 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  4.14 | ppl    62.61 | acc     0.55 | train_ae_norm     1.00\n",
      "[12/200][2799/4361] Loss_D: 0.84449613 (Loss_D_real: 0.45295700 Loss_D_fake: 0.39153910) Loss_G: 0.02622785 Loss_Enh_Dec: -0.17468217\n",
      "| epoch  12 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  4.07 | ppl    58.27 | acc     0.55 | train_ae_norm     1.00\n",
      "[12/200][2899/4361] Loss_D: 0.83602393 (Loss_D_real: 0.50189388 Loss_D_fake: 0.33413005) Loss_G: 0.06123094 Loss_Enh_Dec: -0.15245019\n",
      "| epoch  12 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  4.11 | ppl    60.82 | acc     0.59 | train_ae_norm     1.00\n",
      "[12/200][2999/4361] Loss_D: 0.78162599 (Loss_D_real: 0.38396868 Loss_D_fake: 0.39765733) Loss_G: 0.02714151 Loss_Enh_Dec: -0.12310034\n",
      "| epoch  12 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  4.12 | ppl    61.27 | acc     0.54 | train_ae_norm     1.00\n",
      "[12/200][3099/4361] Loss_D: 0.67816353 (Loss_D_real: 0.34331083 Loss_D_fake: 0.33485270) Loss_G: 0.03517533 Loss_Enh_Dec: -0.17091627\n",
      "| epoch  12 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.13 | loss  4.13 | ppl    62.33 | acc     0.53 | train_ae_norm     1.00\n",
      "[12/200][3199/4361] Loss_D: 0.77636212 (Loss_D_real: 0.40287608 Loss_D_fake: 0.37348604) Loss_G: 0.03873684 Loss_Enh_Dec: -0.20327520\n",
      "| epoch  12 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.53 | loss  4.16 | ppl    64.16 | acc     0.54 | train_ae_norm     1.00\n",
      "[12/200][3299/4361] Loss_D: 0.83568937 (Loss_D_real: 0.43573046 Loss_D_fake: 0.39995891) Loss_G: 0.04410898 Loss_Enh_Dec: -0.25692359\n",
      "| epoch  12 |  3300/ 4361 batches | lr 0.000000 | ms/batch 400.62 | loss  4.17 | ppl    64.87 | acc     0.54 | train_ae_norm     1.00\n",
      "[12/200][3399/4361] Loss_D: 0.77272630 (Loss_D_real: 0.41883004 Loss_D_fake: 0.35389626) Loss_G: 0.04716793 Loss_Enh_Dec: -0.23696649\n",
      "| epoch  12 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.33 | loss  4.14 | ppl    62.75 | acc     0.56 | train_ae_norm     1.00\n",
      "[12/200][3499/4361] Loss_D: 0.86894798 (Loss_D_real: 0.48991537 Loss_D_fake: 0.37903258) Loss_G: 0.02815722 Loss_Enh_Dec: -0.29235646\n",
      "| epoch  12 |  3500/ 4361 batches | lr 0.000000 | ms/batch 399.99 | loss  4.13 | ppl    62.00 | acc     0.54 | train_ae_norm     1.00\n",
      "[12/200][3599/4361] Loss_D: 0.64016056 (Loss_D_real: 0.31062549 Loss_D_fake: 0.32953507) Loss_G: 0.06595736 Loss_Enh_Dec: -0.20139919\n",
      "| epoch  12 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  4.19 | ppl    66.26 | acc     0.54 | train_ae_norm     1.00\n",
      "[12/200][3699/4361] Loss_D: 0.65089631 (Loss_D_real: 0.29521582 Loss_D_fake: 0.35568053) Loss_G: 0.07141974 Loss_Enh_Dec: -0.20721450\n",
      "| epoch  12 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  4.16 | ppl    64.36 | acc     0.50 | train_ae_norm     1.00\n",
      "[12/200][3799/4361] Loss_D: 0.45003760 (Loss_D_real: 0.19753335 Loss_D_fake: 0.25250423) Loss_G: 0.06103640 Loss_Enh_Dec: -0.28336552\n",
      "| epoch  12 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.94 | loss  4.19 | ppl    65.78 | acc     0.57 | train_ae_norm     1.00\n",
      "[12/200][3899/4361] Loss_D: 0.43079787 (Loss_D_real: 0.19979650 Loss_D_fake: 0.23100136) Loss_G: 0.09592480 Loss_Enh_Dec: -0.31679460\n",
      "| epoch  12 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  4.18 | ppl    65.57 | acc     0.51 | train_ae_norm     1.00\n",
      "[12/200][3999/4361] Loss_D: 0.55426985 (Loss_D_real: 0.31543946 Loss_D_fake: 0.23883037) Loss_G: 0.08475655 Loss_Enh_Dec: -0.35824320\n",
      "| epoch  12 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.40 | loss  4.25 | ppl    69.99 | acc     0.53 | train_ae_norm     1.00\n",
      "[12/200][4099/4361] Loss_D: 0.33412421 (Loss_D_real: 0.14535090 Loss_D_fake: 0.18877332) Loss_G: 0.10541236 Loss_Enh_Dec: -0.34653521\n",
      "| epoch  12 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  4.29 | ppl    73.19 | acc     0.50 | train_ae_norm     1.00\n",
      "[12/200][4199/4361] Loss_D: 0.47801691 (Loss_D_real: 0.24367659 Loss_D_fake: 0.23434031) Loss_G: 0.07711437 Loss_Enh_Dec: -0.37786728\n",
      "| epoch  12 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  4.34 | ppl    76.44 | acc     0.52 | train_ae_norm     1.00\n",
      "[12/200][4299/4361] Loss_D: 0.74825084 (Loss_D_real: 0.40928364 Loss_D_fake: 0.33896717) Loss_G: 0.05378044 Loss_Enh_Dec: -0.15627263\n",
      "| epoch  12 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.82 | loss  4.25 | ppl    70.20 | acc     0.53 | train_ae_norm     1.00\n",
      "| end of epoch  12 | time: 1850.90s | test loss  3.96 | test ppl 52.22 | acc 0.602\n",
      "bleu_self:  [1.68750000e-01 4.18255193e-09 1.31101131e-11 7.81019449e-13\n",
      " 1.58315320e-13]\n",
      "bleu_test:  [8.57291666e-01 4.82433239e-01 1.77959743e-01 8.84040716e-02\n",
      " 1.00709677e-04]\n",
      "bleu_self: [0.16875000,0.00000000,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.85729167,0.48243324,0.17795974,0.08840407,0.00010071]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 13 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.704\n",
      "  Average training loss discriminator: 0.768\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.477\n",
      "  Test Loss: 2.427\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  13 |     0/ 4361 batches | lr 0.000000 | ms/batch 864.24 | loss  0.04 | ppl     1.04 | acc     0.56 | train_ae_norm     1.00\n",
      "[13/200][99/4361] Loss_D: 0.79085171 (Loss_D_real: 0.35733876 Loss_D_fake: 0.43351293) Loss_G: 0.06807568 Loss_Enh_Dec: -0.26812950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  13 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.05 | loss  4.24 | ppl    69.56 | acc     0.51 | train_ae_norm     1.00\n",
      "[13/200][199/4361] Loss_D: 0.84366775 (Loss_D_real: 0.43728751 Loss_D_fake: 0.40638027) Loss_G: 0.04743294 Loss_Enh_Dec: -0.20893112\n",
      "| epoch  13 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  4.28 | ppl    72.18 | acc     0.56 | train_ae_norm     1.00\n",
      "[13/200][299/4361] Loss_D: 0.71416807 (Loss_D_real: 0.41109276 Loss_D_fake: 0.30307531) Loss_G: 0.04539827 Loss_Enh_Dec: -0.17274688\n",
      "| epoch  13 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  4.33 | ppl    75.70 | acc     0.46 | train_ae_norm     1.00\n",
      "[13/200][399/4361] Loss_D: 0.80036622 (Loss_D_real: 0.35286933 Loss_D_fake: 0.44749689) Loss_G: 0.05648738 Loss_Enh_Dec: -0.20024042\n",
      "| epoch  13 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  4.17 | ppl    64.76 | acc     0.53 | train_ae_norm     1.00\n",
      "[13/200][499/4361] Loss_D: 0.60111648 (Loss_D_real: 0.32084674 Loss_D_fake: 0.28026974) Loss_G: 0.04907846 Loss_Enh_Dec: -0.21382253\n",
      "| epoch  13 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  4.23 | ppl    68.69 | acc     0.55 | train_ae_norm     1.00\n",
      "[13/200][599/4361] Loss_D: 0.78705978 (Loss_D_real: 0.35517341 Loss_D_fake: 0.43188637) Loss_G: 0.05994723 Loss_Enh_Dec: -0.16778517\n",
      "| epoch  13 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  4.18 | ppl    65.28 | acc     0.49 | train_ae_norm     1.00\n",
      "[13/200][699/4361] Loss_D: 0.59803271 (Loss_D_real: 0.28391698 Loss_D_fake: 0.31411573) Loss_G: 0.05447802 Loss_Enh_Dec: -0.28440043\n",
      "| epoch  13 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  4.24 | ppl    69.59 | acc     0.56 | train_ae_norm     1.00\n",
      "[13/200][799/4361] Loss_D: 0.70193219 (Loss_D_real: 0.40440711 Loss_D_fake: 0.29752505) Loss_G: 0.07731433 Loss_Enh_Dec: -0.24509190\n",
      "| epoch  13 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  4.21 | ppl    67.15 | acc     0.53 | train_ae_norm     1.00\n",
      "[13/200][899/4361] Loss_D: 0.69066799 (Loss_D_real: 0.32123172 Loss_D_fake: 0.36943626) Loss_G: 0.05318770 Loss_Enh_Dec: -0.25131330\n",
      "| epoch  13 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.77 | loss  4.23 | ppl    68.50 | acc     0.56 | train_ae_norm     1.00\n",
      "[13/200][999/4361] Loss_D: 0.77576208 (Loss_D_real: 0.36995110 Loss_D_fake: 0.40581101) Loss_G: 0.05358427 Loss_Enh_Dec: -0.23173653\n",
      "| epoch  13 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  4.22 | ppl    67.96 | acc     0.52 | train_ae_norm     1.00\n",
      "[13/200][1099/4361] Loss_D: 0.65496910 (Loss_D_real: 0.36149979 Loss_D_fake: 0.29346928) Loss_G: 0.04689232 Loss_Enh_Dec: -0.22013311\n",
      "| epoch  13 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  4.23 | ppl    68.71 | acc     0.51 | train_ae_norm     1.00\n",
      "[13/200][1199/4361] Loss_D: 0.85139644 (Loss_D_real: 0.43933758 Loss_D_fake: 0.41205889) Loss_G: 0.07614433 Loss_Enh_Dec: -0.28271064\n",
      "| epoch  13 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  4.28 | ppl    71.97 | acc     0.56 | train_ae_norm     1.00\n",
      "[13/200][1299/4361] Loss_D: 0.83762783 (Loss_D_real: 0.46887851 Loss_D_fake: 0.36874932) Loss_G: 0.05722968 Loss_Enh_Dec: -0.21173923\n",
      "| epoch  13 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.86 | loss  4.26 | ppl    70.68 | acc     0.51 | train_ae_norm     1.00\n",
      "[13/200][1399/4361] Loss_D: 0.94589072 (Loss_D_real: 0.45691401 Loss_D_fake: 0.48897672) Loss_G: 0.05042582 Loss_Enh_Dec: -0.18657845\n",
      "| epoch  13 |  1400/ 4361 batches | lr 0.000000 | ms/batch 399.98 | loss  4.28 | ppl    72.45 | acc     0.48 | train_ae_norm     1.00\n",
      "[13/200][1499/4361] Loss_D: 1.05691254 (Loss_D_real: 0.47275767 Loss_D_fake: 0.58415484) Loss_G: 0.05099551 Loss_Enh_Dec: -0.21258269\n",
      "| epoch  13 |  1500/ 4361 batches | lr 0.000000 | ms/batch 400.76 | loss  4.33 | ppl    75.61 | acc     0.50 | train_ae_norm     1.00\n",
      "[13/200][1599/4361] Loss_D: 0.59801751 (Loss_D_real: 0.29330742 Loss_D_fake: 0.30471009) Loss_G: 0.04188547 Loss_Enh_Dec: -0.30080032\n",
      "| epoch  13 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  4.30 | ppl    73.82 | acc     0.52 | train_ae_norm     1.00\n",
      "[13/200][1699/4361] Loss_D: 0.77686620 (Loss_D_real: 0.37794161 Loss_D_fake: 0.39892459) Loss_G: 0.04788833 Loss_Enh_Dec: -0.19125298\n",
      "| epoch  13 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.39 | loss  4.29 | ppl    72.77 | acc     0.50 | train_ae_norm     1.00\n",
      "[13/200][1799/4361] Loss_D: 0.93950254 (Loss_D_real: 0.56475180 Loss_D_fake: 0.37475073) Loss_G: 0.06595844 Loss_Enh_Dec: -0.15917566\n",
      "| epoch  13 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  4.28 | ppl    71.91 | acc     0.53 | train_ae_norm     1.00\n",
      "[13/200][1899/4361] Loss_D: 0.77932882 (Loss_D_real: 0.35330439 Loss_D_fake: 0.42602441) Loss_G: 0.06728305 Loss_Enh_Dec: -0.24001789\n",
      "| epoch  13 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  4.31 | ppl    74.30 | acc     0.53 | train_ae_norm     1.00\n",
      "[13/200][1999/4361] Loss_D: 0.83528572 (Loss_D_real: 0.42286262 Loss_D_fake: 0.41242310) Loss_G: 0.06712940 Loss_Enh_Dec: -0.24039359\n",
      "| epoch  13 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  4.27 | ppl    71.75 | acc     0.52 | train_ae_norm     1.00\n",
      "[13/200][2099/4361] Loss_D: 0.90477252 (Loss_D_real: 0.44315213 Loss_D_fake: 0.46162039) Loss_G: 0.04552159 Loss_Enh_Dec: -0.19228368\n",
      "| epoch  13 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.76 | loss  4.24 | ppl    69.49 | acc     0.52 | train_ae_norm     1.00\n",
      "[13/200][2199/4361] Loss_D: 0.68039238 (Loss_D_real: 0.37965533 Loss_D_fake: 0.30073708) Loss_G: 0.06622245 Loss_Enh_Dec: -0.16389941\n",
      "| epoch  13 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  4.21 | ppl    67.55 | acc     0.53 | train_ae_norm     1.00\n",
      "[13/200][2299/4361] Loss_D: 0.66551799 (Loss_D_real: 0.32945427 Loss_D_fake: 0.33606371) Loss_G: 0.03881822 Loss_Enh_Dec: -0.13930006\n",
      "| epoch  13 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  4.22 | ppl    67.90 | acc     0.53 | train_ae_norm     1.00\n",
      "[13/200][2399/4361] Loss_D: 0.89516705 (Loss_D_real: 0.49563307 Loss_D_fake: 0.39953399) Loss_G: 0.06504201 Loss_Enh_Dec: -0.16814293\n",
      "| epoch  13 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  4.22 | ppl    68.24 | acc     0.49 | train_ae_norm     1.00\n",
      "[13/200][2499/4361] Loss_D: 0.77760768 (Loss_D_real: 0.39805377 Loss_D_fake: 0.37955388) Loss_G: 0.03851363 Loss_Enh_Dec: -0.16469963\n",
      "| epoch  13 |  2500/ 4361 batches | lr 0.000000 | ms/batch 400.81 | loss  4.27 | ppl    71.47 | acc     0.53 | train_ae_norm     1.00\n",
      "[13/200][2599/4361] Loss_D: 0.74355602 (Loss_D_real: 0.39613551 Loss_D_fake: 0.34742051) Loss_G: 0.07078950 Loss_Enh_Dec: -0.23699816\n",
      "| epoch  13 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  4.21 | ppl    67.21 | acc     0.52 | train_ae_norm     1.00\n",
      "[13/200][2699/4361] Loss_D: 0.86834913 (Loss_D_real: 0.45107645 Loss_D_fake: 0.41727269) Loss_G: 0.04135780 Loss_Enh_Dec: -0.16797513\n",
      "| epoch  13 |  2700/ 4361 batches | lr 0.000000 | ms/batch 400.79 | loss  4.22 | ppl    68.30 | acc     0.56 | train_ae_norm     1.00\n",
      "[13/200][2799/4361] Loss_D: 0.76720273 (Loss_D_real: 0.40749022 Loss_D_fake: 0.35971254) Loss_G: 0.06522512 Loss_Enh_Dec: -0.28952199\n",
      "| epoch  13 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.86 | loss  4.17 | ppl    64.87 | acc     0.52 | train_ae_norm     1.00\n",
      "[13/200][2899/4361] Loss_D: 0.76288170 (Loss_D_real: 0.41204938 Loss_D_fake: 0.35083231) Loss_G: 0.04901400 Loss_Enh_Dec: -0.15533486\n",
      "| epoch  13 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  4.20 | ppl    66.67 | acc     0.56 | train_ae_norm     1.00\n",
      "[13/200][2999/4361] Loss_D: 0.71679711 (Loss_D_real: 0.37859178 Loss_D_fake: 0.33820531) Loss_G: 0.05817130 Loss_Enh_Dec: -0.23464642\n",
      "| epoch  13 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  4.18 | ppl    65.16 | acc     0.54 | train_ae_norm     1.00\n",
      "[13/200][3099/4361] Loss_D: 0.75571847 (Loss_D_real: 0.34933245 Loss_D_fake: 0.40638599) Loss_G: 0.05167396 Loss_Enh_Dec: -0.26973170\n",
      "| epoch  13 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  4.18 | ppl    65.38 | acc     0.51 | train_ae_norm     1.00\n",
      "[13/200][3199/4361] Loss_D: 0.67958224 (Loss_D_real: 0.26886266 Loss_D_fake: 0.41071957) Loss_G: 0.06265493 Loss_Enh_Dec: -0.16436781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  13 |  3200/ 4361 batches | lr 0.000000 | ms/batch 402.06 | loss  4.20 | ppl    66.46 | acc     0.54 | train_ae_norm     1.00\n",
      "[13/200][3299/4361] Loss_D: 0.67713940 (Loss_D_real: 0.31206271 Loss_D_fake: 0.36507666) Loss_G: 0.04592621 Loss_Enh_Dec: -0.22099729\n",
      "| epoch  13 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  4.23 | ppl    68.67 | acc     0.52 | train_ae_norm     1.00\n",
      "[13/200][3399/4361] Loss_D: 0.81787038 (Loss_D_real: 0.40143785 Loss_D_fake: 0.41643256) Loss_G: 0.04641819 Loss_Enh_Dec: -0.23616615\n",
      "| epoch  13 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  4.17 | ppl    64.88 | acc     0.56 | train_ae_norm     1.00\n",
      "[13/200][3499/4361] Loss_D: 0.67318934 (Loss_D_real: 0.34393129 Loss_D_fake: 0.32925805) Loss_G: 0.05990602 Loss_Enh_Dec: -0.24656974\n",
      "| epoch  13 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  4.15 | ppl    63.51 | acc     0.57 | train_ae_norm     1.00\n",
      "[13/200][3599/4361] Loss_D: 0.84562892 (Loss_D_real: 0.46013293 Loss_D_fake: 0.38549599) Loss_G: 0.04433708 Loss_Enh_Dec: -0.25642619\n",
      "| epoch  13 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  4.15 | ppl    63.28 | acc     0.55 | train_ae_norm     1.00\n",
      "[13/200][3699/4361] Loss_D: 0.66578555 (Loss_D_real: 0.35173473 Loss_D_fake: 0.31405079) Loss_G: 0.05727473 Loss_Enh_Dec: -0.26767662\n",
      "| epoch  13 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  4.13 | ppl    61.92 | acc     0.53 | train_ae_norm     1.00\n",
      "[13/200][3799/4361] Loss_D: 0.65589088 (Loss_D_real: 0.33732551 Loss_D_fake: 0.31856537) Loss_G: 0.06639015 Loss_Enh_Dec: -0.28818664\n",
      "| epoch  13 |  3800/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  4.16 | ppl    64.04 | acc     0.57 | train_ae_norm     1.00\n",
      "[13/200][3899/4361] Loss_D: 0.83630514 (Loss_D_real: 0.48939058 Loss_D_fake: 0.34691453) Loss_G: 0.06698115 Loss_Enh_Dec: -0.17548250\n",
      "| epoch  13 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.60 | loss  4.15 | ppl    63.23 | acc     0.52 | train_ae_norm     1.00\n",
      "[13/200][3999/4361] Loss_D: 0.79490805 (Loss_D_real: 0.40254256 Loss_D_fake: 0.39236546) Loss_G: 0.05066926 Loss_Enh_Dec: -0.15792342\n",
      "| epoch  13 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  4.15 | ppl    63.53 | acc     0.56 | train_ae_norm     1.00\n",
      "[13/200][4099/4361] Loss_D: 0.84895194 (Loss_D_real: 0.51771045 Loss_D_fake: 0.33124146) Loss_G: 0.06269857 Loss_Enh_Dec: -0.29144257\n",
      "| epoch  13 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  4.10 | ppl    60.27 | acc     0.53 | train_ae_norm     1.00\n",
      "[13/200][4199/4361] Loss_D: 0.73779774 (Loss_D_real: 0.38291094 Loss_D_fake: 0.35488677) Loss_G: 0.04682064 Loss_Enh_Dec: -0.17019038\n",
      "| epoch  13 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.05 | loss  4.15 | ppl    63.46 | acc     0.56 | train_ae_norm     1.00\n",
      "[13/200][4299/4361] Loss_D: 0.77574778 (Loss_D_real: 0.34748361 Loss_D_fake: 0.42826414) Loss_G: 0.04926090 Loss_Enh_Dec: -0.14149223\n",
      "| epoch  13 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  4.10 | ppl    60.16 | acc     0.55 | train_ae_norm     1.00\n",
      "| end of epoch  13 | time: 1851.64s | test loss  3.84 | test ppl 46.54 | acc 0.615\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 14 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.703\n",
      "  Average training loss discriminator: 0.767\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.477\n",
      "  Test Loss: 2.493\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  14 |     0/ 4361 batches | lr 0.000000 | ms/batch 861.58 | loss  0.04 | ppl     1.04 | acc     0.57 | train_ae_norm     1.00\n",
      "[14/200][99/4361] Loss_D: 0.71986616 (Loss_D_real: 0.37993830 Loss_D_fake: 0.33992788) Loss_G: 0.05899146 Loss_Enh_Dec: -0.10729823\n",
      "| epoch  14 |   100/ 4361 batches | lr 0.000000 | ms/batch 400.85 | loss  4.14 | ppl    62.51 | acc     0.55 | train_ae_norm     1.00\n",
      "[14/200][199/4361] Loss_D: 0.72433990 (Loss_D_real: 0.36142263 Loss_D_fake: 0.36291727) Loss_G: 0.07650716 Loss_Enh_Dec: -0.20421794\n",
      "| epoch  14 |   200/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  4.14 | ppl    62.99 | acc     0.54 | train_ae_norm     1.00\n",
      "[14/200][299/4361] Loss_D: 0.71413863 (Loss_D_real: 0.32154712 Loss_D_fake: 0.39259148) Loss_G: 0.05947063 Loss_Enh_Dec: -0.23408209\n",
      "| epoch  14 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.37 | loss  4.12 | ppl    61.80 | acc     0.51 | train_ae_norm     1.00\n",
      "[14/200][399/4361] Loss_D: 0.78918964 (Loss_D_real: 0.36810160 Loss_D_fake: 0.42108804) Loss_G: 0.05521768 Loss_Enh_Dec: -0.16311859\n",
      "| epoch  14 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  4.08 | ppl    59.32 | acc     0.56 | train_ae_norm     1.00\n",
      "[14/200][499/4361] Loss_D: 0.75159001 (Loss_D_real: 0.38386011 Loss_D_fake: 0.36772990) Loss_G: 0.05838937 Loss_Enh_Dec: -0.17977256\n",
      "| epoch  14 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  4.13 | ppl    61.92 | acc     0.55 | train_ae_norm     1.00\n",
      "[14/200][599/4361] Loss_D: 0.71227288 (Loss_D_real: 0.33541021 Loss_D_fake: 0.37686270) Loss_G: 0.05596201 Loss_Enh_Dec: -0.26008806\n",
      "| epoch  14 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.41 | loss  4.07 | ppl    58.67 | acc     0.49 | train_ae_norm     1.00\n",
      "[14/200][699/4361] Loss_D: 0.82050240 (Loss_D_real: 0.45647010 Loss_D_fake: 0.36403227) Loss_G: 0.06584609 Loss_Enh_Dec: -0.23574172\n",
      "| epoch  14 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.61 | loss  4.12 | ppl    61.73 | acc     0.58 | train_ae_norm     1.00\n",
      "[14/200][799/4361] Loss_D: 0.86740744 (Loss_D_real: 0.47045240 Loss_D_fake: 0.39695507) Loss_G: 0.05641093 Loss_Enh_Dec: -0.19376177\n",
      "| epoch  14 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.80 | loss  4.10 | ppl    60.21 | acc     0.55 | train_ae_norm     1.00\n",
      "[14/200][899/4361] Loss_D: 0.73364395 (Loss_D_real: 0.36705518 Loss_D_fake: 0.36658877) Loss_G: 0.07130341 Loss_Enh_Dec: -0.15692897\n",
      "| epoch  14 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  4.13 | ppl    61.98 | acc     0.59 | train_ae_norm     1.00\n",
      "[14/200][999/4361] Loss_D: 0.68234479 (Loss_D_real: 0.33536693 Loss_D_fake: 0.34697783) Loss_G: 0.06893940 Loss_Enh_Dec: -0.23844086\n",
      "| epoch  14 |  1000/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  4.10 | ppl    60.27 | acc     0.54 | train_ae_norm     1.00\n",
      "[14/200][1099/4361] Loss_D: 0.84161949 (Loss_D_real: 0.39896381 Loss_D_fake: 0.44265565) Loss_G: 0.04072431 Loss_Enh_Dec: -0.16324854\n",
      "| epoch  14 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  4.11 | ppl    60.66 | acc     0.54 | train_ae_norm     1.00\n",
      "[14/200][1199/4361] Loss_D: 0.92707682 (Loss_D_real: 0.51638234 Loss_D_fake: 0.41069448) Loss_G: 0.05203968 Loss_Enh_Dec: -0.24647717\n",
      "| epoch  14 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.91 | loss  4.11 | ppl    61.01 | acc     0.57 | train_ae_norm     1.00\n",
      "[14/200][1299/4361] Loss_D: 0.86667091 (Loss_D_real: 0.46207938 Loss_D_fake: 0.40459153) Loss_G: 0.05334151 Loss_Enh_Dec: -0.25078139\n",
      "| epoch  14 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.59 | loss  4.10 | ppl    60.43 | acc     0.56 | train_ae_norm     1.00\n",
      "[14/200][1399/4361] Loss_D: 0.69194347 (Loss_D_real: 0.33634514 Loss_D_fake: 0.35559833) Loss_G: 0.06935915 Loss_Enh_Dec: -0.15393017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  14 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.71 | loss  4.15 | ppl    63.71 | acc     0.49 | train_ae_norm     1.00\n",
      "[14/200][1499/4361] Loss_D: 0.80666351 (Loss_D_real: 0.27381185 Loss_D_fake: 0.53285164) Loss_G: 0.04661287 Loss_Enh_Dec: -0.19840117\n",
      "| epoch  14 |  1500/ 4361 batches | lr 0.000000 | ms/batch 400.41 | loss  4.20 | ppl    66.68 | acc     0.54 | train_ae_norm     1.00\n",
      "[14/200][1599/4361] Loss_D: 0.82056797 (Loss_D_real: 0.47565240 Loss_D_fake: 0.34491560) Loss_G: 0.03683065 Loss_Enh_Dec: -0.19047478\n",
      "| epoch  14 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  4.15 | ppl    63.72 | acc     0.55 | train_ae_norm     1.00\n",
      "[14/200][1699/4361] Loss_D: 0.78593862 (Loss_D_real: 0.35773250 Loss_D_fake: 0.42820615) Loss_G: 0.04352307 Loss_Enh_Dec: -0.15918416\n",
      "| epoch  14 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  4.17 | ppl    64.90 | acc     0.52 | train_ae_norm     1.00\n",
      "[14/200][1799/4361] Loss_D: 0.66192025 (Loss_D_real: 0.37753785 Loss_D_fake: 0.28438240) Loss_G: 0.07898884 Loss_Enh_Dec: -0.16263565\n",
      "| epoch  14 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.40 | loss  4.12 | ppl    61.86 | acc     0.54 | train_ae_norm     1.00\n",
      "[14/200][1899/4361] Loss_D: 0.88680613 (Loss_D_real: 0.51223040 Loss_D_fake: 0.37457570) Loss_G: 0.05989181 Loss_Enh_Dec: -0.36383992\n",
      "| epoch  14 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  4.19 | ppl    66.05 | acc     0.55 | train_ae_norm     1.00\n",
      "[14/200][1999/4361] Loss_D: 0.78666168 (Loss_D_real: 0.38848802 Loss_D_fake: 0.39817366) Loss_G: 0.05137299 Loss_Enh_Dec: -0.19298311\n",
      "| epoch  14 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.73 | loss  4.15 | ppl    63.33 | acc     0.55 | train_ae_norm     1.00\n",
      "[14/200][2099/4361] Loss_D: 0.72275734 (Loss_D_real: 0.37692389 Loss_D_fake: 0.34583348) Loss_G: 0.04832631 Loss_Enh_Dec: -0.16273506\n",
      "| epoch  14 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.86 | loss  4.17 | ppl    64.80 | acc     0.55 | train_ae_norm     1.00\n",
      "[14/200][2199/4361] Loss_D: 0.87129343 (Loss_D_real: 0.51809478 Loss_D_fake: 0.35319862) Loss_G: 0.05357471 Loss_Enh_Dec: -0.20255475\n",
      "| epoch  14 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  4.18 | ppl    65.58 | acc     0.53 | train_ae_norm     1.00\n",
      "[14/200][2299/4361] Loss_D: 0.89350951 (Loss_D_real: 0.49511468 Loss_D_fake: 0.39839479) Loss_G: 0.06154485 Loss_Enh_Dec: -0.11464613\n",
      "| epoch  14 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  4.18 | ppl    65.15 | acc     0.56 | train_ae_norm     1.00\n",
      "[14/200][2399/4361] Loss_D: 0.78611356 (Loss_D_real: 0.35354182 Loss_D_fake: 0.43257174) Loss_G: 0.06724699 Loss_Enh_Dec: -0.21332930\n",
      "| epoch  14 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  4.17 | ppl    64.78 | acc     0.48 | train_ae_norm     1.00\n",
      "[14/200][2499/4361] Loss_D: 0.73468566 (Loss_D_real: 0.33358812 Loss_D_fake: 0.40109751) Loss_G: 0.04502537 Loss_Enh_Dec: -0.18570207\n",
      "| epoch  14 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  4.19 | ppl    65.98 | acc     0.57 | train_ae_norm     1.00\n",
      "[14/200][2599/4361] Loss_D: 0.80286473 (Loss_D_real: 0.44166431 Loss_D_fake: 0.36120042) Loss_G: 0.05660858 Loss_Enh_Dec: -0.21849255\n",
      "| epoch  14 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  4.17 | ppl    64.49 | acc     0.52 | train_ae_norm     1.00\n",
      "[14/200][2699/4361] Loss_D: 0.70451587 (Loss_D_real: 0.33253789 Loss_D_fake: 0.37197798) Loss_G: 0.06640866 Loss_Enh_Dec: -0.18222259\n",
      "| epoch  14 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  4.18 | ppl    65.31 | acc     0.55 | train_ae_norm     1.00\n",
      "[14/200][2799/4361] Loss_D: 0.54151571 (Loss_D_real: 0.26191035 Loss_D_fake: 0.27960533) Loss_G: 0.05018526 Loss_Enh_Dec: -0.24439679\n",
      "| epoch  14 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  4.11 | ppl    60.77 | acc     0.50 | train_ae_norm     1.00\n",
      "[14/200][2899/4361] Loss_D: 0.77822262 (Loss_D_real: 0.41165930 Loss_D_fake: 0.36656332) Loss_G: 0.05212529 Loss_Enh_Dec: -0.15841047\n",
      "| epoch  14 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  4.15 | ppl    63.27 | acc     0.56 | train_ae_norm     1.00\n",
      "[14/200][2999/4361] Loss_D: 0.74178159 (Loss_D_real: 0.41247210 Loss_D_fake: 0.32930952) Loss_G: 0.07516474 Loss_Enh_Dec: -0.23492847\n",
      "| epoch  14 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  4.16 | ppl    64.11 | acc     0.52 | train_ae_norm     1.00\n",
      "[14/200][3099/4361] Loss_D: 0.68981373 (Loss_D_real: 0.35796547 Loss_D_fake: 0.33184826) Loss_G: 0.04293257 Loss_Enh_Dec: -0.19990900\n",
      "| epoch  14 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  4.17 | ppl    64.73 | acc     0.50 | train_ae_norm     1.00\n",
      "[14/200][3199/4361] Loss_D: 0.58519113 (Loss_D_real: 0.23233730 Loss_D_fake: 0.35285386) Loss_G: 0.04544485 Loss_Enh_Dec: -0.25617355\n",
      "| epoch  14 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  4.17 | ppl    64.60 | acc     0.55 | train_ae_norm     1.00\n",
      "[14/200][3299/4361] Loss_D: 0.76062483 (Loss_D_real: 0.48853815 Loss_D_fake: 0.27208668) Loss_G: 0.05911318 Loss_Enh_Dec: -0.23684195\n",
      "| epoch  14 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  4.20 | ppl    66.53 | acc     0.54 | train_ae_norm     1.00\n",
      "[14/200][3399/4361] Loss_D: 0.76273382 (Loss_D_real: 0.35014170 Loss_D_fake: 0.41259211) Loss_G: 0.04928524 Loss_Enh_Dec: -0.12686686\n",
      "| epoch  14 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  4.17 | ppl    64.59 | acc     0.56 | train_ae_norm     1.00\n",
      "[14/200][3499/4361] Loss_D: 0.59004891 (Loss_D_real: 0.28133580 Loss_D_fake: 0.30871308) Loss_G: 0.07308307 Loss_Enh_Dec: -0.16092041\n",
      "| epoch  14 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  4.11 | ppl    60.86 | acc     0.56 | train_ae_norm     1.00\n",
      "[14/200][3599/4361] Loss_D: 0.62106168 (Loss_D_real: 0.33386415 Loss_D_fake: 0.28719753) Loss_G: 0.05821107 Loss_Enh_Dec: -0.21319108\n",
      "| epoch  14 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  4.11 | ppl    61.16 | acc     0.53 | train_ae_norm     1.00\n",
      "[14/200][3699/4361] Loss_D: 0.77183890 (Loss_D_real: 0.38079390 Loss_D_fake: 0.39104497) Loss_G: 0.06228313 Loss_Enh_Dec: -0.19978108\n",
      "| epoch  14 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  4.11 | ppl    61.12 | acc     0.54 | train_ae_norm     1.00\n",
      "[14/200][3799/4361] Loss_D: 0.75760543 (Loss_D_real: 0.33441377 Loss_D_fake: 0.42319167) Loss_G: 0.06406923 Loss_Enh_Dec: -0.23169771\n",
      "| epoch  14 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  4.15 | ppl    63.42 | acc     0.58 | train_ae_norm     1.00\n",
      "[14/200][3899/4361] Loss_D: 0.77896768 (Loss_D_real: 0.39506322 Loss_D_fake: 0.38390446) Loss_G: 0.07536084 Loss_Enh_Dec: -0.16866010\n",
      "| epoch  14 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  4.14 | ppl    63.05 | acc     0.50 | train_ae_norm     1.00\n",
      "[14/200][3999/4361] Loss_D: 0.59750968 (Loss_D_real: 0.25126374 Loss_D_fake: 0.34624594) Loss_G: 0.03719221 Loss_Enh_Dec: -0.14256656\n",
      "| epoch  14 |  4000/ 4361 batches | lr 0.000000 | ms/batch 402.29 | loss  4.16 | ppl    64.16 | acc     0.55 | train_ae_norm     1.00\n",
      "[14/200][4099/4361] Loss_D: 0.62637091 (Loss_D_real: 0.25100657 Loss_D_fake: 0.37536430) Loss_G: 0.08040481 Loss_Enh_Dec: -0.18760447\n",
      "| epoch  14 |  4100/ 4361 batches | lr 0.000000 | ms/batch 402.08 | loss  4.10 | ppl    60.26 | acc     0.53 | train_ae_norm     1.00\n",
      "[14/200][4199/4361] Loss_D: 0.68328893 (Loss_D_real: 0.22878203 Loss_D_fake: 0.45450693) Loss_G: 0.06405576 Loss_Enh_Dec: -0.19147019\n",
      "| epoch  14 |  4200/ 4361 batches | lr 0.000000 | ms/batch 402.24 | loss  4.13 | ppl    62.46 | acc     0.55 | train_ae_norm     1.00\n",
      "[14/200][4299/4361] Loss_D: 0.71176010 (Loss_D_real: 0.32956564 Loss_D_fake: 0.38219446) Loss_G: 0.06123416 Loss_Enh_Dec: -0.14966539\n",
      "| epoch  14 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  4.08 | ppl    58.95 | acc     0.53 | train_ae_norm     1.00\n",
      "| end of epoch  14 | time: 1851.48s | test loss  3.84 | test ppl 46.44 | acc 0.615\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 15 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.703\n",
      "  Average training loss discriminator: 0.759\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.472\n",
      "  Test Loss: 2.561\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  15 |     0/ 4361 batches | lr 0.000000 | ms/batch 863.90 | loss  0.04 | ppl     1.04 | acc     0.57 | train_ae_norm     1.00\n",
      "[15/200][99/4361] Loss_D: 1.00441813 (Loss_D_real: 0.59765834 Loss_D_fake: 0.40675974) Loss_G: 0.07104383 Loss_Enh_Dec: -0.23432799\n",
      "| epoch  15 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  4.13 | ppl    62.14 | acc     0.53 | train_ae_norm     1.00\n",
      "[15/200][199/4361] Loss_D: 0.81740808 (Loss_D_real: 0.45487857 Loss_D_fake: 0.36252952) Loss_G: 0.05387544 Loss_Enh_Dec: -0.21248904\n",
      "| epoch  15 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  4.14 | ppl    62.55 | acc     0.56 | train_ae_norm     1.00\n",
      "[15/200][299/4361] Loss_D: 0.72896361 (Loss_D_real: 0.31655326 Loss_D_fake: 0.41241032) Loss_G: 0.07683238 Loss_Enh_Dec: -0.19711933\n",
      "| epoch  15 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  4.14 | ppl    63.03 | acc     0.52 | train_ae_norm     1.00\n",
      "[15/200][399/4361] Loss_D: 0.79918885 (Loss_D_real: 0.42971414 Loss_D_fake: 0.36947474) Loss_G: 0.06437833 Loss_Enh_Dec: -0.17223699\n",
      "| epoch  15 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  4.09 | ppl    59.84 | acc     0.57 | train_ae_norm     1.00\n",
      "[15/200][499/4361] Loss_D: 0.62028420 (Loss_D_real: 0.28368908 Loss_D_fake: 0.33659515) Loss_G: 0.07100397 Loss_Enh_Dec: -0.12514614\n",
      "| epoch  15 |   500/ 4361 batches | lr 0.000000 | ms/batch 402.17 | loss  4.15 | ppl    63.23 | acc     0.58 | train_ae_norm     1.00\n",
      "[15/200][599/4361] Loss_D: 0.62127066 (Loss_D_real: 0.27349699 Loss_D_fake: 0.34777364) Loss_G: 0.06460201 Loss_Enh_Dec: -0.19920020\n",
      "| epoch  15 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  4.08 | ppl    59.14 | acc     0.52 | train_ae_norm     1.00\n",
      "[15/200][699/4361] Loss_D: 0.73374116 (Loss_D_real: 0.35609382 Loss_D_fake: 0.37764734) Loss_G: 0.04523821 Loss_Enh_Dec: -0.18805298\n",
      "| epoch  15 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.85 | loss  4.12 | ppl    61.41 | acc     0.58 | train_ae_norm     1.00\n",
      "[15/200][799/4361] Loss_D: 0.72433811 (Loss_D_real: 0.36731300 Loss_D_fake: 0.35702512) Loss_G: 0.05975093 Loss_Enh_Dec: -0.16188429\n",
      "| epoch  15 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  4.06 | ppl    58.24 | acc     0.55 | train_ae_norm     1.00\n",
      "[15/200][899/4361] Loss_D: 0.84678060 (Loss_D_real: 0.40824187 Loss_D_fake: 0.43853873) Loss_G: 0.05056955 Loss_Enh_Dec: -0.13928661\n",
      "| epoch  15 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.74 | loss  4.14 | ppl    62.94 | acc     0.60 | train_ae_norm     1.00\n",
      "[15/200][999/4361] Loss_D: 0.88493735 (Loss_D_real: 0.31446159 Loss_D_fake: 0.57047576) Loss_G: 0.07763733 Loss_Enh_Dec: -0.16023286\n",
      "| epoch  15 |  1000/ 4361 batches | lr 0.000000 | ms/batch 400.78 | loss  4.12 | ppl    61.47 | acc     0.55 | train_ae_norm     1.00\n",
      "[15/200][1099/4361] Loss_D: 0.77586854 (Loss_D_real: 0.40187913 Loss_D_fake: 0.37398940) Loss_G: 0.05515878 Loss_Enh_Dec: -0.16029468\n",
      "| epoch  15 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  4.10 | ppl    60.29 | acc     0.51 | train_ae_norm     1.00\n",
      "[15/200][1199/4361] Loss_D: 0.70131743 (Loss_D_real: 0.37624401 Loss_D_fake: 0.32507342) Loss_G: 0.04135020 Loss_Enh_Dec: -0.26618364\n",
      "| epoch  15 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  4.14 | ppl    62.99 | acc     0.58 | train_ae_norm     1.00\n",
      "[15/200][1299/4361] Loss_D: 0.67871439 (Loss_D_real: 0.33805928 Loss_D_fake: 0.34065515) Loss_G: 0.05206317 Loss_Enh_Dec: -0.18354020\n",
      "| epoch  15 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.93 | loss  4.12 | ppl    61.30 | acc     0.55 | train_ae_norm     1.00\n",
      "[15/200][1399/4361] Loss_D: 0.67354763 (Loss_D_real: 0.32256755 Loss_D_fake: 0.35098004) Loss_G: 0.05508785 Loss_Enh_Dec: -0.22202845\n",
      "| epoch  15 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.08 | loss  4.11 | ppl    61.04 | acc     0.49 | train_ae_norm     1.00\n",
      "[15/200][1499/4361] Loss_D: 0.70902550 (Loss_D_real: 0.37009048 Loss_D_fake: 0.33893502) Loss_G: 0.03603389 Loss_Enh_Dec: -0.11727035\n",
      "| epoch  15 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  4.13 | ppl    62.27 | acc     0.52 | train_ae_norm     1.00\n",
      "[15/200][1599/4361] Loss_D: 0.62459004 (Loss_D_real: 0.25788230 Loss_D_fake: 0.36670777) Loss_G: 0.05938927 Loss_Enh_Dec: -0.22835758\n",
      "| epoch  15 |  1600/ 4361 batches | lr 0.000000 | ms/batch 400.48 | loss  4.09 | ppl    59.90 | acc     0.56 | train_ae_norm     1.00\n",
      "[15/200][1699/4361] Loss_D: 0.75518167 (Loss_D_real: 0.33033580 Loss_D_fake: 0.42484587) Loss_G: 0.04256484 Loss_Enh_Dec: -0.26599798\n",
      "| epoch  15 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  4.08 | ppl    59.26 | acc     0.52 | train_ae_norm     1.00\n",
      "[15/200][1799/4361] Loss_D: 0.76893431 (Loss_D_real: 0.38192019 Loss_D_fake: 0.38701412) Loss_G: 0.06091366 Loss_Enh_Dec: -0.24621920\n",
      "| epoch  15 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  4.07 | ppl    58.54 | acc     0.57 | train_ae_norm     1.00\n",
      "[15/200][1899/4361] Loss_D: 0.87774920 (Loss_D_real: 0.46932209 Loss_D_fake: 0.40842709) Loss_G: 0.04511256 Loss_Enh_Dec: -0.14269376\n",
      "| epoch  15 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.92 | loss  4.11 | ppl    61.00 | acc     0.54 | train_ae_norm     1.00\n",
      "[15/200][1999/4361] Loss_D: 0.86403620 (Loss_D_real: 0.48501953 Loss_D_fake: 0.37901664) Loss_G: 0.05509515 Loss_Enh_Dec: -0.23778601\n",
      "| epoch  15 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.57 | loss  4.06 | ppl    57.76 | acc     0.53 | train_ae_norm     1.00\n",
      "[15/200][2099/4361] Loss_D: 0.73892462 (Loss_D_real: 0.34735721 Loss_D_fake: 0.39156741) Loss_G: 0.06250498 Loss_Enh_Dec: -0.21473685\n",
      "| epoch  15 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.42 | loss  4.07 | ppl    58.81 | acc     0.55 | train_ae_norm     1.00\n",
      "[15/200][2199/4361] Loss_D: 0.84061038 (Loss_D_real: 0.43289673 Loss_D_fake: 0.40771365) Loss_G: 0.04777792 Loss_Enh_Dec: -0.25122115\n",
      "| epoch  15 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.31 | loss  4.05 | ppl    57.33 | acc     0.56 | train_ae_norm     1.00\n",
      "[15/200][2299/4361] Loss_D: 0.64193475 (Loss_D_real: 0.35351023 Loss_D_fake: 0.28842449) Loss_G: 0.06709132 Loss_Enh_Dec: -0.25428089\n",
      "| epoch  15 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.52 | loss  4.04 | ppl    57.09 | acc     0.57 | train_ae_norm     1.00\n",
      "[15/200][2399/4361] Loss_D: 0.87744331 (Loss_D_real: 0.48541963 Loss_D_fake: 0.39202368) Loss_G: 0.05877108 Loss_Enh_Dec: -0.20251656\n",
      "| epoch  15 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  4.02 | ppl    55.87 | acc     0.53 | train_ae_norm     1.00\n",
      "[15/200][2499/4361] Loss_D: 0.70396841 (Loss_D_real: 0.35870719 Loss_D_fake: 0.34526119) Loss_G: 0.04068094 Loss_Enh_Dec: -0.18916482\n",
      "| epoch  15 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  4.10 | ppl    60.09 | acc     0.56 | train_ae_norm     1.00\n",
      "[15/200][2599/4361] Loss_D: 0.63267636 (Loss_D_real: 0.29909319 Loss_D_fake: 0.33358318) Loss_G: 0.06363914 Loss_Enh_Dec: -0.19800605\n",
      "| epoch  15 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  4.06 | ppl    57.94 | acc     0.55 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/200][2699/4361] Loss_D: 0.80697703 (Loss_D_real: 0.43489039 Loss_D_fake: 0.37208661) Loss_G: 0.03904104 Loss_Enh_Dec: -0.21338537\n",
      "| epoch  15 |  2700/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  4.08 | ppl    58.96 | acc     0.54 | train_ae_norm     1.00\n",
      "[15/200][2799/4361] Loss_D: 0.84367108 (Loss_D_real: 0.47999758 Loss_D_fake: 0.36367351) Loss_G: 0.06315795 Loss_Enh_Dec: -0.20976818\n",
      "| epoch  15 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.85 | loss  4.00 | ppl    54.61 | acc     0.52 | train_ae_norm     1.00\n",
      "[15/200][2899/4361] Loss_D: 0.87066156 (Loss_D_real: 0.48541439 Loss_D_fake: 0.38524717) Loss_G: 0.06308474 Loss_Enh_Dec: -0.18286265\n",
      "| epoch  15 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  4.06 | ppl    57.74 | acc     0.58 | train_ae_norm     1.00\n",
      "[15/200][2999/4361] Loss_D: 0.70015889 (Loss_D_real: 0.39658558 Loss_D_fake: 0.30357331) Loss_G: 0.02518850 Loss_Enh_Dec: -0.25478962\n",
      "| epoch  15 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.41 | loss  4.04 | ppl    56.74 | acc     0.57 | train_ae_norm     1.00\n",
      "[15/200][3099/4361] Loss_D: 0.94405794 (Loss_D_real: 0.41433463 Loss_D_fake: 0.52972335) Loss_G: 0.06228992 Loss_Enh_Dec: -0.21497993\n",
      "| epoch  15 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  4.06 | ppl    57.75 | acc     0.53 | train_ae_norm     1.00\n",
      "[15/200][3199/4361] Loss_D: 0.67898595 (Loss_D_real: 0.35522863 Loss_D_fake: 0.32375735) Loss_G: 0.07073897 Loss_Enh_Dec: -0.20615780\n",
      "| epoch  15 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  4.07 | ppl    58.85 | acc     0.56 | train_ae_norm     1.00\n",
      "[15/200][3299/4361] Loss_D: 0.81844807 (Loss_D_real: 0.43502992 Loss_D_fake: 0.38341817) Loss_G: 0.05186958 Loss_Enh_Dec: -0.25589779\n",
      "| epoch  15 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  4.09 | ppl    60.02 | acc     0.56 | train_ae_norm     1.00\n",
      "[15/200][3399/4361] Loss_D: 0.73399282 (Loss_D_real: 0.39755464 Loss_D_fake: 0.33643818) Loss_G: 0.05552058 Loss_Enh_Dec: -0.23697348\n",
      "| epoch  15 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  4.05 | ppl    57.42 | acc     0.59 | train_ae_norm     1.00\n",
      "[15/200][3499/4361] Loss_D: 0.74299085 (Loss_D_real: 0.40948373 Loss_D_fake: 0.33350712) Loss_G: 0.03875247 Loss_Enh_Dec: -0.26141086\n",
      "| epoch  15 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.74 | loss  3.99 | ppl    54.16 | acc     0.58 | train_ae_norm     1.00\n",
      "[15/200][3599/4361] Loss_D: 0.73415083 (Loss_D_real: 0.38441178 Loss_D_fake: 0.34973904) Loss_G: 0.05759779 Loss_Enh_Dec: -0.18369196\n",
      "| epoch  15 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  4.01 | ppl    55.05 | acc     0.53 | train_ae_norm     1.00\n",
      "[15/200][3699/4361] Loss_D: 0.83566898 (Loss_D_real: 0.36923274 Loss_D_fake: 0.46643624) Loss_G: 0.06494785 Loss_Enh_Dec: -0.19993483\n",
      "| epoch  15 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.82 | loss  4.02 | ppl    55.63 | acc     0.56 | train_ae_norm     1.00\n",
      "[15/200][3799/4361] Loss_D: 0.74187219 (Loss_D_real: 0.33587235 Loss_D_fake: 0.40599984) Loss_G: 0.06495514 Loss_Enh_Dec: -0.24463549\n",
      "| epoch  15 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  4.05 | ppl    57.14 | acc     0.60 | train_ae_norm     1.00\n",
      "[15/200][3899/4361] Loss_D: 0.69874275 (Loss_D_real: 0.41979688 Loss_D_fake: 0.27894586) Loss_G: 0.06241209 Loss_Enh_Dec: -0.14263891\n",
      "| epoch  15 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  4.05 | ppl    57.50 | acc     0.52 | train_ae_norm     1.00\n",
      "[15/200][3999/4361] Loss_D: 0.69475001 (Loss_D_real: 0.35751575 Loss_D_fake: 0.33723426) Loss_G: 0.04550776 Loss_Enh_Dec: -0.22650757\n",
      "| epoch  15 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.74 | loss  4.05 | ppl    57.44 | acc     0.57 | train_ae_norm     1.00\n",
      "[15/200][4099/4361] Loss_D: 0.78239524 (Loss_D_real: 0.34821999 Loss_D_fake: 0.43417522) Loss_G: 0.06536825 Loss_Enh_Dec: -0.22646661\n",
      "| epoch  15 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  4.01 | ppl    55.02 | acc     0.55 | train_ae_norm     1.00\n",
      "[15/200][4199/4361] Loss_D: 0.86549568 (Loss_D_real: 0.40121385 Loss_D_fake: 0.46428180) Loss_G: 0.04668532 Loss_Enh_Dec: -0.17684206\n",
      "| epoch  15 |  4200/ 4361 batches | lr 0.000000 | ms/batch 400.83 | loss  4.04 | ppl    56.73 | acc     0.56 | train_ae_norm     1.00\n",
      "[15/200][4299/4361] Loss_D: 0.78471529 (Loss_D_real: 0.42736965 Loss_D_fake: 0.35734567) Loss_G: 0.06241868 Loss_Enh_Dec: -0.28705356\n",
      "| epoch  15 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  4.00 | ppl    54.46 | acc     0.56 | train_ae_norm     1.00\n",
      "| end of epoch  15 | time: 1851.25s | test loss  3.74 | test ppl 42.16 | acc 0.628\n",
      "bleu_self:  [1.41523314e-01 4.30899194e-09 1.50255676e-11 1.01144591e-12\n",
      " 8.42729148e-12]\n",
      "bleu_test:  [8.08432539e-01 3.52390740e-01 2.73819188e-06 8.40277982e-09\n",
      " 5.70249476e-09]\n",
      "bleu_self: [0.14152331,0.00000000,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.80843254,0.35239074,0.00000274,0.00000001,0.00000001]\n",
      "New saving model: epoch 015.\n",
      "Saving models to ./results/yahoo_merge_assigned_results\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 16 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.759\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.470\n",
      "  Test Loss: 2.660\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  16 |     0/ 4361 batches | lr 0.000000 | ms/batch 863.60 | loss  0.04 | ppl     1.04 | acc     0.60 | train_ae_norm     1.00\n",
      "[16/200][99/4361] Loss_D: 0.93052346 (Loss_D_real: 0.37424570 Loss_D_fake: 0.55627775) Loss_G: 0.05432464 Loss_Enh_Dec: -0.21439274\n",
      "| epoch  16 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  4.03 | ppl    56.31 | acc     0.53 | train_ae_norm     1.00\n",
      "[16/200][199/4361] Loss_D: 0.67958963 (Loss_D_real: 0.32365632 Loss_D_fake: 0.35593328) Loss_G: 0.05003364 Loss_Enh_Dec: -0.16310582\n",
      "| epoch  16 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  4.05 | ppl    57.16 | acc     0.58 | train_ae_norm     1.00\n",
      "[16/200][299/4361] Loss_D: 0.68340373 (Loss_D_real: 0.36550173 Loss_D_fake: 0.31790203) Loss_G: 0.07488706 Loss_Enh_Dec: -0.19820487\n",
      "| epoch  16 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  4.05 | ppl    57.50 | acc     0.51 | train_ae_norm     1.00\n",
      "[16/200][399/4361] Loss_D: 0.64391470 (Loss_D_real: 0.35912630 Loss_D_fake: 0.28478840) Loss_G: 0.07377455 Loss_Enh_Dec: -0.17128357\n",
      "| epoch  16 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  3.97 | ppl    53.16 | acc     0.58 | train_ae_norm     1.00\n",
      "[16/200][499/4361] Loss_D: 0.79194558 (Loss_D_real: 0.35583290 Loss_D_fake: 0.43611264) Loss_G: 0.03616520 Loss_Enh_Dec: -0.22028823\n",
      "| epoch  16 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  4.04 | ppl    56.78 | acc     0.56 | train_ae_norm     1.00\n",
      "[16/200][599/4361] Loss_D: 0.78636348 (Loss_D_real: 0.41713676 Loss_D_fake: 0.36922675) Loss_G: 0.06205438 Loss_Enh_Dec: -0.16847676\n",
      "| epoch  16 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  3.97 | ppl    53.23 | acc     0.54 | train_ae_norm     1.00\n",
      "[16/200][699/4361] Loss_D: 0.79466552 (Loss_D_real: 0.34772253 Loss_D_fake: 0.44694299) Loss_G: 0.08141991 Loss_Enh_Dec: -0.21728817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  16 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  4.02 | ppl    55.67 | acc     0.59 | train_ae_norm     1.00\n",
      "[16/200][799/4361] Loss_D: 0.91821027 (Loss_D_real: 0.39269909 Loss_D_fake: 0.52551115) Loss_G: 0.03535310 Loss_Enh_Dec: -0.18858908\n",
      "| epoch  16 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.88 | loss  3.99 | ppl    54.25 | acc     0.56 | train_ae_norm     1.00\n",
      "[16/200][899/4361] Loss_D: 0.72714865 (Loss_D_real: 0.41273409 Loss_D_fake: 0.31441453) Loss_G: 0.05731382 Loss_Enh_Dec: -0.24311367\n",
      "| epoch  16 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  4.04 | ppl    56.86 | acc     0.59 | train_ae_norm     1.00\n",
      "[16/200][999/4361] Loss_D: 0.81655264 (Loss_D_real: 0.52886343 Loss_D_fake: 0.28768921) Loss_G: 0.03999960 Loss_Enh_Dec: -0.21629314\n",
      "| epoch  16 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  4.01 | ppl    54.89 | acc     0.56 | train_ae_norm     1.00\n",
      "[16/200][1099/4361] Loss_D: 0.97563505 (Loss_D_real: 0.60529697 Loss_D_fake: 0.37033805) Loss_G: 0.05037479 Loss_Enh_Dec: -0.15455942\n",
      "| epoch  16 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  4.01 | ppl    55.14 | acc     0.57 | train_ae_norm     1.00\n",
      "[16/200][1199/4361] Loss_D: 0.75704408 (Loss_D_real: 0.36175537 Loss_D_fake: 0.39528868) Loss_G: 0.06566103 Loss_Enh_Dec: -0.18192072\n",
      "| epoch  16 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.73 | loss  4.03 | ppl    56.19 | acc     0.55 | train_ae_norm     1.00\n",
      "[16/200][1299/4361] Loss_D: 0.85896707 (Loss_D_real: 0.33256879 Loss_D_fake: 0.52639824) Loss_G: 0.05781776 Loss_Enh_Dec: -0.17040573\n",
      "| epoch  16 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.81 | loss  4.04 | ppl    56.85 | acc     0.55 | train_ae_norm     1.00\n",
      "[16/200][1399/4361] Loss_D: 0.79791230 (Loss_D_real: 0.35919720 Loss_D_fake: 0.43871510) Loss_G: 0.06106174 Loss_Enh_Dec: -0.23327057\n",
      "| epoch  16 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  4.04 | ppl    57.00 | acc     0.49 | train_ae_norm     1.00\n",
      "[16/200][1499/4361] Loss_D: 0.83518308 (Loss_D_real: 0.42174715 Loss_D_fake: 0.41343594) Loss_G: 0.05840578 Loss_Enh_Dec: -0.18558496\n",
      "| epoch  16 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  4.07 | ppl    58.47 | acc     0.54 | train_ae_norm     1.00\n",
      "[16/200][1599/4361] Loss_D: 0.78830558 (Loss_D_real: 0.38175881 Loss_D_fake: 0.40654677) Loss_G: 0.07157771 Loss_Enh_Dec: -0.21905439\n",
      "| epoch  16 |  1600/ 4361 batches | lr 0.000000 | ms/batch 400.85 | loss  4.03 | ppl    56.36 | acc     0.56 | train_ae_norm     1.00\n",
      "[16/200][1699/4361] Loss_D: 0.69015801 (Loss_D_real: 0.43878672 Loss_D_fake: 0.25137126) Loss_G: 0.04263261 Loss_Enh_Dec: -0.19945207\n",
      "| epoch  16 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.72 | loss  4.03 | ppl    56.03 | acc     0.55 | train_ae_norm     1.00\n",
      "[16/200][1799/4361] Loss_D: 0.83296371 (Loss_D_real: 0.38727498 Loss_D_fake: 0.44568869) Loss_G: 0.04083402 Loss_Enh_Dec: -0.20215981\n",
      "| epoch  16 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  4.01 | ppl    54.94 | acc     0.55 | train_ae_norm     1.00\n",
      "[16/200][1899/4361] Loss_D: 0.94253075 (Loss_D_real: 0.45753908 Loss_D_fake: 0.48499164) Loss_G: 0.03308386 Loss_Enh_Dec: -0.16622773\n",
      "| epoch  16 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.05 | loss  4.07 | ppl    58.74 | acc     0.57 | train_ae_norm     1.00\n",
      "[16/200][1999/4361] Loss_D: 0.82851934 (Loss_D_real: 0.35029566 Loss_D_fake: 0.47822365) Loss_G: 0.04219525 Loss_Enh_Dec: -0.16372131\n",
      "| epoch  16 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  4.00 | ppl    54.60 | acc     0.57 | train_ae_norm     1.00\n",
      "[16/200][2099/4361] Loss_D: 0.71415848 (Loss_D_real: 0.35484537 Loss_D_fake: 0.35931310) Loss_G: 0.05073029 Loss_Enh_Dec: -0.13734615\n",
      "| epoch  16 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  4.03 | ppl    56.38 | acc     0.57 | train_ae_norm     1.00\n",
      "[16/200][2199/4361] Loss_D: 0.90061092 (Loss_D_real: 0.37739247 Loss_D_fake: 0.52321845) Loss_G: 0.05433735 Loss_Enh_Dec: -0.19660258\n",
      "| epoch  16 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  4.01 | ppl    55.18 | acc     0.55 | train_ae_norm     1.00\n",
      "[16/200][2299/4361] Loss_D: 0.68597364 (Loss_D_real: 0.33745754 Loss_D_fake: 0.34851608) Loss_G: 0.04779020 Loss_Enh_Dec: -0.19565184\n",
      "| epoch  16 |  2300/ 4361 batches | lr 0.000000 | ms/batch 402.05 | loss  4.01 | ppl    55.21 | acc     0.58 | train_ae_norm     1.00\n",
      "[16/200][2399/4361] Loss_D: 0.87748730 (Loss_D_real: 0.45431697 Loss_D_fake: 0.42317030) Loss_G: 0.05601140 Loss_Enh_Dec: -0.17714752\n",
      "| epoch  16 |  2400/ 4361 batches | lr 0.000000 | ms/batch 400.73 | loss  4.02 | ppl    55.65 | acc     0.53 | train_ae_norm     1.00\n",
      "[16/200][2499/4361] Loss_D: 0.65223050 (Loss_D_real: 0.29970306 Loss_D_fake: 0.35252744) Loss_G: 0.03745179 Loss_Enh_Dec: -0.21045032\n",
      "| epoch  16 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  4.08 | ppl    58.95 | acc     0.51 | train_ae_norm     1.00\n",
      "[16/200][2599/4361] Loss_D: 0.74023765 (Loss_D_real: 0.38939333 Loss_D_fake: 0.35084432) Loss_G: 0.07060786 Loss_Enh_Dec: -0.28043404\n",
      "| epoch  16 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  4.03 | ppl    56.02 | acc     0.54 | train_ae_norm     1.00\n",
      "[16/200][2699/4361] Loss_D: 0.71631473 (Loss_D_real: 0.33717009 Loss_D_fake: 0.37914464) Loss_G: 0.04604730 Loss_Enh_Dec: -0.24015243\n",
      "| epoch  16 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  4.05 | ppl    57.18 | acc     0.56 | train_ae_norm     1.00\n",
      "[16/200][2799/4361] Loss_D: 0.62784272 (Loss_D_real: 0.25092730 Loss_D_fake: 0.37691543) Loss_G: 0.03879499 Loss_Enh_Dec: -0.29588190\n",
      "| epoch  16 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  3.99 | ppl    54.02 | acc     0.53 | train_ae_norm     1.00\n",
      "[16/200][2899/4361] Loss_D: 1.04422069 (Loss_D_real: 0.55453312 Loss_D_fake: 0.48968756) Loss_G: 0.05305501 Loss_Enh_Dec: -0.26247612\n",
      "| epoch  16 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  4.03 | ppl    56.53 | acc     0.56 | train_ae_norm     1.00\n",
      "[16/200][2999/4361] Loss_D: 0.81402725 (Loss_D_real: 0.39478600 Loss_D_fake: 0.41924125) Loss_G: 0.06107137 Loss_Enh_Dec: -0.23615932\n",
      "| epoch  16 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  4.05 | ppl    57.36 | acc     0.58 | train_ae_norm     1.00\n",
      "[16/200][3099/4361] Loss_D: 0.94550216 (Loss_D_real: 0.46023878 Loss_D_fake: 0.48526341) Loss_G: 0.05270303 Loss_Enh_Dec: -0.24967967\n",
      "| epoch  16 |  3100/ 4361 batches | lr 0.000000 | ms/batch 402.00 | loss  4.08 | ppl    59.27 | acc     0.52 | train_ae_norm     1.00\n",
      "[16/200][3199/4361] Loss_D: 0.81422669 (Loss_D_real: 0.48778686 Loss_D_fake: 0.32643983) Loss_G: 0.04987306 Loss_Enh_Dec: -0.26844233\n",
      "| epoch  16 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  4.10 | ppl    60.32 | acc     0.55 | train_ae_norm     1.00\n",
      "[16/200][3299/4361] Loss_D: 0.80426306 (Loss_D_real: 0.38529763 Loss_D_fake: 0.41896543) Loss_G: 0.06619295 Loss_Enh_Dec: -0.28750554\n",
      "| epoch  16 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  4.09 | ppl    59.69 | acc     0.52 | train_ae_norm     1.00\n",
      "[16/200][3399/4361] Loss_D: 0.84820926 (Loss_D_real: 0.43823230 Loss_D_fake: 0.40997696) Loss_G: 0.03827338 Loss_Enh_Dec: -0.30291286\n",
      "| epoch  16 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  4.08 | ppl    59.28 | acc     0.58 | train_ae_norm     1.00\n",
      "[16/200][3499/4361] Loss_D: 0.73243642 (Loss_D_real: 0.31302959 Loss_D_fake: 0.41940686) Loss_G: 0.05752384 Loss_Enh_Dec: -0.24798539\n",
      "| epoch  16 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  4.04 | ppl    56.94 | acc     0.56 | train_ae_norm     1.00\n",
      "[16/200][3599/4361] Loss_D: 0.87978971 (Loss_D_real: 0.51312411 Loss_D_fake: 0.36666557) Loss_G: 0.02843806 Loss_Enh_Dec: -0.28719652\n",
      "| epoch  16 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  4.08 | ppl    59.13 | acc     0.54 | train_ae_norm     1.00\n",
      "[16/200][3699/4361] Loss_D: 0.77455878 (Loss_D_real: 0.42231783 Loss_D_fake: 0.35224098) Loss_G: 0.06128521 Loss_Enh_Dec: -0.25203997\n",
      "| epoch  16 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.77 | loss  4.09 | ppl    59.86 | acc     0.53 | train_ae_norm     1.00\n",
      "[16/200][3799/4361] Loss_D: 0.75918311 (Loss_D_real: 0.43611321 Loss_D_fake: 0.32306990) Loss_G: 0.04394012 Loss_Enh_Dec: -0.30068737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  16 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  4.11 | ppl    60.99 | acc     0.58 | train_ae_norm     1.00\n",
      "[16/200][3899/4361] Loss_D: 0.80006987 (Loss_D_real: 0.36507842 Loss_D_fake: 0.43499145) Loss_G: 0.02455233 Loss_Enh_Dec: -0.27065474\n",
      "| epoch  16 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  4.11 | ppl    61.05 | acc     0.50 | train_ae_norm     1.00\n",
      "[16/200][3999/4361] Loss_D: 0.83311975 (Loss_D_real: 0.46873444 Loss_D_fake: 0.36438534) Loss_G: 0.05074907 Loss_Enh_Dec: -0.39219609\n",
      "| epoch  16 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  4.15 | ppl    63.21 | acc     0.54 | train_ae_norm     1.00\n",
      "[16/200][4099/4361] Loss_D: 0.91962409 (Loss_D_real: 0.42365450 Loss_D_fake: 0.49596962) Loss_G: 0.07587512 Loss_Enh_Dec: -0.31916329\n",
      "| epoch  16 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  4.10 | ppl    60.16 | acc     0.56 | train_ae_norm     1.00\n",
      "[16/200][4199/4361] Loss_D: 0.89764655 (Loss_D_real: 0.48141962 Loss_D_fake: 0.41622692) Loss_G: 0.03507099 Loss_Enh_Dec: -0.30573776\n",
      "| epoch  16 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  4.11 | ppl    61.16 | acc     0.56 | train_ae_norm     1.00\n",
      "[16/200][4299/4361] Loss_D: 0.98890436 (Loss_D_real: 0.55425543 Loss_D_fake: 0.43464896) Loss_G: 0.03866841 Loss_Enh_Dec: -0.34981605\n",
      "| epoch  16 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  4.07 | ppl    58.28 | acc     0.55 | train_ae_norm     1.00\n",
      "| end of epoch  16 | time: 1852.20s | test loss  3.81 | test ppl 45.10 | acc 0.617\n",
      "bleu_self:  [2.55357143e-01 5.75509118e-09 1.76010232e-11 1.04999609e-12\n",
      " 2.13236614e-13]\n",
      "bleu_test:  [8.23660714e-01 4.81354971e-01 1.70839736e-01 2.67466069e-05\n",
      " 1.53132344e-07]\n",
      "bleu_self: [0.25535714,0.00000001,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.82366071,0.48135497,0.17083974,0.00002675,0.00000015]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 17 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.755\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.455\n",
      "  Test Loss: 2.757\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  17 |     0/ 4361 batches | lr 0.000000 | ms/batch 863.24 | loss  0.04 | ppl     1.04 | acc     0.58 | train_ae_norm     1.00\n",
      "[17/200][99/4361] Loss_D: 0.97955263 (Loss_D_real: 0.52885073 Loss_D_fake: 0.45070189) Loss_G: 0.05927336 Loss_Enh_Dec: -0.22556150\n",
      "| epoch  17 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  4.10 | ppl    60.60 | acc     0.54 | train_ae_norm     1.00\n",
      "[17/200][199/4361] Loss_D: 0.92137516 (Loss_D_real: 0.45439380 Loss_D_fake: 0.46698135) Loss_G: 0.06070929 Loss_Enh_Dec: -0.32234788\n",
      "| epoch  17 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  4.13 | ppl    61.96 | acc     0.56 | train_ae_norm     1.00\n",
      "[17/200][299/4361] Loss_D: 0.91654724 (Loss_D_real: 0.50423646 Loss_D_fake: 0.41231078) Loss_G: 0.03732881 Loss_Enh_Dec: -0.31594706\n",
      "| epoch  17 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.94 | loss  4.14 | ppl    62.87 | acc     0.50 | train_ae_norm     1.00\n",
      "[17/200][399/4361] Loss_D: 0.76338923 (Loss_D_real: 0.37850398 Loss_D_fake: 0.38488525) Loss_G: 0.03614932 Loss_Enh_Dec: -0.36628261\n",
      "| epoch  17 |   400/ 4361 batches | lr 0.000000 | ms/batch 402.15 | loss  4.07 | ppl    58.69 | acc     0.56 | train_ae_norm     1.00\n",
      "[17/200][499/4361] Loss_D: 0.83959579 (Loss_D_real: 0.53034192 Loss_D_fake: 0.30925384) Loss_G: 0.05207788 Loss_Enh_Dec: -0.24621275\n",
      "| epoch  17 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  4.14 | ppl    63.00 | acc     0.57 | train_ae_norm     1.00\n",
      "[17/200][599/4361] Loss_D: 0.94340241 (Loss_D_real: 0.46669230 Loss_D_fake: 0.47671008) Loss_G: 0.05670003 Loss_Enh_Dec: -0.21618715\n",
      "| epoch  17 |   600/ 4361 batches | lr 0.000000 | ms/batch 402.04 | loss  4.06 | ppl    57.76 | acc     0.51 | train_ae_norm     1.00\n",
      "[17/200][699/4361] Loss_D: 1.22247899 (Loss_D_real: 0.80465239 Loss_D_fake: 0.41782662) Loss_G: 0.02847560 Loss_Enh_Dec: -0.32424438\n",
      "| epoch  17 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  4.14 | ppl    62.88 | acc     0.54 | train_ae_norm     1.00\n",
      "[17/200][799/4361] Loss_D: 0.75194222 (Loss_D_real: 0.29601485 Loss_D_fake: 0.45592737) Loss_G: 0.06101744 Loss_Enh_Dec: -0.32343945\n",
      "| epoch  17 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.75 | loss  4.12 | ppl    61.57 | acc     0.53 | train_ae_norm     1.00\n",
      "[17/200][899/4361] Loss_D: 0.83236623 (Loss_D_real: 0.46429658 Loss_D_fake: 0.36806965) Loss_G: 0.02129539 Loss_Enh_Dec: -0.30074856\n",
      "| epoch  17 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  4.13 | ppl    62.27 | acc     0.57 | train_ae_norm     1.00\n",
      "[17/200][999/4361] Loss_D: 0.94610089 (Loss_D_real: 0.52798402 Loss_D_fake: 0.41811687) Loss_G: 0.04362108 Loss_Enh_Dec: -0.38187405\n",
      "| epoch  17 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  4.11 | ppl    60.76 | acc     0.55 | train_ae_norm     1.00\n",
      "[17/200][1099/4361] Loss_D: 0.97590053 (Loss_D_real: 0.46816429 Loss_D_fake: 0.50773621) Loss_G: 0.02922813 Loss_Enh_Dec: -0.29775196\n",
      "| epoch  17 |  1100/ 4361 batches | lr 0.000000 | ms/batch 402.04 | loss  4.09 | ppl    59.81 | acc     0.51 | train_ae_norm     1.00\n",
      "[17/200][1199/4361] Loss_D: 0.83802789 (Loss_D_real: 0.46812177 Loss_D_fake: 0.36990613) Loss_G: 0.01582260 Loss_Enh_Dec: -0.27014518\n",
      "| epoch  17 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  4.15 | ppl    63.70 | acc     0.54 | train_ae_norm     1.00\n",
      "[17/200][1299/4361] Loss_D: 0.89508218 (Loss_D_real: 0.47760767 Loss_D_fake: 0.41747451) Loss_G: 0.02594662 Loss_Enh_Dec: -0.35623798\n",
      "| epoch  17 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  4.18 | ppl    65.10 | acc     0.52 | train_ae_norm     1.00\n",
      "[17/200][1399/4361] Loss_D: 1.00651157 (Loss_D_real: 0.56814349 Loss_D_fake: 0.43836808) Loss_G: 0.01683721 Loss_Enh_Dec: -0.36539182\n",
      "| epoch  17 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  4.14 | ppl    62.64 | acc     0.48 | train_ae_norm     1.00\n",
      "[17/200][1499/4361] Loss_D: 0.72379172 (Loss_D_real: 0.41122374 Loss_D_fake: 0.31256801) Loss_G: 0.07241198 Loss_Enh_Dec: -0.27516133\n",
      "| epoch  17 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  4.18 | ppl    65.15 | acc     0.55 | train_ae_norm     1.00\n",
      "[17/200][1599/4361] Loss_D: 0.80732292 (Loss_D_real: 0.43483180 Loss_D_fake: 0.37249112) Loss_G: 0.02814424 Loss_Enh_Dec: -0.36451027\n",
      "| epoch  17 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  4.15 | ppl    63.63 | acc     0.52 | train_ae_norm     1.00\n",
      "[17/200][1699/4361] Loss_D: 0.95270967 (Loss_D_real: 0.40322757 Loss_D_fake: 0.54948211) Loss_G: 0.04781541 Loss_Enh_Dec: -0.27374068\n",
      "| epoch  17 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  4.14 | ppl    62.66 | acc     0.50 | train_ae_norm     1.00\n",
      "[17/200][1799/4361] Loss_D: 0.64211857 (Loss_D_real: 0.30233955 Loss_D_fake: 0.33977899) Loss_G: 0.04691103 Loss_Enh_Dec: -0.26582056\n",
      "| epoch  17 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  4.12 | ppl    61.60 | acc     0.50 | train_ae_norm     1.00\n",
      "[17/200][1899/4361] Loss_D: 0.91758811 (Loss_D_real: 0.51543188 Loss_D_fake: 0.40215623) Loss_G: 0.04310488 Loss_Enh_Dec: -0.40010405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  17 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  4.20 | ppl    66.53 | acc     0.54 | train_ae_norm     1.00\n",
      "[17/200][1999/4361] Loss_D: 0.84532320 (Loss_D_real: 0.37902302 Loss_D_fake: 0.46630022) Loss_G: 0.06497040 Loss_Enh_Dec: -0.33408451\n",
      "| epoch  17 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  4.13 | ppl    62.36 | acc     0.53 | train_ae_norm     1.00\n",
      "[17/200][2099/4361] Loss_D: 0.82758081 (Loss_D_real: 0.33471105 Loss_D_fake: 0.49286973) Loss_G: 0.06372762 Loss_Enh_Dec: -0.36743590\n",
      "| epoch  17 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  4.17 | ppl    64.72 | acc     0.53 | train_ae_norm     1.00\n",
      "[17/200][2199/4361] Loss_D: 0.87307906 (Loss_D_real: 0.42231888 Loss_D_fake: 0.45076022) Loss_G: 0.03989534 Loss_Enh_Dec: -0.38275358\n",
      "| epoch  17 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  4.18 | ppl    65.13 | acc     0.51 | train_ae_norm     1.00\n",
      "[17/200][2299/4361] Loss_D: 0.91037607 (Loss_D_real: 0.46811503 Loss_D_fake: 0.44226107) Loss_G: 0.01910884 Loss_Enh_Dec: -0.37945578\n",
      "| epoch  17 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.51 | loss  4.15 | ppl    63.22 | acc     0.56 | train_ae_norm     1.00\n",
      "[17/200][2399/4361] Loss_D: 0.85630989 (Loss_D_real: 0.44468197 Loss_D_fake: 0.41162795) Loss_G: 0.03821162 Loss_Enh_Dec: -0.32809672\n",
      "| epoch  17 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  4.16 | ppl    63.82 | acc     0.50 | train_ae_norm     1.00\n",
      "[17/200][2499/4361] Loss_D: 0.97769266 (Loss_D_real: 0.46591514 Loss_D_fake: 0.51177752) Loss_G: 0.04226387 Loss_Enh_Dec: -0.30568102\n",
      "| epoch  17 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  4.19 | ppl    65.98 | acc     0.54 | train_ae_norm     1.00\n",
      "[17/200][2599/4361] Loss_D: 0.80991304 (Loss_D_real: 0.50448585 Loss_D_fake: 0.30542716) Loss_G: 0.05347630 Loss_Enh_Dec: -0.34553123\n",
      "| epoch  17 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  4.17 | ppl    65.02 | acc     0.54 | train_ae_norm     1.00\n",
      "[17/200][2699/4361] Loss_D: 0.79193646 (Loss_D_real: 0.40542358 Loss_D_fake: 0.38651288) Loss_G: 0.05379440 Loss_Enh_Dec: -0.35600033\n",
      "| epoch  17 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  4.15 | ppl    63.68 | acc     0.52 | train_ae_norm     1.00\n",
      "[17/200][2799/4361] Loss_D: 0.89588070 (Loss_D_real: 0.40778601 Loss_D_fake: 0.48809466) Loss_G: 0.03605061 Loss_Enh_Dec: -0.33165121\n",
      "| epoch  17 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  4.11 | ppl    60.99 | acc     0.53 | train_ae_norm     1.00\n",
      "[17/200][2899/4361] Loss_D: 0.96045399 (Loss_D_real: 0.47162744 Loss_D_fake: 0.48882651) Loss_G: 0.04281325 Loss_Enh_Dec: -0.37375247\n",
      "| epoch  17 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  4.15 | ppl    63.16 | acc     0.55 | train_ae_norm     1.00\n",
      "[17/200][2999/4361] Loss_D: 0.85066915 (Loss_D_real: 0.48605046 Loss_D_fake: 0.36461872) Loss_G: 0.04740361 Loss_Enh_Dec: -0.41280538\n",
      "| epoch  17 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  4.16 | ppl    64.31 | acc     0.55 | train_ae_norm     1.00\n",
      "[17/200][3099/4361] Loss_D: 0.63105971 (Loss_D_real: 0.36380714 Loss_D_fake: 0.26725256) Loss_G: 0.04495086 Loss_Enh_Dec: -0.35539061\n",
      "| epoch  17 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  4.19 | ppl    65.76 | acc     0.50 | train_ae_norm     1.00\n",
      "[17/200][3199/4361] Loss_D: 0.82667303 (Loss_D_real: 0.37304243 Loss_D_fake: 0.45363057) Loss_G: 0.03589788 Loss_Enh_Dec: -0.34864002\n",
      "| epoch  17 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  4.52 | ppl    91.89 | acc     0.50 | train_ae_norm     1.00\n",
      "[17/200][3299/4361] Loss_D: 0.73923284 (Loss_D_real: 0.44011837 Loss_D_fake: 0.29911447) Loss_G: 0.05246310 Loss_Enh_Dec: -0.38384143\n",
      "| epoch  17 |  3300/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  4.22 | ppl    67.89 | acc     0.49 | train_ae_norm     1.00\n",
      "[17/200][3399/4361] Loss_D: 1.01582003 (Loss_D_real: 0.46776730 Loss_D_fake: 0.54805267) Loss_G: 0.02706038 Loss_Enh_Dec: -0.27473411\n",
      "| epoch  17 |  3400/ 4361 batches | lr 0.000000 | ms/batch 402.21 | loss  4.23 | ppl    68.71 | acc     0.57 | train_ae_norm     1.00\n",
      "[17/200][3499/4361] Loss_D: 0.94455183 (Loss_D_real: 0.38992643 Loss_D_fake: 0.55462539) Loss_G: 0.03647348 Loss_Enh_Dec: -0.26178083\n",
      "| epoch  17 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  4.17 | ppl    64.84 | acc     0.54 | train_ae_norm     1.00\n",
      "[17/200][3599/4361] Loss_D: 0.88748348 (Loss_D_real: 0.49166557 Loss_D_fake: 0.39581794) Loss_G: 0.02798432 Loss_Enh_Dec: -0.24817534\n",
      "| epoch  17 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  4.23 | ppl    68.89 | acc     0.51 | train_ae_norm     1.00\n",
      "[17/200][3699/4361] Loss_D: 1.00486827 (Loss_D_real: 0.48253393 Loss_D_fake: 0.52233440) Loss_G: 0.01959508 Loss_Enh_Dec: -0.30595872\n",
      "| epoch  17 |  3700/ 4361 batches | lr 0.000000 | ms/batch 402.49 | loss  4.28 | ppl    72.22 | acc     0.48 | train_ae_norm     1.00\n",
      "[17/200][3799/4361] Loss_D: 0.81253541 (Loss_D_real: 0.43675038 Loss_D_fake: 0.37578505) Loss_G: 0.04888076 Loss_Enh_Dec: -0.32778129\n",
      "| epoch  17 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  4.25 | ppl    70.05 | acc     0.55 | train_ae_norm     1.00\n",
      "[17/200][3899/4361] Loss_D: 1.11572492 (Loss_D_real: 0.63376778 Loss_D_fake: 0.48195714) Loss_G: 0.01704853 Loss_Enh_Dec: -0.36479270\n",
      "| epoch  17 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  4.21 | ppl    67.28 | acc     0.50 | train_ae_norm     1.00\n",
      "[17/200][3999/4361] Loss_D: 1.03834939 (Loss_D_real: 0.40856239 Loss_D_fake: 0.62978697) Loss_G: 0.01578709 Loss_Enh_Dec: -0.36398059\n",
      "| epoch  17 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  4.22 | ppl    67.99 | acc     0.51 | train_ae_norm     1.00\n",
      "[17/200][4099/4361] Loss_D: 0.95648253 (Loss_D_real: 0.59210610 Loss_D_fake: 0.36437640) Loss_G: -0.00583946 Loss_Enh_Dec: -0.27403226\n",
      "| epoch  17 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  4.19 | ppl    66.03 | acc     0.53 | train_ae_norm     1.00\n",
      "[17/200][4199/4361] Loss_D: 0.95465064 (Loss_D_real: 0.42848977 Loss_D_fake: 0.52616090) Loss_G: 0.03811119 Loss_Enh_Dec: -0.32102329\n",
      "| epoch  17 |  4200/ 4361 batches | lr 0.000000 | ms/batch 402.48 | loss  4.26 | ppl    70.74 | acc     0.53 | train_ae_norm     1.00\n",
      "[17/200][4299/4361] Loss_D: 0.87063205 (Loss_D_real: 0.47087353 Loss_D_fake: 0.39975852) Loss_G: 0.04473665 Loss_Enh_Dec: -0.42509088\n",
      "| epoch  17 |  4300/ 4361 batches | lr 0.000000 | ms/batch 402.06 | loss  4.20 | ppl    66.41 | acc     0.54 | train_ae_norm     1.00\n",
      "| end of epoch  17 | time: 1853.39s | test loss  3.87 | test ppl 47.83 | acc 0.600\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 18 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.750\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.468\n",
      "  Test Loss: 2.827\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  18 |     0/ 4361 batches | lr 0.000000 | ms/batch 864.43 | loss  0.04 | ppl     1.04 | acc     0.54 | train_ae_norm     1.00\n",
      "[18/200][99/4361] Loss_D: 0.89298153 (Loss_D_real: 0.44341159 Loss_D_fake: 0.44956994) Loss_G: 0.03555996 Loss_Enh_Dec: -0.36013809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  18 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.24 | loss  4.23 | ppl    68.68 | acc     0.52 | train_ae_norm     1.00\n",
      "[18/200][199/4361] Loss_D: 0.95635909 (Loss_D_real: 0.45292979 Loss_D_fake: 0.50342929) Loss_G: 0.02215381 Loss_Enh_Dec: -0.36123598\n",
      "| epoch  18 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  4.25 | ppl    70.03 | acc     0.54 | train_ae_norm     1.00\n",
      "[18/200][299/4361] Loss_D: 0.84468162 (Loss_D_real: 0.45660990 Loss_D_fake: 0.38807175) Loss_G: 0.04758188 Loss_Enh_Dec: -0.28572613\n",
      "| epoch  18 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  4.24 | ppl    69.22 | acc     0.47 | train_ae_norm     1.00\n",
      "[18/200][399/4361] Loss_D: 0.91565228 (Loss_D_real: 0.47266078 Loss_D_fake: 0.44299150) Loss_G: 0.04620230 Loss_Enh_Dec: -0.40849733\n",
      "| epoch  18 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  4.17 | ppl    64.56 | acc     0.53 | train_ae_norm     1.00\n",
      "[18/200][499/4361] Loss_D: 0.89667463 (Loss_D_real: 0.49183846 Loss_D_fake: 0.40483618) Loss_G: 0.03323422 Loss_Enh_Dec: -0.35903499\n",
      "| epoch  18 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  4.24 | ppl    69.12 | acc     0.53 | train_ae_norm     1.00\n",
      "[18/200][599/4361] Loss_D: 0.88679719 (Loss_D_real: 0.34916055 Loss_D_fake: 0.53763664) Loss_G: 0.02462280 Loss_Enh_Dec: -0.31169841\n",
      "| epoch  18 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  4.19 | ppl    66.12 | acc     0.48 | train_ae_norm     1.00\n",
      "[18/200][699/4361] Loss_D: 1.24849999 (Loss_D_real: 0.72820115 Loss_D_fake: 0.52029884) Loss_G: 0.05022941 Loss_Enh_Dec: -0.28609920\n",
      "| epoch  18 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  4.24 | ppl    69.45 | acc     0.53 | train_ae_norm     1.00\n",
      "[18/200][799/4361] Loss_D: 0.79162931 (Loss_D_real: 0.34721988 Loss_D_fake: 0.44440946) Loss_G: 0.01658302 Loss_Enh_Dec: -0.32552898\n",
      "| epoch  18 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  4.19 | ppl    66.11 | acc     0.52 | train_ae_norm     1.00\n",
      "[18/200][899/4361] Loss_D: 1.01803315 (Loss_D_real: 0.65250850 Loss_D_fake: 0.36552462) Loss_G: 0.01223045 Loss_Enh_Dec: -0.40693855\n",
      "| epoch  18 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  4.24 | ppl    69.56 | acc     0.56 | train_ae_norm     1.00\n",
      "[18/200][999/4361] Loss_D: 0.97771382 (Loss_D_real: 0.44568360 Loss_D_fake: 0.53203022) Loss_G: 0.03624377 Loss_Enh_Dec: -0.47047272\n",
      "| epoch  18 |  1000/ 4361 batches | lr 0.000000 | ms/batch 400.51 | loss  4.21 | ppl    67.69 | acc     0.51 | train_ae_norm     1.00\n",
      "[18/200][1099/4361] Loss_D: 0.93060994 (Loss_D_real: 0.48112595 Loss_D_fake: 0.44948396) Loss_G: 0.03474201 Loss_Enh_Dec: -0.36663476\n",
      "| epoch  18 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  4.23 | ppl    68.65 | acc     0.49 | train_ae_norm     1.00\n",
      "[18/200][1199/4361] Loss_D: 0.79369724 (Loss_D_real: 0.36278459 Loss_D_fake: 0.43091261) Loss_G: 0.02956733 Loss_Enh_Dec: -0.38983151\n",
      "| epoch  18 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  4.24 | ppl    69.13 | acc     0.54 | train_ae_norm     1.00\n",
      "[18/200][1299/4361] Loss_D: 0.92448956 (Loss_D_real: 0.40152955 Loss_D_fake: 0.52296001) Loss_G: 0.02706588 Loss_Enh_Dec: -0.42632705\n",
      "| epoch  18 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.81 | loss  4.24 | ppl    69.34 | acc     0.50 | train_ae_norm     1.00\n",
      "[18/200][1399/4361] Loss_D: 0.79200554 (Loss_D_real: 0.39985943 Loss_D_fake: 0.39214608) Loss_G: 0.04272632 Loss_Enh_Dec: -0.38561556\n",
      "| epoch  18 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  4.25 | ppl    70.15 | acc     0.45 | train_ae_norm     1.00\n",
      "[18/200][1499/4361] Loss_D: 0.96012020 (Loss_D_real: 0.46135661 Loss_D_fake: 0.49876356) Loss_G: 0.03159403 Loss_Enh_Dec: -0.50686973\n",
      "| epoch  18 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  4.29 | ppl    72.99 | acc     0.45 | train_ae_norm     1.00\n",
      "[18/200][1599/4361] Loss_D: 0.84949398 (Loss_D_real: 0.44819623 Loss_D_fake: 0.40129775) Loss_G: -0.01455349 Loss_Enh_Dec: -0.35702923\n",
      "| epoch  18 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  4.24 | ppl    69.55 | acc     0.53 | train_ae_norm     1.00\n",
      "[18/200][1699/4361] Loss_D: 1.11493683 (Loss_D_real: 0.53206885 Loss_D_fake: 0.58286798) Loss_G: 0.00979542 Loss_Enh_Dec: -0.45566329\n",
      "| epoch  18 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.74 | loss  4.22 | ppl    67.98 | acc     0.51 | train_ae_norm     1.00\n",
      "[18/200][1799/4361] Loss_D: 0.71651632 (Loss_D_real: 0.31841606 Loss_D_fake: 0.39810026) Loss_G: 0.05080013 Loss_Enh_Dec: -0.41460305\n",
      "| epoch  18 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  4.23 | ppl    68.88 | acc     0.52 | train_ae_norm     1.00\n",
      "[18/200][1899/4361] Loss_D: 0.98166478 (Loss_D_real: 0.51339519 Loss_D_fake: 0.46826956) Loss_G: 0.03449100 Loss_Enh_Dec: -0.36262354\n",
      "| epoch  18 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  4.30 | ppl    73.80 | acc     0.51 | train_ae_norm     1.00\n",
      "[18/200][1999/4361] Loss_D: 1.03089654 (Loss_D_real: 0.49962124 Loss_D_fake: 0.53127527) Loss_G: 0.04017014 Loss_Enh_Dec: -0.35158604\n",
      "| epoch  18 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  4.24 | ppl    69.62 | acc     0.51 | train_ae_norm     1.00\n",
      "[18/200][2099/4361] Loss_D: 0.65919268 (Loss_D_real: 0.33762234 Loss_D_fake: 0.32157037) Loss_G: 0.07484310 Loss_Enh_Dec: -0.31462067\n",
      "| epoch  18 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  4.25 | ppl    70.25 | acc     0.54 | train_ae_norm     1.00\n",
      "[18/200][2199/4361] Loss_D: 1.02224612 (Loss_D_real: 0.41579193 Loss_D_fake: 0.60645413) Loss_G: 0.00199731 Loss_Enh_Dec: -0.31141582\n",
      "| epoch  18 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  4.24 | ppl    69.18 | acc     0.50 | train_ae_norm     1.00\n",
      "[18/200][2299/4361] Loss_D: 0.86194938 (Loss_D_real: 0.53471363 Loss_D_fake: 0.32723576) Loss_G: 0.03564370 Loss_Enh_Dec: -0.53588676\n",
      "| epoch  18 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  4.22 | ppl    68.23 | acc     0.53 | train_ae_norm     1.00\n",
      "[18/200][2399/4361] Loss_D: 0.97678977 (Loss_D_real: 0.37002599 Loss_D_fake: 0.60676378) Loss_G: 0.02491293 Loss_Enh_Dec: -0.58356035\n",
      "| epoch  18 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  4.24 | ppl    69.08 | acc     0.48 | train_ae_norm     1.00\n",
      "[18/200][2499/4361] Loss_D: 0.84974498 (Loss_D_real: 0.37470385 Loss_D_fake: 0.47504112) Loss_G: 0.03620516 Loss_Enh_Dec: -0.42162433\n",
      "| epoch  18 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  4.30 | ppl    73.49 | acc     0.52 | train_ae_norm     1.00\n",
      "[18/200][2599/4361] Loss_D: 0.72564405 (Loss_D_real: 0.35854906 Loss_D_fake: 0.36709499) Loss_G: 0.02505655 Loss_Enh_Dec: -0.36019897\n",
      "| epoch  18 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.80 | loss  4.27 | ppl    71.20 | acc     0.48 | train_ae_norm     1.00\n",
      "[18/200][2699/4361] Loss_D: 0.80866534 (Loss_D_real: 0.50658345 Loss_D_fake: 0.30208188) Loss_G: 0.05868788 Loss_Enh_Dec: -0.39356613\n",
      "| epoch  18 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  4.26 | ppl    70.93 | acc     0.51 | train_ae_norm     1.00\n",
      "[18/200][2799/4361] Loss_D: 0.49036947 (Loss_D_real: 0.26208967 Loss_D_fake: 0.22827980) Loss_G: 0.09196352 Loss_Enh_Dec: -0.49364892\n",
      "| epoch  18 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  4.22 | ppl    67.89 | acc     0.51 | train_ae_norm     1.00\n",
      "[18/200][2899/4361] Loss_D: 0.65633345 (Loss_D_real: 0.34483266 Loss_D_fake: 0.31150076) Loss_G: 0.09266042 Loss_Enh_Dec: -0.34898251\n",
      "| epoch  18 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  4.31 | ppl    74.36 | acc     0.53 | train_ae_norm     1.00\n",
      "[18/200][2999/4361] Loss_D: 0.53858984 (Loss_D_real: 0.28639537 Loss_D_fake: 0.25219449) Loss_G: 0.05989480 Loss_Enh_Dec: -0.36425111\n",
      "| epoch  18 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  4.24 | ppl    69.63 | acc     0.52 | train_ae_norm     1.00\n",
      "[18/200][3099/4361] Loss_D: 0.53245449 (Loss_D_real: 0.28352988 Loss_D_fake: 0.24892464) Loss_G: 0.07917935 Loss_Enh_Dec: -0.17817424\n",
      "| epoch  18 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  4.27 | ppl    71.82 | acc     0.47 | train_ae_norm     1.00\n",
      "[18/200][3199/4361] Loss_D: 0.78658980 (Loss_D_real: 0.47082967 Loss_D_fake: 0.31576014) Loss_G: 0.07793125 Loss_Enh_Dec: -0.24849482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  18 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  4.37 | ppl    79.40 | acc     0.51 | train_ae_norm     1.00\n",
      "[18/200][3299/4361] Loss_D: 0.86206424 (Loss_D_real: 0.40942606 Loss_D_fake: 0.45263821) Loss_G: 0.05900223 Loss_Enh_Dec: -0.23660672\n",
      "| epoch  18 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  4.34 | ppl    76.54 | acc     0.51 | train_ae_norm     1.00\n",
      "[18/200][3399/4361] Loss_D: 0.92543077 (Loss_D_real: 0.48016295 Loss_D_fake: 0.44526786) Loss_G: 0.07495121 Loss_Enh_Dec: -0.26894233\n",
      "| epoch  18 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  4.29 | ppl    72.96 | acc     0.50 | train_ae_norm     1.00\n",
      "[18/200][3499/4361] Loss_D: 0.83856481 (Loss_D_real: 0.41422269 Loss_D_fake: 0.42434213) Loss_G: 0.06992338 Loss_Enh_Dec: -0.31372935\n",
      "| epoch  18 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  4.23 | ppl    68.91 | acc     0.48 | train_ae_norm     1.00\n",
      "[18/200][3599/4361] Loss_D: 0.68179119 (Loss_D_real: 0.27147350 Loss_D_fake: 0.41031766) Loss_G: 0.07100802 Loss_Enh_Dec: -0.34591064\n",
      "| epoch  18 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  4.34 | ppl    76.98 | acc     0.50 | train_ae_norm     1.00\n",
      "[18/200][3699/4361] Loss_D: 0.97013867 (Loss_D_real: 0.56944859 Loss_D_fake: 0.40069005) Loss_G: 0.05085520 Loss_Enh_Dec: -0.39042923\n",
      "| epoch  18 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.08 | loss  4.28 | ppl    72.52 | acc     0.49 | train_ae_norm     1.00\n",
      "[18/200][3799/4361] Loss_D: 0.90819550 (Loss_D_real: 0.53111398 Loss_D_fake: 0.37708148) Loss_G: 0.06723534 Loss_Enh_Dec: -0.34778768\n",
      "| epoch  18 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  4.26 | ppl    71.05 | acc     0.55 | train_ae_norm     1.00\n",
      "[18/200][3899/4361] Loss_D: 0.66112852 (Loss_D_real: 0.29155457 Loss_D_fake: 0.36957392) Loss_G: 0.05200063 Loss_Enh_Dec: -0.46937200\n",
      "| epoch  18 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  4.27 | ppl    71.42 | acc     0.45 | train_ae_norm     1.00\n",
      "[18/200][3999/4361] Loss_D: 0.80281067 (Loss_D_real: 0.41393968 Loss_D_fake: 0.38887095) Loss_G: 0.06637966 Loss_Enh_Dec: -0.36794278\n",
      "| epoch  18 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  4.29 | ppl    72.88 | acc     0.50 | train_ae_norm     1.00\n",
      "[18/200][4099/4361] Loss_D: 0.84845698 (Loss_D_real: 0.46426412 Loss_D_fake: 0.38419282) Loss_G: 0.06201880 Loss_Enh_Dec: -0.29344019\n",
      "| epoch  18 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  4.26 | ppl    70.47 | acc     0.48 | train_ae_norm     1.00\n",
      "[18/200][4199/4361] Loss_D: 0.89057118 (Loss_D_real: 0.60685802 Loss_D_fake: 0.28371316) Loss_G: 0.03251909 Loss_Enh_Dec: -0.37531218\n",
      "| epoch  18 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  4.27 | ppl    71.39 | acc     0.52 | train_ae_norm     1.00\n",
      "[18/200][4299/4361] Loss_D: 0.86023951 (Loss_D_real: 0.40877622 Loss_D_fake: 0.45146331) Loss_G: 0.06645589 Loss_Enh_Dec: -0.46558306\n",
      "| epoch  18 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  4.24 | ppl    69.39 | acc     0.50 | train_ae_norm     1.00\n",
      "| end of epoch  18 | time: 1852.83s | test loss  3.89 | test ppl 48.68 | acc 0.599\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 19 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.745\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.458\n",
      "  Test Loss: 2.939\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  19 |     0/ 4361 batches | lr 0.000000 | ms/batch 864.05 | loss  0.04 | ppl     1.04 | acc     0.55 | train_ae_norm     1.00\n",
      "[19/200][99/4361] Loss_D: 0.88422507 (Loss_D_real: 0.50906610 Loss_D_fake: 0.37515897) Loss_G: 0.02648189 Loss_Enh_Dec: -0.40822467\n",
      "| epoch  19 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.05 | loss  4.22 | ppl    68.21 | acc     0.49 | train_ae_norm     1.00\n",
      "[19/200][199/4361] Loss_D: 0.76335835 (Loss_D_real: 0.41499457 Loss_D_fake: 0.34836382) Loss_G: 0.04467681 Loss_Enh_Dec: -0.36822900\n",
      "| epoch  19 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  4.25 | ppl    70.17 | acc     0.53 | train_ae_norm     1.00\n",
      "[19/200][299/4361] Loss_D: 0.93977636 (Loss_D_real: 0.47759637 Loss_D_fake: 0.46217999) Loss_G: 0.04725025 Loss_Enh_Dec: -0.45881158\n",
      "| epoch  19 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  4.26 | ppl    70.88 | acc     0.46 | train_ae_norm     1.00\n",
      "[19/200][399/4361] Loss_D: 0.81617975 (Loss_D_real: 0.42239654 Loss_D_fake: 0.39378318) Loss_G: 0.02145082 Loss_Enh_Dec: -0.51816708\n",
      "| epoch  19 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  4.19 | ppl    65.78 | acc     0.53 | train_ae_norm     1.00\n",
      "[19/200][499/4361] Loss_D: 0.80122083 (Loss_D_real: 0.49920487 Loss_D_fake: 0.30201596) Loss_G: 0.02862476 Loss_Enh_Dec: -0.37162921\n",
      "| epoch  19 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  4.25 | ppl    70.25 | acc     0.55 | train_ae_norm     1.00\n",
      "[19/200][599/4361] Loss_D: 0.92194927 (Loss_D_real: 0.50806713 Loss_D_fake: 0.41388214) Loss_G: 0.04545273 Loss_Enh_Dec: -0.37222984\n",
      "| epoch  19 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.63 | loss  4.24 | ppl    69.43 | acc     0.47 | train_ae_norm     1.00\n",
      "[19/200][699/4361] Loss_D: 1.13744497 (Loss_D_real: 0.52895486 Loss_D_fake: 0.60849005) Loss_G: 0.04005107 Loss_Enh_Dec: -0.46470976\n",
      "| epoch  19 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.56 | loss  4.27 | ppl    71.47 | acc     0.56 | train_ae_norm     1.00\n",
      "[19/200][799/4361] Loss_D: 0.75266433 (Loss_D_real: 0.46672857 Loss_D_fake: 0.28593579) Loss_G: 0.03780399 Loss_Enh_Dec: -0.51634955\n",
      "| epoch  19 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.67 | loss  4.25 | ppl    70.10 | acc     0.52 | train_ae_norm     1.00\n",
      "[19/200][899/4361] Loss_D: 0.85537696 (Loss_D_real: 0.32934019 Loss_D_fake: 0.52603674) Loss_G: 0.03623705 Loss_Enh_Dec: -0.48098555\n",
      "| epoch  19 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.23 | loss  4.27 | ppl    71.54 | acc     0.56 | train_ae_norm     1.00\n",
      "[19/200][999/4361] Loss_D: 0.66898799 (Loss_D_real: 0.31983259 Loss_D_fake: 0.34915543) Loss_G: 0.05490671 Loss_Enh_Dec: -0.44857532\n",
      "| epoch  19 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  4.25 | ppl    70.34 | acc     0.52 | train_ae_norm     1.00\n",
      "[19/200][1099/4361] Loss_D: 0.87615269 (Loss_D_real: 0.34504688 Loss_D_fake: 0.53110582) Loss_G: 0.06083107 Loss_Enh_Dec: -0.42412186\n",
      "| epoch  19 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.40 | loss  4.25 | ppl    69.78 | acc     0.47 | train_ae_norm     1.00\n",
      "[19/200][1199/4361] Loss_D: 0.79357076 (Loss_D_real: 0.30042416 Loss_D_fake: 0.49314660) Loss_G: 0.05088492 Loss_Enh_Dec: -0.39734733\n",
      "| epoch  19 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  4.28 | ppl    72.36 | acc     0.52 | train_ae_norm     1.00\n",
      "[19/200][1299/4361] Loss_D: 0.58107603 (Loss_D_real: 0.32740068 Loss_D_fake: 0.25367534) Loss_G: 0.06734534 Loss_Enh_Dec: -0.40740463\n",
      "| epoch  19 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.91 | loss  4.28 | ppl    72.47 | acc     0.49 | train_ae_norm     1.00\n",
      "[19/200][1399/4361] Loss_D: 0.58297670 (Loss_D_real: 0.37761241 Loss_D_fake: 0.20536429) Loss_G: 0.07468462 Loss_Enh_Dec: -0.42278281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  19 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.06 | loss  4.27 | ppl    71.51 | acc     0.46 | train_ae_norm     1.00\n",
      "[19/200][1499/4361] Loss_D: 0.46455896 (Loss_D_real: 0.29737887 Loss_D_fake: 0.16718011) Loss_G: 0.10278070 Loss_Enh_Dec: -0.61277992\n",
      "| epoch  19 |  1500/ 4361 batches | lr 0.000000 | ms/batch 400.38 | loss  4.32 | ppl    75.07 | acc     0.46 | train_ae_norm     1.00\n",
      "[19/200][1599/4361] Loss_D: 0.58903193 (Loss_D_real: 0.22595975 Loss_D_fake: 0.36307216) Loss_G: 0.06719339 Loss_Enh_Dec: -0.56341243\n",
      "| epoch  19 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  4.30 | ppl    73.58 | acc     0.52 | train_ae_norm     1.00\n",
      "[19/200][1699/4361] Loss_D: 0.66558945 (Loss_D_real: 0.40470034 Loss_D_fake: 0.26088914) Loss_G: 0.06368359 Loss_Enh_Dec: -0.54492944\n",
      "| epoch  19 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  4.30 | ppl    74.06 | acc     0.47 | train_ae_norm     1.00\n",
      "[19/200][1799/4361] Loss_D: 0.46024108 (Loss_D_real: 0.33035135 Loss_D_fake: 0.12988971) Loss_G: 0.12052470 Loss_Enh_Dec: -0.35718313\n",
      "| epoch  19 |  1800/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  4.48 | ppl    88.47 | acc     0.41 | train_ae_norm     1.00\n",
      "[19/200][1899/4361] Loss_D: 0.80103320 (Loss_D_real: 0.38010043 Loss_D_fake: 0.42093277) Loss_G: 0.02748780 Loss_Enh_Dec: -0.35454473\n",
      "| epoch  19 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  4.36 | ppl    78.55 | acc     0.51 | train_ae_norm     1.00\n",
      "[19/200][1999/4361] Loss_D: 0.74358094 (Loss_D_real: 0.39214069 Loss_D_fake: 0.35144028) Loss_G: 0.04529459 Loss_Enh_Dec: -0.45945874\n",
      "| epoch  19 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  4.29 | ppl    72.90 | acc     0.51 | train_ae_norm     1.00\n",
      "[19/200][2099/4361] Loss_D: 1.04763603 (Loss_D_real: 0.56453288 Loss_D_fake: 0.48310313) Loss_G: 0.03732733 Loss_Enh_Dec: -0.40312108\n",
      "| epoch  19 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  4.28 | ppl    72.19 | acc     0.52 | train_ae_norm     1.00\n",
      "[19/200][2199/4361] Loss_D: 0.80584055 (Loss_D_real: 0.50052619 Loss_D_fake: 0.30531436) Loss_G: 0.07713329 Loss_Enh_Dec: -0.47322279\n",
      "| epoch  19 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  4.28 | ppl    72.36 | acc     0.52 | train_ae_norm     1.00\n",
      "[19/200][2299/4361] Loss_D: 0.81728512 (Loss_D_real: 0.45433101 Loss_D_fake: 0.36295411) Loss_G: 0.03517398 Loss_Enh_Dec: -0.44153857\n",
      "| epoch  19 |  2300/ 4361 batches | lr 0.000000 | ms/batch 402.00 | loss  4.27 | ppl    71.72 | acc     0.50 | train_ae_norm     1.00\n",
      "[19/200][2399/4361] Loss_D: 0.80654079 (Loss_D_real: 0.33024713 Loss_D_fake: 0.47629365) Loss_G: 0.04921741 Loss_Enh_Dec: -0.52257204\n",
      "| epoch  19 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  4.26 | ppl    70.55 | acc     0.48 | train_ae_norm     1.00\n",
      "[19/200][2499/4361] Loss_D: 0.97325176 (Loss_D_real: 0.52523589 Loss_D_fake: 0.44801587) Loss_G: 0.02438264 Loss_Enh_Dec: -0.33343506\n",
      "| epoch  19 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  4.30 | ppl    73.63 | acc     0.53 | train_ae_norm     1.00\n",
      "[19/200][2599/4361] Loss_D: 1.28146887 (Loss_D_real: 0.66224796 Loss_D_fake: 0.61922085) Loss_G: -0.00373621 Loss_Enh_Dec: -0.45384237\n",
      "| epoch  19 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  4.26 | ppl    70.71 | acc     0.51 | train_ae_norm     1.00\n",
      "[19/200][2699/4361] Loss_D: 1.06530094 (Loss_D_real: 0.50107026 Loss_D_fake: 0.56423062) Loss_G: 0.01588178 Loss_Enh_Dec: -0.30650279\n",
      "| epoch  19 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  4.24 | ppl    69.63 | acc     0.52 | train_ae_norm     1.00\n",
      "[19/200][2799/4361] Loss_D: 1.03194308 (Loss_D_real: 0.61356986 Loss_D_fake: 0.41837320) Loss_G: 0.03011145 Loss_Enh_Dec: -0.42395449\n",
      "| epoch  19 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  4.22 | ppl    68.04 | acc     0.53 | train_ae_norm     1.00\n",
      "[19/200][2899/4361] Loss_D: 1.08104873 (Loss_D_real: 0.58393109 Loss_D_fake: 0.49711761) Loss_G: 0.02049276 Loss_Enh_Dec: -0.40000278\n",
      "| epoch  19 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  4.26 | ppl    71.06 | acc     0.54 | train_ae_norm     1.00\n",
      "[19/200][2999/4361] Loss_D: 0.98540592 (Loss_D_real: 0.51842237 Loss_D_fake: 0.46698356) Loss_G: 0.03837905 Loss_Enh_Dec: -0.58924830\n",
      "| epoch  19 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  4.25 | ppl    70.14 | acc     0.49 | train_ae_norm     1.00\n",
      "[19/200][3099/4361] Loss_D: 1.00176597 (Loss_D_real: 0.56392467 Loss_D_fake: 0.43784130) Loss_G: 0.04116214 Loss_Enh_Dec: -0.47934833\n",
      "| epoch  19 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  4.27 | ppl    71.35 | acc     0.50 | train_ae_norm     1.00\n",
      "[19/200][3199/4361] Loss_D: 0.99657691 (Loss_D_real: 0.48702216 Loss_D_fake: 0.50955474) Loss_G: 0.03001280 Loss_Enh_Dec: -0.51549906\n",
      "| epoch  19 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.94 | loss  4.27 | ppl    71.83 | acc     0.54 | train_ae_norm     1.00\n",
      "[19/200][3299/4361] Loss_D: 1.05867207 (Loss_D_real: 0.63879204 Loss_D_fake: 0.41988003) Loss_G: 0.01319695 Loss_Enh_Dec: -0.41118890\n",
      "| epoch  19 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  4.29 | ppl    72.66 | acc     0.50 | train_ae_norm     1.00\n",
      "[19/200][3399/4361] Loss_D: 0.90531105 (Loss_D_real: 0.39831430 Loss_D_fake: 0.50699675) Loss_G: 0.03138290 Loss_Enh_Dec: -0.39295241\n",
      "| epoch  19 |  3400/ 4361 batches | lr 0.000000 | ms/batch 402.11 | loss  4.28 | ppl    71.95 | acc     0.48 | train_ae_norm     1.00\n",
      "[19/200][3499/4361] Loss_D: 1.03518200 (Loss_D_real: 0.47403407 Loss_D_fake: 0.56114793) Loss_G: 0.02339701 Loss_Enh_Dec: -0.44337168\n",
      "| epoch  19 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  4.21 | ppl    67.59 | acc     0.52 | train_ae_norm     1.00\n",
      "[19/200][3599/4361] Loss_D: 0.96259266 (Loss_D_real: 0.53014570 Loss_D_fake: 0.43244696) Loss_G: 0.02335495 Loss_Enh_Dec: -0.32098609\n",
      "| epoch  19 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  4.23 | ppl    68.84 | acc     0.51 | train_ae_norm     1.00\n",
      "[19/200][3699/4361] Loss_D: 0.83775753 (Loss_D_real: 0.37870413 Loss_D_fake: 0.45905340) Loss_G: 0.02729683 Loss_Enh_Dec: -0.48109356\n",
      "| epoch  19 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  4.29 | ppl    73.04 | acc     0.49 | train_ae_norm     1.00\n",
      "[19/200][3799/4361] Loss_D: 0.95942903 (Loss_D_real: 0.42047176 Loss_D_fake: 0.53895724) Loss_G: 0.02683028 Loss_Enh_Dec: -0.47983152\n",
      "| epoch  19 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  4.30 | ppl    73.48 | acc     0.55 | train_ae_norm     1.00\n",
      "[19/200][3899/4361] Loss_D: 1.09120083 (Loss_D_real: 0.48354661 Loss_D_fake: 0.60765421) Loss_G: 0.02429860 Loss_Enh_Dec: -0.34783861\n",
      "| epoch  19 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  4.27 | ppl    71.65 | acc     0.51 | train_ae_norm     1.00\n",
      "[19/200][3999/4361] Loss_D: 0.94064444 (Loss_D_real: 0.58776301 Loss_D_fake: 0.35288143) Loss_G: 0.03069106 Loss_Enh_Dec: -0.49336433\n",
      "| epoch  19 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  4.25 | ppl    70.15 | acc     0.53 | train_ae_norm     1.00\n",
      "[19/200][4099/4361] Loss_D: 0.85937172 (Loss_D_real: 0.43681949 Loss_D_fake: 0.42255223) Loss_G: 0.05063480 Loss_Enh_Dec: -0.52003789\n",
      "| epoch  19 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  4.20 | ppl    66.63 | acc     0.52 | train_ae_norm     1.00\n",
      "[19/200][4199/4361] Loss_D: 0.81689507 (Loss_D_real: 0.39775607 Loss_D_fake: 0.41913900) Loss_G: 0.06379395 Loss_Enh_Dec: -0.45404837\n",
      "| epoch  19 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  4.24 | ppl    69.39 | acc     0.52 | train_ae_norm     1.00\n",
      "[19/200][4299/4361] Loss_D: 0.72523820 (Loss_D_real: 0.35551962 Loss_D_fake: 0.36971861) Loss_G: 0.09574746 Loss_Enh_Dec: -0.41799331\n",
      "| epoch  19 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  4.20 | ppl    66.61 | acc     0.53 | train_ae_norm     1.00\n",
      "| end of epoch  19 | time: 1852.36s | test loss  3.92 | test ppl 50.51 | acc 0.591\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 20 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.742\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.465\n",
      "  Test Loss: 2.965\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  20 |     0/ 4361 batches | lr 0.000000 | ms/batch 863.12 | loss  0.04 | ppl     1.04 | acc     0.55 | train_ae_norm     1.00\n",
      "[20/200][99/4361] Loss_D: 0.71058583 (Loss_D_real: 0.36448354 Loss_D_fake: 0.34610230) Loss_G: 0.04111868 Loss_Enh_Dec: -0.62871593\n",
      "| epoch  20 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.14 | loss  4.22 | ppl    68.36 | acc     0.50 | train_ae_norm     1.00\n",
      "[20/200][199/4361] Loss_D: 0.83668172 (Loss_D_real: 0.44971937 Loss_D_fake: 0.38696238) Loss_G: 0.04574177 Loss_Enh_Dec: -0.44961509\n",
      "| epoch  20 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  4.30 | ppl    73.42 | acc     0.54 | train_ae_norm     1.00\n",
      "[20/200][299/4361] Loss_D: 0.64381075 (Loss_D_real: 0.40334791 Loss_D_fake: 0.24046281) Loss_G: 0.05743529 Loss_Enh_Dec: -0.50649416\n",
      "| epoch  20 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.89 | loss  4.31 | ppl    74.61 | acc     0.50 | train_ae_norm     1.00\n",
      "[20/200][399/4361] Loss_D: 0.74647897 (Loss_D_real: 0.40970987 Loss_D_fake: 0.33676910) Loss_G: 0.04716178 Loss_Enh_Dec: -0.57720786\n",
      "| epoch  20 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  4.21 | ppl    67.09 | acc     0.53 | train_ae_norm     1.00\n",
      "[20/200][499/4361] Loss_D: 0.71795523 (Loss_D_real: 0.39795780 Loss_D_fake: 0.31999740) Loss_G: 0.08162262 Loss_Enh_Dec: -0.53869170\n",
      "| epoch  20 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  4.29 | ppl    72.75 | acc     0.54 | train_ae_norm     1.00\n",
      "[20/200][599/4361] Loss_D: 0.75359416 (Loss_D_real: 0.36803827 Loss_D_fake: 0.38555592) Loss_G: 0.05078455 Loss_Enh_Dec: -0.35555047\n",
      "| epoch  20 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  4.18 | ppl    65.56 | acc     0.46 | train_ae_norm     1.00\n",
      "[20/200][699/4361] Loss_D: 1.14440703 (Loss_D_real: 0.58555353 Loss_D_fake: 0.55885345) Loss_G: 0.05873439 Loss_Enh_Dec: -0.47945195\n",
      "| epoch  20 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.42 | loss  4.25 | ppl    70.17 | acc     0.55 | train_ae_norm     1.00\n",
      "[20/200][799/4361] Loss_D: 0.78045100 (Loss_D_real: 0.40830344 Loss_D_fake: 0.37214756) Loss_G: 0.05362416 Loss_Enh_Dec: -0.45653424\n",
      "| epoch  20 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  4.24 | ppl    69.75 | acc     0.53 | train_ae_norm     1.00\n",
      "[20/200][899/4361] Loss_D: 0.93683118 (Loss_D_real: 0.48253959 Loss_D_fake: 0.45429158) Loss_G: 0.04054933 Loss_Enh_Dec: -0.44091448\n",
      "| epoch  20 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.85 | loss  4.26 | ppl    70.55 | acc     0.52 | train_ae_norm     1.00\n",
      "[20/200][999/4361] Loss_D: 0.68869752 (Loss_D_real: 0.26183122 Loss_D_fake: 0.42686629) Loss_G: 0.03771340 Loss_Enh_Dec: -0.56670755\n",
      "| epoch  20 |  1000/ 4361 batches | lr 0.000000 | ms/batch 400.42 | loss  4.22 | ppl    67.69 | acc     0.53 | train_ae_norm     1.00\n",
      "[20/200][1099/4361] Loss_D: 0.65271878 (Loss_D_real: 0.26170492 Loss_D_fake: 0.39101383) Loss_G: 0.02932074 Loss_Enh_Dec: -0.52476442\n",
      "| epoch  20 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  4.20 | ppl    67.00 | acc     0.49 | train_ae_norm     1.00\n",
      "[20/200][1199/4361] Loss_D: 0.99453974 (Loss_D_real: 0.63005823 Loss_D_fake: 0.36448154) Loss_G: 0.02854361 Loss_Enh_Dec: -0.48251581\n",
      "| epoch  20 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.57 | loss  4.22 | ppl    67.76 | acc     0.53 | train_ae_norm     1.00\n",
      "[20/200][1299/4361] Loss_D: 0.55214977 (Loss_D_real: 0.24787985 Loss_D_fake: 0.30426991) Loss_G: 0.07246931 Loss_Enh_Dec: -0.45682597\n",
      "| epoch  20 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  4.23 | ppl    68.72 | acc     0.53 | train_ae_norm     1.00\n",
      "[20/200][1399/4361] Loss_D: 0.63694322 (Loss_D_real: 0.32487997 Loss_D_fake: 0.31206328) Loss_G: 0.10411979 Loss_Enh_Dec: -0.65322644\n",
      "| epoch  20 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  4.25 | ppl    69.96 | acc     0.48 | train_ae_norm     1.00\n",
      "[20/200][1499/4361] Loss_D: 0.67296064 (Loss_D_real: 0.39382276 Loss_D_fake: 0.27913785) Loss_G: 0.04935908 Loss_Enh_Dec: -0.52014899\n",
      "| epoch  20 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  4.27 | ppl    71.65 | acc     0.48 | train_ae_norm     1.00\n",
      "[20/200][1599/4361] Loss_D: 0.60533440 (Loss_D_real: 0.25804827 Loss_D_fake: 0.34728616) Loss_G: 0.09173350 Loss_Enh_Dec: -0.58577710\n",
      "| epoch  20 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  4.24 | ppl    69.43 | acc     0.51 | train_ae_norm     1.00\n",
      "[20/200][1699/4361] Loss_D: 0.53865504 (Loss_D_real: 0.28949744 Loss_D_fake: 0.24915764) Loss_G: 0.09814884 Loss_Enh_Dec: -0.45094582\n",
      "| epoch  20 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  4.21 | ppl    67.08 | acc     0.50 | train_ae_norm     1.00\n",
      "[20/200][1799/4361] Loss_D: 0.45497373 (Loss_D_real: 0.15586397 Loss_D_fake: 0.29910976) Loss_G: 0.06814237 Loss_Enh_Dec: -0.59695047\n",
      "| epoch  20 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  4.19 | ppl    65.92 | acc     0.52 | train_ae_norm     1.00\n",
      "[20/200][1899/4361] Loss_D: 0.63282222 (Loss_D_real: 0.35004291 Loss_D_fake: 0.28277931) Loss_G: 0.05979819 Loss_Enh_Dec: -0.41780391\n",
      "| epoch  20 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.71 | loss  4.25 | ppl    70.07 | acc     0.50 | train_ae_norm     1.00\n",
      "[20/200][1999/4361] Loss_D: 0.64608049 (Loss_D_real: 0.25468078 Loss_D_fake: 0.39139971) Loss_G: 0.07077239 Loss_Enh_Dec: -0.49578258\n",
      "| epoch  20 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.91 | loss  4.20 | ppl    66.92 | acc     0.50 | train_ae_norm     1.00\n",
      "[20/200][2099/4361] Loss_D: 0.91794097 (Loss_D_real: 0.45497853 Loss_D_fake: 0.46296245) Loss_G: 0.08786879 Loss_Enh_Dec: -0.36582574\n",
      "| epoch  20 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  4.22 | ppl    68.19 | acc     0.54 | train_ae_norm     1.00\n",
      "[20/200][2199/4361] Loss_D: 0.56827766 (Loss_D_real: 0.33701989 Loss_D_fake: 0.23125777) Loss_G: 0.06966075 Loss_Enh_Dec: -0.52120799\n",
      "| epoch  20 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  4.21 | ppl    67.27 | acc     0.51 | train_ae_norm     1.00\n",
      "[20/200][2299/4361] Loss_D: 1.04097509 (Loss_D_real: 0.55851924 Loss_D_fake: 0.48245579) Loss_G: 0.03719929 Loss_Enh_Dec: -0.52569675\n",
      "| epoch  20 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.75 | loss  4.23 | ppl    68.95 | acc     0.55 | train_ae_norm     1.00\n",
      "[20/200][2399/4361] Loss_D: 0.59767556 (Loss_D_real: 0.29780447 Loss_D_fake: 0.29987112) Loss_G: 0.05142761 Loss_Enh_Dec: -0.52173263\n",
      "| epoch  20 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  4.21 | ppl    67.09 | acc     0.49 | train_ae_norm     1.00\n",
      "[20/200][2499/4361] Loss_D: 0.82536137 (Loss_D_real: 0.50169230 Loss_D_fake: 0.32366911) Loss_G: 0.04113684 Loss_Enh_Dec: -0.58813637\n",
      "| epoch  20 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  4.25 | ppl    69.76 | acc     0.53 | train_ae_norm     1.00\n",
      "[20/200][2599/4361] Loss_D: 0.66076672 (Loss_D_real: 0.28681934 Loss_D_fake: 0.37394738) Loss_G: 0.03622000 Loss_Enh_Dec: -0.48136216\n",
      "| epoch  20 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.64 | loss  4.24 | ppl    69.51 | acc     0.51 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/200][2699/4361] Loss_D: 0.77339214 (Loss_D_real: 0.30473942 Loss_D_fake: 0.46865273) Loss_G: 0.02907758 Loss_Enh_Dec: -0.43850064\n",
      "| epoch  20 |  2700/ 4361 batches | lr 0.000000 | ms/batch 400.80 | loss  4.25 | ppl    70.43 | acc     0.52 | train_ae_norm     1.00\n",
      "[20/200][2799/4361] Loss_D: 0.83557111 (Loss_D_real: 0.32291013 Loss_D_fake: 0.51266098) Loss_G: 0.07746001 Loss_Enh_Dec: -0.40976954\n",
      "| epoch  20 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  4.19 | ppl    66.02 | acc     0.47 | train_ae_norm     1.00\n",
      "[20/200][2899/4361] Loss_D: 0.64084399 (Loss_D_real: 0.31574529 Loss_D_fake: 0.32509869) Loss_G: 0.05973517 Loss_Enh_Dec: -0.57540888\n",
      "| epoch  20 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  4.25 | ppl    70.18 | acc     0.51 | train_ae_norm     1.00\n",
      "[20/200][2999/4361] Loss_D: 0.80739075 (Loss_D_real: 0.40978608 Loss_D_fake: 0.39760467) Loss_G: 0.04983616 Loss_Enh_Dec: -0.58720183\n",
      "| epoch  20 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  4.24 | ppl    69.68 | acc     0.53 | train_ae_norm     1.00\n",
      "[20/200][3099/4361] Loss_D: 0.67032611 (Loss_D_real: 0.27472278 Loss_D_fake: 0.39560336) Loss_G: 0.04330138 Loss_Enh_Dec: -0.41147071\n",
      "| epoch  20 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  4.25 | ppl    70.31 | acc     0.49 | train_ae_norm     1.00\n",
      "[20/200][3199/4361] Loss_D: 0.70901406 (Loss_D_real: 0.34962630 Loss_D_fake: 0.35938773) Loss_G: 0.02572031 Loss_Enh_Dec: -0.62284023\n",
      "| epoch  20 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  4.30 | ppl    73.68 | acc     0.51 | train_ae_norm     1.00\n",
      "[20/200][3299/4361] Loss_D: 0.70145631 (Loss_D_real: 0.35516971 Loss_D_fake: 0.34628662) Loss_G: 0.07963077 Loss_Enh_Dec: -0.66766161\n",
      "| epoch  20 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  4.33 | ppl    76.18 | acc     0.49 | train_ae_norm     1.00\n",
      "[20/200][3399/4361] Loss_D: 1.02049959 (Loss_D_real: 0.44922805 Loss_D_fake: 0.57127154) Loss_G: 0.03054483 Loss_Enh_Dec: -0.47556302\n",
      "| epoch  20 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  4.29 | ppl    73.08 | acc     0.50 | train_ae_norm     1.00\n",
      "[20/200][3499/4361] Loss_D: 1.09328091 (Loss_D_real: 0.49007377 Loss_D_fake: 0.60320711) Loss_G: 0.05364224 Loss_Enh_Dec: -0.69620812\n",
      "| epoch  20 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  4.25 | ppl    70.31 | acc     0.50 | train_ae_norm     1.00\n",
      "[20/200][3599/4361] Loss_D: 0.96399301 (Loss_D_real: 0.56628889 Loss_D_fake: 0.39770412) Loss_G: 0.01857803 Loss_Enh_Dec: -0.69463915\n",
      "| epoch  20 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  4.27 | ppl    71.66 | acc     0.53 | train_ae_norm     1.00\n",
      "[20/200][3699/4361] Loss_D: 0.63405871 (Loss_D_real: 0.27013180 Loss_D_fake: 0.36392689) Loss_G: 0.07232466 Loss_Enh_Dec: -0.50412798\n",
      "| epoch  20 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  4.28 | ppl    71.89 | acc     0.48 | train_ae_norm     1.00\n",
      "[20/200][3799/4361] Loss_D: 0.66144013 (Loss_D_real: 0.28438205 Loss_D_fake: 0.37705809) Loss_G: 0.03770935 Loss_Enh_Dec: -0.65116471\n",
      "| epoch  20 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  4.29 | ppl    73.14 | acc     0.53 | train_ae_norm     1.00\n",
      "[20/200][3899/4361] Loss_D: 0.81005991 (Loss_D_real: 0.38291717 Loss_D_fake: 0.42714271) Loss_G: 0.08701888 Loss_Enh_Dec: -0.57136297\n",
      "| epoch  20 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  4.27 | ppl    71.54 | acc     0.47 | train_ae_norm     1.00\n",
      "[20/200][3999/4361] Loss_D: 0.74643505 (Loss_D_real: 0.35103720 Loss_D_fake: 0.39539787) Loss_G: 0.06380516 Loss_Enh_Dec: -0.46518117\n",
      "| epoch  20 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.94 | loss  4.25 | ppl    70.29 | acc     0.52 | train_ae_norm     1.00\n",
      "[20/200][4099/4361] Loss_D: 0.62135303 (Loss_D_real: 0.26154047 Loss_D_fake: 0.35981259) Loss_G: 0.03439224 Loss_Enh_Dec: -0.52121997\n",
      "| epoch  20 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  4.21 | ppl    67.59 | acc     0.50 | train_ae_norm     1.00\n",
      "[20/200][4199/4361] Loss_D: 0.38139299 (Loss_D_real: 0.11948391 Loss_D_fake: 0.26190907) Loss_G: 0.09988944 Loss_Enh_Dec: -0.53661931\n",
      "| epoch  20 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  4.36 | ppl    78.13 | acc     0.53 | train_ae_norm     1.00\n",
      "[20/200][4299/4361] Loss_D: 0.77804196 (Loss_D_real: 0.49496335 Loss_D_fake: 0.28307858) Loss_G: 0.07118028 Loss_Enh_Dec: -0.46937114\n",
      "| epoch  20 |  4300/ 4361 batches | lr 0.000000 | ms/batch 402.05 | loss  4.23 | ppl    68.59 | acc     0.49 | train_ae_norm     1.00\n",
      "| end of epoch  20 | time: 1851.72s | test loss  3.89 | test ppl 48.98 | acc 0.593\n",
      "bleu_self:  [3.26984127e-01 1.47349935e-01 1.35773846e-06 4.43986535e-09\n",
      " 1.67039850e-10]\n",
      "bleu_test:  [8.74851190e-01 5.67601404e-01 2.68850924e-01 4.05234242e-05\n",
      " 2.55727606e-07]\n",
      "bleu_self: [0.32698413,0.14734994,0.00000136,0.00000000,0.00000000]\n",
      "bleu_test: [0.87485119,0.56760140,0.26885092,0.00004052,0.00000026]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 21 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.741\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.472\n",
      "  Test Loss: 2.989\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  21 |     0/ 4361 batches | lr 0.000000 | ms/batch 864.37 | loss  0.04 | ppl     1.04 | acc     0.51 | train_ae_norm     1.00\n",
      "[21/200][99/4361] Loss_D: 0.75496936 (Loss_D_real: 0.38446993 Loss_D_fake: 0.37049943) Loss_G: 0.07006856 Loss_Enh_Dec: -0.57435954\n",
      "| epoch  21 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  4.26 | ppl    70.64 | acc     0.48 | train_ae_norm     1.00\n",
      "[21/200][199/4361] Loss_D: 0.73783827 (Loss_D_real: 0.39150476 Loss_D_fake: 0.34633347) Loss_G: 0.06115993 Loss_Enh_Dec: -0.77970850\n",
      "| epoch  21 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  4.30 | ppl    74.06 | acc     0.57 | train_ae_norm     1.00\n",
      "[21/200][299/4361] Loss_D: 0.77072215 (Loss_D_real: 0.37031850 Loss_D_fake: 0.40040362) Loss_G: 0.04540418 Loss_Enh_Dec: -0.63430876\n",
      "| epoch  21 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.93 | loss  4.28 | ppl    72.51 | acc     0.48 | train_ae_norm     1.00\n",
      "[21/200][399/4361] Loss_D: 0.79310310 (Loss_D_real: 0.47650939 Loss_D_fake: 0.31659371) Loss_G: 0.02974894 Loss_Enh_Dec: -0.65571612\n",
      "| epoch  21 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  4.25 | ppl    70.34 | acc     0.50 | train_ae_norm     1.00\n",
      "[21/200][499/4361] Loss_D: 0.96121234 (Loss_D_real: 0.44473660 Loss_D_fake: 0.51647574) Loss_G: 0.03819833 Loss_Enh_Dec: -0.64880824\n",
      "| epoch  21 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.78 | loss  4.28 | ppl    72.04 | acc     0.53 | train_ae_norm     1.00\n",
      "[21/200][599/4361] Loss_D: 0.65455747 (Loss_D_real: 0.23411322 Loss_D_fake: 0.42044425) Loss_G: 0.04070777 Loss_Enh_Dec: -0.53924513\n",
      "| epoch  21 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  4.20 | ppl    66.99 | acc     0.50 | train_ae_norm     1.00\n",
      "[21/200][699/4361] Loss_D: 1.29126740 (Loss_D_real: 0.62799084 Loss_D_fake: 0.66327661) Loss_G: -0.02188522 Loss_Enh_Dec: -0.56594771\n",
      "| epoch  21 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.75 | loss  4.28 | ppl    72.16 | acc     0.53 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21/200][799/4361] Loss_D: 1.08156347 (Loss_D_real: 0.63037485 Loss_D_fake: 0.45118868) Loss_G: -0.00205098 Loss_Enh_Dec: -0.32285172\n",
      "| epoch  21 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  4.27 | ppl    71.42 | acc     0.51 | train_ae_norm     1.00\n",
      "[21/200][899/4361] Loss_D: 0.72941017 (Loss_D_real: 0.32073975 Loss_D_fake: 0.40867040) Loss_G: 0.03456929 Loss_Enh_Dec: -0.37831447\n",
      "| epoch  21 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  4.31 | ppl    74.33 | acc     0.54 | train_ae_norm     1.00\n",
      "[21/200][999/4361] Loss_D: 0.73582363 (Loss_D_real: 0.36855453 Loss_D_fake: 0.36726910) Loss_G: 0.05751867 Loss_Enh_Dec: -0.33862075\n",
      "| epoch  21 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  4.29 | ppl    73.21 | acc     0.46 | train_ae_norm     1.00\n",
      "[21/200][1099/4361] Loss_D: 0.76940459 (Loss_D_real: 0.52932024 Loss_D_fake: 0.24008434) Loss_G: 0.04221579 Loss_Enh_Dec: -0.41714308\n",
      "| epoch  21 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  4.27 | ppl    71.75 | acc     0.47 | train_ae_norm     1.00\n",
      "[21/200][1199/4361] Loss_D: 0.99756193 (Loss_D_real: 0.52777791 Loss_D_fake: 0.46978402) Loss_G: 0.07757854 Loss_Enh_Dec: -0.23481567\n",
      "| epoch  21 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  4.29 | ppl    72.99 | acc     0.53 | train_ae_norm     1.00\n",
      "[21/200][1299/4361] Loss_D: 0.87266850 (Loss_D_real: 0.40340313 Loss_D_fake: 0.46926540) Loss_G: 0.04789733 Loss_Enh_Dec: -0.38176700\n",
      "| epoch  21 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  4.28 | ppl    72.43 | acc     0.47 | train_ae_norm     1.00\n",
      "[21/200][1399/4361] Loss_D: 0.87496918 (Loss_D_real: 0.47599110 Loss_D_fake: 0.39897808) Loss_G: 0.05125833 Loss_Enh_Dec: -0.38420877\n",
      "| epoch  21 |  1400/ 4361 batches | lr 0.000000 | ms/batch 404.89 | loss  4.29 | ppl    73.07 | acc     0.44 | train_ae_norm     1.00\n",
      "[21/200][1499/4361] Loss_D: 1.00082636 (Loss_D_real: 0.70401192 Loss_D_fake: 0.29681450) Loss_G: 0.04922308 Loss_Enh_Dec: -0.31243229\n",
      "| epoch  21 |  1500/ 4361 batches | lr 0.000000 | ms/batch 402.05 | loss  4.35 | ppl    77.47 | acc     0.46 | train_ae_norm     1.00\n",
      "[21/200][1599/4361] Loss_D: 0.95769638 (Loss_D_real: 0.48905867 Loss_D_fake: 0.46863770) Loss_G: 0.04589464 Loss_Enh_Dec: -0.40315539\n",
      "| epoch  21 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  4.29 | ppl    72.68 | acc     0.50 | train_ae_norm     1.00\n",
      "[21/200][1699/4361] Loss_D: 0.80927426 (Loss_D_real: 0.39014313 Loss_D_fake: 0.41913113) Loss_G: 0.05047917 Loss_Enh_Dec: -0.39056382\n",
      "| epoch  21 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  4.28 | ppl    71.99 | acc     0.49 | train_ae_norm     1.00\n",
      "[21/200][1799/4361] Loss_D: 0.94906151 (Loss_D_real: 0.37935150 Loss_D_fake: 0.56971002) Loss_G: 0.03416574 Loss_Enh_Dec: -0.53415632\n",
      "| epoch  21 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  4.24 | ppl    69.47 | acc     0.51 | train_ae_norm     1.00\n",
      "[21/200][1899/4361] Loss_D: 0.84673560 (Loss_D_real: 0.51204306 Loss_D_fake: 0.33469257) Loss_G: 0.03348083 Loss_Enh_Dec: -0.47936422\n",
      "| epoch  21 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  4.29 | ppl    73.03 | acc     0.51 | train_ae_norm     1.00\n",
      "[21/200][1999/4361] Loss_D: 0.82128155 (Loss_D_real: 0.41480392 Loss_D_fake: 0.40647763) Loss_G: 0.04666940 Loss_Enh_Dec: -0.59123868\n",
      "| epoch  21 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  4.25 | ppl    69.93 | acc     0.49 | train_ae_norm     1.00\n",
      "[21/200][2099/4361] Loss_D: 1.04097104 (Loss_D_real: 0.60314834 Loss_D_fake: 0.43782270) Loss_G: 0.02932617 Loss_Enh_Dec: -0.38951823\n",
      "| epoch  21 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.83 | loss  4.30 | ppl    74.00 | acc     0.52 | train_ae_norm     1.00\n",
      "[21/200][2199/4361] Loss_D: 0.78331274 (Loss_D_real: 0.29834157 Loss_D_fake: 0.48497117) Loss_G: 0.03369346 Loss_Enh_Dec: -0.38448378\n",
      "| epoch  21 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  4.37 | ppl    79.03 | acc     0.52 | train_ae_norm     1.00\n",
      "[21/200][2299/4361] Loss_D: 1.03588009 (Loss_D_real: 0.51473105 Loss_D_fake: 0.52114910) Loss_G: 0.05611758 Loss_Enh_Dec: -0.36294714\n",
      "| epoch  21 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  4.29 | ppl    72.88 | acc     0.52 | train_ae_norm     1.00\n",
      "[21/200][2399/4361] Loss_D: 1.05638289 (Loss_D_real: 0.56043434 Loss_D_fake: 0.49594855) Loss_G: 0.04078853 Loss_Enh_Dec: -0.38267633\n",
      "| epoch  21 |  2400/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  4.31 | ppl    74.22 | acc     0.47 | train_ae_norm     1.00\n",
      "[21/200][2499/4361] Loss_D: 0.74995136 (Loss_D_real: 0.37605605 Loss_D_fake: 0.37389529) Loss_G: 0.00012200 Loss_Enh_Dec: -0.40804550\n",
      "| epoch  21 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  4.35 | ppl    77.31 | acc     0.51 | train_ae_norm     1.00\n",
      "[21/200][2599/4361] Loss_D: 0.86390066 (Loss_D_real: 0.39922696 Loss_D_fake: 0.46467370) Loss_G: 0.00808020 Loss_Enh_Dec: -0.30199918\n",
      "| epoch  21 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  4.27 | ppl    71.25 | acc     0.50 | train_ae_norm     1.00\n",
      "[21/200][2699/4361] Loss_D: 1.13536239 (Loss_D_real: 0.63536537 Loss_D_fake: 0.49999708) Loss_G: 0.04458138 Loss_Enh_Dec: -0.44762889\n",
      "| epoch  21 |  2700/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  4.30 | ppl    73.94 | acc     0.51 | train_ae_norm     1.00\n",
      "[21/200][2799/4361] Loss_D: 0.94533312 (Loss_D_real: 0.47431087 Loss_D_fake: 0.47102222) Loss_G: 0.05464998 Loss_Enh_Dec: -0.42164141\n",
      "| epoch  21 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  4.27 | ppl    71.48 | acc     0.48 | train_ae_norm     1.00\n",
      "[21/200][2899/4361] Loss_D: 0.72392446 (Loss_D_real: 0.43220338 Loss_D_fake: 0.29172108) Loss_G: 0.05781961 Loss_Enh_Dec: -0.47639546\n",
      "| epoch  21 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.83 | loss  4.30 | ppl    73.43 | acc     0.52 | train_ae_norm     1.00\n",
      "[21/200][2999/4361] Loss_D: 0.75660622 (Loss_D_real: 0.40220463 Loss_D_fake: 0.35440156) Loss_G: 0.05423259 Loss_Enh_Dec: -0.55600446\n",
      "| epoch  21 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  4.30 | ppl    73.89 | acc     0.51 | train_ae_norm     1.00\n",
      "[21/200][3099/4361] Loss_D: 0.78092027 (Loss_D_real: 0.47161373 Loss_D_fake: 0.30930650) Loss_G: 0.09214548 Loss_Enh_Dec: -0.44211221\n",
      "| epoch  21 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  4.32 | ppl    74.85 | acc     0.44 | train_ae_norm     1.00\n",
      "[21/200][3199/4361] Loss_D: 0.70270884 (Loss_D_real: 0.32332602 Loss_D_fake: 0.37938285) Loss_G: 0.05174130 Loss_Enh_Dec: -0.49948320\n",
      "| epoch  21 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  4.34 | ppl    76.70 | acc     0.47 | train_ae_norm     1.00\n",
      "[21/200][3299/4361] Loss_D: 0.73454016 (Loss_D_real: 0.35118341 Loss_D_fake: 0.38335675) Loss_G: 0.04342276 Loss_Enh_Dec: -0.61165684\n",
      "| epoch  21 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  4.35 | ppl    77.77 | acc     0.49 | train_ae_norm     1.00\n",
      "[21/200][3399/4361] Loss_D: 0.64259028 (Loss_D_real: 0.32793033 Loss_D_fake: 0.31465995) Loss_G: 0.07689773 Loss_Enh_Dec: -0.61258978\n",
      "| epoch  21 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  4.30 | ppl    73.47 | acc     0.50 | train_ae_norm     1.00\n",
      "[21/200][3499/4361] Loss_D: 0.91831750 (Loss_D_real: 0.42236122 Loss_D_fake: 0.49595627) Loss_G: 0.03751767 Loss_Enh_Dec: -0.65911615\n",
      "| epoch  21 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  4.24 | ppl    69.25 | acc     0.52 | train_ae_norm     1.00\n",
      "[21/200][3599/4361] Loss_D: 0.88668478 (Loss_D_real: 0.46650776 Loss_D_fake: 0.42017698) Loss_G: 0.04733835 Loss_Enh_Dec: -0.71015108\n",
      "| epoch  21 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  4.29 | ppl    72.94 | acc     0.50 | train_ae_norm     1.00\n",
      "[21/200][3699/4361] Loss_D: 0.98550677 (Loss_D_real: 0.68993706 Loss_D_fake: 0.29556975) Loss_G: 0.03260887 Loss_Enh_Dec: -0.63500494\n",
      "| epoch  21 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  4.28 | ppl    72.26 | acc     0.50 | train_ae_norm     1.00\n",
      "[21/200][3799/4361] Loss_D: 1.02029884 (Loss_D_real: 0.55106318 Loss_D_fake: 0.46923566) Loss_G: 0.06707057 Loss_Enh_Dec: -0.48968163\n",
      "| epoch  21 |  3800/ 4361 batches | lr 0.000000 | ms/batch 400.40 | loss  4.32 | ppl    75.31 | acc     0.52 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21/200][3899/4361] Loss_D: 0.96142125 (Loss_D_real: 0.56531233 Loss_D_fake: 0.39610896) Loss_G: 0.04541794 Loss_Enh_Dec: -0.49881896\n",
      "| epoch  21 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.53 | loss  4.31 | ppl    74.53 | acc     0.49 | train_ae_norm     1.00\n",
      "[21/200][3999/4361] Loss_D: 0.78907096 (Loss_D_real: 0.43765959 Loss_D_fake: 0.35141134) Loss_G: 0.05169106 Loss_Enh_Dec: -0.57647818\n",
      "| epoch  21 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  4.36 | ppl    78.46 | acc     0.51 | train_ae_norm     1.00\n",
      "[21/200][4099/4361] Loss_D: 0.42525867 (Loss_D_real: 0.19829170 Loss_D_fake: 0.22696696) Loss_G: 0.09730550 Loss_Enh_Dec: -0.52725160\n",
      "| epoch  21 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  4.31 | ppl    74.10 | acc     0.50 | train_ae_norm     1.00\n",
      "[21/200][4199/4361] Loss_D: 0.38107949 (Loss_D_real: 0.10863083 Loss_D_fake: 0.27244866) Loss_G: 0.09958588 Loss_Enh_Dec: -0.47671849\n",
      "| epoch  21 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  4.31 | ppl    74.54 | acc     0.51 | train_ae_norm     1.00\n",
      "[21/200][4299/4361] Loss_D: 0.28355801 (Loss_D_real: 0.11634991 Loss_D_fake: 0.16720812) Loss_G: 0.09713813 Loss_Enh_Dec: -0.37253216\n",
      "| epoch  21 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  4.24 | ppl    69.54 | acc     0.50 | train_ae_norm     1.00\n",
      "| end of epoch  21 | time: 1852.84s | test loss  3.95 | test ppl 51.73 | acc 0.584\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 22 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.735\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.475\n",
      "  Test Loss: 3.032\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  22 |     0/ 4361 batches | lr 0.000000 | ms/batch 865.00 | loss  0.04 | ppl     1.04 | acc     0.52 | train_ae_norm     1.00\n",
      "[22/200][99/4361] Loss_D: 0.97093433 (Loss_D_real: 0.49956036 Loss_D_fake: 0.47137398) Loss_G: 0.06832626 Loss_Enh_Dec: -0.32886216\n",
      "| epoch  22 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  4.26 | ppl    71.15 | acc     0.48 | train_ae_norm     1.00\n",
      "[22/200][199/4361] Loss_D: 0.37587842 (Loss_D_real: 0.18154532 Loss_D_fake: 0.19433311) Loss_G: 0.11377934 Loss_Enh_Dec: -0.34714794\n",
      "| epoch  22 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.94 | loss  4.29 | ppl    73.03 | acc     0.52 | train_ae_norm     1.00\n",
      "[22/200][299/4361] Loss_D: 0.43550396 (Loss_D_real: 0.14179210 Loss_D_fake: 0.29371184) Loss_G: 0.11945979 Loss_Enh_Dec: -0.53823066\n",
      "| epoch  22 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  4.42 | ppl    82.71 | acc     0.39 | train_ae_norm     1.00\n",
      "[22/200][399/4361] Loss_D: 0.49304113 (Loss_D_real: 0.27054930 Loss_D_fake: 0.22249183) Loss_G: 0.12724541 Loss_Enh_Dec: -0.53812844\n",
      "| epoch  22 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.69 | loss  4.47 | ppl    87.72 | acc     0.44 | train_ae_norm     1.00\n",
      "[22/200][499/4361] Loss_D: 0.39627370 (Loss_D_real: 0.15154085 Loss_D_fake: 0.24473286) Loss_G: 0.15818824 Loss_Enh_Dec: -0.37737033\n",
      "| epoch  22 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  4.76 | ppl   116.32 | acc     0.48 | train_ae_norm     1.00\n",
      "[22/200][599/4361] Loss_D: 0.35695142 (Loss_D_real: 0.19761860 Loss_D_fake: 0.15933283) Loss_G: 0.16093053 Loss_Enh_Dec: -0.57040894\n",
      "| epoch  22 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  4.52 | ppl    91.85 | acc     0.42 | train_ae_norm     1.00\n",
      "[22/200][699/4361] Loss_D: 0.51992285 (Loss_D_real: 0.31663546 Loss_D_fake: 0.20328736) Loss_G: 0.12808494 Loss_Enh_Dec: -0.46806994\n",
      "| epoch  22 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  4.67 | ppl   106.87 | acc     0.42 | train_ae_norm     1.00\n",
      "[22/200][799/4361] Loss_D: 0.39385581 (Loss_D_real: 0.15146312 Loss_D_fake: 0.24239270) Loss_G: 0.13324721 Loss_Enh_Dec: -0.46479329\n",
      "| epoch  22 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  4.70 | ppl   110.24 | acc     0.43 | train_ae_norm     1.00\n",
      "[22/200][899/4361] Loss_D: 0.26568201 (Loss_D_real: 0.06435590 Loss_D_fake: 0.20132610) Loss_G: 0.14295262 Loss_Enh_Dec: -0.35430476\n",
      "| epoch  22 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  4.81 | ppl   122.51 | acc     0.44 | train_ae_norm     1.00\n",
      "[22/200][999/4361] Loss_D: 0.35628474 (Loss_D_real: 0.26477900 Loss_D_fake: 0.09150574) Loss_G: 0.14238690 Loss_Enh_Dec: -0.48365378\n",
      "| epoch  22 |  1000/ 4361 batches | lr 0.000000 | ms/batch 402.24 | loss  4.77 | ppl   118.04 | acc     0.40 | train_ae_norm     1.00\n",
      "[22/200][1099/4361] Loss_D: 0.29749870 (Loss_D_real: 0.18650901 Loss_D_fake: 0.11098969) Loss_G: 0.15111275 Loss_Enh_Dec: -0.59501535\n",
      "| epoch  22 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  4.81 | ppl   122.29 | acc     0.36 | train_ae_norm     1.00\n",
      "[22/200][1199/4361] Loss_D: 0.25282598 (Loss_D_real: 0.08500467 Loss_D_fake: 0.16782132) Loss_G: 0.15744101 Loss_Enh_Dec: -0.48048851\n",
      "| epoch  22 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  4.93 | ppl   138.56 | acc     0.38 | train_ae_norm     1.00\n",
      "[22/200][1299/4361] Loss_D: 0.25880474 (Loss_D_real: 0.13075267 Loss_D_fake: 0.12805206) Loss_G: 0.14215569 Loss_Enh_Dec: -0.60269773\n",
      "| epoch  22 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  4.92 | ppl   136.71 | acc     0.36 | train_ae_norm     1.00\n",
      "[22/200][1399/4361] Loss_D: 0.18138121 (Loss_D_real: 0.04940999 Loss_D_fake: 0.13197123) Loss_G: 0.15135096 Loss_Enh_Dec: -0.47594005\n",
      "| epoch  22 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  4.94 | ppl   139.45 | acc     0.32 | train_ae_norm     1.00\n",
      "[22/200][1499/4361] Loss_D: 0.30106285 (Loss_D_real: 0.18602064 Loss_D_fake: 0.11504221) Loss_G: 0.13855855 Loss_Enh_Dec: -0.64104491\n",
      "| epoch  22 |  1500/ 4361 batches | lr 0.000000 | ms/batch 402.06 | loss  5.03 | ppl   152.90 | acc     0.34 | train_ae_norm     1.00\n",
      "[22/200][1599/4361] Loss_D: 0.20073794 (Loss_D_real: 0.09833376 Loss_D_fake: 0.10240418) Loss_G: 0.14208978 Loss_Enh_Dec: -0.67047095\n",
      "| epoch  22 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  5.01 | ppl   149.30 | acc     0.36 | train_ae_norm     1.00\n",
      "[22/200][1699/4361] Loss_D: 0.23302150 (Loss_D_real: 0.13962434 Loss_D_fake: 0.09339716) Loss_G: 0.17269473 Loss_Enh_Dec: -0.56503028\n",
      "| epoch  22 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.06 | loss  4.99 | ppl   147.25 | acc     0.34 | train_ae_norm     1.00\n",
      "[22/200][1799/4361] Loss_D: 0.23792203 (Loss_D_real: 0.14126554 Loss_D_fake: 0.09665649) Loss_G: 0.16687594 Loss_Enh_Dec: -0.47559118\n",
      "| epoch  22 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  4.98 | ppl   144.78 | acc     0.36 | train_ae_norm     1.00\n",
      "[22/200][1899/4361] Loss_D: 0.22207698 (Loss_D_real: 0.11645120 Loss_D_fake: 0.10562578) Loss_G: 0.14938724 Loss_Enh_Dec: -0.70481187\n",
      "| epoch  22 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  5.04 | ppl   155.08 | acc     0.39 | train_ae_norm     1.00\n",
      "[22/200][1999/4361] Loss_D: 0.24268270 (Loss_D_real: 0.12802351 Loss_D_fake: 0.11465919) Loss_G: 0.14342467 Loss_Enh_Dec: -0.43030563\n",
      "| epoch  22 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  4.98 | ppl   145.62 | acc     0.37 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22/200][2099/4361] Loss_D: 0.18617618 (Loss_D_real: 0.05575432 Loss_D_fake: 0.13042186) Loss_G: 0.16072595 Loss_Enh_Dec: -0.45453468\n",
      "| epoch  22 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  5.03 | ppl   152.45 | acc     0.40 | train_ae_norm     1.00\n",
      "[22/200][2199/4361] Loss_D: 0.16148663 (Loss_D_real: 0.05890999 Loss_D_fake: 0.10257664) Loss_G: 0.16931891 Loss_Enh_Dec: -0.49952617\n",
      "| epoch  22 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  5.00 | ppl   147.82 | acc     0.36 | train_ae_norm     1.00\n",
      "[22/200][2299/4361] Loss_D: 0.12404772 (Loss_D_real: 0.03938917 Loss_D_fake: 0.08465855) Loss_G: 0.17387471 Loss_Enh_Dec: -0.54277992\n",
      "| epoch  22 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  4.97 | ppl   144.05 | acc     0.37 | train_ae_norm     1.00\n",
      "[22/200][2399/4361] Loss_D: 0.23793787 (Loss_D_real: 0.16093200 Loss_D_fake: 0.07700586) Loss_G: 0.18048111 Loss_Enh_Dec: -0.52803928\n",
      "| epoch  22 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  4.95 | ppl   140.70 | acc     0.31 | train_ae_norm     1.00\n",
      "[22/200][2499/4361] Loss_D: 0.10696118 (Loss_D_real: 0.04559053 Loss_D_fake: 0.06137065) Loss_G: 0.18602958 Loss_Enh_Dec: -0.76971620\n",
      "| epoch  22 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  4.98 | ppl   145.64 | acc     0.34 | train_ae_norm     1.00\n",
      "[22/200][2599/4361] Loss_D: 0.16349772 (Loss_D_real: 0.06867486 Loss_D_fake: 0.09482285) Loss_G: 0.16934694 Loss_Enh_Dec: -0.69768006\n",
      "| epoch  22 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  4.95 | ppl   140.85 | acc     0.39 | train_ae_norm     1.00\n",
      "[22/200][2699/4361] Loss_D: 0.14401698 (Loss_D_real: 0.07850643 Loss_D_fake: 0.06551056) Loss_G: 0.18570389 Loss_Enh_Dec: -0.37453142\n",
      "| epoch  22 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  4.93 | ppl   137.70 | acc     0.39 | train_ae_norm     1.00\n",
      "[22/200][2799/4361] Loss_D: 0.13536346 (Loss_D_real: 0.05857248 Loss_D_fake: 0.07679098) Loss_G: 0.16176504 Loss_Enh_Dec: -0.62796086\n",
      "| epoch  22 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  4.84 | ppl   126.56 | acc     0.37 | train_ae_norm     1.00\n",
      "[22/200][2899/4361] Loss_D: 0.22832948 (Loss_D_real: 0.14874464 Loss_D_fake: 0.07958483) Loss_G: 0.17495823 Loss_Enh_Dec: -0.68018430\n",
      "| epoch  22 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  4.84 | ppl   126.12 | acc     0.41 | train_ae_norm     1.00\n",
      "[22/200][2999/4361] Loss_D: 0.10499726 (Loss_D_real: 0.04414800 Loss_D_fake: 0.06084926) Loss_G: 0.18185818 Loss_Enh_Dec: -0.34726578\n",
      "| epoch  22 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  4.82 | ppl   124.00 | acc     0.41 | train_ae_norm     1.00\n",
      "[22/200][3099/4361] Loss_D: 0.12801242 (Loss_D_real: 0.06204247 Loss_D_fake: 0.06596995) Loss_G: 0.19035323 Loss_Enh_Dec: -0.38861674\n",
      "| epoch  22 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  4.80 | ppl   121.31 | acc     0.38 | train_ae_norm     1.00\n",
      "[22/200][3199/4361] Loss_D: 0.11363621 (Loss_D_real: 0.05551367 Loss_D_fake: 0.05812255) Loss_G: 0.19953407 Loss_Enh_Dec: -0.56516737\n",
      "| epoch  22 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  4.81 | ppl   122.43 | acc     0.39 | train_ae_norm     1.00\n",
      "[22/200][3299/4361] Loss_D: 0.17413676 (Loss_D_real: 0.09614503 Loss_D_fake: 0.07799173) Loss_G: 0.18319204 Loss_Enh_Dec: -0.68394995\n",
      "| epoch  22 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  4.79 | ppl   120.17 | acc     0.37 | train_ae_norm     1.00\n",
      "[22/200][3399/4361] Loss_D: 0.09098145 (Loss_D_real: 0.04250810 Loss_D_fake: 0.04847335) Loss_G: 0.18388179 Loss_Enh_Dec: -0.73410141\n",
      "| epoch  22 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.28 | loss  4.76 | ppl   117.14 | acc     0.42 | train_ae_norm     1.00\n",
      "[22/200][3499/4361] Loss_D: 0.10108314 (Loss_D_real: 0.04808792 Loss_D_fake: 0.05299523) Loss_G: 0.19852753 Loss_Enh_Dec: -0.67980403\n",
      "| epoch  22 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  4.67 | ppl   107.09 | acc     0.42 | train_ae_norm     1.00\n",
      "[22/200][3599/4361] Loss_D: 0.12233132 (Loss_D_real: 0.08162484 Loss_D_fake: 0.04070648) Loss_G: 0.17296593 Loss_Enh_Dec: -0.78110456\n",
      "| epoch  22 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  4.65 | ppl   104.40 | acc     0.42 | train_ae_norm     1.00\n",
      "[22/200][3699/4361] Loss_D: 0.16899469 (Loss_D_real: 0.10006513 Loss_D_fake: 0.06892957) Loss_G: 0.18374386 Loss_Enh_Dec: -0.78412050\n",
      "| epoch  22 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  4.64 | ppl   103.93 | acc     0.41 | train_ae_norm     1.00\n",
      "[22/200][3799/4361] Loss_D: 0.30676806 (Loss_D_real: 0.22311310 Loss_D_fake: 0.08365494) Loss_G: 0.15292615 Loss_Enh_Dec: -0.74965328\n",
      "| epoch  22 |  3800/ 4361 batches | lr 0.000000 | ms/batch 400.73 | loss  4.71 | ppl   110.83 | acc     0.45 | train_ae_norm     1.00\n",
      "[22/200][3899/4361] Loss_D: 0.91842163 (Loss_D_real: 0.56165111 Loss_D_fake: 0.35677052) Loss_G: 0.05177498 Loss_Enh_Dec: -0.83315384\n",
      "| epoch  22 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  4.76 | ppl   116.32 | acc     0.40 | train_ae_norm     1.00\n",
      "[22/200][3999/4361] Loss_D: 0.59115601 (Loss_D_real: 0.29978243 Loss_D_fake: 0.29137361) Loss_G: 0.09400769 Loss_Enh_Dec: -0.55858225\n",
      "| epoch  22 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  4.76 | ppl   116.87 | acc     0.44 | train_ae_norm     1.00\n",
      "[22/200][4099/4361] Loss_D: 0.40637711 (Loss_D_real: 0.19191985 Loss_D_fake: 0.21445726) Loss_G: 0.11337490 Loss_Enh_Dec: -0.44312450\n",
      "| epoch  22 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  4.74 | ppl   114.52 | acc     0.39 | train_ae_norm     1.00\n",
      "[22/200][4199/4361] Loss_D: 0.57155228 (Loss_D_real: 0.36185998 Loss_D_fake: 0.20969230) Loss_G: 0.09625912 Loss_Enh_Dec: -0.51757234\n",
      "| epoch  22 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  4.77 | ppl   118.21 | acc     0.39 | train_ae_norm     1.00\n",
      "[22/200][4299/4361] Loss_D: 0.52356881 (Loss_D_real: 0.28544760 Loss_D_fake: 0.23812120) Loss_G: 0.11275051 Loss_Enh_Dec: -0.47958174\n",
      "| epoch  22 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  4.80 | ppl   121.52 | acc     0.41 | train_ae_norm     1.00\n",
      "| end of epoch  22 | time: 1852.56s | test loss  4.36 | test ppl 78.06 | acc 0.501\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 23 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.738\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.470\n",
      "  Test Loss: 3.151\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  23 |     0/ 4361 batches | lr 0.000000 | ms/batch 862.23 | loss  0.04 | ppl     1.05 | acc     0.41 | train_ae_norm     1.00\n",
      "[23/200][99/4361] Loss_D: 0.65526879 (Loss_D_real: 0.41885015 Loss_D_fake: 0.23641860) Loss_G: 0.12240938 Loss_Enh_Dec: -0.56854039\n",
      "| epoch  23 |   100/ 4361 batches | lr 0.000000 | ms/batch 400.86 | loss  4.72 | ppl   112.28 | acc     0.43 | train_ae_norm     1.00\n",
      "[23/200][199/4361] Loss_D: 0.25723484 (Loss_D_real: 0.11580363 Loss_D_fake: 0.14143121) Loss_G: 0.12315917 Loss_Enh_Dec: -0.55107069\n",
      "| epoch  23 |   200/ 4361 batches | lr 0.000000 | ms/batch 400.78 | loss  4.71 | ppl   111.18 | acc     0.43 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/200][299/4361] Loss_D: 0.38089621 (Loss_D_real: 0.22695820 Loss_D_fake: 0.15393800) Loss_G: 0.13565229 Loss_Enh_Dec: -0.54217452\n",
      "| epoch  23 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  4.72 | ppl   112.49 | acc     0.38 | train_ae_norm     1.00\n",
      "[23/200][399/4361] Loss_D: 0.28709033 (Loss_D_real: 0.11695942 Loss_D_fake: 0.17013091) Loss_G: 0.12667874 Loss_Enh_Dec: -0.57771963\n",
      "| epoch  23 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.06 | loss  4.62 | ppl   101.45 | acc     0.41 | train_ae_norm     1.00\n",
      "[23/200][499/4361] Loss_D: 0.31567627 (Loss_D_real: 0.15604912 Loss_D_fake: 0.15962714) Loss_G: 0.12556474 Loss_Enh_Dec: -0.74091798\n",
      "| epoch  23 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.78 | loss  4.69 | ppl   108.64 | acc     0.47 | train_ae_norm     1.00\n",
      "[23/200][599/4361] Loss_D: 0.28271863 (Loss_D_real: 0.15884887 Loss_D_fake: 0.12386977) Loss_G: 0.14078216 Loss_Enh_Dec: -0.68890154\n",
      "| epoch  23 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  4.59 | ppl    98.27 | acc     0.38 | train_ae_norm     1.00\n",
      "[23/200][699/4361] Loss_D: 0.22185472 (Loss_D_real: 0.08528157 Loss_D_fake: 0.13657314) Loss_G: 0.13402304 Loss_Enh_Dec: -0.60085219\n",
      "| epoch  23 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  4.66 | ppl   105.32 | acc     0.43 | train_ae_norm     1.00\n",
      "[23/200][799/4361] Loss_D: 0.19920596 (Loss_D_real: 0.10396002 Loss_D_fake: 0.09524594) Loss_G: 0.15440758 Loss_Enh_Dec: -0.50465131\n",
      "| epoch  23 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  4.58 | ppl    97.74 | acc     0.45 | train_ae_norm     1.00\n",
      "[23/200][899/4361] Loss_D: 0.27425805 (Loss_D_real: 0.12423041 Loss_D_fake: 0.15002763) Loss_G: 0.13977326 Loss_Enh_Dec: -0.76786017\n",
      "| epoch  23 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  4.61 | ppl   100.16 | acc     0.47 | train_ae_norm     1.00\n",
      "[23/200][999/4361] Loss_D: 0.27205768 (Loss_D_real: 0.18072228 Loss_D_fake: 0.09133541) Loss_G: 0.15002759 Loss_Enh_Dec: -0.62203002\n",
      "| epoch  23 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  4.59 | ppl    98.20 | acc     0.45 | train_ae_norm     1.00\n",
      "[23/200][1099/4361] Loss_D: 0.21054864 (Loss_D_real: 0.11553186 Loss_D_fake: 0.09501678) Loss_G: 0.14309816 Loss_Enh_Dec: -0.55859560\n",
      "| epoch  23 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  4.52 | ppl    91.44 | acc     0.46 | train_ae_norm     1.00\n",
      "[23/200][1199/4361] Loss_D: 0.19687554 (Loss_D_real: 0.08735415 Loss_D_fake: 0.10952139) Loss_G: 0.14732197 Loss_Enh_Dec: -0.57401127\n",
      "| epoch  23 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  4.55 | ppl    95.04 | acc     0.45 | train_ae_norm     1.00\n",
      "[23/200][1299/4361] Loss_D: 0.30659875 (Loss_D_real: 0.14920676 Loss_D_fake: 0.15739200) Loss_G: 0.15884390 Loss_Enh_Dec: -0.63954711\n",
      "| epoch  23 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.66 | loss  4.58 | ppl    97.96 | acc     0.42 | train_ae_norm     1.00\n",
      "[23/200][1399/4361] Loss_D: 0.12748995 (Loss_D_real: 0.06383055 Loss_D_fake: 0.06365940) Loss_G: 0.16525963 Loss_Enh_Dec: -0.76085347\n",
      "| epoch  23 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  4.56 | ppl    95.91 | acc     0.35 | train_ae_norm     1.00\n",
      "[23/200][1499/4361] Loss_D: 0.18833640 (Loss_D_real: 0.12733501 Loss_D_fake: 0.06100139) Loss_G: 0.15672140 Loss_Enh_Dec: -0.64318234\n",
      "| epoch  23 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  4.58 | ppl    97.81 | acc     0.44 | train_ae_norm     1.00\n",
      "[23/200][1599/4361] Loss_D: 0.13050114 (Loss_D_real: 0.06489778 Loss_D_fake: 0.06560336) Loss_G: 0.16013668 Loss_Enh_Dec: -0.57998055\n",
      "| epoch  23 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  4.49 | ppl    88.90 | acc     0.47 | train_ae_norm     1.00\n",
      "[23/200][1699/4361] Loss_D: 0.19762018 (Loss_D_real: 0.05877130 Loss_D_fake: 0.13884887) Loss_G: 0.18375014 Loss_Enh_Dec: -0.69291472\n",
      "| epoch  23 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  4.49 | ppl    88.81 | acc     0.42 | train_ae_norm     1.00\n",
      "[23/200][1799/4361] Loss_D: 0.15346688 (Loss_D_real: 0.06873780 Loss_D_fake: 0.08472909) Loss_G: 0.17508221 Loss_Enh_Dec: -0.50046211\n",
      "| epoch  23 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  4.45 | ppl    85.70 | acc     0.48 | train_ae_norm     1.00\n",
      "[23/200][1899/4361] Loss_D: 0.20702112 (Loss_D_real: 0.09328432 Loss_D_fake: 0.11373680) Loss_G: 0.16611724 Loss_Enh_Dec: -0.63904917\n",
      "| epoch  23 |  1900/ 4361 batches | lr 0.000000 | ms/batch 402.42 | loss  4.51 | ppl    90.47 | acc     0.48 | train_ae_norm     1.00\n",
      "[23/200][1999/4361] Loss_D: 0.08990417 (Loss_D_real: 0.02968139 Loss_D_fake: 0.06022277) Loss_G: 0.17461558 Loss_Enh_Dec: -0.64020950\n",
      "| epoch  23 |  2000/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  4.47 | ppl    87.50 | acc     0.43 | train_ae_norm     1.00\n",
      "[23/200][2099/4361] Loss_D: 0.26179364 (Loss_D_real: 0.15753233 Loss_D_fake: 0.10426131) Loss_G: 0.17185712 Loss_Enh_Dec: -0.57637590\n",
      "| epoch  23 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  4.49 | ppl    89.49 | acc     0.48 | train_ae_norm     1.00\n",
      "[23/200][2199/4361] Loss_D: 0.18316364 (Loss_D_real: 0.05911565 Loss_D_fake: 0.12404799) Loss_G: 0.18240379 Loss_Enh_Dec: -0.62881464\n",
      "| epoch  23 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  4.48 | ppl    88.13 | acc     0.47 | train_ae_norm     1.00\n",
      "[23/200][2299/4361] Loss_D: 0.10336047 (Loss_D_real: 0.05265399 Loss_D_fake: 0.05070649) Loss_G: 0.17610689 Loss_Enh_Dec: -0.63938600\n",
      "| epoch  23 |  2300/ 4361 batches | lr 0.000000 | ms/batch 402.22 | loss  4.56 | ppl    95.40 | acc     0.47 | train_ae_norm     1.00\n",
      "[23/200][2399/4361] Loss_D: 0.07379068 (Loss_D_real: 0.03096946 Loss_D_fake: 0.04282122) Loss_G: 0.18923055 Loss_Enh_Dec: -0.76786041\n",
      "| epoch  23 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  4.45 | ppl    85.47 | acc     0.42 | train_ae_norm     1.00\n",
      "[23/200][2499/4361] Loss_D: 0.11730518 (Loss_D_real: 0.08271408 Loss_D_fake: 0.03459110) Loss_G: 0.18579845 Loss_Enh_Dec: -0.57999724\n",
      "| epoch  23 |  2500/ 4361 batches | lr 0.000000 | ms/batch 402.22 | loss  4.44 | ppl    85.12 | acc     0.47 | train_ae_norm     1.00\n",
      "[23/200][2599/4361] Loss_D: 0.12798142 (Loss_D_real: 0.05263785 Loss_D_fake: 0.07534356) Loss_G: 0.17623757 Loss_Enh_Dec: -0.74233133\n",
      "| epoch  23 |  2600/ 4361 batches | lr 0.000000 | ms/batch 402.07 | loss  4.41 | ppl    82.48 | acc     0.47 | train_ae_norm     1.00\n",
      "[23/200][2699/4361] Loss_D: 0.09298171 (Loss_D_real: 0.04809586 Loss_D_fake: 0.04488584) Loss_G: 0.19791006 Loss_Enh_Dec: -0.64145410\n",
      "| epoch  23 |  2700/ 4361 batches | lr 0.000000 | ms/batch 402.31 | loss  4.43 | ppl    83.89 | acc     0.46 | train_ae_norm     1.00\n",
      "[23/200][2799/4361] Loss_D: 0.14916919 (Loss_D_real: 0.09888424 Loss_D_fake: 0.05028495) Loss_G: 0.17921837 Loss_Enh_Dec: -0.59383005\n",
      "| epoch  23 |  2800/ 4361 batches | lr 0.000000 | ms/batch 402.15 | loss  4.39 | ppl    80.34 | acc     0.44 | train_ae_norm     1.00\n",
      "[23/200][2899/4361] Loss_D: 0.12117320 (Loss_D_real: 0.05961575 Loss_D_fake: 0.06155746) Loss_G: 0.17275195 Loss_Enh_Dec: -0.57355559\n",
      "| epoch  23 |  2900/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  4.49 | ppl    88.77 | acc     0.46 | train_ae_norm     1.00\n",
      "[23/200][2999/4361] Loss_D: 0.14866318 (Loss_D_real: 0.08473578 Loss_D_fake: 0.06392740) Loss_G: 0.17707539 Loss_Enh_Dec: -0.51578158\n",
      "| epoch  23 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  4.66 | ppl   105.97 | acc     0.45 | train_ae_norm     1.00\n",
      "[23/200][3099/4361] Loss_D: 0.05528372 (Loss_D_real: 0.02992014 Loss_D_fake: 0.02536358) Loss_G: 0.19487078 Loss_Enh_Dec: -0.61669779\n",
      "| epoch  23 |  3100/ 4361 batches | lr 0.000000 | ms/batch 402.09 | loss  4.50 | ppl    90.41 | acc     0.43 | train_ae_norm     1.00\n",
      "[23/200][3199/4361] Loss_D: 0.05509583 (Loss_D_real: 0.01632066 Loss_D_fake: 0.03877518) Loss_G: 0.19948812 Loss_Enh_Dec: -0.51601958\n",
      "| epoch  23 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  4.48 | ppl    88.65 | acc     0.41 | train_ae_norm     1.00\n",
      "[23/200][3299/4361] Loss_D: 0.12298002 (Loss_D_real: 0.05073935 Loss_D_fake: 0.07224067) Loss_G: 0.18525457 Loss_Enh_Dec: -0.64822429\n",
      "| epoch  23 |  3300/ 4361 batches | lr 0.000000 | ms/batch 402.20 | loss  4.62 | ppl   101.76 | acc     0.46 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/200][3399/4361] Loss_D: 0.07366658 (Loss_D_real: 0.01624930 Loss_D_fake: 0.05741727) Loss_G: 0.20290394 Loss_Enh_Dec: -0.73678714\n",
      "| epoch  23 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  4.48 | ppl    88.18 | acc     0.47 | train_ae_norm     1.00\n",
      "[23/200][3499/4361] Loss_D: 0.07686626 (Loss_D_real: 0.03930934 Loss_D_fake: 0.03755692) Loss_G: 0.19410646 Loss_Enh_Dec: -0.58516276\n",
      "| epoch  23 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  4.38 | ppl    79.46 | acc     0.46 | train_ae_norm     1.00\n",
      "[23/200][3599/4361] Loss_D: 0.09173568 (Loss_D_real: 0.04177930 Loss_D_fake: 0.04995637) Loss_G: 0.20493484 Loss_Enh_Dec: -0.66253829\n",
      "| epoch  23 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  4.40 | ppl    81.07 | acc     0.48 | train_ae_norm     1.00\n",
      "[23/200][3699/4361] Loss_D: 0.07121073 (Loss_D_real: 0.03714109 Loss_D_fake: 0.03406964) Loss_G: 0.19190823 Loss_Enh_Dec: -0.25221977\n",
      "| epoch  23 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  4.38 | ppl    80.09 | acc     0.45 | train_ae_norm     1.00\n",
      "[23/200][3799/4361] Loss_D: 0.11118115 (Loss_D_real: 0.03970239 Loss_D_fake: 0.07147876) Loss_G: 0.20313087 Loss_Enh_Dec: -0.52005202\n",
      "| epoch  23 |  3800/ 4361 batches | lr 0.000000 | ms/batch 402.04 | loss  4.47 | ppl    87.04 | acc     0.49 | train_ae_norm     1.00\n",
      "[23/200][3899/4361] Loss_D: 0.09134246 (Loss_D_real: 0.03506955 Loss_D_fake: 0.05627292) Loss_G: 0.19327764 Loss_Enh_Dec: -0.66825598\n",
      "| epoch  23 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  4.43 | ppl    84.33 | acc     0.44 | train_ae_norm     1.00\n",
      "[23/200][3999/4361] Loss_D: 0.08013828 (Loss_D_real: 0.04872409 Loss_D_fake: 0.03141420) Loss_G: 0.20743363 Loss_Enh_Dec: -0.85545558\n",
      "| epoch  23 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  4.47 | ppl    87.16 | acc     0.47 | train_ae_norm     1.00\n",
      "[23/200][4099/4361] Loss_D: 0.05420849 (Loss_D_real: 0.03682439 Loss_D_fake: 0.01738410) Loss_G: 0.20256415 Loss_Enh_Dec: -0.77762413\n",
      "| epoch  23 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.84 | loss  4.35 | ppl    77.31 | acc     0.48 | train_ae_norm     1.00\n",
      "[23/200][4199/4361] Loss_D: 0.07305019 (Loss_D_real: 0.02785506 Loss_D_fake: 0.04519513) Loss_G: 0.19803725 Loss_Enh_Dec: -0.75385803\n",
      "| epoch  23 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.91 | loss  4.36 | ppl    78.33 | acc     0.47 | train_ae_norm     1.00\n",
      "[23/200][4299/4361] Loss_D: 0.09414442 (Loss_D_real: 0.05484243 Loss_D_fake: 0.03930199) Loss_G: 0.21208143 Loss_Enh_Dec: -0.51154280\n",
      "| epoch  23 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  4.34 | ppl    76.64 | acc     0.49 | train_ae_norm     1.00\n",
      "| end of epoch  23 | time: 1853.24s | test loss  4.02 | test ppl 55.58 | acc 0.556\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 24 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:57.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.703\n",
      "  Average training loss discriminator: 0.724\n",
      "  Training epcoh took: 0:01:27\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.475\n",
      "  Test Loss: 3.257\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  24 |     0/ 4361 batches | lr 0.000000 | ms/batch 868.64 | loss  0.04 | ppl     1.04 | acc     0.49 | train_ae_norm     1.00\n",
      "[24/200][99/4361] Loss_D: 0.05702145 (Loss_D_real: 0.02516286 Loss_D_fake: 0.03185860) Loss_G: 0.21668063 Loss_Enh_Dec: -0.75930101\n",
      "| epoch  24 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  4.35 | ppl    77.30 | acc     0.48 | train_ae_norm     1.00\n",
      "[24/200][199/4361] Loss_D: 0.06778296 (Loss_D_real: 0.04374805 Loss_D_fake: 0.02403492) Loss_G: 0.21230285 Loss_Enh_Dec: -0.71107960\n",
      "| epoch  24 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  4.48 | ppl    88.00 | acc     0.51 | train_ae_norm     1.00\n",
      "[24/200][299/4361] Loss_D: 0.04989176 (Loss_D_real: 0.02663363 Loss_D_fake: 0.02325813) Loss_G: 0.22568421 Loss_Enh_Dec: -0.62056267\n",
      "| epoch  24 |   300/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  4.44 | ppl    84.36 | acc     0.45 | train_ae_norm     1.00\n",
      "[24/200][399/4361] Loss_D: 0.08363502 (Loss_D_real: 0.02024627 Loss_D_fake: 0.06338875) Loss_G: 0.23626852 Loss_Enh_Dec: -0.84961569\n",
      "| epoch  24 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  4.32 | ppl    75.10 | acc     0.47 | train_ae_norm     1.00\n",
      "[24/200][499/4361] Loss_D: 0.04483528 (Loss_D_real: 0.02168035 Loss_D_fake: 0.02315493) Loss_G: 0.23597708 Loss_Enh_Dec: -0.57783395\n",
      "| epoch  24 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  4.39 | ppl    80.60 | acc     0.47 | train_ae_norm     1.00\n",
      "[24/200][599/4361] Loss_D: 0.04838311 (Loss_D_real: 0.01987539 Loss_D_fake: 0.02850772) Loss_G: 0.22825037 Loss_Enh_Dec: -0.63627601\n",
      "| epoch  24 |   600/ 4361 batches | lr 0.000000 | ms/batch 402.09 | loss  4.35 | ppl    77.09 | acc     0.42 | train_ae_norm     1.00\n",
      "[24/200][699/4361] Loss_D: 0.04725037 (Loss_D_real: 0.02599787 Loss_D_fake: 0.02125250) Loss_G: 0.22525261 Loss_Enh_Dec: -0.48224410\n",
      "| epoch  24 |   700/ 4361 batches | lr 0.000000 | ms/batch 402.39 | loss  4.39 | ppl    80.91 | acc     0.51 | train_ae_norm     1.00\n",
      "[24/200][799/4361] Loss_D: 0.04658600 (Loss_D_real: 0.02749975 Loss_D_fake: 0.01908625) Loss_G: 0.22414628 Loss_Enh_Dec: -0.47315320\n",
      "| epoch  24 |   800/ 4361 batches | lr 0.000000 | ms/batch 402.30 | loss  4.36 | ppl    78.01 | acc     0.51 | train_ae_norm     1.00\n",
      "[24/200][899/4361] Loss_D: 0.08989048 (Loss_D_real: 0.02233393 Loss_D_fake: 0.06755655) Loss_G: 0.23033054 Loss_Enh_Dec: -0.35010868\n",
      "| epoch  24 |   900/ 4361 batches | lr 0.000000 | ms/batch 402.06 | loss  4.33 | ppl    75.87 | acc     0.55 | train_ae_norm     1.00\n",
      "[24/200][999/4361] Loss_D: 0.06867896 (Loss_D_real: 0.03823909 Loss_D_fake: 0.03043987) Loss_G: 0.21843641 Loss_Enh_Dec: -0.78193909\n",
      "| epoch  24 |  1000/ 4361 batches | lr 0.000000 | ms/batch 402.21 | loss  4.31 | ppl    74.54 | acc     0.51 | train_ae_norm     1.00\n",
      "[24/200][1099/4361] Loss_D: 0.04299226 (Loss_D_real: 0.01873489 Loss_D_fake: 0.02425737) Loss_G: 0.23144856 Loss_Enh_Dec: -0.39778677\n",
      "| epoch  24 |  1100/ 4361 batches | lr 0.000000 | ms/batch 402.13 | loss  4.33 | ppl    75.61 | acc     0.46 | train_ae_norm     1.00\n",
      "[24/200][1199/4361] Loss_D: 0.04897144 (Loss_D_real: 0.02059676 Loss_D_fake: 0.02837468) Loss_G: 0.24414901 Loss_Enh_Dec: -0.49135032\n",
      "| epoch  24 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  4.33 | ppl    75.81 | acc     0.48 | train_ae_norm     1.00\n",
      "[24/200][1299/4361] Loss_D: 0.04938637 (Loss_D_real: 0.01030058 Loss_D_fake: 0.03908580) Loss_G: 0.22305922 Loss_Enh_Dec: -0.30953208\n",
      "| epoch  24 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  4.32 | ppl    75.42 | acc     0.49 | train_ae_norm     1.00\n",
      "[24/200][1399/4361] Loss_D: 0.08267074 (Loss_D_real: 0.05841980 Loss_D_fake: 0.02425094) Loss_G: 0.20938054 Loss_Enh_Dec: -0.49604613\n",
      "| epoch  24 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  4.31 | ppl    74.66 | acc     0.44 | train_ae_norm     1.00\n",
      "[24/200][1499/4361] Loss_D: 0.07098645 (Loss_D_real: 0.05143109 Loss_D_fake: 0.01955536) Loss_G: 0.21824388 Loss_Enh_Dec: -0.70179510\n",
      "| epoch  24 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  4.32 | ppl    75.41 | acc     0.49 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24/200][1599/4361] Loss_D: 0.05622664 (Loss_D_real: 0.02739789 Loss_D_fake: 0.02882875) Loss_G: 0.21118188 Loss_Enh_Dec: -0.59085387\n",
      "| epoch  24 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  4.31 | ppl    74.10 | acc     0.51 | train_ae_norm     1.00\n",
      "[24/200][1699/4361] Loss_D: 0.04776812 (Loss_D_real: 0.02360401 Loss_D_fake: 0.02416411) Loss_G: 0.23764150 Loss_Enh_Dec: -0.62949729\n",
      "| epoch  24 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  4.30 | ppl    73.77 | acc     0.50 | train_ae_norm     1.00\n",
      "[24/200][1799/4361] Loss_D: 0.09393361 (Loss_D_real: 0.06492414 Loss_D_fake: 0.02900947) Loss_G: 0.22838926 Loss_Enh_Dec: -0.58316910\n",
      "| epoch  24 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  4.25 | ppl    70.27 | acc     0.49 | train_ae_norm     1.00\n",
      "[24/200][1899/4361] Loss_D: 0.03831032 (Loss_D_real: 0.01761284 Loss_D_fake: 0.02069747) Loss_G: 0.21990283 Loss_Enh_Dec: -0.51067019\n",
      "| epoch  24 |  1900/ 4361 batches | lr 0.000000 | ms/batch 402.07 | loss  4.30 | ppl    73.95 | acc     0.50 | train_ae_norm     1.00\n",
      "[24/200][1999/4361] Loss_D: 0.02700951 (Loss_D_real: 0.00961954 Loss_D_fake: 0.01738997) Loss_G: 0.23650952 Loss_Enh_Dec: -0.60935241\n",
      "| epoch  24 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  4.26 | ppl    70.94 | acc     0.50 | train_ae_norm     1.00\n",
      "[24/200][2099/4361] Loss_D: 0.04268483 (Loss_D_real: 0.00689814 Loss_D_fake: 0.03578669) Loss_G: 0.21630637 Loss_Enh_Dec: -0.63210636\n",
      "| epoch  24 |  2100/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  4.28 | ppl    71.98 | acc     0.52 | train_ae_norm     1.00\n",
      "[24/200][2199/4361] Loss_D: 0.10862581 (Loss_D_real: 0.07519833 Loss_D_fake: 0.03342749) Loss_G: 0.19316000 Loss_Enh_Dec: -0.53276312\n",
      "| epoch  24 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  4.25 | ppl    70.38 | acc     0.48 | train_ae_norm     1.00\n",
      "[24/200][2299/4361] Loss_D: 0.19440508 (Loss_D_real: 0.12215125 Loss_D_fake: 0.07225384) Loss_G: 0.19214968 Loss_Enh_Dec: -0.95476341\n",
      "| epoch  24 |  2300/ 4361 batches | lr 0.000000 | ms/batch 402.32 | loss  4.25 | ppl    69.77 | acc     0.52 | train_ae_norm     1.00\n",
      "[24/200][2399/4361] Loss_D: 0.10228329 (Loss_D_real: 0.05121269 Loss_D_fake: 0.05107060) Loss_G: 0.17674243 Loss_Enh_Dec: -0.74920142\n",
      "| epoch  24 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.94 | loss  4.24 | ppl    69.17 | acc     0.49 | train_ae_norm     1.00\n",
      "[24/200][2499/4361] Loss_D: 0.09065451 (Loss_D_real: 0.03227795 Loss_D_fake: 0.05837657) Loss_G: 0.18256532 Loss_Enh_Dec: -0.80575603\n",
      "| epoch  24 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  4.26 | ppl    70.96 | acc     0.52 | train_ae_norm     1.00\n",
      "[24/200][2599/4361] Loss_D: 0.10967121 (Loss_D_real: 0.03911932 Loss_D_fake: 0.07055189) Loss_G: 0.19320178 Loss_Enh_Dec: -0.99110281\n",
      "| epoch  24 |  2600/ 4361 batches | lr 0.000000 | ms/batch 402.47 | loss  4.25 | ppl    70.35 | acc     0.48 | train_ae_norm     1.00\n",
      "[24/200][2699/4361] Loss_D: 0.13465837 (Loss_D_real: 0.10783359 Loss_D_fake: 0.02682477) Loss_G: 0.20077717 Loss_Enh_Dec: -0.83294982\n",
      "| epoch  24 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  4.30 | ppl    73.40 | acc     0.48 | train_ae_norm     1.00\n",
      "[24/200][2799/4361] Loss_D: 0.18423307 (Loss_D_real: 0.15219957 Loss_D_fake: 0.03203350) Loss_G: 0.19053872 Loss_Enh_Dec: -0.74494421\n",
      "| epoch  24 |  2800/ 4361 batches | lr 0.000000 | ms/batch 402.05 | loss  4.27 | ppl    71.45 | acc     0.47 | train_ae_norm     1.00\n",
      "[24/200][2899/4361] Loss_D: 0.07615524 (Loss_D_real: 0.01893811 Loss_D_fake: 0.05721713) Loss_G: 0.20030153 Loss_Enh_Dec: -0.86889690\n",
      "| epoch  24 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  4.36 | ppl    78.28 | acc     0.49 | train_ae_norm     1.00\n",
      "[24/200][2999/4361] Loss_D: 0.09231724 (Loss_D_real: 0.06909320 Loss_D_fake: 0.02322404) Loss_G: 0.20031781 Loss_Enh_Dec: -1.00723231\n",
      "| epoch  24 |  3000/ 4361 batches | lr 0.000000 | ms/batch 402.06 | loss  4.36 | ppl    78.38 | acc     0.47 | train_ae_norm     1.00\n",
      "[24/200][3099/4361] Loss_D: 0.14210099 (Loss_D_real: 0.07219809 Loss_D_fake: 0.06990290) Loss_G: 0.20031460 Loss_Enh_Dec: -0.80058020\n",
      "| epoch  24 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  4.35 | ppl    77.77 | acc     0.47 | train_ae_norm     1.00\n",
      "[24/200][3199/4361] Loss_D: 0.03668673 (Loss_D_real: 0.00943939 Loss_D_fake: 0.02724734) Loss_G: 0.23223542 Loss_Enh_Dec: -1.00966167\n",
      "| epoch  24 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  4.37 | ppl    78.95 | acc     0.46 | train_ae_norm     1.00\n",
      "[24/200][3299/4361] Loss_D: 0.05643634 (Loss_D_real: 0.01851283 Loss_D_fake: 0.03792351) Loss_G: 0.20745638 Loss_Enh_Dec: -0.82049292\n",
      "| epoch  24 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  4.36 | ppl    78.37 | acc     0.48 | train_ae_norm     1.00\n",
      "[24/200][3399/4361] Loss_D: 0.05237470 (Loss_D_real: 0.02307683 Loss_D_fake: 0.02929787) Loss_G: 0.23325090 Loss_Enh_Dec: -0.98149031\n",
      "| epoch  24 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  4.30 | ppl    73.37 | acc     0.50 | train_ae_norm     1.00\n",
      "[24/200][3499/4361] Loss_D: 0.08172914 (Loss_D_real: 0.04087343 Loss_D_fake: 0.04085571) Loss_G: 0.21083049 Loss_Enh_Dec: -0.87850589\n",
      "| epoch  24 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  4.28 | ppl    71.92 | acc     0.49 | train_ae_norm     1.00\n",
      "[24/200][3599/4361] Loss_D: 0.06442341 (Loss_D_real: 0.03593085 Loss_D_fake: 0.02849256) Loss_G: 0.22765164 Loss_Enh_Dec: -0.94039744\n",
      "| epoch  24 |  3600/ 4361 batches | lr 0.000000 | ms/batch 400.58 | loss  4.30 | ppl    73.68 | acc     0.46 | train_ae_norm     1.00\n",
      "[24/200][3699/4361] Loss_D: 0.08015034 (Loss_D_real: 0.05452247 Loss_D_fake: 0.02562788) Loss_G: 0.22258118 Loss_Enh_Dec: -1.02632380\n",
      "| epoch  24 |  3700/ 4361 batches | lr 0.000000 | ms/batch 402.09 | loss  4.26 | ppl    70.98 | acc     0.48 | train_ae_norm     1.00\n",
      "[24/200][3799/4361] Loss_D: 0.05576992 (Loss_D_real: 0.01405710 Loss_D_fake: 0.04171282) Loss_G: 0.20189185 Loss_Enh_Dec: -0.86002207\n",
      "| epoch  24 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  4.33 | ppl    75.76 | acc     0.47 | train_ae_norm     1.00\n",
      "[24/200][3899/4361] Loss_D: 0.10825007 (Loss_D_real: 0.06335285 Loss_D_fake: 0.04489721) Loss_G: 0.21014200 Loss_Enh_Dec: -1.14631462\n",
      "| epoch  24 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  4.32 | ppl    75.00 | acc     0.44 | train_ae_norm     1.00\n",
      "[24/200][3999/4361] Loss_D: 0.14816549 (Loss_D_real: 0.05745324 Loss_D_fake: 0.09071225) Loss_G: 0.19816984 Loss_Enh_Dec: -0.83116001\n",
      "| epoch  24 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  4.35 | ppl    77.57 | acc     0.49 | train_ae_norm     1.00\n",
      "[24/200][4099/4361] Loss_D: 0.07265282 (Loss_D_real: 0.04497265 Loss_D_fake: 0.02768016) Loss_G: 0.21146980 Loss_Enh_Dec: -0.92417568\n",
      "| epoch  24 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.82 | loss  4.28 | ppl    72.36 | acc     0.49 | train_ae_norm     1.00\n",
      "[24/200][4199/4361] Loss_D: 0.18557039 (Loss_D_real: 0.16520971 Loss_D_fake: 0.02036067) Loss_G: 0.19667344 Loss_Enh_Dec: -0.75731152\n",
      "| epoch  24 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  4.29 | ppl    73.33 | acc     0.52 | train_ae_norm     1.00\n",
      "[24/200][4299/4361] Loss_D: 0.07027538 (Loss_D_real: 0.02793968 Loss_D_fake: 0.04233570) Loss_G: 0.23121074 Loss_Enh_Dec: -0.88279516\n",
      "| epoch  24 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.56 | loss  4.28 | ppl    72.05 | acc     0.48 | train_ae_norm     1.00\n",
      "| end of epoch  24 | time: 1854.55s | test loss  3.97 | test ppl 53.07 | acc 0.561\n",
      "bleu_self:  [4.16666666e-01 3.59264093e-01 1.04072936e-01 1.95698507e-05\n",
      " 9.20705837e-07]\n",
      "bleu_test:  [8.81249999e-01 4.82661034e-01 1.97217948e-01 3.86673050e-05\n",
      " 3.02734528e-06]\n",
      "bleu_self: [0.41666667,0.35926409,0.10407294,0.00001957,0.00000092]\n",
      "bleu_test: [0.88125000,0.48266103,0.19721795,0.00003867,0.00000303]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 25 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:18.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.729\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.450\n",
      "  Test Loss: 3.367\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  25 |     0/ 4361 batches | lr 0.000000 | ms/batch 859.27 | loss  0.04 | ppl     1.04 | acc     0.53 | train_ae_norm     1.00\n",
      "[25/200][99/4361] Loss_D: 0.09248419 (Loss_D_real: 0.03730284 Loss_D_fake: 0.05518135) Loss_G: 0.20734175 Loss_Enh_Dec: -0.81178325\n",
      "| epoch  25 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  4.31 | ppl    74.34 | acc     0.49 | train_ae_norm     1.00\n",
      "[25/200][199/4361] Loss_D: 0.07193512 (Loss_D_real: 0.01433256 Loss_D_fake: 0.05760257) Loss_G: 0.22391330 Loss_Enh_Dec: -0.78377992\n",
      "| epoch  25 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  4.30 | ppl    73.98 | acc     0.54 | train_ae_norm     1.00\n",
      "[25/200][299/4361] Loss_D: 0.09520550 (Loss_D_real: 0.04334426 Loss_D_fake: 0.05186124) Loss_G: 0.22058435 Loss_Enh_Dec: -0.78474933\n",
      "| epoch  25 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.53 | loss  4.28 | ppl    72.20 | acc     0.48 | train_ae_norm     1.00\n",
      "[25/200][399/4361] Loss_D: 0.08073239 (Loss_D_real: 0.03104309 Loss_D_fake: 0.04968930) Loss_G: 0.22395034 Loss_Enh_Dec: -0.71407294\n",
      "| epoch  25 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.99 | loss  4.18 | ppl    65.52 | acc     0.51 | train_ae_norm     1.00\n",
      "[25/200][499/4361] Loss_D: 0.04794039 (Loss_D_real: 0.02555626 Loss_D_fake: 0.02238413) Loss_G: 0.21633048 Loss_Enh_Dec: -0.93887538\n",
      "| epoch  25 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  4.25 | ppl    70.01 | acc     0.53 | train_ae_norm     1.00\n",
      "[25/200][599/4361] Loss_D: 0.06036922 (Loss_D_real: 0.03344787 Loss_D_fake: 0.02692135) Loss_G: 0.22664021 Loss_Enh_Dec: -0.92771053\n",
      "| epoch  25 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  4.19 | ppl    65.74 | acc     0.47 | train_ae_norm     1.00\n",
      "[25/200][699/4361] Loss_D: 0.11143811 (Loss_D_real: 0.07973382 Loss_D_fake: 0.03170429) Loss_G: 0.21837281 Loss_Enh_Dec: -0.83282894\n",
      "| epoch  25 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  4.24 | ppl    69.53 | acc     0.53 | train_ae_norm     1.00\n",
      "[25/200][799/4361] Loss_D: 0.04574513 (Loss_D_real: 0.01943357 Loss_D_fake: 0.02631157) Loss_G: 0.21321869 Loss_Enh_Dec: -0.81277198\n",
      "| epoch  25 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.67 | loss  4.20 | ppl    66.87 | acc     0.49 | train_ae_norm     1.00\n",
      "[25/200][899/4361] Loss_D: 0.08281063 (Loss_D_real: 0.05749846 Loss_D_fake: 0.02531217) Loss_G: 0.23352793 Loss_Enh_Dec: -0.85667342\n",
      "| epoch  25 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.24 | loss  4.24 | ppl    69.50 | acc     0.54 | train_ae_norm     1.00\n",
      "[25/200][999/4361] Loss_D: 0.08034171 (Loss_D_real: 0.05604088 Loss_D_fake: 0.02430082) Loss_G: 0.19320650 Loss_Enh_Dec: -0.85508937\n",
      "| epoch  25 |  1000/ 4361 batches | lr 0.000000 | ms/batch 400.49 | loss  4.29 | ppl    72.82 | acc     0.49 | train_ae_norm     1.00\n",
      "[25/200][1099/4361] Loss_D: 0.09142824 (Loss_D_real: 0.05622645 Loss_D_fake: 0.03520178) Loss_G: 0.20014007 Loss_Enh_Dec: -0.78554958\n",
      "| epoch  25 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.63 | loss  4.30 | ppl    73.48 | acc     0.47 | train_ae_norm     1.00\n",
      "[25/200][1199/4361] Loss_D: 0.07364690 (Loss_D_real: 0.01873817 Loss_D_fake: 0.05490873) Loss_G: 0.18859820 Loss_Enh_Dec: -0.75333250\n",
      "| epoch  25 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.63 | loss  4.34 | ppl    76.44 | acc     0.49 | train_ae_norm     1.00\n",
      "[25/200][1299/4361] Loss_D: 0.06345672 (Loss_D_real: 0.02916638 Loss_D_fake: 0.03429034) Loss_G: 0.21295503 Loss_Enh_Dec: -0.95024127\n",
      "| epoch  25 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.81 | loss  4.34 | ppl    76.36 | acc     0.46 | train_ae_norm     1.00\n",
      "[25/200][1399/4361] Loss_D: 0.10880526 (Loss_D_real: 0.04487064 Loss_D_fake: 0.06393462) Loss_G: 0.20783101 Loss_Enh_Dec: -1.01926482\n",
      "| epoch  25 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.15 | loss  4.37 | ppl    78.89 | acc     0.43 | train_ae_norm     1.00\n",
      "[25/200][1499/4361] Loss_D: 0.07149165 (Loss_D_real: 0.00810020 Loss_D_fake: 0.06339145) Loss_G: 0.21633735 Loss_Enh_Dec: -0.95111960\n",
      "| epoch  25 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  4.37 | ppl    79.14 | acc     0.45 | train_ae_norm     1.00\n",
      "[25/200][1599/4361] Loss_D: 0.04110099 (Loss_D_real: 0.01295860 Loss_D_fake: 0.02814239) Loss_G: 0.23356418 Loss_Enh_Dec: -0.53998721\n",
      "| epoch  25 |  1600/ 4361 batches | lr 0.000000 | ms/batch 400.80 | loss  4.33 | ppl    76.11 | acc     0.49 | train_ae_norm     1.00\n",
      "[25/200][1699/4361] Loss_D: 0.03989000 (Loss_D_real: 0.01973121 Loss_D_fake: 0.02015879) Loss_G: 0.22506033 Loss_Enh_Dec: -0.72994399\n",
      "| epoch  25 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.92 | loss  4.29 | ppl    72.65 | acc     0.47 | train_ae_norm     1.00\n",
      "[25/200][1799/4361] Loss_D: 0.07519925 (Loss_D_real: 0.02453617 Loss_D_fake: 0.05066308) Loss_G: 0.22622418 Loss_Enh_Dec: -0.95531493\n",
      "| epoch  25 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.48 | loss  4.29 | ppl    72.74 | acc     0.49 | train_ae_norm     1.00\n",
      "[25/200][1899/4361] Loss_D: 0.09057653 (Loss_D_real: 0.04561129 Loss_D_fake: 0.04496524) Loss_G: 0.19693503 Loss_Enh_Dec: -0.93566245\n",
      "| epoch  25 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.85 | loss  4.34 | ppl    76.79 | acc     0.49 | train_ae_norm     1.00\n",
      "[25/200][1999/4361] Loss_D: 0.08953802 (Loss_D_real: 0.02291007 Loss_D_fake: 0.06662796) Loss_G: 0.21405123 Loss_Enh_Dec: -0.62087458\n",
      "| epoch  25 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  4.30 | ppl    73.45 | acc     0.51 | train_ae_norm     1.00\n",
      "[25/200][2099/4361] Loss_D: 0.10903975 (Loss_D_real: 0.07369781 Loss_D_fake: 0.03534194) Loss_G: 0.22763930 Loss_Enh_Dec: -0.91941369\n",
      "| epoch  25 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.05 | loss  4.33 | ppl    76.31 | acc     0.53 | train_ae_norm     1.00\n",
      "[25/200][2199/4361] Loss_D: 0.03885899 (Loss_D_real: 0.00686862 Loss_D_fake: 0.03199037) Loss_G: 0.23587024 Loss_Enh_Dec: -1.15913951\n",
      "| epoch  25 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  4.31 | ppl    74.39 | acc     0.48 | train_ae_norm     1.00\n",
      "[25/200][2299/4361] Loss_D: 0.04923815 (Loss_D_real: 0.02947792 Loss_D_fake: 0.01976023) Loss_G: 0.24498966 Loss_Enh_Dec: -0.78386956\n",
      "| epoch  25 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  4.34 | ppl    76.92 | acc     0.52 | train_ae_norm     1.00\n",
      "[25/200][2399/4361] Loss_D: 0.03763835 (Loss_D_real: 0.01476706 Loss_D_fake: 0.02287128) Loss_G: 0.22640991 Loss_Enh_Dec: -1.02454853\n",
      "| epoch  25 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  4.33 | ppl    75.78 | acc     0.46 | train_ae_norm     1.00\n",
      "[25/200][2499/4361] Loss_D: 0.07025905 (Loss_D_real: 0.02291976 Loss_D_fake: 0.04733929) Loss_G: 0.21847634 Loss_Enh_Dec: -0.90601873\n",
      "| epoch  25 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  4.38 | ppl    79.96 | acc     0.51 | train_ae_norm     1.00\n",
      "[25/200][2599/4361] Loss_D: 0.14338377 (Loss_D_real: 0.10802575 Loss_D_fake: 0.03535803) Loss_G: 0.24174337 Loss_Enh_Dec: -0.81224221\n",
      "| epoch  25 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  4.34 | ppl    77.03 | acc     0.48 | train_ae_norm     1.00\n",
      "[25/200][2699/4361] Loss_D: 0.03739044 (Loss_D_real: 0.01439321 Loss_D_fake: 0.02299723) Loss_G: 0.23005329 Loss_Enh_Dec: -0.66040671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  25 |  2700/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  4.39 | ppl    80.88 | acc     0.44 | train_ae_norm     1.00\n",
      "[25/200][2799/4361] Loss_D: 0.05432684 (Loss_D_real: 0.02887665 Loss_D_fake: 0.02545018) Loss_G: 0.25264955 Loss_Enh_Dec: -0.68801516\n",
      "| epoch  25 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  4.33 | ppl    75.59 | acc     0.48 | train_ae_norm     1.00\n",
      "[25/200][2899/4361] Loss_D: 0.04346242 (Loss_D_real: 0.01396716 Loss_D_fake: 0.02949526) Loss_G: 0.22967958 Loss_Enh_Dec: -0.86194795\n",
      "| epoch  25 |  2900/ 4361 batches | lr 0.000000 | ms/batch 402.16 | loss  4.37 | ppl    79.31 | acc     0.48 | train_ae_norm     1.00\n",
      "[25/200][2999/4361] Loss_D: 0.04618986 (Loss_D_real: 0.02403639 Loss_D_fake: 0.02215347) Loss_G: 0.23966442 Loss_Enh_Dec: -0.89202636\n",
      "| epoch  25 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  4.33 | ppl    75.96 | acc     0.48 | train_ae_norm     1.00\n",
      "[25/200][3099/4361] Loss_D: 0.03572028 (Loss_D_real: 0.01479409 Loss_D_fake: 0.02092619) Loss_G: 0.25558707 Loss_Enh_Dec: -0.81031430\n",
      "| epoch  25 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.66 | loss  4.34 | ppl    76.52 | acc     0.47 | train_ae_norm     1.00\n",
      "[25/200][3199/4361] Loss_D: 0.04178442 (Loss_D_real: 0.01087541 Loss_D_fake: 0.03090900) Loss_G: 0.22299068 Loss_Enh_Dec: -1.13430929\n",
      "| epoch  25 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.52 | loss  4.35 | ppl    77.16 | acc     0.45 | train_ae_norm     1.00\n",
      "[25/200][3299/4361] Loss_D: 0.03801304 (Loss_D_real: 0.01315541 Loss_D_fake: 0.02485763) Loss_G: 0.23204580 Loss_Enh_Dec: -1.01017094\n",
      "| epoch  25 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  4.36 | ppl    78.29 | acc     0.47 | train_ae_norm     1.00\n",
      "[25/200][3399/4361] Loss_D: 0.02665010 (Loss_D_real: 0.00863518 Loss_D_fake: 0.01801492) Loss_G: 0.25912821 Loss_Enh_Dec: -0.82281172\n",
      "| epoch  25 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.79 | loss  4.34 | ppl    76.85 | acc     0.49 | train_ae_norm     1.00\n",
      "[25/200][3499/4361] Loss_D: 0.02647361 (Loss_D_real: 0.00977102 Loss_D_fake: 0.01670259) Loss_G: 0.25435480 Loss_Enh_Dec: -1.21370542\n",
      "| epoch  25 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.91 | loss  4.26 | ppl    71.06 | acc     0.49 | train_ae_norm     1.00\n",
      "[25/200][3599/4361] Loss_D: 0.02004343 (Loss_D_real: 0.00989269 Loss_D_fake: 0.01015074) Loss_G: 0.24227925 Loss_Enh_Dec: -0.98953229\n",
      "| epoch  25 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  4.27 | ppl    71.72 | acc     0.48 | train_ae_norm     1.00\n",
      "[25/200][3699/4361] Loss_D: 0.06804505 (Loss_D_real: 0.05037993 Loss_D_fake: 0.01766512) Loss_G: 0.25668898 Loss_Enh_Dec: -1.13642001\n",
      "| epoch  25 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.63 | loss  4.28 | ppl    72.33 | acc     0.46 | train_ae_norm     1.00\n",
      "[25/200][3799/4361] Loss_D: 0.02092700 (Loss_D_real: 0.01304295 Loss_D_fake: 0.00788405) Loss_G: 0.28966206 Loss_Enh_Dec: -0.95214510\n",
      "| epoch  25 |  3800/ 4361 batches | lr 0.000000 | ms/batch 400.54 | loss  4.46 | ppl    86.83 | acc     0.54 | train_ae_norm     1.00\n",
      "[25/200][3899/4361] Loss_D: 0.07820429 (Loss_D_real: 0.04146277 Loss_D_fake: 0.03674152) Loss_G: 0.25409508 Loss_Enh_Dec: -0.89960015\n",
      "| epoch  25 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  4.31 | ppl    74.72 | acc     0.48 | train_ae_norm     1.00\n",
      "[25/200][3999/4361] Loss_D: 0.01862129 (Loss_D_real: 0.00913313 Loss_D_fake: 0.00948817) Loss_G: 0.25005004 Loss_Enh_Dec: -1.07650054\n",
      "| epoch  25 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  4.30 | ppl    73.90 | acc     0.49 | train_ae_norm     1.00\n",
      "[25/200][4099/4361] Loss_D: 0.10984927 (Loss_D_real: 0.07471512 Loss_D_fake: 0.03513415) Loss_G: 0.25743696 Loss_Enh_Dec: -0.98834896\n",
      "| epoch  25 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  4.22 | ppl    68.15 | acc     0.49 | train_ae_norm     1.00\n",
      "[25/200][4199/4361] Loss_D: 0.02223330 (Loss_D_real: 0.00897706 Loss_D_fake: 0.01325623) Loss_G: 0.26619178 Loss_Enh_Dec: -0.97376233\n",
      "| epoch  25 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  4.24 | ppl    69.32 | acc     0.52 | train_ae_norm     1.00\n",
      "[25/200][4299/4361] Loss_D: 0.06552006 (Loss_D_real: 0.03174182 Loss_D_fake: 0.03377824) Loss_G: 0.23987091 Loss_Enh_Dec: -1.00146556\n",
      "| epoch  25 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  4.21 | ppl    67.38 | acc     0.51 | train_ae_norm     1.00\n",
      "| end of epoch  25 | time: 1850.92s | test loss  3.98 | test ppl 53.75 | acc 0.564\n",
      "bleu_self:  [3.24900794e-01 6.50291492e-09 1.87454555e-11 1.05520868e-12\n",
      " 1.96565863e-13]\n",
      "bleu_test:  [7.27430555e-01 3.05237208e-01 2.18129970e-06 6.13241650e-09\n",
      " 1.89397358e-10]\n",
      "bleu_self: [0.32490079,0.00000001,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.72743056,0.30523721,0.00000218,0.00000001,0.00000000]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 26 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.724\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.463\n",
      "  Test Loss: 3.400\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  26 |     0/ 4361 batches | lr 0.000000 | ms/batch 865.44 | loss  0.04 | ppl     1.04 | acc     0.53 | train_ae_norm     1.00\n",
      "[26/200][99/4361] Loss_D: 0.05225914 (Loss_D_real: 0.01926310 Loss_D_fake: 0.03299604) Loss_G: 0.25951216 Loss_Enh_Dec: -0.80782270\n",
      "| epoch  26 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  4.22 | ppl    67.78 | acc     0.50 | train_ae_norm     1.00\n",
      "[26/200][199/4361] Loss_D: 0.03236687 (Loss_D_real: 0.01998190 Loss_D_fake: 0.01238497) Loss_G: 0.25727296 Loss_Enh_Dec: -0.82778531\n",
      "| epoch  26 |   200/ 4361 batches | lr 0.000000 | ms/batch 400.60 | loss  4.26 | ppl    70.90 | acc     0.53 | train_ae_norm     1.00\n",
      "[26/200][299/4361] Loss_D: 0.04027843 (Loss_D_real: 0.02292946 Loss_D_fake: 0.01734897) Loss_G: 0.25998753 Loss_Enh_Dec: -1.22051167\n",
      "| epoch  26 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  4.23 | ppl    68.79 | acc     0.49 | train_ae_norm     1.00\n",
      "[26/200][399/4361] Loss_D: 0.02568470 (Loss_D_real: 0.01160137 Loss_D_fake: 0.01408333) Loss_G: 0.23315416 Loss_Enh_Dec: -1.02781022\n",
      "| epoch  26 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  4.16 | ppl    63.97 | acc     0.51 | train_ae_norm     1.00\n",
      "[26/200][499/4361] Loss_D: 0.02090221 (Loss_D_real: 0.00462881 Loss_D_fake: 0.01627340) Loss_G: 0.26435873 Loss_Enh_Dec: -1.07940221\n",
      "| epoch  26 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  4.26 | ppl    70.70 | acc     0.52 | train_ae_norm     1.00\n",
      "[26/200][599/4361] Loss_D: 0.03056756 (Loss_D_real: 0.01657152 Loss_D_fake: 0.01399604) Loss_G: 0.26102951 Loss_Enh_Dec: -1.07708263\n",
      "| epoch  26 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  4.22 | ppl    67.84 | acc     0.44 | train_ae_norm     1.00\n",
      "[26/200][699/4361] Loss_D: 0.02807657 (Loss_D_real: 0.00776628 Loss_D_fake: 0.02031030) Loss_G: 0.26002315 Loss_Enh_Dec: -1.28714406\n",
      "| epoch  26 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  4.30 | ppl    73.80 | acc     0.53 | train_ae_norm     1.00\n",
      "[26/200][799/4361] Loss_D: 0.08253386 (Loss_D_real: 0.06897601 Loss_D_fake: 0.01355785) Loss_G: 0.24775878 Loss_Enh_Dec: -0.88572830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  26 |   800/ 4361 batches | lr 0.000000 | ms/batch 402.19 | loss  4.29 | ppl    72.85 | acc     0.49 | train_ae_norm     1.00\n",
      "[26/200][899/4361] Loss_D: 0.05296314 (Loss_D_real: 0.01960831 Loss_D_fake: 0.03335483) Loss_G: 0.25861013 Loss_Enh_Dec: -0.75757599\n",
      "| epoch  26 |   900/ 4361 batches | lr 0.000000 | ms/batch 402.00 | loss  4.30 | ppl    73.91 | acc     0.54 | train_ae_norm     1.00\n",
      "[26/200][999/4361] Loss_D: 0.03564500 (Loss_D_real: 0.00939846 Loss_D_fake: 0.02624653) Loss_G: 0.23935771 Loss_Enh_Dec: -0.99226838\n",
      "| epoch  26 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  4.31 | ppl    74.58 | acc     0.49 | train_ae_norm     1.00\n",
      "[26/200][1099/4361] Loss_D: 0.06013688 (Loss_D_real: 0.04236945 Loss_D_fake: 0.01776742) Loss_G: 0.25438029 Loss_Enh_Dec: -0.92202377\n",
      "| epoch  26 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  4.29 | ppl    72.77 | acc     0.49 | train_ae_norm     1.00\n",
      "[26/200][1199/4361] Loss_D: 0.02907197 (Loss_D_real: 0.01530122 Loss_D_fake: 0.01377075) Loss_G: 0.26181668 Loss_Enh_Dec: -0.97481757\n",
      "| epoch  26 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  4.28 | ppl    72.45 | acc     0.52 | train_ae_norm     1.00\n",
      "[26/200][1299/4361] Loss_D: 0.06997114 (Loss_D_real: 0.05421411 Loss_D_fake: 0.01575704) Loss_G: 0.27813825 Loss_Enh_Dec: -1.01461768\n",
      "| epoch  26 |  1300/ 4361 batches | lr 0.000000 | ms/batch 402.25 | loss  4.26 | ppl    71.10 | acc     0.51 | train_ae_norm     1.00\n",
      "[26/200][1399/4361] Loss_D: 0.03946753 (Loss_D_real: 0.01962335 Loss_D_fake: 0.01984418) Loss_G: 0.25317311 Loss_Enh_Dec: -0.97439903\n",
      "| epoch  26 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  4.25 | ppl    70.03 | acc     0.44 | train_ae_norm     1.00\n",
      "[26/200][1499/4361] Loss_D: 0.07839418 (Loss_D_real: 0.05234218 Loss_D_fake: 0.02605200) Loss_G: 0.26111335 Loss_Enh_Dec: -1.09540617\n",
      "| epoch  26 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  4.27 | ppl    71.78 | acc     0.47 | train_ae_norm     1.00\n",
      "[26/200][1599/4361] Loss_D: 0.01911464 (Loss_D_real: 0.00709632 Loss_D_fake: 0.01201831) Loss_G: 0.26224431 Loss_Enh_Dec: -1.05107713\n",
      "| epoch  26 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  4.23 | ppl    68.66 | acc     0.55 | train_ae_norm     1.00\n",
      "[26/200][1699/4361] Loss_D: 0.02286961 (Loss_D_real: 0.00820046 Loss_D_fake: 0.01466915) Loss_G: 0.25797483 Loss_Enh_Dec: -1.06012368\n",
      "| epoch  26 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  4.22 | ppl    67.99 | acc     0.47 | train_ae_norm     1.00\n",
      "[26/200][1799/4361] Loss_D: 0.01611511 (Loss_D_real: 0.00686729 Loss_D_fake: 0.00924782) Loss_G: 0.28951403 Loss_Enh_Dec: -0.91156256\n",
      "| epoch  26 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  4.20 | ppl    66.88 | acc     0.51 | train_ae_norm     1.00\n",
      "[26/200][1899/4361] Loss_D: 0.05558120 (Loss_D_real: 0.04018765 Loss_D_fake: 0.01539355) Loss_G: 0.24526548 Loss_Enh_Dec: -1.06160891\n",
      "| epoch  26 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  4.31 | ppl    74.60 | acc     0.50 | train_ae_norm     1.00\n",
      "[26/200][1999/4361] Loss_D: 0.02115300 (Loss_D_real: 0.00802418 Loss_D_fake: 0.01312882) Loss_G: 0.26606950 Loss_Enh_Dec: -0.98305333\n",
      "| epoch  26 |  2000/ 4361 batches | lr 0.000000 | ms/batch 402.08 | loss  4.24 | ppl    69.18 | acc     0.50 | train_ae_norm     1.00\n",
      "[26/200][2099/4361] Loss_D: 0.02003621 (Loss_D_real: 0.00539274 Loss_D_fake: 0.01464347) Loss_G: 0.25497931 Loss_Enh_Dec: -0.84333915\n",
      "| epoch  26 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  4.26 | ppl    70.57 | acc     0.51 | train_ae_norm     1.00\n",
      "[26/200][2199/4361] Loss_D: 0.02856953 (Loss_D_real: 0.01468877 Loss_D_fake: 0.01388076) Loss_G: 0.24754934 Loss_Enh_Dec: -1.00226820\n",
      "| epoch  26 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  4.24 | ppl    69.25 | acc     0.49 | train_ae_norm     1.00\n",
      "[26/200][2299/4361] Loss_D: 0.02255648 (Loss_D_real: 0.00575875 Loss_D_fake: 0.01679773) Loss_G: 0.26948237 Loss_Enh_Dec: -0.71387571\n",
      "| epoch  26 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  4.21 | ppl    67.36 | acc     0.53 | train_ae_norm     1.00\n",
      "[26/200][2399/4361] Loss_D: 0.02313148 (Loss_D_real: 0.00362652 Loss_D_fake: 0.01950496) Loss_G: 0.24949284 Loss_Enh_Dec: -0.79981118\n",
      "| epoch  26 |  2400/ 4361 batches | lr 0.000000 | ms/batch 402.12 | loss  4.22 | ppl    68.14 | acc     0.45 | train_ae_norm     1.00\n",
      "[26/200][2499/4361] Loss_D: 0.02756865 (Loss_D_real: 0.00922180 Loss_D_fake: 0.01834684) Loss_G: 0.24980830 Loss_Enh_Dec: -1.04148221\n",
      "| epoch  26 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  4.27 | ppl    71.80 | acc     0.50 | train_ae_norm     1.00\n",
      "[26/200][2599/4361] Loss_D: 0.02169607 (Loss_D_real: 0.00525757 Loss_D_fake: 0.01643850) Loss_G: 0.26478481 Loss_Enh_Dec: -0.78247303\n",
      "| epoch  26 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  4.24 | ppl    69.15 | acc     0.49 | train_ae_norm     1.00\n",
      "[26/200][2699/4361] Loss_D: 0.01960254 (Loss_D_real: 0.00548642 Loss_D_fake: 0.01411611) Loss_G: 0.25619125 Loss_Enh_Dec: -0.50744659\n",
      "| epoch  26 |  2700/ 4361 batches | lr 0.000000 | ms/batch 402.31 | loss  4.29 | ppl    72.72 | acc     0.50 | train_ae_norm     1.00\n",
      "[26/200][2799/4361] Loss_D: 0.03624710 (Loss_D_real: 0.01894119 Loss_D_fake: 0.01730591) Loss_G: 0.28668791 Loss_Enh_Dec: -0.68223679\n",
      "| epoch  26 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  4.21 | ppl    67.40 | acc     0.47 | train_ae_norm     1.00\n",
      "[26/200][2899/4361] Loss_D: 0.03227924 (Loss_D_real: 0.02262212 Loss_D_fake: 0.00965712) Loss_G: 0.26418129 Loss_Enh_Dec: -0.97024745\n",
      "| epoch  26 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  4.22 | ppl    67.93 | acc     0.54 | train_ae_norm     1.00\n",
      "[26/200][2999/4361] Loss_D: 0.03033557 (Loss_D_real: 0.02045604 Loss_D_fake: 0.00987952) Loss_G: 0.27284011 Loss_Enh_Dec: -0.90656817\n",
      "| epoch  26 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  4.19 | ppl    66.32 | acc     0.51 | train_ae_norm     1.00\n",
      "[26/200][3099/4361] Loss_D: 0.02048589 (Loss_D_real: 0.00515938 Loss_D_fake: 0.01532651) Loss_G: 0.27694747 Loss_Enh_Dec: -0.98024714\n",
      "| epoch  26 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  4.20 | ppl    66.84 | acc     0.47 | train_ae_norm     1.00\n",
      "[26/200][3199/4361] Loss_D: 0.02107473 (Loss_D_real: 0.00933428 Loss_D_fake: 0.01174045) Loss_G: 0.27684772 Loss_Enh_Dec: -0.89315647\n",
      "| epoch  26 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  4.23 | ppl    68.60 | acc     0.47 | train_ae_norm     1.00\n",
      "[26/200][3299/4361] Loss_D: 0.01004827 (Loss_D_real: 0.00321577 Loss_D_fake: 0.00683250) Loss_G: 0.26345441 Loss_Enh_Dec: -1.16176224\n",
      "| epoch  26 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  4.29 | ppl    72.78 | acc     0.50 | train_ae_norm     1.00\n",
      "[26/200][3399/4361] Loss_D: 0.01426787 (Loss_D_real: 0.00503036 Loss_D_fake: 0.00923751) Loss_G: 0.25789353 Loss_Enh_Dec: -1.22462964\n",
      "| epoch  26 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  4.20 | ppl    66.51 | acc     0.55 | train_ae_norm     1.00\n",
      "[26/200][3499/4361] Loss_D: 0.01178397 (Loss_D_real: 0.00484985 Loss_D_fake: 0.00693412) Loss_G: 0.25961977 Loss_Enh_Dec: -1.00400341\n",
      "| epoch  26 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  4.14 | ppl    62.52 | acc     0.52 | train_ae_norm     1.00\n",
      "[26/200][3599/4361] Loss_D: 0.03188202 (Loss_D_real: 0.02160415 Loss_D_fake: 0.01027787) Loss_G: 0.28826410 Loss_Enh_Dec: -0.79626924\n",
      "| epoch  26 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  4.14 | ppl    62.77 | acc     0.51 | train_ae_norm     1.00\n",
      "[26/200][3699/4361] Loss_D: 0.02182300 (Loss_D_real: 0.00873050 Loss_D_fake: 0.01309250) Loss_G: 0.26211879 Loss_Enh_Dec: -1.05413425\n",
      "| epoch  26 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  4.13 | ppl    62.42 | acc     0.50 | train_ae_norm     1.00\n",
      "[26/200][3799/4361] Loss_D: 0.01530164 (Loss_D_real: 0.00739934 Loss_D_fake: 0.00790230) Loss_G: 0.26579079 Loss_Enh_Dec: -0.87351561\n",
      "| epoch  26 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  4.14 | ppl    62.93 | acc     0.57 | train_ae_norm     1.00\n",
      "[26/200][3899/4361] Loss_D: 0.05884670 (Loss_D_real: 0.03699583 Loss_D_fake: 0.02185087) Loss_G: 0.26372343 Loss_Enh_Dec: -1.30961931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  26 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  4.17 | ppl    64.66 | acc     0.48 | train_ae_norm     1.00\n",
      "[26/200][3999/4361] Loss_D: 0.02443878 (Loss_D_real: 0.01227204 Loss_D_fake: 0.01216674) Loss_G: 0.27853337 Loss_Enh_Dec: -1.02975571\n",
      "| epoch  26 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  4.19 | ppl    66.30 | acc     0.51 | train_ae_norm     1.00\n",
      "[26/200][4099/4361] Loss_D: 0.04734094 (Loss_D_real: 0.01469144 Loss_D_fake: 0.03264950) Loss_G: 0.28960887 Loss_Enh_Dec: -0.74920124\n",
      "| epoch  26 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  4.15 | ppl    63.56 | acc     0.48 | train_ae_norm     1.00\n",
      "[26/200][4199/4361] Loss_D: 0.01455241 (Loss_D_real: 0.00524452 Loss_D_fake: 0.00930789) Loss_G: 0.26726133 Loss_Enh_Dec: -0.58976835\n",
      "| epoch  26 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  4.19 | ppl    65.92 | acc     0.55 | train_ae_norm     1.00\n",
      "[26/200][4299/4361] Loss_D: 0.02007100 (Loss_D_real: 0.00991313 Loss_D_fake: 0.01015787) Loss_G: 0.26897803 Loss_Enh_Dec: -0.60886306\n",
      "| epoch  26 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  4.12 | ppl    61.48 | acc     0.54 | train_ae_norm     1.00\n",
      "| end of epoch  26 | time: 1853.25s | test loss  3.85 | test ppl 46.83 | acc 0.597\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 27 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.722\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.470\n",
      "  Test Loss: 3.457\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  27 |     0/ 4361 batches | lr 0.000000 | ms/batch 865.64 | loss  0.04 | ppl     1.04 | acc     0.54 | train_ae_norm     1.00\n",
      "[27/200][99/4361] Loss_D: 0.04451144 (Loss_D_real: 0.03362868 Loss_D_fake: 0.01088277) Loss_G: 0.26972085 Loss_Enh_Dec: -0.92539734\n",
      "| epoch  27 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  4.13 | ppl    62.23 | acc     0.51 | train_ae_norm     1.00\n",
      "[27/200][199/4361] Loss_D: 0.02177217 (Loss_D_real: 0.00744357 Loss_D_fake: 0.01432860) Loss_G: 0.26659307 Loss_Enh_Dec: -0.98653871\n",
      "| epoch  27 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  4.14 | ppl    62.87 | acc     0.55 | train_ae_norm     1.00\n",
      "[27/200][299/4361] Loss_D: 0.03641884 (Loss_D_real: 0.02642298 Loss_D_fake: 0.00999585) Loss_G: 0.25749052 Loss_Enh_Dec: -0.79346353\n",
      "| epoch  27 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  4.14 | ppl    62.69 | acc     0.52 | train_ae_norm     1.00\n",
      "[27/200][399/4361] Loss_D: 0.02165193 (Loss_D_real: 0.00773629 Loss_D_fake: 0.01391563) Loss_G: 0.25594351 Loss_Enh_Dec: -0.76515239\n",
      "| epoch  27 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  4.05 | ppl    57.23 | acc     0.53 | train_ae_norm     1.00\n",
      "[27/200][499/4361] Loss_D: 0.08183727 (Loss_D_real: 0.05398607 Loss_D_fake: 0.02785120) Loss_G: 0.24445319 Loss_Enh_Dec: -1.03949702\n",
      "| epoch  27 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  4.13 | ppl    62.18 | acc     0.57 | train_ae_norm     1.00\n",
      "[27/200][599/4361] Loss_D: 0.03389739 (Loss_D_real: 0.00308050 Loss_D_fake: 0.03081688) Loss_G: 0.27291161 Loss_Enh_Dec: -1.23464680\n",
      "| epoch  27 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.94 | loss  4.09 | ppl    60.03 | acc     0.49 | train_ae_norm     1.00\n",
      "[27/200][699/4361] Loss_D: 0.01515881 (Loss_D_real: 0.00240462 Loss_D_fake: 0.01275419) Loss_G: 0.26346549 Loss_Enh_Dec: -1.02976120\n",
      "| epoch  27 |   700/ 4361 batches | lr 0.000000 | ms/batch 402.19 | loss  4.16 | ppl    64.07 | acc     0.54 | train_ae_norm     1.00\n",
      "[27/200][799/4361] Loss_D: 0.04816391 (Loss_D_real: 0.02764842 Loss_D_fake: 0.02051549) Loss_G: 0.27929115 Loss_Enh_Dec: -0.99204105\n",
      "| epoch  27 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  4.10 | ppl    60.55 | acc     0.51 | train_ae_norm     1.00\n",
      "[27/200][899/4361] Loss_D: 0.01875712 (Loss_D_real: 0.00573153 Loss_D_fake: 0.01302559) Loss_G: 0.26893625 Loss_Enh_Dec: -1.11006808\n",
      "| epoch  27 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  4.14 | ppl    62.63 | acc     0.54 | train_ae_norm     1.00\n",
      "[27/200][999/4361] Loss_D: 0.02578905 (Loss_D_real: 0.01036714 Loss_D_fake: 0.01542191) Loss_G: 0.23398808 Loss_Enh_Dec: -1.09338367\n",
      "| epoch  27 |  1000/ 4361 batches | lr 0.000000 | ms/batch 402.27 | loss  4.12 | ppl    61.43 | acc     0.53 | train_ae_norm     1.00\n",
      "[27/200][1099/4361] Loss_D: 0.01711925 (Loss_D_real: 0.00534392 Loss_D_fake: 0.01177532) Loss_G: 0.27233049 Loss_Enh_Dec: -1.16053474\n",
      "| epoch  27 |  1100/ 4361 batches | lr 0.000000 | ms/batch 402.78 | loss  4.10 | ppl    60.35 | acc     0.51 | train_ae_norm     1.00\n",
      "[27/200][1199/4361] Loss_D: 0.03261939 (Loss_D_real: 0.02157070 Loss_D_fake: 0.01104869) Loss_G: 0.26525441 Loss_Enh_Dec: -0.95410079\n",
      "| epoch  27 |  1200/ 4361 batches | lr 0.000000 | ms/batch 402.05 | loss  4.14 | ppl    62.69 | acc     0.53 | train_ae_norm     1.00\n",
      "[27/200][1299/4361] Loss_D: 0.12711045 (Loss_D_real: 0.09847262 Loss_D_fake: 0.02863783) Loss_G: 0.26909944 Loss_Enh_Dec: -1.29869497\n",
      "| epoch  27 |  1300/ 4361 batches | lr 0.000000 | ms/batch 402.09 | loss  4.13 | ppl    62.34 | acc     0.51 | train_ae_norm     1.00\n",
      "[27/200][1399/4361] Loss_D: 0.02117346 (Loss_D_real: 0.00749010 Loss_D_fake: 0.01368336) Loss_G: 0.25161526 Loss_Enh_Dec: -0.92948341\n",
      "| epoch  27 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  4.10 | ppl    60.45 | acc     0.46 | train_ae_norm     1.00\n",
      "[27/200][1499/4361] Loss_D: 0.06477201 (Loss_D_real: 0.05584558 Loss_D_fake: 0.00892643) Loss_G: 0.27007061 Loss_Enh_Dec: -0.95100099\n",
      "| epoch  27 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  4.16 | ppl    63.77 | acc     0.53 | train_ae_norm     1.00\n",
      "[27/200][1599/4361] Loss_D: 0.01608855 (Loss_D_real: 0.00953148 Loss_D_fake: 0.00655706) Loss_G: 0.31419179 Loss_Enh_Dec: -1.05867565\n",
      "| epoch  27 |  1600/ 4361 batches | lr 0.000000 | ms/batch 402.14 | loss  4.11 | ppl    60.95 | acc     0.53 | train_ae_norm     1.00\n",
      "[27/200][1699/4361] Loss_D: 0.08796024 (Loss_D_real: 0.07451496 Loss_D_fake: 0.01344528) Loss_G: 0.26212081 Loss_Enh_Dec: -1.12940013\n",
      "| epoch  27 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  4.11 | ppl    61.13 | acc     0.50 | train_ae_norm     1.00\n",
      "[27/200][1799/4361] Loss_D: 0.07852880 (Loss_D_real: 0.04018102 Loss_D_fake: 0.03834778) Loss_G: 0.24965410 Loss_Enh_Dec: -1.02572095\n",
      "| epoch  27 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  4.12 | ppl    61.45 | acc     0.51 | train_ae_norm     1.00\n",
      "[27/200][1899/4361] Loss_D: 0.02406395 (Loss_D_real: 0.01202261 Loss_D_fake: 0.01204134) Loss_G: 0.24852657 Loss_Enh_Dec: -1.04642808\n",
      "| epoch  27 |  1900/ 4361 batches | lr 0.000000 | ms/batch 402.22 | loss  4.14 | ppl    62.89 | acc     0.54 | train_ae_norm     1.00\n",
      "[27/200][1999/4361] Loss_D: 0.02289502 (Loss_D_real: 0.00927297 Loss_D_fake: 0.01362204) Loss_G: 0.27627203 Loss_Enh_Dec: -1.02723277\n",
      "| epoch  27 |  2000/ 4361 batches | lr 0.000000 | ms/batch 402.32 | loss  4.10 | ppl    60.09 | acc     0.50 | train_ae_norm     1.00\n",
      "[27/200][2099/4361] Loss_D: 0.02120287 (Loss_D_real: 0.00737429 Loss_D_fake: 0.01382857) Loss_G: 0.26856002 Loss_Enh_Dec: -1.04322517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  27 |  2100/ 4361 batches | lr 0.000000 | ms/batch 402.19 | loss  4.11 | ppl    60.89 | acc     0.53 | train_ae_norm     1.00\n",
      "[27/200][2199/4361] Loss_D: 0.02633579 (Loss_D_real: 0.00811329 Loss_D_fake: 0.01822250) Loss_G: 0.29853562 Loss_Enh_Dec: -1.11888981\n",
      "| epoch  27 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  4.11 | ppl    61.23 | acc     0.55 | train_ae_norm     1.00\n",
      "[27/200][2299/4361] Loss_D: 0.06378215 (Loss_D_real: 0.05290471 Loss_D_fake: 0.01087744) Loss_G: 0.26223484 Loss_Enh_Dec: -1.23040926\n",
      "| epoch  27 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  4.09 | ppl    59.91 | acc     0.53 | train_ae_norm     1.00\n",
      "[27/200][2399/4361] Loss_D: 0.03734780 (Loss_D_real: 0.02069056 Loss_D_fake: 0.01665724) Loss_G: 0.25467023 Loss_Enh_Dec: -1.27815330\n",
      "| epoch  27 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  4.10 | ppl    60.54 | acc     0.50 | train_ae_norm     1.00\n",
      "[27/200][2499/4361] Loss_D: 0.01997600 (Loss_D_real: 0.00459831 Loss_D_fake: 0.01537769) Loss_G: 0.25305736 Loss_Enh_Dec: -1.28726959\n",
      "| epoch  27 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  4.12 | ppl    61.43 | acc     0.54 | train_ae_norm     1.00\n",
      "[27/200][2599/4361] Loss_D: 0.02026682 (Loss_D_real: 0.00844995 Loss_D_fake: 0.01181687) Loss_G: 0.25671807 Loss_Enh_Dec: -1.14136684\n",
      "| epoch  27 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  4.10 | ppl    60.11 | acc     0.51 | train_ae_norm     1.00\n",
      "[27/200][2699/4361] Loss_D: 0.02838215 (Loss_D_real: 0.02077778 Loss_D_fake: 0.00760437) Loss_G: 0.28216329 Loss_Enh_Dec: -1.08361423\n",
      "| epoch  27 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  4.15 | ppl    63.35 | acc     0.51 | train_ae_norm     1.00\n",
      "[27/200][2799/4361] Loss_D: 0.01621127 (Loss_D_real: 0.00192914 Loss_D_fake: 0.01428213) Loss_G: 0.27402160 Loss_Enh_Dec: -0.84147114\n",
      "| epoch  27 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  4.02 | ppl    55.97 | acc     0.50 | train_ae_norm     1.00\n",
      "[27/200][2899/4361] Loss_D: 0.04423283 (Loss_D_real: 0.02092846 Loss_D_fake: 0.02330437) Loss_G: 0.28722963 Loss_Enh_Dec: -0.67291981\n",
      "| epoch  27 |  2900/ 4361 batches | lr 0.000000 | ms/batch 402.05 | loss  4.08 | ppl    59.17 | acc     0.51 | train_ae_norm     1.00\n",
      "[27/200][2999/4361] Loss_D: 0.02354211 (Loss_D_real: 0.01112036 Loss_D_fake: 0.01242175) Loss_G: 0.26594803 Loss_Enh_Dec: -0.98211843\n",
      "| epoch  27 |  3000/ 4361 batches | lr 0.000000 | ms/batch 402.24 | loss  4.09 | ppl    59.78 | acc     0.54 | train_ae_norm     1.00\n",
      "[27/200][3099/4361] Loss_D: 0.02828595 (Loss_D_real: 0.01060243 Loss_D_fake: 0.01768352) Loss_G: 0.25403839 Loss_Enh_Dec: -1.16515863\n",
      "| epoch  27 |  3100/ 4361 batches | lr 0.000000 | ms/batch 402.31 | loss  4.10 | ppl    60.04 | acc     0.52 | train_ae_norm     1.00\n",
      "[27/200][3199/4361] Loss_D: 0.13669020 (Loss_D_real: 0.12415823 Loss_D_fake: 0.01253198) Loss_G: 0.26058561 Loss_Enh_Dec: -1.37226772\n",
      "| epoch  27 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  4.10 | ppl    60.45 | acc     0.52 | train_ae_norm     1.00\n",
      "[27/200][3299/4361] Loss_D: 0.04749174 (Loss_D_real: 0.02258981 Loss_D_fake: 0.02490193) Loss_G: 0.26694295 Loss_Enh_Dec: -1.24701977\n",
      "| epoch  27 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  4.14 | ppl    63.11 | acc     0.50 | train_ae_norm     1.00\n",
      "[27/200][3399/4361] Loss_D: 0.04538229 (Loss_D_real: 0.01134946 Loss_D_fake: 0.03403284) Loss_G: 0.27196911 Loss_Enh_Dec: -1.21343994\n",
      "| epoch  27 |  3400/ 4361 batches | lr 0.000000 | ms/batch 402.21 | loss  4.09 | ppl    59.85 | acc     0.55 | train_ae_norm     1.00\n",
      "[27/200][3499/4361] Loss_D: 0.02858829 (Loss_D_real: 0.01950104 Loss_D_fake: 0.00908725) Loss_G: 0.28363013 Loss_Enh_Dec: -1.21543205\n",
      "| epoch  27 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  4.02 | ppl    55.68 | acc     0.56 | train_ae_norm     1.00\n",
      "[27/200][3599/4361] Loss_D: 0.02832412 (Loss_D_real: 0.01937161 Loss_D_fake: 0.00895251) Loss_G: 0.30572602 Loss_Enh_Dec: -1.04932964\n",
      "| epoch  27 |  3600/ 4361 batches | lr 0.000000 | ms/batch 402.62 | loss  4.04 | ppl    56.96 | acc     0.54 | train_ae_norm     1.00\n",
      "[27/200][3699/4361] Loss_D: 0.01501642 (Loss_D_real: 0.00335778 Loss_D_fake: 0.01165864) Loss_G: 0.26535651 Loss_Enh_Dec: -1.16116941\n",
      "| epoch  27 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  4.05 | ppl    57.59 | acc     0.52 | train_ae_norm     1.00\n",
      "[27/200][3799/4361] Loss_D: 0.04009746 (Loss_D_real: 0.00513707 Loss_D_fake: 0.03496039) Loss_G: 0.26088023 Loss_Enh_Dec: -1.28751016\n",
      "| epoch  27 |  3800/ 4361 batches | lr 0.000000 | ms/batch 402.07 | loss  4.06 | ppl    57.80 | acc     0.60 | train_ae_norm     1.00\n",
      "[27/200][3899/4361] Loss_D: 0.03376312 (Loss_D_real: 0.02427576 Loss_D_fake: 0.00948736) Loss_G: 0.28293198 Loss_Enh_Dec: -1.09569013\n",
      "| epoch  27 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  4.03 | ppl    56.25 | acc     0.49 | train_ae_norm     1.00\n",
      "[27/200][3999/4361] Loss_D: 0.08194040 (Loss_D_real: 0.06948313 Loss_D_fake: 0.01245727) Loss_G: 0.26357919 Loss_Enh_Dec: -1.05546832\n",
      "| epoch  27 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  4.03 | ppl    56.16 | acc     0.55 | train_ae_norm     1.00\n",
      "[27/200][4099/4361] Loss_D: 0.01612096 (Loss_D_real: 0.00785810 Loss_D_fake: 0.00826285) Loss_G: 0.25795698 Loss_Enh_Dec: -0.72528267\n",
      "| epoch  27 |  4100/ 4361 batches | lr 0.000000 | ms/batch 402.11 | loss  3.97 | ppl    52.77 | acc     0.54 | train_ae_norm     1.00\n",
      "[27/200][4199/4361] Loss_D: 0.02879055 (Loss_D_real: 0.00794144 Loss_D_fake: 0.02084912) Loss_G: 0.27367917 Loss_Enh_Dec: -0.97277051\n",
      "| epoch  27 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  4.00 | ppl    54.53 | acc     0.57 | train_ae_norm     1.00\n",
      "[27/200][4299/4361] Loss_D: 0.01309307 (Loss_D_real: 0.00322120 Loss_D_fake: 0.00987187) Loss_G: 0.28181544 Loss_Enh_Dec: -1.10748279\n",
      "| epoch  27 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  3.95 | ppl    52.00 | acc     0.56 | train_ae_norm     1.00\n",
      "| end of epoch  27 | time: 1854.73s | test loss  3.76 | test ppl 42.81 | acc 0.604\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 28 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.727\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.468\n",
      "  Test Loss: 3.440\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  28 |     0/ 4361 batches | lr 0.000000 | ms/batch 861.79 | loss  0.04 | ppl     1.04 | acc     0.59 | train_ae_norm     1.00\n",
      "[28/200][99/4361] Loss_D: 0.02044550 (Loss_D_real: 0.00532231 Loss_D_fake: 0.01512320) Loss_G: 0.27588993 Loss_Enh_Dec: -1.19001043\n",
      "| epoch  28 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  3.98 | ppl    53.69 | acc     0.54 | train_ae_norm     1.00\n",
      "[28/200][199/4361] Loss_D: 0.00866941 (Loss_D_real: 0.00391391 Loss_D_fake: 0.00475550) Loss_G: 0.29469854 Loss_Enh_Dec: -1.20908117\n",
      "| epoch  28 |   200/ 4361 batches | lr 0.000000 | ms/batch 400.90 | loss  4.02 | ppl    55.57 | acc     0.57 | train_ae_norm     1.00\n",
      "[28/200][299/4361] Loss_D: 0.01397206 (Loss_D_real: 0.00817931 Loss_D_fake: 0.00579275) Loss_G: 0.27963406 Loss_Enh_Dec: -1.36991024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  28 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  4.01 | ppl    54.99 | acc     0.52 | train_ae_norm     1.00\n",
      "[28/200][399/4361] Loss_D: 0.01503472 (Loss_D_real: 0.00433981 Loss_D_fake: 0.01069491) Loss_G: 0.27698380 Loss_Enh_Dec: -1.17407107\n",
      "| epoch  28 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  3.94 | ppl    51.29 | acc     0.56 | train_ae_norm     1.00\n",
      "[28/200][499/4361] Loss_D: 0.02935276 (Loss_D_real: 0.01777070 Loss_D_fake: 0.01158206) Loss_G: 0.28502518 Loss_Enh_Dec: -1.26613498\n",
      "| epoch  28 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.99 | loss  3.99 | ppl    53.94 | acc     0.56 | train_ae_norm     1.00\n",
      "[28/200][599/4361] Loss_D: 0.01974972 (Loss_D_real: 0.00646375 Loss_D_fake: 0.01328597) Loss_G: 0.29186511 Loss_Enh_Dec: -1.14555252\n",
      "| epoch  28 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  3.92 | ppl    50.36 | acc     0.51 | train_ae_norm     1.00\n",
      "[28/200][699/4361] Loss_D: 0.01873909 (Loss_D_real: 0.01004052 Loss_D_fake: 0.00869857) Loss_G: 0.25959906 Loss_Enh_Dec: -1.24347293\n",
      "| epoch  28 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  3.99 | ppl    54.25 | acc     0.58 | train_ae_norm     1.00\n",
      "[28/200][799/4361] Loss_D: 0.01650321 (Loss_D_real: 0.00331125 Loss_D_fake: 0.01319195) Loss_G: 0.30527911 Loss_Enh_Dec: -1.33257854\n",
      "| epoch  28 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  3.95 | ppl    51.97 | acc     0.56 | train_ae_norm     1.00\n",
      "[28/200][899/4361] Loss_D: 0.01242801 (Loss_D_real: 0.00237458 Loss_D_fake: 0.01005343) Loss_G: 0.28268680 Loss_Enh_Dec: -1.21252930\n",
      "| epoch  28 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  3.97 | ppl    52.90 | acc     0.59 | train_ae_norm     1.00\n",
      "[28/200][999/4361] Loss_D: 0.01374722 (Loss_D_real: 0.00717705 Loss_D_fake: 0.00657017) Loss_G: 0.29679018 Loss_Enh_Dec: -1.27593553\n",
      "| epoch  28 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  3.94 | ppl    51.64 | acc     0.58 | train_ae_norm     1.00\n",
      "[28/200][1099/4361] Loss_D: 0.04744685 (Loss_D_real: 0.00951726 Loss_D_fake: 0.03792959) Loss_G: 0.32108620 Loss_Enh_Dec: -1.21957624\n",
      "| epoch  28 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  3.94 | ppl    51.64 | acc     0.55 | train_ae_norm     1.00\n",
      "[28/200][1199/4361] Loss_D: 0.01857284 (Loss_D_real: 0.01087236 Loss_D_fake: 0.00770047) Loss_G: 0.28920460 Loss_Enh_Dec: -1.13738155\n",
      "| epoch  28 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  3.96 | ppl    52.43 | acc     0.58 | train_ae_norm     1.00\n",
      "[28/200][1299/4361] Loss_D: 0.02831309 (Loss_D_real: 0.01646889 Loss_D_fake: 0.01184420) Loss_G: 0.29894075 Loss_Enh_Dec: -0.93268961\n",
      "| epoch  28 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  3.98 | ppl    53.51 | acc     0.54 | train_ae_norm     1.00\n",
      "[28/200][1399/4361] Loss_D: 0.01103680 (Loss_D_real: 0.00245244 Loss_D_fake: 0.00858435) Loss_G: 0.31209445 Loss_Enh_Dec: -1.24077868\n",
      "| epoch  28 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  3.98 | ppl    53.65 | acc     0.50 | train_ae_norm     1.00\n",
      "[28/200][1499/4361] Loss_D: 0.02118472 (Loss_D_real: 0.01402948 Loss_D_fake: 0.00715524) Loss_G: 0.30760315 Loss_Enh_Dec: -1.06278837\n",
      "| epoch  28 |  1500/ 4361 batches | lr 0.000000 | ms/batch 400.85 | loss  4.01 | ppl    55.13 | acc     0.49 | train_ae_norm     1.00\n",
      "[28/200][1599/4361] Loss_D: 0.01235501 (Loss_D_real: 0.00499867 Loss_D_fake: 0.00735633) Loss_G: 0.30137286 Loss_Enh_Dec: -1.14968145\n",
      "| epoch  28 |  1600/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  3.99 | ppl    53.96 | acc     0.56 | train_ae_norm     1.00\n",
      "[28/200][1699/4361] Loss_D: 0.01334322 (Loss_D_real: 0.00379626 Loss_D_fake: 0.00954696) Loss_G: 0.31896672 Loss_Enh_Dec: -1.26436388\n",
      "| epoch  28 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  3.96 | ppl    52.66 | acc     0.52 | train_ae_norm     1.00\n",
      "[28/200][1799/4361] Loss_D: 0.01280891 (Loss_D_real: 0.00526614 Loss_D_fake: 0.00754277) Loss_G: 0.29983020 Loss_Enh_Dec: -1.31858623\n",
      "| epoch  28 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.84 | loss  3.96 | ppl    52.68 | acc     0.55 | train_ae_norm     1.00\n",
      "[28/200][1899/4361] Loss_D: 0.01964756 (Loss_D_real: 0.01213108 Loss_D_fake: 0.00751648) Loss_G: 0.33333182 Loss_Enh_Dec: -0.78283453\n",
      "| epoch  28 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  4.02 | ppl    55.63 | acc     0.57 | train_ae_norm     1.00\n",
      "[28/200][1999/4361] Loss_D: 0.04047467 (Loss_D_real: 0.00825546 Loss_D_fake: 0.03221921) Loss_G: 0.33275852 Loss_Enh_Dec: -1.12834811\n",
      "| epoch  28 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  3.95 | ppl    51.94 | acc     0.53 | train_ae_norm     1.00\n",
      "[28/200][2099/4361] Loss_D: 0.01291589 (Loss_D_real: 0.00737418 Loss_D_fake: 0.00554171) Loss_G: 0.32496005 Loss_Enh_Dec: -1.26477516\n",
      "| epoch  28 |  2100/ 4361 batches | lr 0.000000 | ms/batch 402.08 | loss  3.97 | ppl    53.11 | acc     0.57 | train_ae_norm     1.00\n",
      "[28/200][2199/4361] Loss_D: 0.00822437 (Loss_D_real: 0.00166350 Loss_D_fake: 0.00656088) Loss_G: 0.31128776 Loss_Enh_Dec: -1.25191820\n",
      "| epoch  28 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  3.98 | ppl    53.28 | acc     0.54 | train_ae_norm     1.00\n",
      "[28/200][2299/4361] Loss_D: 0.03755002 (Loss_D_real: 0.02165173 Loss_D_fake: 0.01589830) Loss_G: 0.32368127 Loss_Enh_Dec: -1.01913857\n",
      "| epoch  28 |  2300/ 4361 batches | lr 0.000000 | ms/batch 402.25 | loss  3.98 | ppl    53.64 | acc     0.53 | train_ae_norm     1.00\n",
      "[28/200][2399/4361] Loss_D: 0.00996929 (Loss_D_real: 0.00620006 Loss_D_fake: 0.00376923) Loss_G: 0.31390977 Loss_Enh_Dec: -1.21255815\n",
      "| epoch  28 |  2400/ 4361 batches | lr 0.000000 | ms/batch 402.08 | loss  3.97 | ppl    52.72 | acc     0.52 | train_ae_norm     1.00\n",
      "[28/200][2499/4361] Loss_D: 0.02402275 (Loss_D_real: 0.00892614 Loss_D_fake: 0.01509661) Loss_G: 0.30288315 Loss_Enh_Dec: -1.06053102\n",
      "| epoch  28 |  2500/ 4361 batches | lr 0.000000 | ms/batch 403.04 | loss  4.01 | ppl    55.19 | acc     0.56 | train_ae_norm     1.00\n",
      "[28/200][2599/4361] Loss_D: 0.02526457 (Loss_D_real: 0.02257246 Loss_D_fake: 0.00269211) Loss_G: 0.36949238 Loss_Enh_Dec: -1.28791034\n",
      "| epoch  28 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  4.01 | ppl    55.36 | acc     0.53 | train_ae_norm     1.00\n",
      "[28/200][2699/4361] Loss_D: 0.04096837 (Loss_D_real: 0.02607661 Loss_D_fake: 0.01489175) Loss_G: 0.31996131 Loss_Enh_Dec: -1.18725002\n",
      "| epoch  28 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  4.04 | ppl    56.81 | acc     0.52 | train_ae_norm     1.00\n",
      "[28/200][2799/4361] Loss_D: 0.03893660 (Loss_D_real: 0.02330991 Loss_D_fake: 0.01562669) Loss_G: 0.30127293 Loss_Enh_Dec: -1.22379231\n",
      "| epoch  28 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  3.96 | ppl    52.58 | acc     0.52 | train_ae_norm     1.00\n",
      "[28/200][2899/4361] Loss_D: 0.06892867 (Loss_D_real: 0.05515065 Loss_D_fake: 0.01377802) Loss_G: 0.26070461 Loss_Enh_Dec: -0.98180717\n",
      "| epoch  28 |  2900/ 4361 batches | lr 0.000000 | ms/batch 402.16 | loss  4.03 | ppl    56.10 | acc     0.56 | train_ae_norm     1.00\n",
      "[28/200][2999/4361] Loss_D: 0.04313112 (Loss_D_real: 0.03288920 Loss_D_fake: 0.01024192) Loss_G: 0.32461202 Loss_Enh_Dec: -1.22381437\n",
      "| epoch  28 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  4.02 | ppl    55.97 | acc     0.54 | train_ae_norm     1.00\n",
      "[28/200][3099/4361] Loss_D: 0.02696801 (Loss_D_real: 0.01478579 Loss_D_fake: 0.01218222) Loss_G: 0.27851161 Loss_Enh_Dec: -1.42191958\n",
      "| epoch  28 |  3100/ 4361 batches | lr 0.000000 | ms/batch 402.04 | loss  4.04 | ppl    56.72 | acc     0.51 | train_ae_norm     1.00\n",
      "[28/200][3199/4361] Loss_D: 0.02840810 (Loss_D_real: 0.01392273 Loss_D_fake: 0.01448536) Loss_G: 0.31636938 Loss_Enh_Dec: -1.19259107\n",
      "| epoch  28 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  4.07 | ppl    58.46 | acc     0.52 | train_ae_norm     1.00\n",
      "[28/200][3299/4361] Loss_D: 0.04180198 (Loss_D_real: 0.02847813 Loss_D_fake: 0.01332385) Loss_G: 0.26910996 Loss_Enh_Dec: -0.96523398\n",
      "| epoch  28 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  4.07 | ppl    58.49 | acc     0.52 | train_ae_norm     1.00\n",
      "[28/200][3399/4361] Loss_D: 0.04349614 (Loss_D_real: 0.02338825 Loss_D_fake: 0.02010789) Loss_G: 0.28429601 Loss_Enh_Dec: -1.18027341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  28 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  4.03 | ppl    56.19 | acc     0.57 | train_ae_norm     1.00\n",
      "[28/200][3499/4361] Loss_D: 0.01274922 (Loss_D_real: 0.00433509 Loss_D_fake: 0.00841413) Loss_G: 0.29017004 Loss_Enh_Dec: -1.17693508\n",
      "| epoch  28 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  3.97 | ppl    52.94 | acc     0.56 | train_ae_norm     1.00\n",
      "[28/200][3599/4361] Loss_D: 0.01821311 (Loss_D_real: 0.00369475 Loss_D_fake: 0.01451835) Loss_G: 0.28946218 Loss_Enh_Dec: -1.27620459\n",
      "| epoch  28 |  3600/ 4361 batches | lr 0.000000 | ms/batch 402.59 | loss  4.01 | ppl    55.05 | acc     0.52 | train_ae_norm     1.00\n",
      "[28/200][3699/4361] Loss_D: 0.15358132 (Loss_D_real: 0.13845912 Loss_D_fake: 0.01512221) Loss_G: 0.27494916 Loss_Enh_Dec: -1.43056893\n",
      "| epoch  28 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  4.02 | ppl    55.75 | acc     0.52 | train_ae_norm     1.00\n",
      "[28/200][3799/4361] Loss_D: 0.05671057 (Loss_D_real: 0.02561436 Loss_D_fake: 0.03109621) Loss_G: 0.29350001 Loss_Enh_Dec: -1.16476786\n",
      "| epoch  28 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  4.06 | ppl    57.71 | acc     0.59 | train_ae_norm     1.00\n",
      "[28/200][3899/4361] Loss_D: 0.02469782 (Loss_D_real: 0.01406250 Loss_D_fake: 0.01063532) Loss_G: 0.27495027 Loss_Enh_Dec: -1.13397789\n",
      "| epoch  28 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  4.04 | ppl    57.01 | acc     0.51 | train_ae_norm     1.00\n",
      "[28/200][3999/4361] Loss_D: 0.01599101 (Loss_D_real: 0.00108540 Loss_D_fake: 0.01490561) Loss_G: 0.30492255 Loss_Enh_Dec: -1.15343845\n",
      "| epoch  28 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  4.07 | ppl    58.38 | acc     0.54 | train_ae_norm     1.00\n",
      "[28/200][4099/4361] Loss_D: 0.04580437 (Loss_D_real: 0.02955138 Loss_D_fake: 0.01625298) Loss_G: 0.28854778 Loss_Enh_Dec: -0.93761444\n",
      "| epoch  28 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  4.00 | ppl    54.78 | acc     0.54 | train_ae_norm     1.00\n",
      "[28/200][4199/4361] Loss_D: 0.02139340 (Loss_D_real: 0.00505977 Loss_D_fake: 0.01633364) Loss_G: 0.29147711 Loss_Enh_Dec: -0.75743383\n",
      "| epoch  28 |  4200/ 4361 batches | lr 0.000000 | ms/batch 400.73 | loss  4.03 | ppl    56.51 | acc     0.58 | train_ae_norm     1.00\n",
      "[28/200][4299/4361] Loss_D: 0.05266656 (Loss_D_real: 0.03977787 Loss_D_fake: 0.01288869) Loss_G: 0.25937709 Loss_Enh_Dec: -1.14644170\n",
      "| epoch  28 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  3.99 | ppl    53.82 | acc     0.53 | train_ae_norm     1.00\n",
      "| end of epoch  28 | time: 1852.84s | test loss  3.76 | test ppl 43.04 | acc 0.605\n",
      "bleu_self:  [3.17708333e-01 1.29681725e-01 1.14061117e-06 3.55789688e-09\n",
      " 1.17432117e-10]\n",
      "bleu_test:  [9.36507936e-01 6.00995430e-01 1.01357841e-01 1.52634398e-05\n",
      " 8.17467104e-08]\n",
      "bleu_self: [0.31770833,0.12968173,0.00000114,0.00000000,0.00000000]\n",
      "bleu_test: [0.93650794,0.60099543,0.10135784,0.00001526,0.00000008]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 29 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.718\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.463\n",
      "  Test Loss: 3.598\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  29 |     0/ 4361 batches | lr 0.000000 | ms/batch 866.60 | loss  0.04 | ppl     1.04 | acc     0.58 | train_ae_norm     1.00\n",
      "[29/200][99/4361] Loss_D: 0.03288073 (Loss_D_real: 0.01629766 Loss_D_fake: 0.01658307) Loss_G: 0.27826580 Loss_Enh_Dec: -0.89497036\n",
      "| epoch  29 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  3.98 | ppl    53.45 | acc     0.51 | train_ae_norm     1.00\n",
      "[29/200][199/4361] Loss_D: 0.03095346 (Loss_D_real: 0.00403759 Loss_D_fake: 0.02691588) Loss_G: 0.27309439 Loss_Enh_Dec: -1.19430625\n",
      "| epoch  29 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  4.00 | ppl    54.56 | acc     0.58 | train_ae_norm     1.00\n",
      "[29/200][299/4361] Loss_D: 0.02893005 (Loss_D_real: 0.01556162 Loss_D_fake: 0.01336843) Loss_G: 0.27630788 Loss_Enh_Dec: -0.33363658\n",
      "| epoch  29 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  3.99 | ppl    53.97 | acc     0.52 | train_ae_norm     1.00\n",
      "[29/200][399/4361] Loss_D: 0.10404150 (Loss_D_real: 0.09181885 Loss_D_fake: 0.01222265) Loss_G: 0.31761950 Loss_Enh_Dec: -1.22558331\n",
      "| epoch  29 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.72 | loss  3.91 | ppl    50.11 | acc     0.57 | train_ae_norm     1.00\n",
      "[29/200][499/4361] Loss_D: 0.03016733 (Loss_D_real: 0.01512053 Loss_D_fake: 0.01504680) Loss_G: 0.26178446 Loss_Enh_Dec: -1.10926402\n",
      "| epoch  29 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  3.98 | ppl    53.27 | acc     0.56 | train_ae_norm     1.00\n",
      "[29/200][599/4361] Loss_D: 0.03883466 (Loss_D_real: 0.01944461 Loss_D_fake: 0.01939005) Loss_G: 0.26478204 Loss_Enh_Dec: -1.27337396\n",
      "| epoch  29 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  3.92 | ppl    50.22 | acc     0.50 | train_ae_norm     1.00\n",
      "[29/200][699/4361] Loss_D: 0.04855836 (Loss_D_real: 0.01691131 Loss_D_fake: 0.03164706) Loss_G: 0.28803498 Loss_Enh_Dec: -1.30500793\n",
      "| epoch  29 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  3.97 | ppl    53.04 | acc     0.58 | train_ae_norm     1.00\n",
      "[29/200][799/4361] Loss_D: 0.00977193 (Loss_D_real: 0.00260456 Loss_D_fake: 0.00716737) Loss_G: 0.28929335 Loss_Enh_Dec: -1.03046298\n",
      "| epoch  29 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  3.93 | ppl    51.00 | acc     0.55 | train_ae_norm     1.00\n",
      "[29/200][899/4361] Loss_D: 0.02115942 (Loss_D_real: 0.00775818 Loss_D_fake: 0.01340124) Loss_G: 0.25737202 Loss_Enh_Dec: -1.17567325\n",
      "| epoch  29 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  3.93 | ppl    50.65 | acc     0.60 | train_ae_norm     1.00\n",
      "[29/200][999/4361] Loss_D: 0.01352129 (Loss_D_real: 0.00227933 Loss_D_fake: 0.01124196) Loss_G: 0.27258956 Loss_Enh_Dec: -1.07435405\n",
      "| epoch  29 |  1000/ 4361 batches | lr 0.000000 | ms/batch 402.05 | loss  3.90 | ppl    49.63 | acc     0.56 | train_ae_norm     1.00\n",
      "[29/200][1099/4361] Loss_D: 0.04848054 (Loss_D_real: 0.01777283 Loss_D_fake: 0.03070771) Loss_G: 0.29214841 Loss_Enh_Dec: -0.81805795\n",
      "| epoch  29 |  1100/ 4361 batches | lr 0.000000 | ms/batch 402.33 | loss  3.90 | ppl    49.30 | acc     0.53 | train_ae_norm     1.00\n",
      "[29/200][1199/4361] Loss_D: 0.04264547 (Loss_D_real: 0.00340127 Loss_D_fake: 0.03924420) Loss_G: 0.28826794 Loss_Enh_Dec: -1.22208965\n",
      "| epoch  29 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  3.92 | ppl    50.49 | acc     0.57 | train_ae_norm     1.00\n",
      "[29/200][1299/4361] Loss_D: 0.02437249 (Loss_D_real: 0.01773971 Loss_D_fake: 0.00663278) Loss_G: 0.28868222 Loss_Enh_Dec: -0.99567395\n",
      "| epoch  29 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  3.91 | ppl    49.80 | acc     0.55 | train_ae_norm     1.00\n",
      "[29/200][1399/4361] Loss_D: 0.02438474 (Loss_D_real: 0.01088016 Loss_D_fake: 0.01350458) Loss_G: 0.27882555 Loss_Enh_Dec: -1.03220367\n",
      "| epoch  29 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  3.91 | ppl    50.00 | acc     0.49 | train_ae_norm     1.00\n",
      "[29/200][1499/4361] Loss_D: 0.00904111 (Loss_D_real: 0.00293527 Loss_D_fake: 0.00610584) Loss_G: 0.28080407 Loss_Enh_Dec: -0.84543800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  29 |  1500/ 4361 batches | lr 0.000000 | ms/batch 402.24 | loss  3.95 | ppl    51.85 | acc     0.54 | train_ae_norm     1.00\n",
      "[29/200][1599/4361] Loss_D: 0.03751982 (Loss_D_real: 0.00845385 Loss_D_fake: 0.02906597) Loss_G: 0.32333156 Loss_Enh_Dec: -0.90684456\n",
      "| epoch  29 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  3.91 | ppl    50.09 | acc     0.55 | train_ae_norm     1.00\n",
      "[29/200][1699/4361] Loss_D: 0.02139116 (Loss_D_real: 0.00812394 Loss_D_fake: 0.01326722) Loss_G: 0.28235036 Loss_Enh_Dec: -0.38882685\n",
      "| epoch  29 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  3.93 | ppl    50.89 | acc     0.55 | train_ae_norm     1.00\n",
      "[29/200][1799/4361] Loss_D: 0.02966225 (Loss_D_real: 0.01354001 Loss_D_fake: 0.01612224) Loss_G: 0.30774999 Loss_Enh_Dec: -0.93949395\n",
      "| epoch  29 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  3.90 | ppl    49.16 | acc     0.57 | train_ae_norm     1.00\n",
      "[29/200][1899/4361] Loss_D: 0.08633949 (Loss_D_real: 0.01254638 Loss_D_fake: 0.07379311) Loss_G: 0.37054706 Loss_Enh_Dec: -0.82971156\n",
      "| epoch  29 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  4.00 | ppl    54.76 | acc     0.56 | train_ae_norm     1.00\n",
      "[29/200][1999/4361] Loss_D: 0.02324905 (Loss_D_real: 0.00639823 Loss_D_fake: 0.01685082) Loss_G: 0.28517810 Loss_Enh_Dec: -0.95340198\n",
      "| epoch  29 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  3.96 | ppl    52.48 | acc     0.53 | train_ae_norm     1.00\n",
      "[29/200][2099/4361] Loss_D: 0.04023454 (Loss_D_real: 0.01270638 Loss_D_fake: 0.02752816) Loss_G: 0.33982763 Loss_Enh_Dec: -0.84814149\n",
      "| epoch  29 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  3.95 | ppl    52.13 | acc     0.55 | train_ae_norm     1.00\n",
      "[29/200][2199/4361] Loss_D: 0.02658325 (Loss_D_real: 0.00534280 Loss_D_fake: 0.02124045) Loss_G: 0.29894224 Loss_Enh_Dec: -0.75553560\n",
      "| epoch  29 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  3.96 | ppl    52.30 | acc     0.55 | train_ae_norm     1.00\n",
      "[29/200][2299/4361] Loss_D: 0.04451697 (Loss_D_real: 0.02991793 Loss_D_fake: 0.01459904) Loss_G: 0.25383049 Loss_Enh_Dec: -0.45718542\n",
      "| epoch  29 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.88 | loss  3.97 | ppl    52.75 | acc     0.55 | train_ae_norm     1.00\n",
      "[29/200][2399/4361] Loss_D: 0.04160782 (Loss_D_real: 0.01225587 Loss_D_fake: 0.02935195) Loss_G: 0.29247054 Loss_Enh_Dec: -0.62701243\n",
      "| epoch  29 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  3.97 | ppl    52.92 | acc     0.54 | train_ae_norm     1.00\n",
      "[29/200][2499/4361] Loss_D: 0.02589595 (Loss_D_real: 0.01027830 Loss_D_fake: 0.01561764) Loss_G: 0.30234432 Loss_Enh_Dec: -0.25702810\n",
      "| epoch  29 |  2500/ 4361 batches | lr 0.000000 | ms/batch 400.55 | loss  3.99 | ppl    53.93 | acc     0.56 | train_ae_norm     1.00\n",
      "[29/200][2599/4361] Loss_D: 0.06033401 (Loss_D_real: 0.04820539 Loss_D_fake: 0.01212863) Loss_G: 0.25247088 Loss_Enh_Dec: -0.88148606\n",
      "| epoch  29 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.57 | loss  3.95 | ppl    52.01 | acc     0.56 | train_ae_norm     1.00\n",
      "[29/200][2699/4361] Loss_D: 0.01658782 (Loss_D_real: 0.00195624 Loss_D_fake: 0.01463157) Loss_G: 0.27351937 Loss_Enh_Dec: -1.10175133\n",
      "| epoch  29 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  3.99 | ppl    53.94 | acc     0.57 | train_ae_norm     1.00\n",
      "[29/200][2799/4361] Loss_D: 0.05088300 (Loss_D_real: 0.02802725 Loss_D_fake: 0.02285574) Loss_G: 0.27273184 Loss_Enh_Dec: -1.02698076\n",
      "| epoch  29 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  3.92 | ppl    50.54 | acc     0.52 | train_ae_norm     1.00\n",
      "[29/200][2899/4361] Loss_D: 0.02124828 (Loss_D_real: 0.00371540 Loss_D_fake: 0.01753287) Loss_G: 0.30450007 Loss_Enh_Dec: -1.16617084\n",
      "| epoch  29 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.58 | loss  3.97 | ppl    52.78 | acc     0.54 | train_ae_norm     1.00\n",
      "[29/200][2999/4361] Loss_D: 0.02394841 (Loss_D_real: 0.01670676 Loss_D_fake: 0.00724165) Loss_G: 0.32364541 Loss_Enh_Dec: -1.05410421\n",
      "| epoch  29 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  3.96 | ppl    52.38 | acc     0.53 | train_ae_norm     1.00\n",
      "[29/200][3099/4361] Loss_D: 0.02219381 (Loss_D_real: 0.01445978 Loss_D_fake: 0.00773403) Loss_G: 0.26949945 Loss_Enh_Dec: -1.31492543\n",
      "| epoch  29 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  3.96 | ppl    52.62 | acc     0.51 | train_ae_norm     1.00\n",
      "[29/200][3199/4361] Loss_D: 0.03637880 (Loss_D_real: 0.02530900 Loss_D_fake: 0.01106980) Loss_G: 0.30201319 Loss_Enh_Dec: -1.38484704\n",
      "| epoch  29 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  3.99 | ppl    53.96 | acc     0.53 | train_ae_norm     1.00\n",
      "[29/200][3299/4361] Loss_D: 0.16993538 (Loss_D_real: 0.13283333 Loss_D_fake: 0.03710204) Loss_G: 0.29074153 Loss_Enh_Dec: -1.22361827\n",
      "| epoch  29 |  3300/ 4361 batches | lr 0.000000 | ms/batch 400.31 | loss  4.01 | ppl    55.12 | acc     0.53 | train_ae_norm     1.00\n",
      "[29/200][3399/4361] Loss_D: 0.02243523 (Loss_D_real: 0.00259252 Loss_D_fake: 0.01984271) Loss_G: 0.29705420 Loss_Enh_Dec: -0.92811090\n",
      "| epoch  29 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.05 | loss  3.99 | ppl    54.13 | acc     0.56 | train_ae_norm     1.00\n",
      "[29/200][3499/4361] Loss_D: 0.01793159 (Loss_D_real: 0.00419038 Loss_D_fake: 0.01374121) Loss_G: 0.27871457 Loss_Enh_Dec: -0.85682982\n",
      "| epoch  29 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.72 | loss  3.93 | ppl    50.91 | acc     0.56 | train_ae_norm     1.00\n",
      "[29/200][3599/4361] Loss_D: 0.06682443 (Loss_D_real: 0.03015489 Loss_D_fake: 0.03666954) Loss_G: 0.27474999 Loss_Enh_Dec: -0.81165642\n",
      "| epoch  29 |  3600/ 4361 batches | lr 0.000000 | ms/batch 400.40 | loss  3.92 | ppl    50.54 | acc     0.56 | train_ae_norm     1.00\n",
      "[29/200][3699/4361] Loss_D: 0.06646645 (Loss_D_real: 0.04147924 Loss_D_fake: 0.02498720) Loss_G: 0.29166076 Loss_Enh_Dec: -0.60992277\n",
      "| epoch  29 |  3700/ 4361 batches | lr 0.000000 | ms/batch 399.81 | loss  3.94 | ppl    51.42 | acc     0.52 | train_ae_norm     1.00\n",
      "[29/200][3799/4361] Loss_D: 0.04362383 (Loss_D_real: 0.02263636 Loss_D_fake: 0.02098747) Loss_G: 0.29687291 Loss_Enh_Dec: -0.35339651\n",
      "| epoch  29 |  3800/ 4361 batches | lr 0.000000 | ms/batch 399.81 | loss  3.97 | ppl    53.01 | acc     0.57 | train_ae_norm     1.00\n",
      "[29/200][3899/4361] Loss_D: 0.03324922 (Loss_D_real: 0.01151022 Loss_D_fake: 0.02173900) Loss_G: 0.28351182 Loss_Enh_Dec: -0.35228440\n",
      "| epoch  29 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.88 | loss  3.96 | ppl    52.61 | acc     0.54 | train_ae_norm     1.00\n",
      "[29/200][3999/4361] Loss_D: 0.04301200 (Loss_D_real: 0.02941105 Loss_D_fake: 0.01360095) Loss_G: 0.28960377 Loss_Enh_Dec: -0.74978334\n",
      "| epoch  29 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  3.95 | ppl    51.97 | acc     0.56 | train_ae_norm     1.00\n",
      "[29/200][4099/4361] Loss_D: 0.07630321 (Loss_D_real: 0.05052292 Loss_D_fake: 0.02578029) Loss_G: 0.32235152 Loss_Enh_Dec: -0.79687035\n",
      "| epoch  29 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.78 | loss  3.93 | ppl    50.93 | acc     0.55 | train_ae_norm     1.00\n",
      "[29/200][4199/4361] Loss_D: 0.03509484 (Loss_D_real: 0.00546015 Loss_D_fake: 0.02963469) Loss_G: 0.30352402 Loss_Enh_Dec: -0.59401721\n",
      "| epoch  29 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  3.98 | ppl    53.69 | acc     0.57 | train_ae_norm     1.00\n",
      "[29/200][4299/4361] Loss_D: 0.05278707 (Loss_D_real: 0.04326275 Loss_D_fake: 0.00952432) Loss_G: 0.29684240 Loss_Enh_Dec: -0.57646078\n",
      "| epoch  29 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  3.95 | ppl    51.87 | acc     0.56 | train_ae_norm     1.00\n",
      "| end of epoch  29 | time: 1851.72s | test loss  3.76 | test ppl 42.86 | acc 0.607\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 30 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.719\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.468\n",
      "  Test Loss: 3.617\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  30 |     0/ 4361 batches | lr 0.000000 | ms/batch 863.50 | loss  0.04 | ppl     1.04 | acc     0.57 | train_ae_norm     1.00\n",
      "[30/200][99/4361] Loss_D: 0.05026804 (Loss_D_real: 0.02335973 Loss_D_fake: 0.02690831) Loss_G: 0.29732123 Loss_Enh_Dec: -1.00415170\n",
      "| epoch  30 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  3.96 | ppl    52.24 | acc     0.54 | train_ae_norm     1.00\n",
      "[30/200][199/4361] Loss_D: 0.03959362 (Loss_D_real: 0.03287452 Loss_D_fake: 0.00671910) Loss_G: 0.29806575 Loss_Enh_Dec: -1.27041399\n",
      "| epoch  30 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  3.98 | ppl    53.42 | acc     0.54 | train_ae_norm     1.00\n",
      "[30/200][299/4361] Loss_D: 0.03520973 (Loss_D_real: 0.01794651 Loss_D_fake: 0.01726323) Loss_G: 0.29531991 Loss_Enh_Dec: -1.22381866\n",
      "| epoch  30 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  3.98 | ppl    53.46 | acc     0.52 | train_ae_norm     1.00\n",
      "[30/200][399/4361] Loss_D: 0.03667890 (Loss_D_real: 0.01056525 Loss_D_fake: 0.02611365) Loss_G: 0.28938088 Loss_Enh_Dec: -1.08638227\n",
      "| epoch  30 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.74 | loss  3.87 | ppl    48.09 | acc     0.55 | train_ae_norm     1.00\n",
      "[30/200][499/4361] Loss_D: 0.03124465 (Loss_D_real: 0.01404115 Loss_D_fake: 0.01720350) Loss_G: 0.26754150 Loss_Enh_Dec: -1.24538767\n",
      "| epoch  30 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  3.95 | ppl    51.84 | acc     0.58 | train_ae_norm     1.00\n",
      "[30/200][599/4361] Loss_D: 0.04004135 (Loss_D_real: 0.02292161 Loss_D_fake: 0.01711974) Loss_G: 0.25081387 Loss_Enh_Dec: -0.91613621\n",
      "| epoch  30 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  3.91 | ppl    49.69 | acc     0.53 | train_ae_norm     1.00\n",
      "[30/200][699/4361] Loss_D: 0.02653836 (Loss_D_real: 0.01666459 Loss_D_fake: 0.00987376) Loss_G: 0.29643726 Loss_Enh_Dec: -1.23983991\n",
      "| epoch  30 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  3.93 | ppl    50.65 | acc     0.60 | train_ae_norm     1.00\n",
      "[30/200][799/4361] Loss_D: 0.02360664 (Loss_D_real: 0.00982301 Loss_D_fake: 0.01378363) Loss_G: 0.30621478 Loss_Enh_Dec: -1.31343579\n",
      "| epoch  30 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  3.90 | ppl    49.46 | acc     0.57 | train_ae_norm     1.00\n",
      "[30/200][899/4361] Loss_D: 0.04062852 (Loss_D_real: 0.02330054 Loss_D_fake: 0.01732797) Loss_G: 0.26926810 Loss_Enh_Dec: -0.95048296\n",
      "| epoch  30 |   900/ 4361 batches | lr 0.000000 | ms/batch 402.00 | loss  3.91 | ppl    50.01 | acc     0.58 | train_ae_norm     1.00\n",
      "[30/200][999/4361] Loss_D: 0.04484851 (Loss_D_real: 0.01739438 Loss_D_fake: 0.02745414) Loss_G: 0.28246766 Loss_Enh_Dec: -1.34537780\n",
      "| epoch  30 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  3.89 | ppl    48.91 | acc     0.56 | train_ae_norm     1.00\n",
      "[30/200][1099/4361] Loss_D: 0.07169662 (Loss_D_real: 0.04024457 Loss_D_fake: 0.03145204) Loss_G: 0.26953074 Loss_Enh_Dec: -1.45016217\n",
      "| epoch  30 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  3.91 | ppl    49.76 | acc     0.49 | train_ae_norm     1.00\n",
      "[30/200][1199/4361] Loss_D: 0.08631758 (Loss_D_real: 0.05473708 Loss_D_fake: 0.03158050) Loss_G: 0.33099589 Loss_Enh_Dec: -1.25970721\n",
      "| epoch  30 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  3.97 | ppl    52.81 | acc     0.58 | train_ae_norm     1.00\n",
      "[30/200][1299/4361] Loss_D: 0.03506382 (Loss_D_real: 0.01905937 Loss_D_fake: 0.01600445) Loss_G: 0.28664654 Loss_Enh_Dec: -0.97055018\n",
      "| epoch  30 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  3.93 | ppl    50.83 | acc     0.52 | train_ae_norm     1.00\n",
      "[30/200][1399/4361] Loss_D: 0.02168085 (Loss_D_real: 0.00364442 Loss_D_fake: 0.01803643) Loss_G: 0.29603544 Loss_Enh_Dec: -1.12257183\n",
      "| epoch  30 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  3.94 | ppl    51.56 | acc     0.50 | train_ae_norm     1.00\n",
      "[30/200][1499/4361] Loss_D: 0.01257310 (Loss_D_real: 0.00438953 Loss_D_fake: 0.00818356) Loss_G: 0.29678905 Loss_Enh_Dec: -1.31294155\n",
      "| epoch  30 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  3.96 | ppl    52.30 | acc     0.54 | train_ae_norm     1.00\n",
      "[30/200][1599/4361] Loss_D: 0.01547583 (Loss_D_real: 0.00412136 Loss_D_fake: 0.01135447) Loss_G: 0.28204724 Loss_Enh_Dec: -1.01785517\n",
      "| epoch  30 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  3.93 | ppl    50.68 | acc     0.55 | train_ae_norm     1.00\n",
      "[30/200][1699/4361] Loss_D: 0.02398223 (Loss_D_real: 0.00734179 Loss_D_fake: 0.01664045) Loss_G: 0.32126257 Loss_Enh_Dec: -1.20419109\n",
      "| epoch  30 |  1700/ 4361 batches | lr 0.000000 | ms/batch 402.19 | loss  3.94 | ppl    51.62 | acc     0.53 | train_ae_norm     1.00\n",
      "[30/200][1799/4361] Loss_D: 0.08489376 (Loss_D_real: 0.06632370 Loss_D_fake: 0.01857006) Loss_G: 0.26236543 Loss_Enh_Dec: -1.16324997\n",
      "| epoch  30 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  3.90 | ppl    49.19 | acc     0.57 | train_ae_norm     1.00\n",
      "[30/200][1899/4361] Loss_D: 0.02370351 (Loss_D_real: 0.00612669 Loss_D_fake: 0.01757682) Loss_G: 0.27811420 Loss_Enh_Dec: -0.75691885\n",
      "| epoch  30 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  3.97 | ppl    52.91 | acc     0.55 | train_ae_norm     1.00\n",
      "[30/200][1999/4361] Loss_D: 0.05624865 (Loss_D_real: 0.04170934 Loss_D_fake: 0.01453931) Loss_G: 0.28210324 Loss_Enh_Dec: -0.50814569\n",
      "| epoch  30 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  3.90 | ppl    49.30 | acc     0.57 | train_ae_norm     1.00\n",
      "[30/200][2099/4361] Loss_D: 0.02629534 (Loss_D_real: 0.00344078 Loss_D_fake: 0.02285456) Loss_G: 0.27564049 Loss_Enh_Dec: -1.03638303\n",
      "| epoch  30 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  3.91 | ppl    49.96 | acc     0.57 | train_ae_norm     1.00\n",
      "[30/200][2199/4361] Loss_D: 0.01783014 (Loss_D_real: 0.00376230 Loss_D_fake: 0.01406784) Loss_G: 0.29910618 Loss_Enh_Dec: -0.73613018\n",
      "| epoch  30 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  3.90 | ppl    49.55 | acc     0.55 | train_ae_norm     1.00\n",
      "[30/200][2299/4361] Loss_D: 0.02960909 (Loss_D_real: 0.02321385 Loss_D_fake: 0.00639524) Loss_G: 0.28313977 Loss_Enh_Dec: -0.96171302\n",
      "| epoch  30 |  2300/ 4361 batches | lr 0.000000 | ms/batch 402.66 | loss  3.93 | ppl    50.67 | acc     0.54 | train_ae_norm     1.00\n",
      "[30/200][2399/4361] Loss_D: 0.06090572 (Loss_D_real: 0.04034258 Loss_D_fake: 0.02056314) Loss_G: 0.28324851 Loss_Enh_Dec: -0.61795062\n",
      "| epoch  30 |  2400/ 4361 batches | lr 0.000000 | ms/batch 402.14 | loss  3.89 | ppl    48.97 | acc     0.52 | train_ae_norm     1.00\n",
      "[30/200][2499/4361] Loss_D: 0.21834654 (Loss_D_real: 0.00571237 Loss_D_fake: 0.21263418) Loss_G: 0.35167941 Loss_Enh_Dec: -1.16793287\n",
      "| epoch  30 |  2500/ 4361 batches | lr 0.000000 | ms/batch 402.49 | loss  3.95 | ppl    51.77 | acc     0.58 | train_ae_norm     1.00\n",
      "[30/200][2599/4361] Loss_D: 0.04943913 (Loss_D_real: 0.02806317 Loss_D_fake: 0.02137597) Loss_G: 0.27942464 Loss_Enh_Dec: -1.03793156\n",
      "| epoch  30 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  3.91 | ppl    50.08 | acc     0.54 | train_ae_norm     1.00\n",
      "[30/200][2699/4361] Loss_D: 0.03569454 (Loss_D_real: 0.00309341 Loss_D_fake: 0.03260113) Loss_G: 0.30145100 Loss_Enh_Dec: -1.13244092\n",
      "| epoch  30 |  2700/ 4361 batches | lr 0.000000 | ms/batch 400.80 | loss  3.91 | ppl    49.72 | acc     0.57 | train_ae_norm     1.00\n",
      "[30/200][2799/4361] Loss_D: 0.02958266 (Loss_D_real: 0.00850892 Loss_D_fake: 0.02107374) Loss_G: 0.32992187 Loss_Enh_Dec: -0.92660755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  30 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  3.86 | ppl    47.38 | acc     0.54 | train_ae_norm     1.00\n",
      "[30/200][2899/4361] Loss_D: 0.03728964 (Loss_D_real: 0.02783206 Loss_D_fake: 0.00945758) Loss_G: 0.31216127 Loss_Enh_Dec: -1.19124377\n",
      "| epoch  30 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  3.88 | ppl    48.23 | acc     0.59 | train_ae_norm     1.00\n",
      "[30/200][2999/4361] Loss_D: 0.01427097 (Loss_D_real: 0.00627243 Loss_D_fake: 0.00799854) Loss_G: 0.34792542 Loss_Enh_Dec: -1.27516305\n",
      "| epoch  30 |  3000/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  3.86 | ppl    47.63 | acc     0.58 | train_ae_norm     1.00\n",
      "[30/200][3099/4361] Loss_D: 0.03078642 (Loss_D_real: 0.00483973 Loss_D_fake: 0.02594669) Loss_G: 0.32411152 Loss_Enh_Dec: -1.47190821\n",
      "| epoch  30 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  3.84 | ppl    46.58 | acc     0.55 | train_ae_norm     1.00\n",
      "[30/200][3199/4361] Loss_D: 0.02108639 (Loss_D_real: 0.00709263 Loss_D_fake: 0.01399377) Loss_G: 0.33075568 Loss_Enh_Dec: -1.15646899\n",
      "| epoch  30 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  3.84 | ppl    46.74 | acc     0.57 | train_ae_norm     1.00\n",
      "[30/200][3299/4361] Loss_D: 0.01295364 (Loss_D_real: 0.00539956 Loss_D_fake: 0.00755409) Loss_G: 0.28678921 Loss_Enh_Dec: -1.10843790\n",
      "| epoch  30 |  3300/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  3.84 | ppl    46.46 | acc     0.58 | train_ae_norm     1.00\n",
      "[30/200][3399/4361] Loss_D: 0.02677621 (Loss_D_real: 0.00393702 Loss_D_fake: 0.02283919) Loss_G: 0.31664744 Loss_Enh_Dec: -1.21174037\n",
      "| epoch  30 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  3.86 | ppl    47.66 | acc     0.58 | train_ae_norm     1.00\n",
      "[30/200][3499/4361] Loss_D: 0.15684019 (Loss_D_real: 0.15057114 Loss_D_fake: 0.00626905) Loss_G: 0.29407787 Loss_Enh_Dec: -1.19332230\n",
      "| epoch  30 |  3500/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  3.77 | ppl    43.18 | acc     0.59 | train_ae_norm     1.00\n",
      "[30/200][3599/4361] Loss_D: 0.07545111 (Loss_D_real: 0.06695460 Loss_D_fake: 0.00849651) Loss_G: 0.39859921 Loss_Enh_Dec: -1.36782265\n",
      "| epoch  30 |  3600/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  3.79 | ppl    44.46 | acc     0.59 | train_ae_norm     1.00\n",
      "[30/200][3699/4361] Loss_D: 0.02467335 (Loss_D_real: 0.01228284 Loss_D_fake: 0.01239051) Loss_G: 0.30051991 Loss_Enh_Dec: -1.27256954\n",
      "| epoch  30 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  3.80 | ppl    44.73 | acc     0.55 | train_ae_norm     1.00\n",
      "[30/200][3799/4361] Loss_D: 0.04568079 (Loss_D_real: 0.01638858 Loss_D_fake: 0.02929220) Loss_G: 0.26948267 Loss_Enh_Dec: -1.08841765\n",
      "| epoch  30 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  3.83 | ppl    45.90 | acc     0.59 | train_ae_norm     1.00\n",
      "[30/200][3899/4361] Loss_D: 0.03740177 (Loss_D_real: 0.01437618 Loss_D_fake: 0.02302559) Loss_G: 0.28172871 Loss_Enh_Dec: -0.72123450\n",
      "| epoch  30 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  3.82 | ppl    45.52 | acc     0.53 | train_ae_norm     1.00\n",
      "[30/200][3999/4361] Loss_D: 0.03511938 (Loss_D_real: 0.01658384 Loss_D_fake: 0.01853554) Loss_G: 0.27715436 Loss_Enh_Dec: -0.81811565\n",
      "| epoch  30 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  3.82 | ppl    45.72 | acc     0.57 | train_ae_norm     1.00\n",
      "[30/200][4099/4361] Loss_D: 0.01141997 (Loss_D_real: 0.00430752 Loss_D_fake: 0.00711245) Loss_G: 0.28422543 Loss_Enh_Dec: -0.93171704\n",
      "| epoch  30 |  4100/ 4361 batches | lr 0.000000 | ms/batch 402.28 | loss  3.78 | ppl    43.79 | acc     0.57 | train_ae_norm     1.00\n",
      "[30/200][4199/4361] Loss_D: 0.01259512 (Loss_D_real: 0.00274866 Loss_D_fake: 0.00984646) Loss_G: 0.27673358 Loss_Enh_Dec: -0.95427513\n",
      "| epoch  30 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  3.81 | ppl    45.27 | acc     0.60 | train_ae_norm     1.00\n",
      "[30/200][4299/4361] Loss_D: 0.04604389 (Loss_D_real: 0.00854333 Loss_D_fake: 0.03750056) Loss_G: 0.29568300 Loss_Enh_Dec: -1.04433906\n",
      "| epoch  30 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  3.77 | ppl    43.48 | acc     0.58 | train_ae_norm     1.00\n",
      "| end of epoch  30 | time: 1853.21s | test loss  3.63 | test ppl 37.83 | acc 0.627\n",
      "bleu_self:  [7.34126984e-02 2.20044053e-09 7.28372149e-12 2.46320757e-12\n",
      " 1.70103030e-11]\n",
      "bleu_test:  [7.37698412e-01 2.13585107e-01 1.97521323e-06 5.93975714e-07\n",
      " 4.33334167e-07]\n",
      "bleu_self: [0.07341270,0.00000000,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.73769841,0.21358511,0.00000198,0.00000059,0.00000043]\n",
      "New saving model: epoch 030.\n",
      "Saving models to ./results/yahoo_merge_assigned_results\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 31 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.718\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.472\n",
      "  Test Loss: 3.636\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  31 |     0/ 4361 batches | lr 0.000000 | ms/batch 866.55 | loss  0.03 | ppl     1.04 | acc     0.61 | train_ae_norm     1.00\n",
      "[31/200][99/4361] Loss_D: 0.01877985 (Loss_D_real: 0.00522538 Loss_D_fake: 0.01355446) Loss_G: 0.29791829 Loss_Enh_Dec: -1.04762232\n",
      "| epoch  31 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  3.80 | ppl    44.68 | acc     0.55 | train_ae_norm     1.00\n",
      "[31/200][199/4361] Loss_D: 0.06825770 (Loss_D_real: 0.03047617 Loss_D_fake: 0.03778153) Loss_G: 0.31132868 Loss_Enh_Dec: -1.05152762\n",
      "| epoch  31 |   200/ 4361 batches | lr 0.000000 | ms/batch 402.25 | loss  3.84 | ppl    46.65 | acc     0.59 | train_ae_norm     1.00\n",
      "[31/200][299/4361] Loss_D: 0.01230099 (Loss_D_real: 0.00181047 Loss_D_fake: 0.01049052) Loss_G: 0.31229314 Loss_Enh_Dec: -1.32889700\n",
      "| epoch  31 |   300/ 4361 batches | lr 0.000000 | ms/batch 402.28 | loss  3.83 | ppl    45.90 | acc     0.56 | train_ae_norm     1.00\n",
      "[31/200][399/4361] Loss_D: 0.01491840 (Loss_D_real: 0.00293924 Loss_D_fake: 0.01197917) Loss_G: 0.27877083 Loss_Enh_Dec: -1.39863145\n",
      "| epoch  31 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  3.75 | ppl    42.35 | acc     0.59 | train_ae_norm     1.00\n",
      "[31/200][499/4361] Loss_D: 0.06316977 (Loss_D_real: 0.05499333 Loss_D_fake: 0.00817644) Loss_G: 0.30052078 Loss_Enh_Dec: -1.15218532\n",
      "| epoch  31 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  3.81 | ppl    45.33 | acc     0.60 | train_ae_norm     1.00\n",
      "[31/200][599/4361] Loss_D: 0.02199213 (Loss_D_real: 0.00849173 Loss_D_fake: 0.01350040) Loss_G: 0.31141624 Loss_Enh_Dec: -1.23712718\n",
      "| epoch  31 |   600/ 4361 batches | lr 0.000000 | ms/batch 402.50 | loss  3.77 | ppl    43.25 | acc     0.53 | train_ae_norm     1.00\n",
      "[31/200][699/4361] Loss_D: 0.01859928 (Loss_D_real: 0.00759590 Loss_D_fake: 0.01100339) Loss_G: 0.31037250 Loss_Enh_Dec: -1.09569085\n",
      "| epoch  31 |   700/ 4361 batches | lr 0.000000 | ms/batch 402.11 | loss  3.84 | ppl    46.30 | acc     0.62 | train_ae_norm     1.00\n",
      "[31/200][799/4361] Loss_D: 0.02991525 (Loss_D_real: 0.01140209 Loss_D_fake: 0.01851315) Loss_G: 0.32012960 Loss_Enh_Dec: -0.79733783\n",
      "| epoch  31 |   800/ 4361 batches | lr 0.000000 | ms/batch 402.19 | loss  3.81 | ppl    44.98 | acc     0.56 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31/200][899/4361] Loss_D: 0.03109912 (Loss_D_real: 0.00596936 Loss_D_fake: 0.02512976) Loss_G: 0.29948446 Loss_Enh_Dec: -0.56116629\n",
      "| epoch  31 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  3.83 | ppl    46.03 | acc     0.58 | train_ae_norm     1.00\n",
      "[31/200][999/4361] Loss_D: 0.12098791 (Loss_D_real: 0.10001531 Loss_D_fake: 0.02097259) Loss_G: 0.32011595 Loss_Enh_Dec: -0.95660889\n",
      "| epoch  31 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  3.79 | ppl    44.38 | acc     0.57 | train_ae_norm     1.00\n",
      "[31/200][1099/4361] Loss_D: 0.01917037 (Loss_D_real: 0.01133789 Loss_D_fake: 0.00783248) Loss_G: 0.28159890 Loss_Enh_Dec: -1.01754141\n",
      "| epoch  31 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  3.80 | ppl    44.79 | acc     0.54 | train_ae_norm     1.00\n",
      "[31/200][1199/4361] Loss_D: 0.00966048 (Loss_D_real: 0.00569840 Loss_D_fake: 0.00396208) Loss_G: 0.37636933 Loss_Enh_Dec: -1.30536973\n",
      "| epoch  31 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  3.82 | ppl    45.52 | acc     0.58 | train_ae_norm     1.00\n",
      "[31/200][1299/4361] Loss_D: 0.04383856 (Loss_D_real: 0.02709424 Loss_D_fake: 0.01674432) Loss_G: 0.29906031 Loss_Enh_Dec: -1.25918114\n",
      "| epoch  31 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  3.82 | ppl    45.49 | acc     0.58 | train_ae_norm     1.00\n",
      "[31/200][1399/4361] Loss_D: 0.04425488 (Loss_D_real: 0.02099241 Loss_D_fake: 0.02326247) Loss_G: 0.29097578 Loss_Enh_Dec: -0.89494532\n",
      "| epoch  31 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  3.81 | ppl    45.22 | acc     0.51 | train_ae_norm     1.00\n",
      "[31/200][1499/4361] Loss_D: 0.03776424 (Loss_D_real: 0.02361474 Loss_D_fake: 0.01414950) Loss_G: 0.31433043 Loss_Enh_Dec: -0.86628097\n",
      "| epoch  31 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  3.85 | ppl    46.99 | acc     0.53 | train_ae_norm     1.00\n",
      "[31/200][1599/4361] Loss_D: 0.18632509 (Loss_D_real: 0.16798276 Loss_D_fake: 0.01834233) Loss_G: 0.28110099 Loss_Enh_Dec: -0.62689942\n",
      "| epoch  31 |  1600/ 4361 batches | lr 0.000000 | ms/batch 402.40 | loss  3.82 | ppl    45.46 | acc     0.56 | train_ae_norm     1.00\n",
      "[31/200][1699/4361] Loss_D: 0.04654787 (Loss_D_real: 0.02193778 Loss_D_fake: 0.02461008) Loss_G: 0.31052169 Loss_Enh_Dec: -0.75538206\n",
      "| epoch  31 |  1700/ 4361 batches | lr 0.000000 | ms/batch 402.41 | loss  3.79 | ppl    44.30 | acc     0.55 | train_ae_norm     1.00\n",
      "[31/200][1799/4361] Loss_D: 0.01621691 (Loss_D_real: 0.00517713 Loss_D_fake: 0.01103978) Loss_G: 0.30345199 Loss_Enh_Dec: -0.72187060\n",
      "| epoch  31 |  1800/ 4361 batches | lr 0.000000 | ms/batch 402.30 | loss  3.85 | ppl    47.19 | acc     0.54 | train_ae_norm     1.00\n",
      "[31/200][1899/4361] Loss_D: 0.01828322 (Loss_D_real: 0.00510405 Loss_D_fake: 0.01317918) Loss_G: 0.29715571 Loss_Enh_Dec: -0.61871558\n",
      "| epoch  31 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  3.99 | ppl    54.21 | acc     0.60 | train_ae_norm     1.00\n",
      "[31/200][1999/4361] Loss_D: 0.03199147 (Loss_D_real: 0.02195993 Loss_D_fake: 0.01003154) Loss_G: 0.31954327 Loss_Enh_Dec: -0.98471308\n",
      "| epoch  31 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  3.80 | ppl    44.60 | acc     0.57 | train_ae_norm     1.00\n",
      "[31/200][2099/4361] Loss_D: 0.01365603 (Loss_D_real: 0.00262811 Loss_D_fake: 0.01102792) Loss_G: 0.27624747 Loss_Enh_Dec: -0.96419472\n",
      "| epoch  31 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  3.81 | ppl    45.03 | acc     0.60 | train_ae_norm     1.00\n",
      "[31/200][2199/4361] Loss_D: 0.02560096 (Loss_D_real: 0.01353350 Loss_D_fake: 0.01206746) Loss_G: 0.29342625 Loss_Enh_Dec: -0.64681959\n",
      "| epoch  31 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  3.81 | ppl    44.93 | acc     0.58 | train_ae_norm     1.00\n",
      "[31/200][2299/4361] Loss_D: 0.02447100 (Loss_D_real: 0.00667975 Loss_D_fake: 0.01779125) Loss_G: 0.32166100 Loss_Enh_Dec: -0.78392977\n",
      "| epoch  31 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  3.78 | ppl    43.62 | acc     0.59 | train_ae_norm     1.00\n",
      "[31/200][2399/4361] Loss_D: 0.02077143 (Loss_D_real: 0.01062562 Loss_D_fake: 0.01014582) Loss_G: 0.28733426 Loss_Enh_Dec: -1.09785306\n",
      "| epoch  31 |  2400/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  3.77 | ppl    43.58 | acc     0.54 | train_ae_norm     1.00\n",
      "[31/200][2499/4361] Loss_D: 0.04630207 (Loss_D_real: 0.00187562 Loss_D_fake: 0.04442645) Loss_G: 0.27202699 Loss_Enh_Dec: -0.68487185\n",
      "| epoch  31 |  2500/ 4361 batches | lr 0.000000 | ms/batch 402.11 | loss  3.81 | ppl    45.12 | acc     0.57 | train_ae_norm     1.00\n",
      "[31/200][2599/4361] Loss_D: 0.02774412 (Loss_D_real: 0.00252325 Loss_D_fake: 0.02522087) Loss_G: 0.30707815 Loss_Enh_Dec: -1.16019320\n",
      "| epoch  31 |  2600/ 4361 batches | lr 0.000000 | ms/batch 402.00 | loss  3.79 | ppl    44.30 | acc     0.56 | train_ae_norm     1.00\n",
      "[31/200][2699/4361] Loss_D: 0.01773623 (Loss_D_real: 0.00971693 Loss_D_fake: 0.00801930) Loss_G: 0.31128192 Loss_Enh_Dec: -1.25846088\n",
      "| epoch  31 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  3.81 | ppl    45.12 | acc     0.57 | train_ae_norm     1.00\n",
      "[31/200][2799/4361] Loss_D: 0.01553159 (Loss_D_real: 0.00528436 Loss_D_fake: 0.01024723) Loss_G: 0.28149167 Loss_Enh_Dec: -1.20083272\n",
      "| epoch  31 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  3.73 | ppl    41.84 | acc     0.57 | train_ae_norm     1.00\n",
      "[31/200][2899/4361] Loss_D: 0.02828803 (Loss_D_real: 0.01824524 Loss_D_fake: 0.01004279) Loss_G: 0.31069979 Loss_Enh_Dec: -1.16545212\n",
      "| epoch  31 |  2900/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  3.78 | ppl    43.95 | acc     0.61 | train_ae_norm     1.00\n",
      "[31/200][2999/4361] Loss_D: 0.01945469 (Loss_D_real: 0.00605548 Loss_D_fake: 0.01339921) Loss_G: 0.31580177 Loss_Enh_Dec: -0.97019637\n",
      "| epoch  31 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  3.78 | ppl    44.02 | acc     0.58 | train_ae_norm     1.00\n",
      "[31/200][3099/4361] Loss_D: 0.02409703 (Loss_D_real: 0.00495088 Loss_D_fake: 0.01914615) Loss_G: 0.29569227 Loss_Enh_Dec: -1.02232993\n",
      "| epoch  31 |  3100/ 4361 batches | lr 0.000000 | ms/batch 402.09 | loss  3.79 | ppl    44.23 | acc     0.56 | train_ae_norm     1.00\n",
      "[31/200][3199/4361] Loss_D: 0.04401650 (Loss_D_real: 0.01812047 Loss_D_fake: 0.02589603) Loss_G: 0.33838579 Loss_Enh_Dec: -1.36286056\n",
      "| epoch  31 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  3.80 | ppl    44.65 | acc     0.57 | train_ae_norm     1.00\n",
      "[31/200][3299/4361] Loss_D: 0.03051581 (Loss_D_real: 0.00580998 Loss_D_fake: 0.02470583) Loss_G: 0.29983497 Loss_Enh_Dec: -1.34297550\n",
      "| epoch  31 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  3.83 | ppl    45.96 | acc     0.57 | train_ae_norm     1.00\n",
      "[31/200][3399/4361] Loss_D: 0.02259886 (Loss_D_real: 0.01193867 Loss_D_fake: 0.01066019) Loss_G: 0.30532756 Loss_Enh_Dec: -1.09616911\n",
      "| epoch  31 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.74 | loss  3.80 | ppl    44.66 | acc     0.59 | train_ae_norm     1.00\n",
      "[31/200][3499/4361] Loss_D: 0.05414909 (Loss_D_real: 0.04386844 Loss_D_fake: 0.01028064) Loss_G: 0.32483003 Loss_Enh_Dec: -0.89840412\n",
      "| epoch  31 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.52 | loss  3.73 | ppl    41.59 | acc     0.59 | train_ae_norm     1.00\n",
      "[31/200][3599/4361] Loss_D: 0.02501524 (Loss_D_real: 0.01419138 Loss_D_fake: 0.01082386) Loss_G: 0.29067984 Loss_Enh_Dec: -1.02746081\n",
      "| epoch  31 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  3.74 | ppl    41.92 | acc     0.59 | train_ae_norm     1.00\n",
      "[31/200][3699/4361] Loss_D: 0.32904220 (Loss_D_real: 0.03317314 Loss_D_fake: 0.29586905) Loss_G: 0.54733026 Loss_Enh_Dec: -1.01799357\n",
      "| epoch  31 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.89 | loss  3.76 | ppl    42.88 | acc     0.58 | train_ae_norm     1.00\n",
      "[31/200][3799/4361] Loss_D: 0.01538020 (Loss_D_real: 0.00672867 Loss_D_fake: 0.00865153) Loss_G: 0.32763997 Loss_Enh_Dec: -0.73855162\n",
      "| epoch  31 |  3800/ 4361 batches | lr 0.000000 | ms/batch 400.42 | loss  3.78 | ppl    43.87 | acc     0.60 | train_ae_norm     1.00\n",
      "[31/200][3899/4361] Loss_D: 0.05073054 (Loss_D_real: 0.03906175 Loss_D_fake: 0.01166879) Loss_G: 0.30035242 Loss_Enh_Dec: -1.02284646\n",
      "| epoch  31 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  3.76 | ppl    42.82 | acc     0.56 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31/200][3999/4361] Loss_D: 0.00992980 (Loss_D_real: 0.00267516 Loss_D_fake: 0.00725465) Loss_G: 0.35021076 Loss_Enh_Dec: -1.27540231\n",
      "| epoch  31 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  3.78 | ppl    44.02 | acc     0.57 | train_ae_norm     1.00\n",
      "[31/200][4099/4361] Loss_D: 0.04167500 (Loss_D_real: 0.03248236 Loss_D_fake: 0.00919264) Loss_G: 0.33971611 Loss_Enh_Dec: -1.23186982\n",
      "| epoch  31 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  3.73 | ppl    41.79 | acc     0.58 | train_ae_norm     1.00\n",
      "[31/200][4199/4361] Loss_D: 0.01252860 (Loss_D_real: 0.00551111 Loss_D_fake: 0.00701749) Loss_G: 0.28551060 Loss_Enh_Dec: -1.36445153\n",
      "| epoch  31 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  3.77 | ppl    43.36 | acc     0.60 | train_ae_norm     1.00\n",
      "[31/200][4299/4361] Loss_D: 0.00567307 (Loss_D_real: 0.00217507 Loss_D_fake: 0.00349800) Loss_G: 0.29002324 Loss_Enh_Dec: -1.42581213\n",
      "| epoch  31 |  4300/ 4361 batches | lr 0.000000 | ms/batch 402.11 | loss  3.72 | ppl    41.24 | acc     0.60 | train_ae_norm     1.00\n",
      "| end of epoch  31 | time: 1854.24s | test loss  3.62 | test ppl 37.50 | acc 0.627\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 32 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.721\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.492\n",
      "  Test Loss: 3.517\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  32 |     0/ 4361 batches | lr 0.000000 | ms/batch 867.07 | loss  0.03 | ppl     1.04 | acc     0.60 | train_ae_norm     1.00\n",
      "[32/200][99/4361] Loss_D: 0.01148601 (Loss_D_real: 0.00537601 Loss_D_fake: 0.00611001) Loss_G: 0.37954414 Loss_Enh_Dec: -0.99737686\n",
      "| epoch  32 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  3.73 | ppl    41.62 | acc     0.57 | train_ae_norm     1.00\n",
      "[32/200][199/4361] Loss_D: 0.01498465 (Loss_D_real: 0.00519095 Loss_D_fake: 0.00979371) Loss_G: 0.29935241 Loss_Enh_Dec: -1.38311231\n",
      "| epoch  32 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  3.74 | ppl    42.26 | acc     0.60 | train_ae_norm     1.00\n",
      "[32/200][299/4361] Loss_D: 0.01118905 (Loss_D_real: 0.00311526 Loss_D_fake: 0.00807379) Loss_G: 0.34233236 Loss_Enh_Dec: -0.77473879\n",
      "| epoch  32 |   300/ 4361 batches | lr 0.000000 | ms/batch 402.42 | loss  3.84 | ppl    46.43 | acc     0.58 | train_ae_norm     1.00\n",
      "[32/200][399/4361] Loss_D: 0.04293618 (Loss_D_real: 0.03536793 Loss_D_fake: 0.00756826) Loss_G: 0.29079065 Loss_Enh_Dec: -1.09168267\n",
      "| epoch  32 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  3.64 | ppl    38.06 | acc     0.60 | train_ae_norm     1.00\n",
      "[32/200][499/4361] Loss_D: 0.01154364 (Loss_D_real: 0.00711395 Loss_D_fake: 0.00442969) Loss_G: 0.28830370 Loss_Enh_Dec: -0.70640695\n",
      "| epoch  32 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  3.72 | ppl    41.39 | acc     0.60 | train_ae_norm     1.00\n",
      "[32/200][599/4361] Loss_D: 0.00693723 (Loss_D_real: 0.00170036 Loss_D_fake: 0.00523687) Loss_G: 0.28411931 Loss_Enh_Dec: -0.56784439\n",
      "| epoch  32 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  3.69 | ppl    40.07 | acc     0.53 | train_ae_norm     1.00\n",
      "[32/200][699/4361] Loss_D: 0.01895611 (Loss_D_real: 0.00097685 Loss_D_fake: 0.01797926) Loss_G: 0.30150148 Loss_Enh_Dec: -0.38211417\n",
      "| epoch  32 |   700/ 4361 batches | lr 0.000000 | ms/batch 402.52 | loss  3.72 | ppl    41.47 | acc     0.62 | train_ae_norm     1.00\n",
      "[32/200][799/4361] Loss_D: 0.00872573 (Loss_D_real: 0.00388677 Loss_D_fake: 0.00483895) Loss_G: 0.31863260 Loss_Enh_Dec: -0.77926534\n",
      "| epoch  32 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  3.68 | ppl    39.46 | acc     0.63 | train_ae_norm     1.00\n",
      "[32/200][899/4361] Loss_D: 0.04176567 (Loss_D_real: 0.03422922 Loss_D_fake: 0.00753646) Loss_G: 0.30792484 Loss_Enh_Dec: -1.23780727\n",
      "| epoch  32 |   900/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  3.70 | ppl    40.25 | acc     0.62 | train_ae_norm     1.00\n",
      "[32/200][999/4361] Loss_D: 0.03681151 (Loss_D_real: 0.03406931 Loss_D_fake: 0.00274219) Loss_G: 0.37878665 Loss_Enh_Dec: -1.56505382\n",
      "| epoch  32 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  3.66 | ppl    39.02 | acc     0.60 | train_ae_norm     1.00\n",
      "[32/200][1099/4361] Loss_D: 0.00700034 (Loss_D_real: 0.00204163 Loss_D_fake: 0.00495871) Loss_G: 0.31204286 Loss_Enh_Dec: -0.85932714\n",
      "| epoch  32 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  3.66 | ppl    38.85 | acc     0.58 | train_ae_norm     1.00\n",
      "[32/200][1199/4361] Loss_D: 0.03260307 (Loss_D_real: 0.02493120 Loss_D_fake: 0.00767188) Loss_G: 0.32504654 Loss_Enh_Dec: -0.56020141\n",
      "| epoch  32 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  3.69 | ppl    40.19 | acc     0.63 | train_ae_norm     1.00\n",
      "[32/200][1299/4361] Loss_D: 0.03755428 (Loss_D_real: 0.02283044 Loss_D_fake: 0.01472384) Loss_G: 0.32560068 Loss_Enh_Dec: -0.82604408\n",
      "| epoch  32 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  3.68 | ppl    39.81 | acc     0.59 | train_ae_norm     1.00\n",
      "[32/200][1399/4361] Loss_D: 0.03353591 (Loss_D_real: 0.02109379 Loss_D_fake: 0.01244212) Loss_G: 0.34508359 Loss_Enh_Dec: -1.16296947\n",
      "| epoch  32 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  3.71 | ppl    40.85 | acc     0.55 | train_ae_norm     1.00\n",
      "[32/200][1499/4361] Loss_D: 0.02016457 (Loss_D_real: 0.01520373 Loss_D_fake: 0.00496085) Loss_G: 0.31529310 Loss_Enh_Dec: -1.30926037\n",
      "| epoch  32 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  3.75 | ppl    42.53 | acc     0.59 | train_ae_norm     1.00\n",
      "[32/200][1599/4361] Loss_D: 0.01224294 (Loss_D_real: 0.00240066 Loss_D_fake: 0.00984228) Loss_G: 0.32578942 Loss_Enh_Dec: -1.45236957\n",
      "| epoch  32 |  1600/ 4361 batches | lr 0.000000 | ms/batch 402.19 | loss  3.68 | ppl    39.84 | acc     0.58 | train_ae_norm     1.00\n",
      "[32/200][1699/4361] Loss_D: 0.02555429 (Loss_D_real: 0.00497030 Loss_D_fake: 0.02058399) Loss_G: 0.36379644 Loss_Enh_Dec: -1.15306926\n",
      "| epoch  32 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  3.66 | ppl    39.03 | acc     0.57 | train_ae_norm     1.00\n",
      "[32/200][1799/4361] Loss_D: 0.01426489 (Loss_D_real: 0.00857515 Loss_D_fake: 0.00568974) Loss_G: 0.37340942 Loss_Enh_Dec: -1.24183071\n",
      "| epoch  32 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  3.64 | ppl    37.98 | acc     0.61 | train_ae_norm     1.00\n",
      "[32/200][1899/4361] Loss_D: 0.00800232 (Loss_D_real: 0.00413919 Loss_D_fake: 0.00386313) Loss_G: 0.32788444 Loss_Enh_Dec: -1.06345463\n",
      "| epoch  32 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  3.71 | ppl    40.72 | acc     0.60 | train_ae_norm     1.00\n",
      "[32/200][1999/4361] Loss_D: 0.01187350 (Loss_D_real: 0.00669742 Loss_D_fake: 0.00517608) Loss_G: 0.33353660 Loss_Enh_Dec: -0.82316285\n",
      "| epoch  32 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  3.63 | ppl    37.86 | acc     0.59 | train_ae_norm     1.00\n",
      "[32/200][2099/4361] Loss_D: 0.03005194 (Loss_D_real: 0.01451103 Loss_D_fake: 0.01554092) Loss_G: 0.33284912 Loss_Enh_Dec: -1.35033977\n",
      "| epoch  32 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  3.66 | ppl    38.79 | acc     0.58 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32/200][2199/4361] Loss_D: 0.01813951 (Loss_D_real: 0.01184343 Loss_D_fake: 0.00629608) Loss_G: 0.35285211 Loss_Enh_Dec: -0.94951242\n",
      "| epoch  32 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  3.66 | ppl    39.02 | acc     0.61 | train_ae_norm     1.00\n",
      "[32/200][2299/4361] Loss_D: 0.01253270 (Loss_D_real: 0.00486967 Loss_D_fake: 0.00766303) Loss_G: 0.35827893 Loss_Enh_Dec: -1.23600101\n",
      "| epoch  32 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  3.66 | ppl    38.80 | acc     0.61 | train_ae_norm     1.00\n",
      "[32/200][2399/4361] Loss_D: 0.01441760 (Loss_D_real: 0.01127923 Loss_D_fake: 0.00313837) Loss_G: 0.33772677 Loss_Enh_Dec: -1.16471183\n",
      "| epoch  32 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  3.73 | ppl    41.50 | acc     0.56 | train_ae_norm     1.00\n",
      "[32/200][2499/4361] Loss_D: 0.01370842 (Loss_D_real: 0.00403541 Loss_D_fake: 0.00967301) Loss_G: 0.31622067 Loss_Enh_Dec: -1.12564301\n",
      "| epoch  32 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  3.68 | ppl    39.83 | acc     0.61 | train_ae_norm     1.00\n",
      "[32/200][2599/4361] Loss_D: 0.01622923 (Loss_D_real: 0.00897554 Loss_D_fake: 0.00725368) Loss_G: 0.33724666 Loss_Enh_Dec: -0.78178722\n",
      "| epoch  32 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  3.66 | ppl    38.93 | acc     0.57 | train_ae_norm     1.00\n",
      "[32/200][2699/4361] Loss_D: 0.02215066 (Loss_D_real: 0.01683534 Loss_D_fake: 0.00531532) Loss_G: 0.32031122 Loss_Enh_Dec: -1.05378711\n",
      "| epoch  32 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  3.67 | ppl    39.12 | acc     0.58 | train_ae_norm     1.00\n",
      "[32/200][2799/4361] Loss_D: 0.00879026 (Loss_D_real: 0.00449330 Loss_D_fake: 0.00429695) Loss_G: 0.33982056 Loss_Enh_Dec: -0.84240973\n",
      "| epoch  32 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  3.63 | ppl    37.61 | acc     0.57 | train_ae_norm     1.00\n",
      "[32/200][2899/4361] Loss_D: 0.00770196 (Loss_D_real: 0.00254486 Loss_D_fake: 0.00515710) Loss_G: 0.36536977 Loss_Enh_Dec: -0.71418488\n",
      "| epoch  32 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  3.66 | ppl    38.76 | acc     0.59 | train_ae_norm     1.00\n",
      "[32/200][2999/4361] Loss_D: 0.01307333 (Loss_D_real: 0.00927571 Loss_D_fake: 0.00379761) Loss_G: 0.38815960 Loss_Enh_Dec: -1.00418508\n",
      "| epoch  32 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  3.70 | ppl    40.55 | acc     0.58 | train_ae_norm     1.00\n",
      "[32/200][3099/4361] Loss_D: 0.01008516 (Loss_D_real: 0.00405959 Loss_D_fake: 0.00602558) Loss_G: 0.31170794 Loss_Enh_Dec: -1.23220587\n",
      "| epoch  32 |  3100/ 4361 batches | lr 0.000000 | ms/batch 402.05 | loss  3.69 | ppl    39.92 | acc     0.58 | train_ae_norm     1.00\n",
      "[32/200][3199/4361] Loss_D: 0.00532437 (Loss_D_real: 0.00375816 Loss_D_fake: 0.00156621) Loss_G: 0.38204858 Loss_Enh_Dec: -0.77183050\n",
      "| epoch  32 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  3.73 | ppl    41.60 | acc     0.61 | train_ae_norm     1.00\n",
      "[32/200][3299/4361] Loss_D: 0.01112964 (Loss_D_real: 0.00692415 Loss_D_fake: 0.00420549) Loss_G: 0.35929188 Loss_Enh_Dec: -1.08718395\n",
      "| epoch  32 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  3.68 | ppl    39.54 | acc     0.60 | train_ae_norm     1.00\n",
      "[32/200][3399/4361] Loss_D: 0.00856673 (Loss_D_real: 0.00158062 Loss_D_fake: 0.00698611) Loss_G: 0.31786859 Loss_Enh_Dec: -1.15898311\n",
      "| epoch  32 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.99 | loss  3.67 | ppl    39.37 | acc     0.58 | train_ae_norm     1.00\n",
      "[32/200][3499/4361] Loss_D: 0.12139956 (Loss_D_real: 0.11946560 Loss_D_fake: 0.00193395) Loss_G: 0.39057466 Loss_Enh_Dec: -1.20272326\n",
      "| epoch  32 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  3.62 | ppl    37.29 | acc     0.60 | train_ae_norm     1.00\n",
      "[32/200][3599/4361] Loss_D: 0.01673692 (Loss_D_real: 0.00852484 Loss_D_fake: 0.00821209) Loss_G: 0.39645988 Loss_Enh_Dec: -1.18104959\n",
      "| epoch  32 |  3600/ 4361 batches | lr 0.000000 | ms/batch 402.03 | loss  3.64 | ppl    38.08 | acc     0.62 | train_ae_norm     1.00\n",
      "[32/200][3699/4361] Loss_D: 0.00891300 (Loss_D_real: 0.00293071 Loss_D_fake: 0.00598229) Loss_G: 0.35472667 Loss_Enh_Dec: -0.83123225\n",
      "| epoch  32 |  3700/ 4361 batches | lr 0.000000 | ms/batch 402.42 | loss  3.64 | ppl    38.19 | acc     0.58 | train_ae_norm     1.00\n",
      "[32/200][3799/4361] Loss_D: 0.03388433 (Loss_D_real: 0.02493307 Loss_D_fake: 0.00895127) Loss_G: 0.30368420 Loss_Enh_Dec: -0.93021154\n",
      "| epoch  32 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  3.65 | ppl    38.36 | acc     0.64 | train_ae_norm     1.00\n",
      "[32/200][3899/4361] Loss_D: 0.01529329 (Loss_D_real: 0.00645964 Loss_D_fake: 0.00883365) Loss_G: 0.30652204 Loss_Enh_Dec: -1.11882126\n",
      "| epoch  32 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  3.66 | ppl    38.84 | acc     0.57 | train_ae_norm     1.00\n",
      "[32/200][3999/4361] Loss_D: 0.01061851 (Loss_D_real: 0.00425928 Loss_D_fake: 0.00635923) Loss_G: 0.35310861 Loss_Enh_Dec: -1.35719383\n",
      "| epoch  32 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  3.67 | ppl    39.29 | acc     0.61 | train_ae_norm     1.00\n",
      "[32/200][4099/4361] Loss_D: 0.01317668 (Loss_D_real: 0.00265404 Loss_D_fake: 0.01052264) Loss_G: 0.29545251 Loss_Enh_Dec: -1.28737569\n",
      "| epoch  32 |  4100/ 4361 batches | lr 0.000000 | ms/batch 402.03 | loss  3.66 | ppl    38.77 | acc     0.58 | train_ae_norm     1.00\n",
      "[32/200][4199/4361] Loss_D: 0.01785429 (Loss_D_real: 0.01261868 Loss_D_fake: 0.00523561) Loss_G: 0.32915327 Loss_Enh_Dec: -0.92374098\n",
      "| epoch  32 |  4200/ 4361 batches | lr 0.000000 | ms/batch 402.53 | loss  3.70 | ppl    40.33 | acc     0.61 | train_ae_norm     1.00\n",
      "[32/200][4299/4361] Loss_D: 0.00919982 (Loss_D_real: 0.00394914 Loss_D_fake: 0.00525068) Loss_G: 0.31324899 Loss_Enh_Dec: -0.95271987\n",
      "| epoch  32 |  4300/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  3.65 | ppl    38.52 | acc     0.60 | train_ae_norm     1.00\n",
      "| end of epoch  32 | time: 1854.65s | test loss  3.52 | test ppl 33.94 | acc 0.644\n",
      "bleu_self:  [1.33680556e-01 5.25546597e-02 4.85866819e-07 1.62679477e-09\n",
      " 3.99491878e-10]\n",
      "bleu_test:  [6.68055555e-01 1.56238003e-01 1.14747552e-06 3.43411665e-09\n",
      " 8.53538475e-10]\n",
      "bleu_self: [0.13368056,0.05255466,0.00000049,0.00000000,0.00000000]\n",
      "bleu_test: [0.66805556,0.15623800,0.00000115,0.00000000,0.00000000]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 33 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.719\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.490\n",
      "  Test Loss: 3.550\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  33 |     0/ 4361 batches | lr 0.000000 | ms/batch 865.25 | loss  0.03 | ppl     1.03 | acc     0.63 | train_ae_norm     1.00\n",
      "[33/200][99/4361] Loss_D: 0.03712476 (Loss_D_real: 0.03228818 Loss_D_fake: 0.00483659) Loss_G: 0.33865830 Loss_Enh_Dec: -1.02451611\n",
      "| epoch  33 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.22 | loss  3.68 | ppl    39.72 | acc     0.53 | train_ae_norm     1.00\n",
      "[33/200][199/4361] Loss_D: 0.01735205 (Loss_D_real: 0.00380430 Loss_D_fake: 0.01354774) Loss_G: 0.34902319 Loss_Enh_Dec: -0.67566735\n",
      "| epoch  33 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  3.72 | ppl    41.43 | acc     0.60 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33/200][299/4361] Loss_D: 0.01401756 (Loss_D_real: 0.00614401 Loss_D_fake: 0.00787354) Loss_G: 0.34048203 Loss_Enh_Dec: -1.10815144\n",
      "| epoch  33 |   300/ 4361 batches | lr 0.000000 | ms/batch 402.26 | loss  3.72 | ppl    41.14 | acc     0.58 | train_ae_norm     1.00\n",
      "[33/200][399/4361] Loss_D: 0.01890448 (Loss_D_real: 0.00956311 Loss_D_fake: 0.00934137) Loss_G: 0.35260826 Loss_Enh_Dec: -0.58023465\n",
      "| epoch  33 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  3.63 | ppl    37.54 | acc     0.61 | train_ae_norm     1.00\n",
      "[33/200][499/4361] Loss_D: 0.00610249 (Loss_D_real: 0.00236683 Loss_D_fake: 0.00373566) Loss_G: 0.33097148 Loss_Enh_Dec: -0.86899871\n",
      "| epoch  33 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  3.74 | ppl    41.92 | acc     0.61 | train_ae_norm     1.00\n",
      "[33/200][599/4361] Loss_D: 0.00926257 (Loss_D_real: 0.00606920 Loss_D_fake: 0.00319338) Loss_G: 0.41061154 Loss_Enh_Dec: -0.40960789\n",
      "| epoch  33 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  3.65 | ppl    38.37 | acc     0.55 | train_ae_norm     1.00\n",
      "[33/200][699/4361] Loss_D: 0.00974842 (Loss_D_real: 0.00465655 Loss_D_fake: 0.00509188) Loss_G: 0.30862901 Loss_Enh_Dec: -0.74453318\n",
      "| epoch  33 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  3.71 | ppl    40.69 | acc     0.62 | train_ae_norm     1.00\n",
      "[33/200][799/4361] Loss_D: 0.00694346 (Loss_D_real: 0.00472499 Loss_D_fake: 0.00221847) Loss_G: 0.36045036 Loss_Enh_Dec: -0.68324369\n",
      "| epoch  33 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  3.66 | ppl    38.82 | acc     0.60 | train_ae_norm     1.00\n",
      "[33/200][899/4361] Loss_D: 0.00691076 (Loss_D_real: 0.00176422 Loss_D_fake: 0.00514654) Loss_G: 0.38132176 Loss_Enh_Dec: -0.72011417\n",
      "| epoch  33 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  3.66 | ppl    38.74 | acc     0.62 | train_ae_norm     1.00\n",
      "[33/200][999/4361] Loss_D: 0.01654758 (Loss_D_real: 0.00219099 Loss_D_fake: 0.01435659) Loss_G: 0.29756936 Loss_Enh_Dec: -0.89570868\n",
      "| epoch  33 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  3.64 | ppl    38.25 | acc     0.62 | train_ae_norm     1.00\n",
      "[33/200][1099/4361] Loss_D: 0.01571243 (Loss_D_real: 0.00391840 Loss_D_fake: 0.01179403) Loss_G: 0.31486017 Loss_Enh_Dec: -0.99120933\n",
      "| epoch  33 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  3.62 | ppl    37.28 | acc     0.59 | train_ae_norm     1.00\n",
      "[33/200][1199/4361] Loss_D: 0.01418025 (Loss_D_real: 0.00472884 Loss_D_fake: 0.00945141) Loss_G: 0.33171090 Loss_Enh_Dec: -0.76874846\n",
      "| epoch  33 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.48 | loss  3.64 | ppl    38.07 | acc     0.62 | train_ae_norm     1.00\n",
      "[33/200][1299/4361] Loss_D: 0.01063899 (Loss_D_real: 0.00272157 Loss_D_fake: 0.00791742) Loss_G: 0.32117957 Loss_Enh_Dec: -0.51334161\n",
      "| epoch  33 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  3.67 | ppl    39.38 | acc     0.59 | train_ae_norm     1.00\n",
      "[33/200][1399/4361] Loss_D: 0.03102311 (Loss_D_real: 0.00685770 Loss_D_fake: 0.02416541) Loss_G: 0.31370464 Loss_Enh_Dec: -0.46015549\n",
      "| epoch  33 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  3.65 | ppl    38.66 | acc     0.54 | train_ae_norm     1.00\n",
      "[33/200][1499/4361] Loss_D: 0.03636188 (Loss_D_real: 0.02798736 Loss_D_fake: 0.00837451) Loss_G: 0.34444007 Loss_Enh_Dec: -0.47796759\n",
      "| epoch  33 |  1500/ 4361 batches | lr 0.000000 | ms/batch 402.16 | loss  3.70 | ppl    40.53 | acc     0.57 | train_ae_norm     1.00\n",
      "[33/200][1599/4361] Loss_D: 0.03430815 (Loss_D_real: 0.01709763 Loss_D_fake: 0.01721052) Loss_G: 0.51196080 Loss_Enh_Dec: -0.73201436\n",
      "| epoch  33 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  3.66 | ppl    38.76 | acc     0.61 | train_ae_norm     1.00\n",
      "[33/200][1699/4361] Loss_D: 0.03491428 (Loss_D_real: 0.02969375 Loss_D_fake: 0.00522053) Loss_G: 0.31924039 Loss_Enh_Dec: -0.76948947\n",
      "| epoch  33 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  3.66 | ppl    38.83 | acc     0.60 | train_ae_norm     1.00\n",
      "[33/200][1799/4361] Loss_D: 0.01184989 (Loss_D_real: 0.00598404 Loss_D_fake: 0.00586586) Loss_G: 0.34855697 Loss_Enh_Dec: -0.94525605\n",
      "| epoch  33 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  3.63 | ppl    37.71 | acc     0.59 | train_ae_norm     1.00\n",
      "[33/200][1899/4361] Loss_D: 0.05561936 (Loss_D_real: 0.00709421 Loss_D_fake: 0.04852515) Loss_G: 0.39514530 Loss_Enh_Dec: -0.90870392\n",
      "| epoch  33 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  3.69 | ppl    40.15 | acc     0.61 | train_ae_norm     1.00\n",
      "[33/200][1999/4361] Loss_D: 0.01193695 (Loss_D_real: 0.00805638 Loss_D_fake: 0.00388057) Loss_G: 0.45897895 Loss_Enh_Dec: -0.88079137\n",
      "| epoch  33 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  3.62 | ppl    37.50 | acc     0.60 | train_ae_norm     1.00\n",
      "[33/200][2099/4361] Loss_D: 0.01175265 (Loss_D_real: 0.00811222 Loss_D_fake: 0.00364043) Loss_G: 0.31438333 Loss_Enh_Dec: -1.13752139\n",
      "| epoch  33 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  3.64 | ppl    38.28 | acc     0.59 | train_ae_norm     1.00\n",
      "[33/200][2199/4361] Loss_D: 0.00408038 (Loss_D_real: 0.00118902 Loss_D_fake: 0.00289136) Loss_G: 0.33578119 Loss_Enh_Dec: -0.74367803\n",
      "| epoch  33 |  2200/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  3.66 | ppl    38.82 | acc     0.59 | train_ae_norm     1.00\n",
      "[33/200][2299/4361] Loss_D: 0.02030773 (Loss_D_real: 0.01473164 Loss_D_fake: 0.00557609) Loss_G: 0.32723224 Loss_Enh_Dec: -1.28115761\n",
      "| epoch  33 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  3.65 | ppl    38.41 | acc     0.61 | train_ae_norm     1.00\n",
      "[33/200][2399/4361] Loss_D: 0.01389806 (Loss_D_real: 0.00168120 Loss_D_fake: 0.01221686) Loss_G: 0.31761310 Loss_Enh_Dec: -1.13839829\n",
      "| epoch  33 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.99 | loss  3.66 | ppl    38.70 | acc     0.58 | train_ae_norm     1.00\n",
      "[33/200][2499/4361] Loss_D: 0.00755843 (Loss_D_real: 0.00136277 Loss_D_fake: 0.00619565) Loss_G: 0.33337802 Loss_Enh_Dec: -0.89125746\n",
      "| epoch  33 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  3.71 | ppl    40.98 | acc     0.62 | train_ae_norm     1.00\n",
      "[33/200][2599/4361] Loss_D: 0.01000797 (Loss_D_real: 0.00692921 Loss_D_fake: 0.00307876) Loss_G: 0.32159832 Loss_Enh_Dec: -0.88053268\n",
      "| epoch  33 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  3.67 | ppl    39.26 | acc     0.57 | train_ae_norm     1.00\n",
      "[33/200][2699/4361] Loss_D: 0.00596392 (Loss_D_real: 0.00073110 Loss_D_fake: 0.00523282) Loss_G: 0.32357821 Loss_Enh_Dec: -1.29118085\n",
      "| epoch  33 |  2700/ 4361 batches | lr 0.000000 | ms/batch 402.65 | loss  3.71 | ppl    40.92 | acc     0.60 | train_ae_norm     1.00\n",
      "[33/200][2799/4361] Loss_D: 0.01332735 (Loss_D_real: 0.00083394 Loss_D_fake: 0.01249341) Loss_G: 0.33065861 Loss_Enh_Dec: -1.45328248\n",
      "| epoch  33 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  3.62 | ppl    37.31 | acc     0.59 | train_ae_norm     1.00\n",
      "[33/200][2899/4361] Loss_D: 0.00843734 (Loss_D_real: 0.00346178 Loss_D_fake: 0.00497556) Loss_G: 0.32798490 Loss_Enh_Dec: -1.25249207\n",
      "| epoch  33 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  3.68 | ppl    39.50 | acc     0.60 | train_ae_norm     1.00\n",
      "[33/200][2999/4361] Loss_D: 0.01730258 (Loss_D_real: 0.01231824 Loss_D_fake: 0.00498434) Loss_G: 0.33368632 Loss_Enh_Dec: -1.41306949\n",
      "| epoch  33 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  3.68 | ppl    39.78 | acc     0.60 | train_ae_norm     1.00\n",
      "[33/200][3099/4361] Loss_D: 0.10397540 (Loss_D_real: 0.09735566 Loss_D_fake: 0.00661975) Loss_G: 0.32165694 Loss_Enh_Dec: -1.38418353\n",
      "| epoch  33 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  3.70 | ppl    40.29 | acc     0.59 | train_ae_norm     1.00\n",
      "[33/200][3199/4361] Loss_D: 0.01479116 (Loss_D_real: 0.01281129 Loss_D_fake: 0.00197986) Loss_G: 0.39631554 Loss_Enh_Dec: -1.16646028\n",
      "| epoch  33 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  3.74 | ppl    42.15 | acc     0.57 | train_ae_norm     1.00\n",
      "[33/200][3299/4361] Loss_D: 0.04202559 (Loss_D_real: 0.03658651 Loss_D_fake: 0.00543908) Loss_G: 0.30113479 Loss_Enh_Dec: -1.40830743\n",
      "| epoch  33 |  3300/ 4361 batches | lr 0.000000 | ms/batch 400.91 | loss  3.77 | ppl    43.34 | acc     0.60 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33/200][3399/4361] Loss_D: 0.01011462 (Loss_D_real: 0.00272643 Loss_D_fake: 0.00738818) Loss_G: 0.36566469 Loss_Enh_Dec: -1.38878703\n",
      "| epoch  33 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  3.73 | ppl    41.49 | acc     0.59 | train_ae_norm     1.00\n",
      "[33/200][3499/4361] Loss_D: 0.01467701 (Loss_D_real: 0.00283253 Loss_D_fake: 0.01184447) Loss_G: 0.30500886 Loss_Enh_Dec: -1.08668554\n",
      "| epoch  33 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  3.68 | ppl    39.75 | acc     0.61 | train_ae_norm     1.00\n",
      "[33/200][3599/4361] Loss_D: 0.06073876 (Loss_D_real: 0.05082101 Loss_D_fake: 0.00991776) Loss_G: 0.34230047 Loss_Enh_Dec: -1.10507154\n",
      "| epoch  33 |  3600/ 4361 batches | lr 0.000000 | ms/batch 402.11 | loss  3.72 | ppl    41.40 | acc     0.59 | train_ae_norm     1.00\n",
      "[33/200][3699/4361] Loss_D: 0.06510981 (Loss_D_real: 0.06060772 Loss_D_fake: 0.00450209) Loss_G: 0.34776163 Loss_Enh_Dec: -1.02370465\n",
      "| epoch  33 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  3.77 | ppl    43.29 | acc     0.55 | train_ae_norm     1.00\n",
      "[33/200][3799/4361] Loss_D: 0.01166032 (Loss_D_real: 0.00176177 Loss_D_fake: 0.00989855) Loss_G: 0.37936613 Loss_Enh_Dec: -1.18981063\n",
      "| epoch  33 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  3.73 | ppl    41.56 | acc     0.61 | train_ae_norm     1.00\n",
      "[33/200][3899/4361] Loss_D: 0.04445718 (Loss_D_real: 0.03976922 Loss_D_fake: 0.00468797) Loss_G: 0.36930352 Loss_Enh_Dec: -1.27180731\n",
      "| epoch  33 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  3.72 | ppl    41.33 | acc     0.56 | train_ae_norm     1.00\n",
      "[33/200][3999/4361] Loss_D: 0.03496008 (Loss_D_real: 0.02252374 Loss_D_fake: 0.01243634) Loss_G: 0.29909715 Loss_Enh_Dec: -1.36066139\n",
      "| epoch  33 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  3.75 | ppl    42.33 | acc     0.58 | train_ae_norm     1.00\n",
      "[33/200][4099/4361] Loss_D: 0.01208636 (Loss_D_real: 0.00589478 Loss_D_fake: 0.00619158) Loss_G: 0.35991582 Loss_Enh_Dec: -0.93625420\n",
      "| epoch  33 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  3.69 | ppl    39.99 | acc     0.59 | train_ae_norm     1.00\n",
      "[33/200][4199/4361] Loss_D: 0.02410427 (Loss_D_real: 0.01507248 Loss_D_fake: 0.00903179) Loss_G: 0.35780045 Loss_Enh_Dec: -1.31091964\n",
      "| epoch  33 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  3.74 | ppl    42.16 | acc     0.59 | train_ae_norm     1.00\n",
      "[33/200][4299/4361] Loss_D: 0.01085984 (Loss_D_real: 0.00195786 Loss_D_fake: 0.00890198) Loss_G: 0.31995389 Loss_Enh_Dec: -1.05577028\n",
      "| epoch  33 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  3.70 | ppl    40.39 | acc     0.58 | train_ae_norm     1.00\n",
      "| end of epoch  33 | time: 1853.70s | test loss  3.59 | test ppl 36.14 | acc 0.631\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 34 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:57.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.713\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.480\n",
      "  Test Loss: 3.699\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  34 |     0/ 4361 batches | lr 0.000000 | ms/batch 868.06 | loss  0.03 | ppl     1.04 | acc     0.61 | train_ae_norm     1.00\n",
      "[34/200][99/4361] Loss_D: 0.02556602 (Loss_D_real: 0.01534319 Loss_D_fake: 0.01022282) Loss_G: 0.35377160 Loss_Enh_Dec: -1.41776502\n",
      "| epoch  34 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.20 | loss  3.72 | ppl    41.20 | acc     0.57 | train_ae_norm     1.00\n",
      "[34/200][199/4361] Loss_D: 0.00751955 (Loss_D_real: 0.00254581 Loss_D_fake: 0.00497374) Loss_G: 0.34093779 Loss_Enh_Dec: -1.17115462\n",
      "| epoch  34 |   200/ 4361 batches | lr 0.000000 | ms/batch 402.12 | loss  3.76 | ppl    42.86 | acc     0.60 | train_ae_norm     1.00\n",
      "[34/200][299/4361] Loss_D: 0.00304520 (Loss_D_real: 0.00105703 Loss_D_fake: 0.00198817) Loss_G: 0.35431632 Loss_Enh_Dec: -1.15135717\n",
      "| epoch  34 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  3.74 | ppl    42.23 | acc     0.55 | train_ae_norm     1.00\n",
      "[34/200][399/4361] Loss_D: 0.04240020 (Loss_D_real: 0.03814505 Loss_D_fake: 0.00425515) Loss_G: 0.33779553 Loss_Enh_Dec: -1.19545591\n",
      "| epoch  34 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  3.64 | ppl    38.24 | acc     0.62 | train_ae_norm     1.00\n",
      "[34/200][499/4361] Loss_D: 0.03494483 (Loss_D_real: 0.00196089 Loss_D_fake: 0.03298394) Loss_G: 0.33535439 Loss_Enh_Dec: -1.51703644\n",
      "| epoch  34 |   500/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  3.72 | ppl    41.36 | acc     0.60 | train_ae_norm     1.00\n",
      "[34/200][599/4361] Loss_D: 0.09403209 (Loss_D_real: 0.08825038 Loss_D_fake: 0.00578170) Loss_G: 0.32128122 Loss_Enh_Dec: -1.12360179\n",
      "| epoch  34 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  3.67 | ppl    39.39 | acc     0.55 | train_ae_norm     1.00\n",
      "[34/200][699/4361] Loss_D: 0.03177498 (Loss_D_real: 0.02357805 Loss_D_fake: 0.00819693) Loss_G: 0.37634572 Loss_Enh_Dec: -0.64773697\n",
      "| epoch  34 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  3.71 | ppl    41.02 | acc     0.63 | train_ae_norm     1.00\n",
      "[34/200][799/4361] Loss_D: 0.00660752 (Loss_D_real: 0.00484319 Loss_D_fake: 0.00176432) Loss_G: 0.34558856 Loss_Enh_Dec: -1.27586675\n",
      "| epoch  34 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  3.68 | ppl    39.55 | acc     0.59 | train_ae_norm     1.00\n",
      "[34/200][899/4361] Loss_D: 0.03470212 (Loss_D_real: 0.02728675 Loss_D_fake: 0.00741537) Loss_G: 0.31417570 Loss_Enh_Dec: -0.77826917\n",
      "| epoch  34 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  3.71 | ppl    40.70 | acc     0.62 | train_ae_norm     1.00\n",
      "[34/200][999/4361] Loss_D: 0.01257538 (Loss_D_real: 0.00291431 Loss_D_fake: 0.00966107) Loss_G: 0.31004411 Loss_Enh_Dec: -0.73554879\n",
      "| epoch  34 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  3.70 | ppl    40.55 | acc     0.60 | train_ae_norm     1.00\n",
      "[34/200][1099/4361] Loss_D: 0.01098820 (Loss_D_real: 0.00461977 Loss_D_fake: 0.00636843) Loss_G: 0.31353366 Loss_Enh_Dec: -0.90608597\n",
      "| epoch  34 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  3.69 | ppl    40.20 | acc     0.56 | train_ae_norm     1.00\n",
      "[34/200][1199/4361] Loss_D: 0.00934817 (Loss_D_real: 0.00722134 Loss_D_fake: 0.00212683) Loss_G: 0.51102239 Loss_Enh_Dec: -0.86581326\n",
      "| epoch  34 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  3.74 | ppl    42.11 | acc     0.61 | train_ae_norm     1.00\n",
      "[34/200][1299/4361] Loss_D: 0.01803675 (Loss_D_real: 0.00651052 Loss_D_fake: 0.01152623) Loss_G: 0.31276873 Loss_Enh_Dec: -1.05975378\n",
      "| epoch  34 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  3.75 | ppl    42.31 | acc     0.59 | train_ae_norm     1.00\n",
      "[34/200][1399/4361] Loss_D: 0.03565681 (Loss_D_real: 0.00668580 Loss_D_fake: 0.02897100) Loss_G: 0.38672987 Loss_Enh_Dec: -0.75424021\n",
      "| epoch  34 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  3.75 | ppl    42.50 | acc     0.54 | train_ae_norm     1.00\n",
      "[34/200][1499/4361] Loss_D: 0.02835611 (Loss_D_real: 0.02159534 Loss_D_fake: 0.00676077) Loss_G: 0.32015261 Loss_Enh_Dec: -0.76120776\n",
      "| epoch  34 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.99 | loss  3.78 | ppl    43.84 | acc     0.57 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34/200][1599/4361] Loss_D: 0.03177774 (Loss_D_real: 0.02601618 Loss_D_fake: 0.00576156) Loss_G: 0.33348024 Loss_Enh_Dec: -0.49118719\n",
      "| epoch  34 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  3.74 | ppl    42.01 | acc     0.60 | train_ae_norm     1.00\n",
      "[34/200][1699/4361] Loss_D: 0.03473125 (Loss_D_real: 0.00875338 Loss_D_fake: 0.02597788) Loss_G: 0.30728036 Loss_Enh_Dec: -0.43392855\n",
      "| epoch  34 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  3.73 | ppl    41.59 | acc     0.56 | train_ae_norm     1.00\n",
      "[34/200][1799/4361] Loss_D: 0.00927728 (Loss_D_real: 0.00309245 Loss_D_fake: 0.00618484) Loss_G: 0.44269830 Loss_Enh_Dec: -0.38277358\n",
      "| epoch  34 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  3.70 | ppl    40.46 | acc     0.59 | train_ae_norm     1.00\n",
      "[34/200][1899/4361] Loss_D: 0.00679155 (Loss_D_real: 0.00139006 Loss_D_fake: 0.00540149) Loss_G: 0.29869500 Loss_Enh_Dec: -0.98652333\n",
      "| epoch  34 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  3.76 | ppl    43.07 | acc     0.60 | train_ae_norm     1.00\n",
      "[34/200][1999/4361] Loss_D: 0.14889051 (Loss_D_real: 0.14362238 Loss_D_fake: 0.00526813) Loss_G: 0.38887873 Loss_Enh_Dec: -0.11642303\n",
      "| epoch  34 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  3.68 | ppl    39.67 | acc     0.60 | train_ae_norm     1.00\n",
      "[34/200][2099/4361] Loss_D: 0.01452267 (Loss_D_real: 0.00595689 Loss_D_fake: 0.00856578) Loss_G: 0.32229400 Loss_Enh_Dec: -0.78790259\n",
      "| epoch  34 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.91 | loss  3.70 | ppl    40.42 | acc     0.59 | train_ae_norm     1.00\n",
      "[34/200][2199/4361] Loss_D: 0.00987299 (Loss_D_real: 0.00432873 Loss_D_fake: 0.00554426) Loss_G: 0.29745021 Loss_Enh_Dec: -0.84808266\n",
      "| epoch  34 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  3.70 | ppl    40.33 | acc     0.59 | train_ae_norm     1.00\n",
      "[34/200][2299/4361] Loss_D: 0.01517574 (Loss_D_real: 0.00625150 Loss_D_fake: 0.00892424) Loss_G: 0.32507262 Loss_Enh_Dec: -0.70480728\n",
      "| epoch  34 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  3.67 | ppl    39.34 | acc     0.60 | train_ae_norm     1.00\n",
      "[34/200][2399/4361] Loss_D: 0.03478516 (Loss_D_real: 0.02004543 Loss_D_fake: 0.01473973) Loss_G: 0.31687790 Loss_Enh_Dec: -0.91163427\n",
      "| epoch  34 |  2400/ 4361 batches | lr 0.000000 | ms/batch 402.55 | loss  3.69 | ppl    40.02 | acc     0.57 | train_ae_norm     1.00\n",
      "[34/200][2499/4361] Loss_D: 0.00992370 (Loss_D_real: 0.00304798 Loss_D_fake: 0.00687572) Loss_G: 0.33997652 Loss_Enh_Dec: -0.72328311\n",
      "| epoch  34 |  2500/ 4361 batches | lr 0.000000 | ms/batch 402.30 | loss  3.72 | ppl    41.24 | acc     0.61 | train_ae_norm     1.00\n",
      "[34/200][2599/4361] Loss_D: 0.01208181 (Loss_D_real: 0.00332413 Loss_D_fake: 0.00875768) Loss_G: 0.29821190 Loss_Enh_Dec: -0.92462063\n",
      "| epoch  34 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  3.69 | ppl    40.09 | acc     0.57 | train_ae_norm     1.00\n",
      "[34/200][2699/4361] Loss_D: 0.00832280 (Loss_D_real: 0.00125201 Loss_D_fake: 0.00707078) Loss_G: 0.30395105 Loss_Enh_Dec: -1.13623989\n",
      "| epoch  34 |  2700/ 4361 batches | lr 0.000000 | ms/batch 402.12 | loss  3.69 | ppl    40.18 | acc     0.60 | train_ae_norm     1.00\n",
      "[34/200][2799/4361] Loss_D: 0.01335648 (Loss_D_real: 0.00774209 Loss_D_fake: 0.00561439) Loss_G: 0.36305881 Loss_Enh_Dec: -1.13866937\n",
      "| epoch  34 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  3.66 | ppl    38.76 | acc     0.58 | train_ae_norm     1.00\n",
      "[34/200][2899/4361] Loss_D: 0.02495742 (Loss_D_real: 0.01955502 Loss_D_fake: 0.00540240) Loss_G: 0.30419412 Loss_Enh_Dec: -1.27149999\n",
      "| epoch  34 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  3.68 | ppl    39.79 | acc     0.61 | train_ae_norm     1.00\n",
      "[34/200][2999/4361] Loss_D: 0.00427527 (Loss_D_real: 0.00311333 Loss_D_fake: 0.00116194) Loss_G: 0.52413934 Loss_Enh_Dec: -1.16966808\n",
      "| epoch  34 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.72 | loss  3.68 | ppl    39.70 | acc     0.60 | train_ae_norm     1.00\n",
      "[34/200][3099/4361] Loss_D: 0.02350002 (Loss_D_real: 0.01830409 Loss_D_fake: 0.00519593) Loss_G: 0.32299706 Loss_Enh_Dec: -0.97577983\n",
      "| epoch  34 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  3.69 | ppl    39.86 | acc     0.54 | train_ae_norm     1.00\n",
      "[34/200][3199/4361] Loss_D: 0.00594588 (Loss_D_real: 0.00215236 Loss_D_fake: 0.00379352) Loss_G: 0.34458134 Loss_Enh_Dec: -1.29378271\n",
      "| epoch  34 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  3.69 | ppl    39.96 | acc     0.58 | train_ae_norm     1.00\n",
      "[34/200][3299/4361] Loss_D: 0.02342119 (Loss_D_real: 0.01366468 Loss_D_fake: 0.00975651) Loss_G: 0.31762537 Loss_Enh_Dec: -1.34130394\n",
      "| epoch  34 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  3.73 | ppl    41.66 | acc     0.59 | train_ae_norm     1.00\n",
      "[34/200][3399/4361] Loss_D: 0.01233442 (Loss_D_real: 0.00639756 Loss_D_fake: 0.00593686) Loss_G: 0.29940784 Loss_Enh_Dec: -1.22448444\n",
      "| epoch  34 |  3400/ 4361 batches | lr 0.000000 | ms/batch 402.54 | loss  3.72 | ppl    41.13 | acc     0.57 | train_ae_norm     1.00\n",
      "[34/200][3499/4361] Loss_D: 0.01142072 (Loss_D_real: 0.00265843 Loss_D_fake: 0.00876229) Loss_G: 0.30356446 Loss_Enh_Dec: -1.29631841\n",
      "| epoch  34 |  3500/ 4361 batches | lr 0.000000 | ms/batch 402.07 | loss  3.65 | ppl    38.36 | acc     0.59 | train_ae_norm     1.00\n",
      "[34/200][3599/4361] Loss_D: 0.04902818 (Loss_D_real: 0.03868458 Loss_D_fake: 0.01034360) Loss_G: 0.26356462 Loss_Enh_Dec: -1.04142654\n",
      "| epoch  34 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  3.64 | ppl    38.00 | acc     0.60 | train_ae_norm     1.00\n",
      "[34/200][3699/4361] Loss_D: 0.03877036 (Loss_D_real: 0.02229108 Loss_D_fake: 0.01647929) Loss_G: 0.30872017 Loss_Enh_Dec: -0.89202070\n",
      "| epoch  34 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  3.65 | ppl    38.45 | acc     0.59 | train_ae_norm     1.00\n",
      "[34/200][3799/4361] Loss_D: 0.01435529 (Loss_D_real: 0.00464192 Loss_D_fake: 0.00971337) Loss_G: 0.33123520 Loss_Enh_Dec: -0.72138876\n",
      "| epoch  34 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  3.68 | ppl    39.64 | acc     0.64 | train_ae_norm     1.00\n",
      "[34/200][3899/4361] Loss_D: 0.02215386 (Loss_D_real: 0.00275231 Loss_D_fake: 0.01940155) Loss_G: 0.32631823 Loss_Enh_Dec: -0.78141755\n",
      "| epoch  34 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  3.68 | ppl    39.76 | acc     0.58 | train_ae_norm     1.00\n",
      "[34/200][3999/4361] Loss_D: 0.01509100 (Loss_D_real: 0.00525062 Loss_D_fake: 0.00984037) Loss_G: 0.33202723 Loss_Enh_Dec: -0.73500043\n",
      "| epoch  34 |  4000/ 4361 batches | lr 0.000000 | ms/batch 402.12 | loss  3.70 | ppl    40.64 | acc     0.60 | train_ae_norm     1.00\n",
      "[34/200][4099/4361] Loss_D: 0.02013542 (Loss_D_real: 0.01432696 Loss_D_fake: 0.00580846) Loss_G: 0.30034608 Loss_Enh_Dec: -0.82755804\n",
      "| epoch  34 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  3.99 | ppl    53.83 | acc     0.45 | train_ae_norm     1.00\n",
      "[34/200][4199/4361] Loss_D: 0.06396831 (Loss_D_real: 0.00343467 Loss_D_fake: 0.06053364) Loss_G: 0.41501364 Loss_Enh_Dec: -0.79165781\n",
      "| epoch  34 |  4200/ 4361 batches | lr 0.000000 | ms/batch 402.66 | loss  4.64 | ppl   103.13 | acc     0.52 | train_ae_norm     1.00\n",
      "[34/200][4299/4361] Loss_D: 0.01591418 (Loss_D_real: 0.00629898 Loss_D_fake: 0.00961520) Loss_G: 0.33558702 Loss_Enh_Dec: -1.17920148\n",
      "| epoch  34 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  4.47 | ppl    86.97 | acc     0.42 | train_ae_norm     1.00\n",
      "| end of epoch  34 | time: 1854.41s | test loss  4.83 | test ppl 125.12 | acc 0.391\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 35 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.703\n",
      "  Average training loss discriminator: 0.710\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.475\n",
      "  Test Loss: 3.742\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  35 |     0/ 4361 batches | lr 0.000000 | ms/batch 866.37 | loss  0.05 | ppl     1.05 | acc     0.40 | train_ae_norm     1.00\n",
      "[35/200][99/4361] Loss_D: 0.05318007 (Loss_D_real: 0.03003431 Loss_D_fake: 0.02314575) Loss_G: 0.40515938 Loss_Enh_Dec: -1.42102838\n",
      "| epoch  35 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  4.85 | ppl   127.46 | acc     0.43 | train_ae_norm     1.00\n",
      "[35/200][199/4361] Loss_D: 0.01164913 (Loss_D_real: 0.00344123 Loss_D_fake: 0.00820790) Loss_G: 0.33610991 Loss_Enh_Dec: -1.38410723\n",
      "| epoch  35 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  4.85 | ppl   127.84 | acc     0.39 | train_ae_norm     1.00\n",
      "[35/200][299/4361] Loss_D: 0.03052603 (Loss_D_real: 0.00848184 Loss_D_fake: 0.02204419) Loss_G: 0.32628152 Loss_Enh_Dec: -1.60450196\n",
      "| epoch  35 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  4.81 | ppl   122.23 | acc     0.37 | train_ae_norm     1.00\n",
      "[35/200][399/4361] Loss_D: 0.02037818 (Loss_D_real: 0.01317553 Loss_D_fake: 0.00720265) Loss_G: 0.32499757 Loss_Enh_Dec: -1.61156881\n",
      "| epoch  35 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  4.68 | ppl   107.79 | acc     0.44 | train_ae_norm     1.00\n",
      "[35/200][499/4361] Loss_D: 0.01488082 (Loss_D_real: 0.00718002 Loss_D_fake: 0.00770079) Loss_G: 0.32942900 Loss_Enh_Dec: -1.13376653\n",
      "| epoch  35 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  4.69 | ppl   109.07 | acc     0.44 | train_ae_norm     1.00\n",
      "[35/200][599/4361] Loss_D: 0.02021285 (Loss_D_real: 0.00738121 Loss_D_fake: 0.01283165) Loss_G: 0.32537040 Loss_Enh_Dec: -1.15366137\n",
      "| epoch  35 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  4.63 | ppl   102.29 | acc     0.38 | train_ae_norm     1.00\n",
      "[35/200][699/4361] Loss_D: 0.01205730 (Loss_D_real: 0.00266416 Loss_D_fake: 0.00939314) Loss_G: 0.32236889 Loss_Enh_Dec: -1.24307466\n",
      "| epoch  35 |   700/ 4361 batches | lr 0.000000 | ms/batch 402.68 | loss  4.62 | ppl   101.94 | acc     0.43 | train_ae_norm     1.00\n",
      "[35/200][799/4361] Loss_D: 0.01971807 (Loss_D_real: 0.01026985 Loss_D_fake: 0.00944822) Loss_G: 0.55057389 Loss_Enh_Dec: -1.01614857\n",
      "| epoch  35 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  4.59 | ppl    98.60 | acc     0.44 | train_ae_norm     1.00\n",
      "[35/200][899/4361] Loss_D: 0.02168958 (Loss_D_real: 0.00323659 Loss_D_fake: 0.01845299) Loss_G: 0.33721849 Loss_Enh_Dec: -1.28950298\n",
      "| epoch  35 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  4.59 | ppl    98.94 | acc     0.45 | train_ae_norm     1.00\n",
      "[35/200][999/4361] Loss_D: 0.01142546 (Loss_D_real: 0.00428610 Loss_D_fake: 0.00713936) Loss_G: 0.32913491 Loss_Enh_Dec: -1.28112984\n",
      "| epoch  35 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  4.57 | ppl    96.45 | acc     0.42 | train_ae_norm     1.00\n",
      "[35/200][1099/4361] Loss_D: 0.13850930 (Loss_D_real: 0.00628966 Loss_D_fake: 0.13221964) Loss_G: 0.38449979 Loss_Enh_Dec: -1.34056175\n",
      "| epoch  35 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  4.53 | ppl    93.21 | acc     0.45 | train_ae_norm     1.00\n",
      "[35/200][1199/4361] Loss_D: 0.03280783 (Loss_D_real: 0.00985158 Loss_D_fake: 0.02295625) Loss_G: 0.33935434 Loss_Enh_Dec: -1.13311863\n",
      "| epoch  35 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.91 | loss  4.48 | ppl    88.21 | acc     0.44 | train_ae_norm     1.00\n",
      "[35/200][1299/4361] Loss_D: 0.02679272 (Loss_D_real: 0.01230441 Loss_D_fake: 0.01448830) Loss_G: 0.33959892 Loss_Enh_Dec: -1.18378294\n",
      "| epoch  35 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  4.43 | ppl    83.96 | acc     0.41 | train_ae_norm     1.00\n",
      "[35/200][1399/4361] Loss_D: 1.65510106 (Loss_D_real: 0.00305061 Loss_D_fake: 1.65205050) Loss_G: 0.50689495 Loss_Enh_Dec: -1.08553159\n",
      "| epoch  35 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  4.20 | ppl    66.96 | acc     0.43 | train_ae_norm     1.00\n",
      "[35/200][1499/4361] Loss_D: 0.03171951 (Loss_D_real: 0.02365376 Loss_D_fake: 0.00806575) Loss_G: 0.29960489 Loss_Enh_Dec: -1.13955534\n",
      "| epoch  35 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  4.08 | ppl    59.44 | acc     0.53 | train_ae_norm     1.00\n",
      "[35/200][1599/4361] Loss_D: 0.03219631 (Loss_D_real: 0.01922092 Loss_D_fake: 0.01297539) Loss_G: 0.29804161 Loss_Enh_Dec: -0.39667824\n",
      "| epoch  35 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  3.89 | ppl    49.11 | acc     0.57 | train_ae_norm     1.00\n",
      "[35/200][1699/4361] Loss_D: 0.07892570 (Loss_D_real: 0.05450279 Loss_D_fake: 0.02442291) Loss_G: 0.32211465 Loss_Enh_Dec: -0.71049082\n",
      "| epoch  35 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  3.85 | ppl    46.89 | acc     0.56 | train_ae_norm     1.00\n",
      "[35/200][1799/4361] Loss_D: 0.02915722 (Loss_D_real: 0.01532418 Loss_D_fake: 0.01383304) Loss_G: 0.30269346 Loss_Enh_Dec: -0.66359359\n",
      "| epoch  35 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  3.78 | ppl    43.91 | acc     0.58 | train_ae_norm     1.00\n",
      "[35/200][1899/4361] Loss_D: 0.05890603 (Loss_D_real: 0.03899569 Loss_D_fake: 0.01991034) Loss_G: 0.32219300 Loss_Enh_Dec: -0.86127967\n",
      "| epoch  35 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  3.82 | ppl    45.78 | acc     0.59 | train_ae_norm     1.00\n",
      "[35/200][1999/4361] Loss_D: 0.04449310 (Loss_D_real: 0.03995612 Loss_D_fake: 0.00453698) Loss_G: 0.33733967 Loss_Enh_Dec: -0.72943246\n",
      "| epoch  35 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  3.76 | ppl    43.02 | acc     0.60 | train_ae_norm     1.00\n",
      "[35/200][2099/4361] Loss_D: 0.02518245 (Loss_D_real: 0.01027790 Loss_D_fake: 0.01490455) Loss_G: 0.36257395 Loss_Enh_Dec: -0.26990548\n",
      "| epoch  35 |  2100/ 4361 batches | lr 0.000000 | ms/batch 402.13 | loss  3.78 | ppl    43.79 | acc     0.59 | train_ae_norm     1.00\n",
      "[35/200][2199/4361] Loss_D: 0.01699821 (Loss_D_real: 0.00263719 Loss_D_fake: 0.01436102) Loss_G: 0.30260724 Loss_Enh_Dec: -0.44003209\n",
      "| epoch  35 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  3.79 | ppl    44.25 | acc     0.60 | train_ae_norm     1.00\n",
      "[35/200][2299/4361] Loss_D: 0.06213862 (Loss_D_real: 0.04815665 Loss_D_fake: 0.01398197) Loss_G: 0.28781343 Loss_Enh_Dec: -0.72379589\n",
      "| epoch  35 |  2300/ 4361 batches | lr 0.000000 | ms/batch 402.92 | loss  3.80 | ppl    44.82 | acc     0.58 | train_ae_norm     1.00\n",
      "[35/200][2399/4361] Loss_D: 0.02503480 (Loss_D_real: 0.02220318 Loss_D_fake: 0.00283162) Loss_G: 0.34742200 Loss_Enh_Dec: -0.41061378\n",
      "| epoch  35 |  2400/ 4361 batches | lr 0.000000 | ms/batch 402.48 | loss  3.77 | ppl    43.26 | acc     0.55 | train_ae_norm     1.00\n",
      "[35/200][2499/4361] Loss_D: 0.01243418 (Loss_D_real: 0.00272407 Loss_D_fake: 0.00971011) Loss_G: 0.28558281 Loss_Enh_Dec: -0.36426815\n",
      "| epoch  35 |  2500/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  3.79 | ppl    44.10 | acc     0.59 | train_ae_norm     1.00\n",
      "[35/200][2599/4361] Loss_D: 0.01341819 (Loss_D_real: 0.00535429 Loss_D_fake: 0.00806390) Loss_G: 0.32092085 Loss_Enh_Dec: -0.29244089\n",
      "| epoch  35 |  2600/ 4361 batches | lr 0.000000 | ms/batch 402.19 | loss  3.77 | ppl    43.56 | acc     0.57 | train_ae_norm     1.00\n",
      "[35/200][2699/4361] Loss_D: 0.01350299 (Loss_D_real: 0.00296650 Loss_D_fake: 0.01053649) Loss_G: 0.32729474 Loss_Enh_Dec: -0.21774642\n",
      "| epoch  35 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  3.78 | ppl    43.87 | acc     0.56 | train_ae_norm     1.00\n",
      "[35/200][2799/4361] Loss_D: 0.00803316 (Loss_D_real: 0.00213073 Loss_D_fake: 0.00590243) Loss_G: 0.34807262 Loss_Enh_Dec: -0.65110928\n",
      "| epoch  35 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  3.70 | ppl    40.30 | acc     0.57 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35/200][2899/4361] Loss_D: 0.02048650 (Loss_D_real: 0.01102375 Loss_D_fake: 0.00946274) Loss_G: 0.29643181 Loss_Enh_Dec: -0.37315825\n",
      "| epoch  35 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  3.73 | ppl    41.58 | acc     0.59 | train_ae_norm     1.00\n",
      "[35/200][2999/4361] Loss_D: 0.06019558 (Loss_D_real: 0.04945004 Loss_D_fake: 0.01074554) Loss_G: 0.32337865 Loss_Enh_Dec: -0.22666223\n",
      "| epoch  35 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  3.73 | ppl    41.81 | acc     0.56 | train_ae_norm     1.00\n",
      "[35/200][3099/4361] Loss_D: 0.02120469 (Loss_D_real: 0.01519718 Loss_D_fake: 0.00600751) Loss_G: 0.30118340 Loss_Enh_Dec: -0.35775661\n",
      "| epoch  35 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  3.77 | ppl    43.45 | acc     0.54 | train_ae_norm     1.00\n",
      "[35/200][3199/4361] Loss_D: 0.02646044 (Loss_D_real: 0.01935195 Loss_D_fake: 0.00710849) Loss_G: 0.31031165 Loss_Enh_Dec: -0.63907760\n",
      "| epoch  35 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  3.82 | ppl    45.64 | acc     0.58 | train_ae_norm     1.00\n",
      "[35/200][3299/4361] Loss_D: 0.02131935 (Loss_D_real: 0.00854586 Loss_D_fake: 0.01277349) Loss_G: 0.30338886 Loss_Enh_Dec: -1.01818347\n",
      "| epoch  35 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  3.79 | ppl    44.18 | acc     0.59 | train_ae_norm     1.00\n",
      "[35/200][3399/4361] Loss_D: 0.05886467 (Loss_D_real: 0.02841692 Loss_D_fake: 0.03044775) Loss_G: 0.60610980 Loss_Enh_Dec: -0.99192697\n",
      "| epoch  35 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  3.75 | ppl    42.38 | acc     0.58 | train_ae_norm     1.00\n",
      "[35/200][3499/4361] Loss_D: 0.00855642 (Loss_D_real: 0.00167982 Loss_D_fake: 0.00687660) Loss_G: 0.31438279 Loss_Enh_Dec: -0.82064199\n",
      "| epoch  35 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  3.69 | ppl    40.19 | acc     0.60 | train_ae_norm     1.00\n",
      "[35/200][3599/4361] Loss_D: 0.00792926 (Loss_D_real: 0.00172189 Loss_D_fake: 0.00620737) Loss_G: 0.33104429 Loss_Enh_Dec: -0.67110252\n",
      "| epoch  35 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  3.71 | ppl    40.88 | acc     0.60 | train_ae_norm     1.00\n",
      "[35/200][3699/4361] Loss_D: 0.06449652 (Loss_D_real: 0.05343381 Loss_D_fake: 0.01106271) Loss_G: 0.32392111 Loss_Enh_Dec: -1.28677666\n",
      "| epoch  35 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  3.72 | ppl    41.38 | acc     0.57 | train_ae_norm     1.00\n",
      "[35/200][3799/4361] Loss_D: 0.01333735 (Loss_D_real: 0.00679329 Loss_D_fake: 0.00654406) Loss_G: 0.35480890 Loss_Enh_Dec: -0.72733229\n",
      "| epoch  35 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  3.73 | ppl    41.62 | acc     0.61 | train_ae_norm     1.00\n",
      "[35/200][3899/4361] Loss_D: 0.06789407 (Loss_D_real: 0.05967810 Loss_D_fake: 0.00821597) Loss_G: 0.30052790 Loss_Enh_Dec: -1.24455249\n",
      "| epoch  35 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  3.75 | ppl    42.37 | acc     0.52 | train_ae_norm     1.00\n",
      "[35/200][3999/4361] Loss_D: 0.03154246 (Loss_D_real: 0.00982325 Loss_D_fake: 0.02171921) Loss_G: 0.30845299 Loss_Enh_Dec: -1.32688332\n",
      "| epoch  35 |  4000/ 4361 batches | lr 0.000000 | ms/batch 402.31 | loss  3.75 | ppl    42.69 | acc     0.61 | train_ae_norm     1.00\n",
      "[35/200][4099/4361] Loss_D: 0.01826629 (Loss_D_real: 0.00392678 Loss_D_fake: 0.01433951) Loss_G: 0.28416118 Loss_Enh_Dec: -1.53668392\n",
      "| epoch  35 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  3.72 | ppl    41.18 | acc     0.59 | train_ae_norm     1.00\n",
      "[35/200][4199/4361] Loss_D: 0.03937151 (Loss_D_real: 0.02100868 Loss_D_fake: 0.01836283) Loss_G: 0.26699391 Loss_Enh_Dec: -1.45289743\n",
      "| epoch  35 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  3.76 | ppl    42.81 | acc     0.61 | train_ae_norm     1.00\n",
      "[35/200][4299/4361] Loss_D: 0.06882040 (Loss_D_real: 0.00361191 Loss_D_fake: 0.06520849) Loss_G: 0.27954784 Loss_Enh_Dec: -1.11419296\n",
      "| epoch  35 |  4300/ 4361 batches | lr 0.000000 | ms/batch 402.21 | loss  3.71 | ppl    41.06 | acc     0.58 | train_ae_norm     1.00\n",
      "| end of epoch  35 | time: 1854.00s | test loss  3.56 | test ppl 35.05 | acc 0.633\n",
      "bleu_self:  [2.10568263e-01 4.59912711e-05 4.60842678e-06 1.47286384e-06\n",
      " 7.56146251e-07]\n",
      "bleu_test:  [7.72916666e-01 1.06819183e-01 1.32872400e-05 3.97373851e-06\n",
      " 2.00851876e-06]\n",
      "bleu_self: [0.21056826,0.00004599,0.00000461,0.00000147,0.00000076]\n",
      "bleu_test: [0.77291667,0.10681918,0.00001329,0.00000397,0.00000201]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 36 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.721\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.472\n",
      "  Test Loss: 3.827\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  36 |     0/ 4361 batches | lr 0.000000 | ms/batch 863.34 | loss  0.04 | ppl     1.04 | acc     0.61 | train_ae_norm     1.00\n",
      "[36/200][99/4361] Loss_D: 0.05305779 (Loss_D_real: 0.04265107 Loss_D_fake: 0.01040673) Loss_G: 0.30665413 Loss_Enh_Dec: -1.03673577\n",
      "| epoch  36 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  3.73 | ppl    41.66 | acc     0.56 | train_ae_norm     1.00\n",
      "[36/200][199/4361] Loss_D: 0.16066505 (Loss_D_real: 0.09165457 Loss_D_fake: 0.06901048) Loss_G: 0.28936228 Loss_Enh_Dec: -1.00941551\n",
      "| epoch  36 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  3.74 | ppl    42.16 | acc     0.60 | train_ae_norm     1.00\n",
      "[36/200][299/4361] Loss_D: 0.07012420 (Loss_D_real: 0.01655164 Loss_D_fake: 0.05357256) Loss_G: 0.28708908 Loss_Enh_Dec: -1.22573030\n",
      "| epoch  36 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  3.75 | ppl    42.70 | acc     0.54 | train_ae_norm     1.00\n",
      "[36/200][399/4361] Loss_D: 0.10669357 (Loss_D_real: 0.06719391 Loss_D_fake: 0.03949966) Loss_G: 0.25950870 Loss_Enh_Dec: -0.87315428\n",
      "| epoch  36 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  3.67 | ppl    39.36 | acc     0.60 | train_ae_norm     1.00\n",
      "[36/200][499/4361] Loss_D: 0.60387802 (Loss_D_real: 0.00649526 Loss_D_fake: 0.59738278) Loss_G: 0.28955513 Loss_Enh_Dec: -1.29103839\n",
      "| epoch  36 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  3.74 | ppl    41.99 | acc     0.61 | train_ae_norm     1.00\n",
      "[36/200][599/4361] Loss_D: 0.02976625 (Loss_D_real: 0.00525599 Loss_D_fake: 0.02451027) Loss_G: 0.30592987 Loss_Enh_Dec: -0.76252079\n",
      "| epoch  36 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  3.68 | ppl    39.51 | acc     0.55 | train_ae_norm     1.00\n",
      "[36/200][699/4361] Loss_D: 0.05343643 (Loss_D_real: 0.01428566 Loss_D_fake: 0.03915077) Loss_G: 0.31661639 Loss_Enh_Dec: -1.02182066\n",
      "| epoch  36 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  3.73 | ppl    41.55 | acc     0.60 | train_ae_norm     1.00\n",
      "[36/200][799/4361] Loss_D: 0.09525743 (Loss_D_real: 0.03016940 Loss_D_fake: 0.06508803) Loss_G: 0.35234070 Loss_Enh_Dec: -0.70364141\n",
      "| epoch  36 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  3.71 | ppl    40.79 | acc     0.59 | train_ae_norm     1.00\n",
      "[36/200][899/4361] Loss_D: 0.21167892 (Loss_D_real: 0.18821888 Loss_D_fake: 0.02346005) Loss_G: 0.25846076 Loss_Enh_Dec: -1.06843245\n",
      "| epoch  36 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  3.70 | ppl    40.59 | acc     0.61 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36/200][999/4361] Loss_D: 0.06329595 (Loss_D_real: 0.04205224 Loss_D_fake: 0.02124371) Loss_G: 0.37016773 Loss_Enh_Dec: -0.27429143\n",
      "| epoch  36 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  3.68 | ppl    39.68 | acc     0.58 | train_ae_norm     1.00\n",
      "[36/200][1099/4361] Loss_D: 0.07772225 (Loss_D_real: 0.06318324 Loss_D_fake: 0.01453901) Loss_G: 0.31332669 Loss_Enh_Dec: -1.24838471\n",
      "| epoch  36 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  3.69 | ppl    39.86 | acc     0.57 | train_ae_norm     1.00\n",
      "[36/200][1199/4361] Loss_D: 0.03172272 (Loss_D_real: 0.01397930 Loss_D_fake: 0.01774341) Loss_G: 0.27147529 Loss_Enh_Dec: -0.51386529\n",
      "| epoch  36 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  3.70 | ppl    40.26 | acc     0.60 | train_ae_norm     1.00\n",
      "[36/200][1299/4361] Loss_D: 0.04084889 (Loss_D_real: 0.02877619 Loss_D_fake: 0.01207270) Loss_G: 0.27152821 Loss_Enh_Dec: -1.48539412\n",
      "| epoch  36 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  3.71 | ppl    40.88 | acc     0.61 | train_ae_norm     1.00\n",
      "[36/200][1399/4361] Loss_D: 0.05139550 (Loss_D_real: 0.01706325 Loss_D_fake: 0.03433225) Loss_G: 0.38052413 Loss_Enh_Dec: -1.03432500\n",
      "| epoch  36 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  3.71 | ppl    40.94 | acc     0.55 | train_ae_norm     1.00\n",
      "[36/200][1499/4361] Loss_D: 0.02757132 (Loss_D_real: 0.02183839 Loss_D_fake: 0.00573294) Loss_G: 0.28860751 Loss_Enh_Dec: -0.71357375\n",
      "| epoch  36 |  1500/ 4361 batches | lr 0.000000 | ms/batch 402.09 | loss  3.75 | ppl    42.67 | acc     0.56 | train_ae_norm     1.00\n",
      "[36/200][1599/4361] Loss_D: 0.02947022 (Loss_D_real: 0.01135500 Loss_D_fake: 0.01811522) Loss_G: 0.30829906 Loss_Enh_Dec: -0.97889435\n",
      "| epoch  36 |  1600/ 4361 batches | lr 0.000000 | ms/batch 402.28 | loss  3.70 | ppl    40.48 | acc     0.60 | train_ae_norm     1.00\n",
      "[36/200][1699/4361] Loss_D: 0.09267011 (Loss_D_real: 0.00262293 Loss_D_fake: 0.09004718) Loss_G: 0.36083630 Loss_Enh_Dec: -0.76019633\n",
      "| epoch  36 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  3.71 | ppl    40.79 | acc     0.59 | train_ae_norm     1.00\n",
      "[36/200][1799/4361] Loss_D: 0.04431423 (Loss_D_real: 0.01345519 Loss_D_fake: 0.03085904) Loss_G: 0.33136758 Loss_Enh_Dec: -0.67371529\n",
      "| epoch  36 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  3.67 | ppl    39.21 | acc     0.60 | train_ae_norm     1.00\n",
      "[36/200][1899/4361] Loss_D: 0.01471034 (Loss_D_real: 0.00398868 Loss_D_fake: 0.01072166) Loss_G: 0.25707203 Loss_Enh_Dec: -0.85359782\n",
      "| epoch  36 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  3.73 | ppl    41.84 | acc     0.62 | train_ae_norm     1.00\n",
      "[36/200][1999/4361] Loss_D: 0.02697910 (Loss_D_real: 0.01152963 Loss_D_fake: 0.01544947) Loss_G: 0.26806626 Loss_Enh_Dec: -0.99566805\n",
      "| epoch  36 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  3.69 | ppl    39.90 | acc     0.57 | train_ae_norm     1.00\n",
      "[36/200][2099/4361] Loss_D: 0.04735002 (Loss_D_real: 0.00529717 Loss_D_fake: 0.04205285) Loss_G: 0.29961586 Loss_Enh_Dec: -1.05911016\n",
      "| epoch  36 |  2100/ 4361 batches | lr 0.000000 | ms/batch 402.56 | loss  3.71 | ppl    40.65 | acc     0.58 | train_ae_norm     1.00\n",
      "[36/200][2199/4361] Loss_D: 0.06039919 (Loss_D_real: 0.03524373 Loss_D_fake: 0.02515546) Loss_G: 0.34346923 Loss_Enh_Dec: -0.99947947\n",
      "| epoch  36 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  3.70 | ppl    40.43 | acc     0.60 | train_ae_norm     1.00\n",
      "[36/200][2299/4361] Loss_D: 0.05328047 (Loss_D_real: 0.03713800 Loss_D_fake: 0.01614246) Loss_G: 0.30851445 Loss_Enh_Dec: -1.04351032\n",
      "| epoch  36 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  3.69 | ppl    40.18 | acc     0.62 | train_ae_norm     1.00\n",
      "[36/200][2399/4361] Loss_D: 0.02083079 (Loss_D_real: 0.01270431 Loss_D_fake: 0.00812648) Loss_G: 0.31916952 Loss_Enh_Dec: -1.18252075\n",
      "| epoch  36 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  3.70 | ppl    40.38 | acc     0.57 | train_ae_norm     1.00\n",
      "[36/200][2499/4361] Loss_D: 0.09756735 (Loss_D_real: 0.07968491 Loss_D_fake: 0.01788245) Loss_G: 0.30874318 Loss_Enh_Dec: -1.09152555\n",
      "| epoch  36 |  2500/ 4361 batches | lr 0.000000 | ms/batch 400.77 | loss  3.72 | ppl    41.28 | acc     0.60 | train_ae_norm     1.00\n",
      "[36/200][2599/4361] Loss_D: 0.07829607 (Loss_D_real: 0.02748340 Loss_D_fake: 0.05081266) Loss_G: 0.40645581 Loss_Enh_Dec: -1.12627161\n",
      "| epoch  36 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.75 | loss  3.72 | ppl    41.43 | acc     0.56 | train_ae_norm     1.00\n",
      "[36/200][2699/4361] Loss_D: 0.02954484 (Loss_D_real: 0.01391691 Loss_D_fake: 0.01562793) Loss_G: 0.36016923 Loss_Enh_Dec: -1.22089565\n",
      "| epoch  36 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  3.72 | ppl    41.21 | acc     0.56 | train_ae_norm     1.00\n",
      "[36/200][2799/4361] Loss_D: 0.02259018 (Loss_D_real: 0.00888255 Loss_D_fake: 0.01370763) Loss_G: 0.27663895 Loss_Enh_Dec: -1.23012817\n",
      "| epoch  36 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  3.67 | ppl    39.08 | acc     0.54 | train_ae_norm     1.00\n",
      "[36/200][2899/4361] Loss_D: 0.02784120 (Loss_D_real: 0.00870636 Loss_D_fake: 0.01913484) Loss_G: 0.27375573 Loss_Enh_Dec: -0.88149673\n",
      "| epoch  36 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  3.71 | ppl    41.02 | acc     0.61 | train_ae_norm     1.00\n",
      "[36/200][2999/4361] Loss_D: 0.12668982 (Loss_D_real: 0.11670649 Loss_D_fake: 0.00998333) Loss_G: 0.37323546 Loss_Enh_Dec: -0.79687077\n",
      "| epoch  36 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  3.69 | ppl    40.17 | acc     0.63 | train_ae_norm     1.00\n",
      "[36/200][3099/4361] Loss_D: 0.12857185 (Loss_D_real: 0.04890108 Loss_D_fake: 0.07967078) Loss_G: 0.32697171 Loss_Enh_Dec: -0.76078969\n",
      "| epoch  36 |  3100/ 4361 batches | lr 0.000000 | ms/batch 402.19 | loss  3.69 | ppl    40.06 | acc     0.57 | train_ae_norm     1.00\n",
      "[36/200][3199/4361] Loss_D: 0.03717967 (Loss_D_real: 0.01466192 Loss_D_fake: 0.02251775) Loss_G: 0.37222838 Loss_Enh_Dec: -1.11258876\n",
      "| epoch  36 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  3.72 | ppl    41.45 | acc     0.59 | train_ae_norm     1.00\n",
      "[36/200][3299/4361] Loss_D: 0.12871459 (Loss_D_real: 0.11097892 Loss_D_fake: 0.01773568) Loss_G: 0.35191542 Loss_Enh_Dec: -0.65173101\n",
      "| epoch  36 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  3.73 | ppl    41.53 | acc     0.59 | train_ae_norm     1.00\n",
      "[36/200][3399/4361] Loss_D: 0.11708088 (Loss_D_real: 0.07210627 Loss_D_fake: 0.04497461) Loss_G: 0.25521603 Loss_Enh_Dec: -1.16759646\n",
      "| epoch  36 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  3.70 | ppl    40.32 | acc     0.59 | train_ae_norm     1.00\n",
      "[36/200][3499/4361] Loss_D: 0.01969671 (Loss_D_real: 0.00743947 Loss_D_fake: 0.01225724) Loss_G: 0.30145156 Loss_Enh_Dec: -1.47987318\n",
      "| epoch  36 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  3.65 | ppl    38.41 | acc     0.60 | train_ae_norm     1.00\n",
      "[36/200][3599/4361] Loss_D: 0.07116095 (Loss_D_real: 0.00966844 Loss_D_fake: 0.06149251) Loss_G: 0.26189002 Loss_Enh_Dec: -1.89315248\n",
      "| epoch  36 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  3.66 | ppl    38.96 | acc     0.59 | train_ae_norm     1.00\n",
      "[36/200][3699/4361] Loss_D: 0.07356080 (Loss_D_real: 0.01425739 Loss_D_fake: 0.05930340) Loss_G: 0.24407244 Loss_Enh_Dec: -1.53537023\n",
      "| epoch  36 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  3.69 | ppl    39.89 | acc     0.55 | train_ae_norm     1.00\n",
      "[36/200][3799/4361] Loss_D: 0.02409034 (Loss_D_real: 0.00143823 Loss_D_fake: 0.02265211) Loss_G: 0.31012568 Loss_Enh_Dec: -1.76699257\n",
      "| epoch  36 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  3.74 | ppl    41.93 | acc     0.63 | train_ae_norm     1.00\n",
      "[36/200][3899/4361] Loss_D: 0.01226522 (Loss_D_real: 0.00289775 Loss_D_fake: 0.00936747) Loss_G: 0.31316963 Loss_Enh_Dec: -1.32121027\n",
      "| epoch  36 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  3.71 | ppl    40.67 | acc     0.53 | train_ae_norm     1.00\n",
      "[36/200][3999/4361] Loss_D: 0.04898060 (Loss_D_real: 0.02988868 Loss_D_fake: 0.01909192) Loss_G: 0.27454910 Loss_Enh_Dec: -1.09395468\n",
      "| epoch  36 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.90 | loss  3.73 | ppl    41.52 | acc     0.57 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36/200][4099/4361] Loss_D: 0.02811099 (Loss_D_real: 0.01339681 Loss_D_fake: 0.01471418) Loss_G: 0.37115827 Loss_Enh_Dec: -0.99850047\n",
      "| epoch  36 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  3.67 | ppl    39.10 | acc     0.56 | train_ae_norm     1.00\n",
      "[36/200][4199/4361] Loss_D: 0.03146667 (Loss_D_real: 0.00619223 Loss_D_fake: 0.02527444) Loss_G: 0.29103929 Loss_Enh_Dec: -1.36012137\n",
      "| epoch  36 |  4200/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  3.72 | ppl    41.44 | acc     0.61 | train_ae_norm     1.00\n",
      "[36/200][4299/4361] Loss_D: 0.06490597 (Loss_D_real: 0.02067445 Loss_D_fake: 0.04423152) Loss_G: 0.31034407 Loss_Enh_Dec: -0.99660999\n",
      "| epoch  36 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  3.70 | ppl    40.52 | acc     0.57 | train_ae_norm     1.00\n",
      "| end of epoch  36 | time: 1853.16s | test loss  3.54 | test ppl 34.48 | acc 0.638\n",
      "bleu_self:  [4.07589286e-01 2.52815019e-01 9.39816859e-02 1.63543511e-05\n",
      " 1.01308596e-07]\n",
      "bleu_test:  [8.62351190e-01 4.40144233e-01 3.22912453e-06 9.41684212e-09\n",
      " 3.09603964e-10]\n",
      "bleu_self: [0.40758929,0.25281502,0.09398169,0.00001635,0.00000010]\n",
      "bleu_test: [0.86235119,0.44014423,0.00000323,0.00000001,0.00000000]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 37 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.717\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.472\n",
      "  Test Loss: 3.766\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  37 |     0/ 4361 batches | lr 0.000000 | ms/batch 864.42 | loss  0.03 | ppl     1.03 | acc     0.63 | train_ae_norm     1.00\n",
      "[37/200][99/4361] Loss_D: 0.20507027 (Loss_D_real: 0.19137812 Loss_D_fake: 0.01369215) Loss_G: 0.31702009 Loss_Enh_Dec: -1.13642538\n",
      "| epoch  37 |   100/ 4361 batches | lr 0.000000 | ms/batch 400.79 | loss  3.74 | ppl    42.12 | acc     0.56 | train_ae_norm     1.00\n",
      "[37/200][199/4361] Loss_D: 0.04619829 (Loss_D_real: 0.01122146 Loss_D_fake: 0.03497683) Loss_G: 0.27918822 Loss_Enh_Dec: -1.47371960\n",
      "| epoch  37 |   200/ 4361 batches | lr 0.000000 | ms/batch 400.83 | loss  3.78 | ppl    44.01 | acc     0.59 | train_ae_norm     1.00\n",
      "[37/200][299/4361] Loss_D: 0.08960602 (Loss_D_real: 0.04888612 Loss_D_fake: 0.04071990) Loss_G: 0.25837135 Loss_Enh_Dec: -1.29356420\n",
      "| epoch  37 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  3.76 | ppl    43.02 | acc     0.55 | train_ae_norm     1.00\n",
      "[37/200][399/4361] Loss_D: 0.03821644 (Loss_D_real: 0.02217642 Loss_D_fake: 0.01604002) Loss_G: 0.26954362 Loss_Enh_Dec: -1.30855262\n",
      "| epoch  37 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  3.66 | ppl    38.68 | acc     0.58 | train_ae_norm     1.00\n",
      "[37/200][499/4361] Loss_D: 0.06417379 (Loss_D_real: 0.04959638 Loss_D_fake: 0.01457741) Loss_G: 0.29342601 Loss_Enh_Dec: -1.18785632\n",
      "| epoch  37 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  3.72 | ppl    41.06 | acc     0.62 | train_ae_norm     1.00\n",
      "[37/200][599/4361] Loss_D: 0.01486382 (Loss_D_real: 0.00297295 Loss_D_fake: 0.01189087) Loss_G: 0.27525780 Loss_Enh_Dec: -1.39543140\n",
      "| epoch  37 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  3.66 | ppl    38.73 | acc     0.55 | train_ae_norm     1.00\n",
      "[37/200][699/4361] Loss_D: 0.09695446 (Loss_D_real: 0.05998109 Loss_D_fake: 0.03697337) Loss_G: 0.27297828 Loss_Enh_Dec: -1.47839725\n",
      "| epoch  37 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  3.72 | ppl    41.13 | acc     0.63 | train_ae_norm     1.00\n",
      "[37/200][799/4361] Loss_D: 0.03452110 (Loss_D_real: 0.00206101 Loss_D_fake: 0.03246009) Loss_G: 0.27150351 Loss_Enh_Dec: -1.32870746\n",
      "| epoch  37 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  3.69 | ppl    40.05 | acc     0.61 | train_ae_norm     1.00\n",
      "[37/200][899/4361] Loss_D: 0.02036958 (Loss_D_real: 0.00500170 Loss_D_fake: 0.01536788) Loss_G: 0.25821900 Loss_Enh_Dec: -1.45765579\n",
      "| epoch  37 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.65 | loss  3.70 | ppl    40.53 | acc     0.63 | train_ae_norm     1.00\n",
      "[37/200][999/4361] Loss_D: 0.04829688 (Loss_D_real: 0.02281262 Loss_D_fake: 0.02548426) Loss_G: 0.32301173 Loss_Enh_Dec: -1.31650639\n",
      "| epoch  37 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  3.69 | ppl    39.93 | acc     0.57 | train_ae_norm     1.00\n",
      "[37/200][1099/4361] Loss_D: 0.04893524 (Loss_D_real: 0.03338820 Loss_D_fake: 0.01554704) Loss_G: 0.29700384 Loss_Enh_Dec: -1.33155096\n",
      "| epoch  37 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.78 | loss  3.69 | ppl    40.05 | acc     0.56 | train_ae_norm     1.00\n",
      "[37/200][1199/4361] Loss_D: 0.04159044 (Loss_D_real: 0.03242974 Loss_D_fake: 0.00916070) Loss_G: 0.36616024 Loss_Enh_Dec: -1.41059899\n",
      "| epoch  37 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  3.70 | ppl    40.60 | acc     0.60 | train_ae_norm     1.00\n",
      "[37/200][1299/4361] Loss_D: 0.02953858 (Loss_D_real: 0.01381819 Loss_D_fake: 0.01572039) Loss_G: 0.30104986 Loss_Enh_Dec: -1.25459766\n",
      "| epoch  37 |  1300/ 4361 batches | lr 0.000000 | ms/batch 402.00 | loss  3.71 | ppl    41.00 | acc     0.60 | train_ae_norm     1.00\n",
      "[37/200][1399/4361] Loss_D: 0.11027647 (Loss_D_real: 0.05702561 Loss_D_fake: 0.05325086) Loss_G: 0.34928331 Loss_Enh_Dec: -1.35697079\n",
      "| epoch  37 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.06 | loss  3.70 | ppl    40.44 | acc     0.51 | train_ae_norm     1.00\n",
      "[37/200][1499/4361] Loss_D: 0.04864403 (Loss_D_real: 0.00510153 Loss_D_fake: 0.04354250) Loss_G: 0.30427495 Loss_Enh_Dec: -1.38739729\n",
      "| epoch  37 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  3.74 | ppl    41.92 | acc     0.55 | train_ae_norm     1.00\n",
      "[37/200][1599/4361] Loss_D: 0.04027505 (Loss_D_real: 0.02708376 Loss_D_fake: 0.01319129) Loss_G: 0.27681360 Loss_Enh_Dec: -1.18015897\n",
      "| epoch  37 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  3.71 | ppl    40.82 | acc     0.61 | train_ae_norm     1.00\n",
      "[37/200][1699/4361] Loss_D: 0.04865452 (Loss_D_real: 0.00303413 Loss_D_fake: 0.04562039) Loss_G: 0.26463178 Loss_Enh_Dec: -1.52296054\n",
      "| epoch  37 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.75 | loss  3.70 | ppl    40.54 | acc     0.58 | train_ae_norm     1.00\n",
      "[37/200][1799/4361] Loss_D: 0.09158036 (Loss_D_real: 0.07954392 Loss_D_fake: 0.01203644) Loss_G: 0.29027915 Loss_Enh_Dec: -0.82117128\n",
      "| epoch  37 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  3.64 | ppl    38.19 | acc     0.60 | train_ae_norm     1.00\n",
      "[37/200][1899/4361] Loss_D: 0.14157258 (Loss_D_real: 0.12844318 Loss_D_fake: 0.01312940) Loss_G: 0.31271479 Loss_Enh_Dec: -1.29552794\n",
      "| epoch  37 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.73 | loss  3.72 | ppl    41.43 | acc     0.60 | train_ae_norm     1.00\n",
      "[37/200][1999/4361] Loss_D: 0.01827676 (Loss_D_real: 0.00731010 Loss_D_fake: 0.01096667) Loss_G: 0.30075151 Loss_Enh_Dec: -1.13412559\n",
      "| epoch  37 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  3.65 | ppl    38.53 | acc     0.58 | train_ae_norm     1.00\n",
      "[37/200][2099/4361] Loss_D: 0.03449785 (Loss_D_real: 0.02413578 Loss_D_fake: 0.01036207) Loss_G: 0.28131175 Loss_Enh_Dec: -1.59894979\n",
      "| epoch  37 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  3.68 | ppl    39.64 | acc     0.59 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37/200][2199/4361] Loss_D: 0.01110720 (Loss_D_real: 0.00190412 Loss_D_fake: 0.00920308) Loss_G: 0.33991754 Loss_Enh_Dec: -1.55623019\n",
      "| epoch  37 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  3.69 | ppl    40.02 | acc     0.59 | train_ae_norm     1.00\n",
      "[37/200][2299/4361] Loss_D: 0.05057864 (Loss_D_real: 0.03158757 Loss_D_fake: 0.01899107) Loss_G: 0.28021431 Loss_Enh_Dec: -1.50290930\n",
      "| epoch  37 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  3.66 | ppl    38.97 | acc     0.61 | train_ae_norm     1.00\n",
      "[37/200][2399/4361] Loss_D: 0.03531947 (Loss_D_real: 0.00809129 Loss_D_fake: 0.02722819) Loss_G: 0.38267156 Loss_Enh_Dec: -1.50008702\n",
      "| epoch  37 |  2400/ 4361 batches | lr 0.000000 | ms/batch 402.05 | loss  3.66 | ppl    38.97 | acc     0.57 | train_ae_norm     1.00\n",
      "[37/200][2499/4361] Loss_D: 0.02326164 (Loss_D_real: 0.01107307 Loss_D_fake: 0.01218857) Loss_G: 0.29046470 Loss_Enh_Dec: -1.28308618\n",
      "| epoch  37 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  3.71 | ppl    40.84 | acc     0.63 | train_ae_norm     1.00\n",
      "[37/200][2599/4361] Loss_D: 0.04393798 (Loss_D_real: 0.03165242 Loss_D_fake: 0.01228556) Loss_G: 0.28486139 Loss_Enh_Dec: -1.36835253\n",
      "| epoch  37 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  3.68 | ppl    39.76 | acc     0.57 | train_ae_norm     1.00\n",
      "[37/200][2699/4361] Loss_D: 0.02146356 (Loss_D_real: 0.01047774 Loss_D_fake: 0.01098582) Loss_G: 0.28831720 Loss_Enh_Dec: -1.36260986\n",
      "| epoch  37 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  3.72 | ppl    41.06 | acc     0.59 | train_ae_norm     1.00\n",
      "[37/200][2799/4361] Loss_D: 0.08588617 (Loss_D_real: 0.04146358 Loss_D_fake: 0.04442260) Loss_G: 0.27593726 Loss_Enh_Dec: -1.51105487\n",
      "| epoch  37 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  3.66 | ppl    38.73 | acc     0.57 | train_ae_norm     1.00\n",
      "[37/200][2899/4361] Loss_D: 0.02482239 (Loss_D_real: 0.01389427 Loss_D_fake: 0.01092812) Loss_G: 0.26949102 Loss_Enh_Dec: -1.40244293\n",
      "| epoch  37 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  3.70 | ppl    40.50 | acc     0.58 | train_ae_norm     1.00\n",
      "[37/200][2999/4361] Loss_D: 0.01572484 (Loss_D_real: 0.00308324 Loss_D_fake: 0.01264161) Loss_G: 0.25950372 Loss_Enh_Dec: -1.01352537\n",
      "| epoch  37 |  3000/ 4361 batches | lr 0.000000 | ms/batch 402.00 | loss  3.70 | ppl    40.41 | acc     0.59 | train_ae_norm     1.00\n",
      "[37/200][3099/4361] Loss_D: 0.06771283 (Loss_D_real: 0.03057208 Loss_D_fake: 0.03714074) Loss_G: 0.32096395 Loss_Enh_Dec: -1.36771548\n",
      "| epoch  37 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  3.72 | ppl    41.17 | acc     0.57 | train_ae_norm     1.00\n",
      "[37/200][3199/4361] Loss_D: 0.19700733 (Loss_D_real: 0.16157353 Loss_D_fake: 0.03543379) Loss_G: 0.29043436 Loss_Enh_Dec: -1.57515061\n",
      "| epoch  37 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  3.73 | ppl    41.84 | acc     0.63 | train_ae_norm     1.00\n",
      "[37/200][3299/4361] Loss_D: 0.05926067 (Loss_D_real: 0.04514122 Loss_D_fake: 0.01411946) Loss_G: 0.29695842 Loss_Enh_Dec: -1.74144673\n",
      "| epoch  37 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  3.73 | ppl    41.62 | acc     0.58 | train_ae_norm     1.00\n",
      "[37/200][3399/4361] Loss_D: 0.02774365 (Loss_D_real: 0.01206427 Loss_D_fake: 0.01567937) Loss_G: 0.32626358 Loss_Enh_Dec: -1.39657390\n",
      "| epoch  37 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  3.69 | ppl    40.08 | acc     0.59 | train_ae_norm     1.00\n",
      "[37/200][3499/4361] Loss_D: 0.12955396 (Loss_D_real: 0.11310953 Loss_D_fake: 0.01644444) Loss_G: 0.24689789 Loss_Enh_Dec: -1.13001442\n",
      "| epoch  37 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  3.63 | ppl    37.78 | acc     0.60 | train_ae_norm     1.00\n",
      "[37/200][3599/4361] Loss_D: 0.02577938 (Loss_D_real: 0.01152037 Loss_D_fake: 0.01425901) Loss_G: 0.27642402 Loss_Enh_Dec: -1.08987272\n",
      "| epoch  37 |  3600/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  3.67 | ppl    39.39 | acc     0.60 | train_ae_norm     1.00\n",
      "[37/200][3699/4361] Loss_D: 0.02627732 (Loss_D_real: 0.00843979 Loss_D_fake: 0.01783753) Loss_G: 0.28414121 Loss_Enh_Dec: -1.13529205\n",
      "| epoch  37 |  3700/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  3.68 | ppl    39.73 | acc     0.56 | train_ae_norm     1.00\n",
      "[37/200][3799/4361] Loss_D: 0.12213862 (Loss_D_real: 0.10811116 Loss_D_fake: 0.01402746) Loss_G: 0.27856693 Loss_Enh_Dec: -1.33479905\n",
      "| epoch  37 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  3.70 | ppl    40.29 | acc     0.61 | train_ae_norm     1.00\n",
      "[37/200][3899/4361] Loss_D: 0.02329410 (Loss_D_real: 0.00472019 Loss_D_fake: 0.01857391) Loss_G: 0.29704115 Loss_Enh_Dec: -0.99671841\n",
      "| epoch  37 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  3.70 | ppl    40.39 | acc     0.54 | train_ae_norm     1.00\n",
      "[37/200][3999/4361] Loss_D: 0.04047556 (Loss_D_real: 0.03084882 Loss_D_fake: 0.00962674) Loss_G: 0.28466636 Loss_Enh_Dec: -1.62423062\n",
      "| epoch  37 |  4000/ 4361 batches | lr 0.000000 | ms/batch 402.35 | loss  3.72 | ppl    41.25 | acc     0.60 | train_ae_norm     1.00\n",
      "[37/200][4099/4361] Loss_D: 0.01790268 (Loss_D_real: 0.00337288 Loss_D_fake: 0.01452980) Loss_G: 0.26162055 Loss_Enh_Dec: -1.47443342\n",
      "| epoch  37 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.99 | loss  3.68 | ppl    39.83 | acc     0.57 | train_ae_norm     1.00\n",
      "[37/200][4199/4361] Loss_D: 0.01633078 (Loss_D_real: 0.00423183 Loss_D_fake: 0.01209895) Loss_G: 0.28061244 Loss_Enh_Dec: -1.46227062\n",
      "| epoch  37 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  3.71 | ppl    40.77 | acc     0.63 | train_ae_norm     1.00\n",
      "[37/200][4299/4361] Loss_D: 0.06032254 (Loss_D_real: 0.03830208 Loss_D_fake: 0.02202046) Loss_G: 0.30353868 Loss_Enh_Dec: -1.60894096\n",
      "| epoch  37 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  3.66 | ppl    38.82 | acc     0.58 | train_ae_norm     1.00\n",
      "| end of epoch  37 | time: 1852.82s | test loss  3.52 | test ppl 33.74 | acc 0.642\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 38 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.715\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.482\n",
      "  Test Loss: 3.807\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  38 |     0/ 4361 batches | lr 0.000000 | ms/batch 866.96 | loss  0.03 | ppl     1.03 | acc     0.61 | train_ae_norm     1.00\n",
      "[38/200][99/4361] Loss_D: 0.05663606 (Loss_D_real: 0.02564235 Loss_D_fake: 0.03099371) Loss_G: 0.24112082 Loss_Enh_Dec: -1.72408164\n",
      "| epoch  38 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  3.67 | ppl    39.32 | acc     0.56 | train_ae_norm     1.00\n",
      "[38/200][199/4361] Loss_D: 0.14039373 (Loss_D_real: 0.10701495 Loss_D_fake: 0.03337879) Loss_G: 0.28729254 Loss_Enh_Dec: -1.48797512\n",
      "| epoch  38 |   200/ 4361 batches | lr 0.000000 | ms/batch 402.28 | loss  3.70 | ppl    40.60 | acc     0.62 | train_ae_norm     1.00\n",
      "[38/200][299/4361] Loss_D: 0.01131540 (Loss_D_real: 0.00605205 Loss_D_fake: 0.00526335) Loss_G: 0.30098018 Loss_Enh_Dec: -1.28470695\n",
      "| epoch  38 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  3.70 | ppl    40.60 | acc     0.57 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38/200][399/4361] Loss_D: 0.02111973 (Loss_D_real: 0.00936551 Loss_D_fake: 0.01175422) Loss_G: 0.31159827 Loss_Enh_Dec: -1.26343155\n",
      "| epoch  38 |   400/ 4361 batches | lr 0.000000 | ms/batch 402.39 | loss  3.60 | ppl    36.62 | acc     0.59 | train_ae_norm     1.00\n",
      "[38/200][499/4361] Loss_D: 0.03528559 (Loss_D_real: 0.00609665 Loss_D_fake: 0.02918894) Loss_G: 0.31112164 Loss_Enh_Dec: -1.68137515\n",
      "| epoch  38 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  3.67 | ppl    39.42 | acc     0.60 | train_ae_norm     1.00\n",
      "[38/200][599/4361] Loss_D: 0.03530757 (Loss_D_real: 0.02771565 Loss_D_fake: 0.00759192) Loss_G: 0.29464966 Loss_Enh_Dec: -1.35234940\n",
      "| epoch  38 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  3.63 | ppl    37.90 | acc     0.56 | train_ae_norm     1.00\n",
      "[38/200][699/4361] Loss_D: 0.22625896 (Loss_D_real: 0.15096581 Loss_D_fake: 0.07529316) Loss_G: 0.30339432 Loss_Enh_Dec: -1.21307266\n",
      "| epoch  38 |   700/ 4361 batches | lr 0.000000 | ms/batch 402.18 | loss  3.68 | ppl    39.77 | acc     0.62 | train_ae_norm     1.00\n",
      "[38/200][799/4361] Loss_D: 0.01443424 (Loss_D_real: 0.00439807 Loss_D_fake: 0.01003616) Loss_G: 0.25355124 Loss_Enh_Dec: -1.26597250\n",
      "| epoch  38 |   800/ 4361 batches | lr 0.000000 | ms/batch 402.24 | loss  3.64 | ppl    38.17 | acc     0.57 | train_ae_norm     1.00\n",
      "[38/200][899/4361] Loss_D: 0.01404388 (Loss_D_real: 0.00388463 Loss_D_fake: 0.01015925) Loss_G: 0.33400789 Loss_Enh_Dec: -1.02563822\n",
      "| epoch  38 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.06 | loss  3.66 | ppl    38.88 | acc     0.63 | train_ae_norm     1.00\n",
      "[38/200][999/4361] Loss_D: 0.02501893 (Loss_D_real: 0.02103950 Loss_D_fake: 0.00397943) Loss_G: 0.42307988 Loss_Enh_Dec: -1.09084845\n",
      "| epoch  38 |  1000/ 4361 batches | lr 0.000000 | ms/batch 402.15 | loss  3.66 | ppl    38.72 | acc     0.60 | train_ae_norm     1.00\n",
      "[38/200][1099/4361] Loss_D: 0.01472416 (Loss_D_real: 0.00634337 Loss_D_fake: 0.00838079) Loss_G: 0.27740866 Loss_Enh_Dec: -1.07016814\n",
      "| epoch  38 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  3.63 | ppl    37.75 | acc     0.58 | train_ae_norm     1.00\n",
      "[38/200][1199/4361] Loss_D: 0.02071646 (Loss_D_real: 0.00639581 Loss_D_fake: 0.01432065) Loss_G: 0.29438779 Loss_Enh_Dec: -1.13101995\n",
      "| epoch  38 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  3.65 | ppl    38.31 | acc     0.61 | train_ae_norm     1.00\n",
      "[38/200][1299/4361] Loss_D: 0.02721012 (Loss_D_real: 0.00360316 Loss_D_fake: 0.02360696) Loss_G: 0.33145306 Loss_Enh_Dec: -1.27343023\n",
      "| epoch  38 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  3.66 | ppl    38.77 | acc     0.60 | train_ae_norm     1.00\n",
      "[38/200][1399/4361] Loss_D: 0.08303483 (Loss_D_real: 0.07645419 Loss_D_fake: 0.00658064) Loss_G: 0.54043239 Loss_Enh_Dec: -0.85338765\n",
      "| epoch  38 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  3.66 | ppl    38.97 | acc     0.56 | train_ae_norm     1.00\n",
      "[38/200][1499/4361] Loss_D: 0.00400831 (Loss_D_real: 0.00165500 Loss_D_fake: 0.00235331) Loss_G: 0.36633262 Loss_Enh_Dec: -1.18269539\n",
      "| epoch  38 |  1500/ 4361 batches | lr 0.000000 | ms/batch 402.00 | loss  3.70 | ppl    40.46 | acc     0.56 | train_ae_norm     1.00\n",
      "[38/200][1599/4361] Loss_D: 0.07354851 (Loss_D_real: 0.05184773 Loss_D_fake: 0.02170078) Loss_G: 0.33761415 Loss_Enh_Dec: -1.45352972\n",
      "| epoch  38 |  1600/ 4361 batches | lr 0.000000 | ms/batch 402.03 | loss  3.68 | ppl    39.57 | acc     0.58 | train_ae_norm     1.00\n",
      "[38/200][1699/4361] Loss_D: 0.06086595 (Loss_D_real: 0.04574887 Loss_D_fake: 0.01511708) Loss_G: 0.29349762 Loss_Enh_Dec: -1.84470880\n",
      "| epoch  38 |  1700/ 4361 batches | lr 0.000000 | ms/batch 402.18 | loss  3.66 | ppl    38.76 | acc     0.57 | train_ae_norm     1.00\n",
      "[38/200][1799/4361] Loss_D: 0.06758131 (Loss_D_real: 0.00266807 Loss_D_fake: 0.06491324) Loss_G: 0.33095863 Loss_Enh_Dec: -1.59939194\n",
      "| epoch  38 |  1800/ 4361 batches | lr 0.000000 | ms/batch 402.56 | loss  3.63 | ppl    37.61 | acc     0.62 | train_ae_norm     1.00\n",
      "[38/200][1899/4361] Loss_D: 0.10570327 (Loss_D_real: 0.07707325 Loss_D_fake: 0.02863002) Loss_G: 0.33046553 Loss_Enh_Dec: -1.63299370\n",
      "| epoch  38 |  1900/ 4361 batches | lr 0.000000 | ms/batch 402.34 | loss  3.69 | ppl    39.99 | acc     0.60 | train_ae_norm     1.00\n",
      "[38/200][1999/4361] Loss_D: 0.02973811 (Loss_D_real: 0.00656024 Loss_D_fake: 0.02317787) Loss_G: 0.33784634 Loss_Enh_Dec: -1.15927446\n",
      "| epoch  38 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  3.61 | ppl    37.11 | acc     0.59 | train_ae_norm     1.00\n",
      "[38/200][2099/4361] Loss_D: 0.17763375 (Loss_D_real: 0.04350750 Loss_D_fake: 0.13412625) Loss_G: 0.29500598 Loss_Enh_Dec: -1.54955935\n",
      "| epoch  38 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  3.65 | ppl    38.44 | acc     0.57 | train_ae_norm     1.00\n",
      "[38/200][2199/4361] Loss_D: 0.03317742 (Loss_D_real: 0.01310852 Loss_D_fake: 0.02006890) Loss_G: 0.28735349 Loss_Enh_Dec: -0.67384911\n",
      "| epoch  38 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  3.63 | ppl    37.60 | acc     0.59 | train_ae_norm     1.00\n",
      "[38/200][2299/4361] Loss_D: 0.03586596 (Loss_D_real: 0.02675216 Loss_D_fake: 0.00911380) Loss_G: 0.32883510 Loss_Enh_Dec: -1.19614875\n",
      "| epoch  38 |  2300/ 4361 batches | lr 0.000000 | ms/batch 402.04 | loss  3.62 | ppl    37.18 | acc     0.63 | train_ae_norm     1.00\n",
      "[38/200][2399/4361] Loss_D: 0.05406976 (Loss_D_real: 0.04285400 Loss_D_fake: 0.01121576) Loss_G: 0.32649431 Loss_Enh_Dec: -1.12299275\n",
      "| epoch  38 |  2400/ 4361 batches | lr 0.000000 | ms/batch 402.12 | loss  3.61 | ppl    37.09 | acc     0.58 | train_ae_norm     1.00\n",
      "[38/200][2499/4361] Loss_D: 0.01481780 (Loss_D_real: 0.00750106 Loss_D_fake: 0.00731674) Loss_G: 0.27913037 Loss_Enh_Dec: -0.97206116\n",
      "| epoch  38 |  2500/ 4361 batches | lr 0.000000 | ms/batch 402.40 | loss  3.65 | ppl    38.50 | acc     0.61 | train_ae_norm     1.00\n",
      "[38/200][2599/4361] Loss_D: 0.02098062 (Loss_D_real: 0.00566444 Loss_D_fake: 0.01531617) Loss_G: 0.31705949 Loss_Enh_Dec: -1.27530849\n",
      "| epoch  38 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  3.62 | ppl    37.26 | acc     0.59 | train_ae_norm     1.00\n",
      "[38/200][2699/4361] Loss_D: 0.10269696 (Loss_D_real: 0.08814928 Loss_D_fake: 0.01454768) Loss_G: 0.29810837 Loss_Enh_Dec: -1.59466100\n",
      "| epoch  38 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  3.64 | ppl    38.02 | acc     0.61 | train_ae_norm     1.00\n",
      "[38/200][2799/4361] Loss_D: 0.03018500 (Loss_D_real: 0.02103643 Loss_D_fake: 0.00914856) Loss_G: 0.30413204 Loss_Enh_Dec: -1.66430175\n",
      "| epoch  38 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  3.60 | ppl    36.61 | acc     0.59 | train_ae_norm     1.00\n",
      "[38/200][2899/4361] Loss_D: 0.08423761 (Loss_D_real: 0.00789889 Loss_D_fake: 0.07633871) Loss_G: 0.39824778 Loss_Enh_Dec: -1.38367963\n",
      "| epoch  38 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  3.65 | ppl    38.52 | acc     0.59 | train_ae_norm     1.00\n",
      "[38/200][2999/4361] Loss_D: 0.01621222 (Loss_D_real: 0.00361332 Loss_D_fake: 0.01259890) Loss_G: 0.28569397 Loss_Enh_Dec: -0.84453863\n",
      "| epoch  38 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  3.66 | ppl    38.77 | acc     0.58 | train_ae_norm     1.00\n",
      "[38/200][3099/4361] Loss_D: 0.03251212 (Loss_D_real: 0.02197818 Loss_D_fake: 0.01053394) Loss_G: 0.31517228 Loss_Enh_Dec: -0.45617697\n",
      "| epoch  38 |  3100/ 4361 batches | lr 0.000000 | ms/batch 402.25 | loss  3.66 | ppl    39.03 | acc     0.58 | train_ae_norm     1.00\n",
      "[38/200][3199/4361] Loss_D: 0.02737445 (Loss_D_real: 0.01996525 Loss_D_fake: 0.00740921) Loss_G: 0.30794570 Loss_Enh_Dec: -0.82895011\n",
      "| epoch  38 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  3.68 | ppl    39.47 | acc     0.62 | train_ae_norm     1.00\n",
      "[38/200][3299/4361] Loss_D: 0.06664273 (Loss_D_real: 0.03568104 Loss_D_fake: 0.03096169) Loss_G: 0.33280438 Loss_Enh_Dec: -0.74248689\n",
      "| epoch  38 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  3.69 | ppl    40.15 | acc     0.60 | train_ae_norm     1.00\n",
      "[38/200][3399/4361] Loss_D: 0.03137151 (Loss_D_real: 0.02276977 Loss_D_fake: 0.00860174) Loss_G: 0.36490780 Loss_Enh_Dec: -0.70848078\n",
      "| epoch  38 |  3400/ 4361 batches | lr 0.000000 | ms/batch 402.36 | loss  3.66 | ppl    38.96 | acc     0.58 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38/200][3499/4361] Loss_D: 0.05079265 (Loss_D_real: 0.04145371 Loss_D_fake: 0.00933894) Loss_G: 0.32231686 Loss_Enh_Dec: -0.73301047\n",
      "| epoch  38 |  3500/ 4361 batches | lr 0.000000 | ms/batch 402.31 | loss  3.59 | ppl    36.32 | acc     0.60 | train_ae_norm     1.00\n",
      "[38/200][3599/4361] Loss_D: 0.04037683 (Loss_D_real: 0.03006030 Loss_D_fake: 0.01031653) Loss_G: 0.29387131 Loss_Enh_Dec: -0.95263195\n",
      "| epoch  38 |  3600/ 4361 batches | lr 0.000000 | ms/batch 402.16 | loss  3.61 | ppl    36.87 | acc     0.61 | train_ae_norm     1.00\n",
      "[38/200][3699/4361] Loss_D: 0.03255938 (Loss_D_real: 0.00680419 Loss_D_fake: 0.02575520) Loss_G: 0.29069850 Loss_Enh_Dec: -1.17314303\n",
      "| epoch  38 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  3.62 | ppl    37.17 | acc     0.58 | train_ae_norm     1.00\n",
      "[38/200][3799/4361] Loss_D: 0.01230628 (Loss_D_real: 0.00709169 Loss_D_fake: 0.00521460) Loss_G: 0.32532158 Loss_Enh_Dec: -0.82546198\n",
      "| epoch  38 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  3.63 | ppl    37.75 | acc     0.62 | train_ae_norm     1.00\n",
      "[38/200][3899/4361] Loss_D: 0.01336032 (Loss_D_real: 0.00671107 Loss_D_fake: 0.00664925) Loss_G: 0.32508513 Loss_Enh_Dec: -0.94104511\n",
      "| epoch  38 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  3.64 | ppl    38.12 | acc     0.57 | train_ae_norm     1.00\n",
      "[38/200][3999/4361] Loss_D: 0.05893925 (Loss_D_real: 0.04564524 Loss_D_fake: 0.01329401) Loss_G: 0.31159472 Loss_Enh_Dec: -0.85230464\n",
      "| epoch  38 |  4000/ 4361 batches | lr 0.000000 | ms/batch 402.17 | loss  3.63 | ppl    37.66 | acc     0.61 | train_ae_norm     1.00\n",
      "[38/200][4099/4361] Loss_D: 0.01964276 (Loss_D_real: 0.00703473 Loss_D_fake: 0.01260803) Loss_G: 0.30961367 Loss_Enh_Dec: -1.05134857\n",
      "| epoch  38 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  3.57 | ppl    35.63 | acc     0.57 | train_ae_norm     1.00\n",
      "[38/200][4199/4361] Loss_D: 0.02534929 (Loss_D_real: 0.01266345 Loss_D_fake: 0.01268584) Loss_G: 0.27889106 Loss_Enh_Dec: -0.99346489\n",
      "| epoch  38 |  4200/ 4361 batches | lr 0.000000 | ms/batch 402.27 | loss  3.63 | ppl    37.54 | acc     0.61 | train_ae_norm     1.00\n",
      "[38/200][4299/4361] Loss_D: 0.01882653 (Loss_D_real: 0.00949037 Loss_D_fake: 0.00933616) Loss_G: 0.34590527 Loss_Enh_Dec: -1.05265069\n",
      "| epoch  38 |  4300/ 4361 batches | lr 0.000000 | ms/batch 402.50 | loss  3.58 | ppl    35.71 | acc     0.61 | train_ae_norm     1.00\n",
      "| end of epoch  38 | time: 1855.11s | test loss  3.41 | test ppl 30.34 | acc 0.652\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 39 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:57.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.708\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.470\n",
      "  Test Loss: 3.974\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  39 |     0/ 4361 batches | lr 0.000000 | ms/batch 868.29 | loss  0.03 | ppl     1.03 | acc     0.63 | train_ae_norm     1.00\n",
      "[39/200][99/4361] Loss_D: 0.00604620 (Loss_D_real: 0.00214018 Loss_D_fake: 0.00390602) Loss_G: 0.32652569 Loss_Enh_Dec: -0.60645610\n",
      "| epoch  39 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  3.57 | ppl    35.52 | acc     0.58 | train_ae_norm     1.00\n",
      "[39/200][199/4361] Loss_D: 0.01614432 (Loss_D_real: 0.01147543 Loss_D_fake: 0.00466889) Loss_G: 0.34307277 Loss_Enh_Dec: -1.18893242\n",
      "| epoch  39 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  3.59 | ppl    36.15 | acc     0.62 | train_ae_norm     1.00\n",
      "[39/200][299/4361] Loss_D: 0.04133853 (Loss_D_real: 0.01204767 Loss_D_fake: 0.02929086) Loss_G: 0.35400507 Loss_Enh_Dec: -0.98469347\n",
      "| epoch  39 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  3.60 | ppl    36.45 | acc     0.57 | train_ae_norm     1.00\n",
      "[39/200][399/4361] Loss_D: 0.03972894 (Loss_D_real: 0.02459638 Loss_D_fake: 0.01513255) Loss_G: 0.29411760 Loss_Enh_Dec: -0.58793384\n",
      "| epoch  39 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  3.52 | ppl    33.90 | acc     0.62 | train_ae_norm     1.00\n",
      "[39/200][499/4361] Loss_D: 0.04870702 (Loss_D_real: 0.04254746 Loss_D_fake: 0.00615957) Loss_G: 0.30507350 Loss_Enh_Dec: -0.73403996\n",
      "| epoch  39 |   500/ 4361 batches | lr 0.000000 | ms/batch 402.55 | loss  3.57 | ppl    35.60 | acc     0.62 | train_ae_norm     1.00\n",
      "[39/200][599/4361] Loss_D: 0.02506397 (Loss_D_real: 0.01770350 Loss_D_fake: 0.00736047) Loss_G: 0.33283845 Loss_Enh_Dec: -0.69384360\n",
      "| epoch  39 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  3.56 | ppl    35.21 | acc     0.54 | train_ae_norm     1.00\n",
      "[39/200][699/4361] Loss_D: 0.08137941 (Loss_D_real: 0.00446703 Loss_D_fake: 0.07691238) Loss_G: 0.39998642 Loss_Enh_Dec: -0.70652461\n",
      "| epoch  39 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  3.58 | ppl    35.93 | acc     0.63 | train_ae_norm     1.00\n",
      "[39/200][799/4361] Loss_D: 0.03965732 (Loss_D_real: 0.02924191 Loss_D_fake: 0.01041541) Loss_G: 0.35910487 Loss_Enh_Dec: -0.82786769\n",
      "| epoch  39 |   800/ 4361 batches | lr 0.000000 | ms/batch 402.24 | loss  3.57 | ppl    35.49 | acc     0.59 | train_ae_norm     1.00\n",
      "[39/200][899/4361] Loss_D: 0.10645293 (Loss_D_real: 0.08583583 Loss_D_fake: 0.02061710) Loss_G: 0.31202695 Loss_Enh_Dec: -0.93546736\n",
      "| epoch  39 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  3.58 | ppl    35.93 | acc     0.64 | train_ae_norm     1.00\n",
      "[39/200][999/4361] Loss_D: 0.00700319 (Loss_D_real: 0.00327137 Loss_D_fake: 0.00373182) Loss_G: 0.31318137 Loss_Enh_Dec: -1.28920126\n",
      "| epoch  39 |  1000/ 4361 batches | lr 0.000000 | ms/batch 400.85 | loss  3.57 | ppl    35.69 | acc     0.58 | train_ae_norm     1.00\n",
      "[39/200][1099/4361] Loss_D: 0.05298162 (Loss_D_real: 0.04096670 Loss_D_fake: 0.01201491) Loss_G: 0.35101849 Loss_Enh_Dec: -1.25920093\n",
      "| epoch  39 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.91 | loss  3.55 | ppl    34.75 | acc     0.59 | train_ae_norm     1.00\n",
      "[39/200][1199/4361] Loss_D: 0.01229488 (Loss_D_real: 0.00416634 Loss_D_fake: 0.00812854) Loss_G: 0.31868893 Loss_Enh_Dec: -1.37690675\n",
      "| epoch  39 |  1200/ 4361 batches | lr 0.000000 | ms/batch 402.64 | loss  3.57 | ppl    35.57 | acc     0.63 | train_ae_norm     1.00\n",
      "[39/200][1299/4361] Loss_D: 0.02659743 (Loss_D_real: 0.00868049 Loss_D_fake: 0.01791694) Loss_G: 0.33151969 Loss_Enh_Dec: -1.05985546\n",
      "| epoch  39 |  1300/ 4361 batches | lr 0.000000 | ms/batch 402.22 | loss  3.57 | ppl    35.42 | acc     0.62 | train_ae_norm     1.00\n",
      "[39/200][1399/4361] Loss_D: 0.03598039 (Loss_D_real: 0.00759428 Loss_D_fake: 0.02838611) Loss_G: 0.32630616 Loss_Enh_Dec: -1.16457844\n",
      "| epoch  39 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  3.58 | ppl    35.93 | acc     0.56 | train_ae_norm     1.00\n",
      "[39/200][1499/4361] Loss_D: 0.03290949 (Loss_D_real: 0.01960718 Loss_D_fake: 0.01330231) Loss_G: 0.28791490 Loss_Enh_Dec: -1.13636291\n",
      "| epoch  39 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  3.61 | ppl    36.84 | acc     0.58 | train_ae_norm     1.00\n",
      "[39/200][1599/4361] Loss_D: 0.03478144 (Loss_D_real: 0.02469471 Loss_D_fake: 0.01008673) Loss_G: 0.30824372 Loss_Enh_Dec: -1.46823561\n",
      "| epoch  39 |  1600/ 4361 batches | lr 0.000000 | ms/batch 402.23 | loss  3.57 | ppl    35.56 | acc     0.60 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39/200][1699/4361] Loss_D: 0.19553091 (Loss_D_real: 0.18152641 Loss_D_fake: 0.01400450) Loss_G: 0.30540034 Loss_Enh_Dec: -1.21928489\n",
      "| epoch  39 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  3.55 | ppl    34.91 | acc     0.57 | train_ae_norm     1.00\n",
      "[39/200][1799/4361] Loss_D: 0.08085936 (Loss_D_real: 0.07569014 Loss_D_fake: 0.00516922) Loss_G: 0.34300384 Loss_Enh_Dec: -1.29366410\n",
      "| epoch  39 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  3.52 | ppl    33.86 | acc     0.62 | train_ae_norm     1.00\n",
      "[39/200][1899/4361] Loss_D: 0.13495734 (Loss_D_real: 0.12632737 Loss_D_fake: 0.00862997) Loss_G: 0.28343275 Loss_Enh_Dec: -1.26101911\n",
      "| epoch  39 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  3.58 | ppl    35.99 | acc     0.63 | train_ae_norm     1.00\n",
      "[39/200][1999/4361] Loss_D: 0.01442305 (Loss_D_real: 0.00626639 Loss_D_fake: 0.00815667) Loss_G: 0.31828648 Loss_Enh_Dec: -1.53867447\n",
      "| epoch  39 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  3.54 | ppl    34.33 | acc     0.59 | train_ae_norm     1.00\n",
      "[39/200][2099/4361] Loss_D: 0.04015053 (Loss_D_real: 0.01889367 Loss_D_fake: 0.02125686) Loss_G: 0.35812777 Loss_Enh_Dec: -1.63716543\n",
      "| epoch  39 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  3.55 | ppl    34.71 | acc     0.62 | train_ae_norm     1.00\n",
      "[39/200][2199/4361] Loss_D: 0.01107111 (Loss_D_real: 0.00153899 Loss_D_fake: 0.00953213) Loss_G: 0.29883409 Loss_Enh_Dec: -1.42526078\n",
      "| epoch  39 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  3.55 | ppl    34.81 | acc     0.62 | train_ae_norm     1.00\n",
      "[39/200][2299/4361] Loss_D: 0.05480208 (Loss_D_real: 0.03322630 Loss_D_fake: 0.02157578) Loss_G: 0.30802923 Loss_Enh_Dec: -1.35528517\n",
      "| epoch  39 |  2300/ 4361 batches | lr 0.000000 | ms/batch 402.04 | loss  3.51 | ppl    33.57 | acc     0.62 | train_ae_norm     1.00\n",
      "[39/200][2399/4361] Loss_D: 0.02195562 (Loss_D_real: 0.01057912 Loss_D_fake: 0.01137650) Loss_G: 0.33039463 Loss_Enh_Dec: -0.78253627\n",
      "| epoch  39 |  2400/ 4361 batches | lr 0.000000 | ms/batch 402.38 | loss  3.52 | ppl    33.93 | acc     0.60 | train_ae_norm     1.00\n",
      "[39/200][2499/4361] Loss_D: 0.04140650 (Loss_D_real: 0.00744862 Loss_D_fake: 0.03395788) Loss_G: 0.29463041 Loss_Enh_Dec: -1.08684993\n",
      "| epoch  39 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  3.56 | ppl    35.02 | acc     0.62 | train_ae_norm     1.00\n",
      "[39/200][2599/4361] Loss_D: 0.01797310 (Loss_D_real: 0.01348566 Loss_D_fake: 0.00448744) Loss_G: 0.35584140 Loss_Enh_Dec: -1.18538427\n",
      "| epoch  39 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  3.54 | ppl    34.32 | acc     0.59 | train_ae_norm     1.00\n",
      "[39/200][2699/4361] Loss_D: 0.04183853 (Loss_D_real: 0.03647351 Loss_D_fake: 0.00536502) Loss_G: 0.32798538 Loss_Enh_Dec: -1.39338422\n",
      "| epoch  39 |  2700/ 4361 batches | lr 0.000000 | ms/batch 402.45 | loss  3.56 | ppl    35.12 | acc     0.63 | train_ae_norm     1.00\n",
      "[39/200][2799/4361] Loss_D: 0.01687341 (Loss_D_real: 0.00708932 Loss_D_fake: 0.00978410) Loss_G: 0.31829834 Loss_Enh_Dec: -0.78685898\n",
      "| epoch  39 |  2800/ 4361 batches | lr 0.000000 | ms/batch 402.74 | loss  3.49 | ppl    32.64 | acc     0.61 | train_ae_norm     1.00\n",
      "[39/200][2899/4361] Loss_D: 0.02557681 (Loss_D_real: 0.01845815 Loss_D_fake: 0.00711866) Loss_G: 0.37060335 Loss_Enh_Dec: -0.65659422\n",
      "| epoch  39 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  3.54 | ppl    34.44 | acc     0.60 | train_ae_norm     1.00\n",
      "[39/200][2999/4361] Loss_D: 0.04699429 (Loss_D_real: 0.03791516 Loss_D_fake: 0.00907913) Loss_G: 0.29985404 Loss_Enh_Dec: -1.12030303\n",
      "| epoch  39 |  3000/ 4361 batches | lr 0.000000 | ms/batch 402.28 | loss  3.55 | ppl    34.98 | acc     0.62 | train_ae_norm     1.00\n",
      "[39/200][3099/4361] Loss_D: 0.02251406 (Loss_D_real: 0.00723198 Loss_D_fake: 0.01528208) Loss_G: 0.30151585 Loss_Enh_Dec: -1.13768065\n",
      "| epoch  39 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  3.57 | ppl    35.47 | acc     0.59 | train_ae_norm     1.00\n",
      "[39/200][3199/4361] Loss_D: 0.02079397 (Loss_D_real: 0.01038083 Loss_D_fake: 0.01041313) Loss_G: 0.33359155 Loss_Enh_Dec: -0.96213478\n",
      "| epoch  39 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  3.59 | ppl    36.13 | acc     0.63 | train_ae_norm     1.00\n",
      "[39/200][3299/4361] Loss_D: 0.01312141 (Loss_D_real: 0.00724852 Loss_D_fake: 0.00587289) Loss_G: 0.31827375 Loss_Enh_Dec: -1.07302368\n",
      "| epoch  39 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  3.61 | ppl    37.11 | acc     0.61 | train_ae_norm     1.00\n",
      "[39/200][3599/4361] Loss_D: 0.06114584 (Loss_D_real: 0.05524616 Loss_D_fake: 0.00589968) Loss_G: 0.37017640 Loss_Enh_Dec: -1.04425132\n",
      "| epoch  39 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  3.53 | ppl    34.25 | acc     0.64 | train_ae_norm     1.00\n",
      "[39/200][3699/4361] Loss_D: 0.01655754 (Loss_D_real: 0.00501582 Loss_D_fake: 0.01154171) Loss_G: 0.32590455 Loss_Enh_Dec: -1.32122982\n",
      "| epoch  39 |  3700/ 4361 batches | lr 0.000000 | ms/batch 402.17 | loss  3.55 | ppl    34.71 | acc     0.58 | train_ae_norm     1.00\n",
      "[39/200][3799/4361] Loss_D: 0.00798157 (Loss_D_real: 0.00570431 Loss_D_fake: 0.00227726) Loss_G: 0.31350729 Loss_Enh_Dec: -1.29534185\n",
      "| epoch  39 |  3800/ 4361 batches | lr 0.000000 | ms/batch 402.09 | loss  3.58 | ppl    35.78 | acc     0.63 | train_ae_norm     1.00\n",
      "[39/200][3899/4361] Loss_D: 0.02086314 (Loss_D_real: 0.01437101 Loss_D_fake: 0.00649213) Loss_G: 0.35207134 Loss_Enh_Dec: -1.32832491\n",
      "| epoch  39 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  3.57 | ppl    35.38 | acc     0.57 | train_ae_norm     1.00\n",
      "[39/200][3999/4361] Loss_D: 0.05901160 (Loss_D_real: 0.00448640 Loss_D_fake: 0.05452520) Loss_G: 0.34177214 Loss_Enh_Dec: -1.38627589\n",
      "| epoch  39 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  3.60 | ppl    36.43 | acc     0.62 | train_ae_norm     1.00\n",
      "[39/200][4099/4361] Loss_D: 0.03626456 (Loss_D_real: 0.03420259 Loss_D_fake: 0.00206198) Loss_G: 0.34820706 Loss_Enh_Dec: -1.32246900\n",
      "| epoch  39 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  3.54 | ppl    34.36 | acc     0.58 | train_ae_norm     1.00\n",
      "[39/200][4199/4361] Loss_D: 0.01006033 (Loss_D_real: 0.00411333 Loss_D_fake: 0.00594699) Loss_G: 0.33969411 Loss_Enh_Dec: -1.12229455\n",
      "| epoch  39 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  3.57 | ppl    35.61 | acc     0.64 | train_ae_norm     1.00\n",
      "[39/200][4299/4361] Loss_D: 0.01570988 (Loss_D_real: 0.00631457 Loss_D_fake: 0.00939530) Loss_G: 0.30722141 Loss_Enh_Dec: -1.44897676\n",
      "| epoch  39 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  3.51 | ppl    33.42 | acc     0.61 | train_ae_norm     1.00\n",
      "| end of epoch  39 | time: 1854.76s | test loss  3.39 | test ppl 29.66 | acc 0.655\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 40 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.709\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.472\n",
      "  Test Loss: 3.919\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  40 |     0/ 4361 batches | lr 0.000000 | ms/batch 867.46 | loss  0.03 | ppl     1.03 | acc     0.64 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40/200][99/4361] Loss_D: 0.00833093 (Loss_D_real: 0.00291725 Loss_D_fake: 0.00541368) Loss_G: 0.32741687 Loss_Enh_Dec: -1.01766241\n",
      "| epoch  40 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.06 | loss  3.54 | ppl    34.50 | acc     0.59 | train_ae_norm     1.00\n",
      "[40/200][199/4361] Loss_D: 0.05465482 (Loss_D_real: 0.02339814 Loss_D_fake: 0.03125668) Loss_G: 0.32700077 Loss_Enh_Dec: -1.39574337\n",
      "| epoch  40 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  3.56 | ppl    35.21 | acc     0.63 | train_ae_norm     1.00\n",
      "[40/200][299/4361] Loss_D: 0.00822841 (Loss_D_real: 0.00305533 Loss_D_fake: 0.00517308) Loss_G: 0.32509682 Loss_Enh_Dec: -1.36195171\n",
      "| epoch  40 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  3.58 | ppl    35.84 | acc     0.59 | train_ae_norm     1.00\n",
      "[40/200][399/4361] Loss_D: 0.07837366 (Loss_D_real: 0.05839328 Loss_D_fake: 0.01998038) Loss_G: 0.53778344 Loss_Enh_Dec: -0.70406258\n",
      "| epoch  40 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  3.49 | ppl    32.70 | acc     0.63 | train_ae_norm     1.00\n",
      "[40/200][499/4361] Loss_D: 0.01128619 (Loss_D_real: 0.00312307 Loss_D_fake: 0.00816313) Loss_G: 0.33826417 Loss_Enh_Dec: -0.83739084\n",
      "| epoch  40 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  3.56 | ppl    35.11 | acc     0.63 | train_ae_norm     1.00\n",
      "[40/200][599/4361] Loss_D: 0.01301952 (Loss_D_real: 0.00238097 Loss_D_fake: 0.01063856) Loss_G: 0.35814223 Loss_Enh_Dec: -1.31476915\n",
      "| epoch  40 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  3.52 | ppl    33.90 | acc     0.59 | train_ae_norm     1.00\n",
      "[40/200][699/4361] Loss_D: 0.03624068 (Loss_D_real: 0.01859325 Loss_D_fake: 0.01764743) Loss_G: 0.37867656 Loss_Enh_Dec: -1.16064131\n",
      "| epoch  40 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  3.57 | ppl    35.40 | acc     0.63 | train_ae_norm     1.00\n",
      "[40/200][799/4361] Loss_D: 0.02330360 (Loss_D_real: 0.01627633 Loss_D_fake: 0.00702727) Loss_G: 0.40521970 Loss_Enh_Dec: -1.10298109\n",
      "| epoch  40 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  3.54 | ppl    34.48 | acc     0.61 | train_ae_norm     1.00\n",
      "[40/200][899/4361] Loss_D: 0.12113053 (Loss_D_real: 0.05063948 Loss_D_fake: 0.07049105) Loss_G: 0.14009123 Loss_Enh_Dec: -0.72153533\n",
      "| epoch  40 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  3.56 | ppl    35.09 | acc     0.64 | train_ae_norm     1.00\n",
      "[40/200][999/4361] Loss_D: 0.06555381 (Loss_D_real: 0.02919780 Loss_D_fake: 0.03635602) Loss_G: 0.16501474 Loss_Enh_Dec: -0.92088521\n",
      "| epoch  40 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  3.52 | ppl    33.69 | acc     0.64 | train_ae_norm     1.00\n",
      "[40/200][1099/4361] Loss_D: 0.05824994 (Loss_D_real: 0.03344387 Loss_D_fake: 0.02480607) Loss_G: 0.18369840 Loss_Enh_Dec: -0.79090375\n",
      "| epoch  40 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  3.50 | ppl    33.05 | acc     0.62 | train_ae_norm     1.00\n",
      "[40/200][1199/4361] Loss_D: 0.03682334 (Loss_D_real: 0.01475328 Loss_D_fake: 0.02207007) Loss_G: 0.19041438 Loss_Enh_Dec: -0.77690363\n",
      "| epoch  40 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  3.51 | ppl    33.38 | acc     0.62 | train_ae_norm     1.00\n",
      "[40/200][1299/4361] Loss_D: 0.02983040 (Loss_D_real: 0.00941087 Loss_D_fake: 0.02041953) Loss_G: 0.20751643 Loss_Enh_Dec: -0.92678225\n",
      "| epoch  40 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  3.54 | ppl    34.43 | acc     0.61 | train_ae_norm     1.00\n",
      "[40/200][1399/4361] Loss_D: 0.03970446 (Loss_D_real: 0.02254694 Loss_D_fake: 0.01715752) Loss_G: 0.20131575 Loss_Enh_Dec: -0.73939556\n",
      "| epoch  40 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  3.52 | ppl    33.76 | acc     0.55 | train_ae_norm     1.00\n",
      "[40/200][1499/4361] Loss_D: 0.02309697 (Loss_D_real: 0.00677980 Loss_D_fake: 0.01631717) Loss_G: 0.20509544 Loss_Enh_Dec: -0.22545329\n",
      "| epoch  40 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  3.54 | ppl    34.48 | acc     0.60 | train_ae_norm     1.00\n",
      "[40/200][1599/4361] Loss_D: 0.03156059 (Loss_D_real: 0.01091758 Loss_D_fake: 0.02064301) Loss_G: 0.20417801 Loss_Enh_Dec: -0.27957961\n",
      "| epoch  40 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  3.53 | ppl    33.96 | acc     0.62 | train_ae_norm     1.00\n",
      "[40/200][1699/4361] Loss_D: 0.02067099 (Loss_D_real: 0.00369570 Loss_D_fake: 0.01697529) Loss_G: 0.20629458 Loss_Enh_Dec: -0.41140148\n",
      "| epoch  40 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  3.49 | ppl    32.84 | acc     0.60 | train_ae_norm     1.00\n",
      "[40/200][1799/4361] Loss_D: 0.07114642 (Loss_D_real: 0.05337987 Loss_D_fake: 0.01776655) Loss_G: 0.21035297 Loss_Enh_Dec: -0.50018996\n",
      "| epoch  40 |  1800/ 4361 batches | lr 0.000000 | ms/batch 402.20 | loss  3.48 | ppl    32.31 | acc     0.61 | train_ae_norm     1.00\n",
      "[40/200][1899/4361] Loss_D: 0.02420950 (Loss_D_real: 0.00429665 Loss_D_fake: 0.01991285) Loss_G: 0.21063676 Loss_Enh_Dec: -0.94878179\n",
      "| epoch  40 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  3.55 | ppl    34.96 | acc     0.64 | train_ae_norm     1.00\n",
      "[40/200][1999/4361] Loss_D: 0.01743501 (Loss_D_real: 0.00566622 Loss_D_fake: 0.01176879) Loss_G: 0.21990728 Loss_Enh_Dec: -0.75869882\n",
      "| epoch  40 |  2000/ 4361 batches | lr 0.000000 | ms/batch 402.14 | loss  3.46 | ppl    31.96 | acc     0.61 | train_ae_norm     1.00\n",
      "[40/200][2099/4361] Loss_D: 0.01660510 (Loss_D_real: 0.00149057 Loss_D_fake: 0.01511454) Loss_G: 0.22214435 Loss_Enh_Dec: -0.89178133\n",
      "| epoch  40 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  3.50 | ppl    33.08 | acc     0.62 | train_ae_norm     1.00\n",
      "[40/200][2199/4361] Loss_D: 0.03125637 (Loss_D_real: 0.01198005 Loss_D_fake: 0.01927632) Loss_G: 0.22807537 Loss_Enh_Dec: -0.78701395\n",
      "| epoch  40 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  3.50 | ppl    33.01 | acc     0.62 | train_ae_norm     1.00\n",
      "[40/200][2299/4361] Loss_D: 0.03742654 (Loss_D_real: 0.02704537 Loss_D_fake: 0.01038117) Loss_G: 0.23018722 Loss_Enh_Dec: -0.88777494\n",
      "| epoch  40 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  3.48 | ppl    32.52 | acc     0.61 | train_ae_norm     1.00\n",
      "[40/200][2399/4361] Loss_D: 0.01599474 (Loss_D_real: 0.00450213 Loss_D_fake: 0.01149261) Loss_G: 0.22965896 Loss_Enh_Dec: -1.28595078\n",
      "| epoch  40 |  2400/ 4361 batches | lr 0.000000 | ms/batch 402.62 | loss  3.48 | ppl    32.49 | acc     0.59 | train_ae_norm     1.00\n",
      "[40/200][2499/4361] Loss_D: 0.02190145 (Loss_D_real: 0.01201023 Loss_D_fake: 0.00989122) Loss_G: 0.23604611 Loss_Enh_Dec: -1.28850055\n",
      "| epoch  40 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  3.53 | ppl    34.07 | acc     0.61 | train_ae_norm     1.00\n",
      "[40/200][2599/4361] Loss_D: 0.04311212 (Loss_D_real: 0.03054769 Loss_D_fake: 0.01256443) Loss_G: 0.23141852 Loss_Enh_Dec: -1.41053295\n",
      "| epoch  40 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  3.51 | ppl    33.38 | acc     0.59 | train_ae_norm     1.00\n",
      "[40/200][2699/4361] Loss_D: 0.02275190 (Loss_D_real: 0.00497032 Loss_D_fake: 0.01778158) Loss_G: 0.24800734 Loss_Enh_Dec: -1.16531789\n",
      "| epoch  40 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  3.53 | ppl    34.11 | acc     0.60 | train_ae_norm     1.00\n",
      "[40/200][2799/4361] Loss_D: 0.01589728 (Loss_D_real: 0.00445967 Loss_D_fake: 0.01143760) Loss_G: 0.22913900 Loss_Enh_Dec: -1.09956241\n",
      "| epoch  40 |  2800/ 4361 batches | lr 0.000000 | ms/batch 402.32 | loss  3.46 | ppl    31.90 | acc     0.62 | train_ae_norm     1.00\n",
      "[40/200][2899/4361] Loss_D: 0.02097810 (Loss_D_real: 0.00701331 Loss_D_fake: 0.01396479) Loss_G: 0.27806690 Loss_Enh_Dec: -0.74495804\n",
      "| epoch  40 |  2900/ 4361 batches | lr 0.000000 | ms/batch 402.16 | loss  3.51 | ppl    33.30 | acc     0.64 | train_ae_norm     1.00\n",
      "[40/200][2999/4361] Loss_D: 0.01844414 (Loss_D_real: 0.00593471 Loss_D_fake: 0.01250943) Loss_G: 0.25326124 Loss_Enh_Dec: -0.44122753\n",
      "| epoch  40 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  3.51 | ppl    33.59 | acc     0.61 | train_ae_norm     1.00\n",
      "[40/200][3099/4361] Loss_D: 0.06155922 (Loss_D_real: 0.00814937 Loss_D_fake: 0.05340984) Loss_G: 0.24168539 Loss_Enh_Dec: -0.95637625\n",
      "| epoch  40 |  3100/ 4361 batches | lr 0.000000 | ms/batch 402.33 | loss  3.52 | ppl    33.84 | acc     0.59 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40/200][3199/4361] Loss_D: 0.16098671 (Loss_D_real: 0.02297350 Loss_D_fake: 0.13801321) Loss_G: 0.50041085 Loss_Enh_Dec: -0.81088775\n",
      "| epoch  40 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  3.56 | ppl    35.24 | acc     0.62 | train_ae_norm     1.00\n",
      "[40/200][3299/4361] Loss_D: 0.01440699 (Loss_D_real: 0.00286189 Loss_D_fake: 0.01154511) Loss_G: 0.24328171 Loss_Enh_Dec: -1.18902469\n",
      "| epoch  40 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  3.57 | ppl    35.62 | acc     0.61 | train_ae_norm     1.00\n",
      "[40/200][3399/4361] Loss_D: 0.04963849 (Loss_D_real: 0.04044256 Loss_D_fake: 0.00919593) Loss_G: 0.24814974 Loss_Enh_Dec: -1.17182279\n",
      "| epoch  40 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  3.53 | ppl    34.08 | acc     0.61 | train_ae_norm     1.00\n",
      "[40/200][3499/4361] Loss_D: 0.22453745 (Loss_D_real: 0.01373246 Loss_D_fake: 0.21080498) Loss_G: 0.27590623 Loss_Enh_Dec: -1.22816420\n",
      "| epoch  40 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.71 | loss  3.49 | ppl    32.91 | acc     0.62 | train_ae_norm     1.00\n",
      "[40/200][3599/4361] Loss_D: 0.01654391 (Loss_D_real: 0.00350763 Loss_D_fake: 0.01303628) Loss_G: 0.24988174 Loss_Enh_Dec: -1.53039002\n",
      "| epoch  40 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  3.51 | ppl    33.51 | acc     0.63 | train_ae_norm     1.00\n",
      "[40/200][3699/4361] Loss_D: 0.01721443 (Loss_D_real: 0.00799828 Loss_D_fake: 0.00921616) Loss_G: 0.25006023 Loss_Enh_Dec: -1.06014156\n",
      "| epoch  40 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  3.51 | ppl    33.58 | acc     0.60 | train_ae_norm     1.00\n",
      "[40/200][3799/4361] Loss_D: 0.03872639 (Loss_D_real: 0.02219660 Loss_D_fake: 0.01652978) Loss_G: 0.22397368 Loss_Enh_Dec: -0.91930771\n",
      "| epoch  40 |  3800/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  3.56 | ppl    35.04 | acc     0.65 | train_ae_norm     1.00\n",
      "[40/200][3899/4361] Loss_D: 0.01406992 (Loss_D_real: 0.00524620 Loss_D_fake: 0.00882371) Loss_G: 0.25230762 Loss_Enh_Dec: -0.96659100\n",
      "| epoch  40 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  3.54 | ppl    34.55 | acc     0.57 | train_ae_norm     1.00\n",
      "[40/200][3999/4361] Loss_D: 0.01311423 (Loss_D_real: 0.00523514 Loss_D_fake: 0.00787909) Loss_G: 0.26692721 Loss_Enh_Dec: -1.11200392\n",
      "| epoch  40 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  3.59 | ppl    36.27 | acc     0.62 | train_ae_norm     1.00\n",
      "[40/200][4099/4361] Loss_D: 0.05741889 (Loss_D_real: 0.04377767 Loss_D_fake: 0.01364122) Loss_G: 0.24605706 Loss_Enh_Dec: -0.58228827\n",
      "| epoch  40 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  3.53 | ppl    34.09 | acc     0.61 | train_ae_norm     1.00\n",
      "[40/200][4199/4361] Loss_D: 0.02767983 (Loss_D_real: 0.00338604 Loss_D_fake: 0.02429379) Loss_G: 0.37721071 Loss_Enh_Dec: -0.73487031\n",
      "| epoch  40 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  3.58 | ppl    36.05 | acc     0.65 | train_ae_norm     1.00\n",
      "[40/200][4299/4361] Loss_D: 0.01234529 (Loss_D_real: 0.00208814 Loss_D_fake: 0.01025715) Loss_G: 0.26816973 Loss_Enh_Dec: -0.97079432\n",
      "| epoch  40 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.74 | loss  3.51 | ppl    33.49 | acc     0.64 | train_ae_norm     1.00\n",
      "| end of epoch  40 | time: 1853.89s | test loss  3.45 | test ppl 31.52 | acc 0.652\n",
      "bleu_self:  [4.22916666e-01 3.21078812e-01 3.63122117e-06 1.60586867e-06\n",
      " 1.19855153e-06]\n",
      "bleu_test:  [6.77777777e-01 4.63914870e-01 4.98715659e-02 8.79395867e-06\n",
      " 1.27355883e-06]\n",
      "bleu_self: [0.42291667,0.32107881,0.00000363,0.00000161,0.00000120]\n",
      "bleu_test: [0.67777778,0.46391487,0.04987157,0.00000879,0.00000127]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 41 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:18.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.721\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.482\n",
      "  Test Loss: 3.792\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  41 |     0/ 4361 batches | lr 0.000000 | ms/batch 860.90 | loss  0.03 | ppl     1.03 | acc     0.67 | train_ae_norm     1.00\n",
      "[41/200][99/4361] Loss_D: 0.01254108 (Loss_D_real: 0.00303459 Loss_D_fake: 0.00950649) Loss_G: 0.30291843 Loss_Enh_Dec: -1.55486286\n",
      "| epoch  41 |   100/ 4361 batches | lr 0.000000 | ms/batch 400.82 | loss  3.54 | ppl    34.41 | acc     0.57 | train_ae_norm     1.00\n",
      "[41/200][199/4361] Loss_D: 0.02717994 (Loss_D_real: 0.00709716 Loss_D_fake: 0.02008278) Loss_G: 0.27951503 Loss_Enh_Dec: -1.43054998\n",
      "| epoch  41 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  3.59 | ppl    36.41 | acc     0.62 | train_ae_norm     1.00\n",
      "[41/200][299/4361] Loss_D: 0.00980703 (Loss_D_real: 0.00459008 Loss_D_fake: 0.00521694) Loss_G: 0.35137430 Loss_Enh_Dec: -1.51534927\n",
      "| epoch  41 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  3.59 | ppl    36.09 | acc     0.57 | train_ae_norm     1.00\n",
      "[41/200][399/4361] Loss_D: 0.08624680 (Loss_D_real: 0.03742430 Loss_D_fake: 0.04882249) Loss_G: 0.46800384 Loss_Enh_Dec: -1.40377104\n",
      "| epoch  41 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  3.49 | ppl    32.78 | acc     0.62 | train_ae_norm     1.00\n",
      "[41/200][499/4361] Loss_D: 0.02719206 (Loss_D_real: 0.01997864 Loss_D_fake: 0.00721342) Loss_G: 0.29718408 Loss_Enh_Dec: -1.34215856\n",
      "| epoch  41 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  3.56 | ppl    35.20 | acc     0.63 | train_ae_norm     1.00\n",
      "[41/200][599/4361] Loss_D: 0.01613899 (Loss_D_real: 0.01250785 Loss_D_fake: 0.00363114) Loss_G: 0.37588117 Loss_Enh_Dec: -1.37386310\n",
      "| epoch  41 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  3.52 | ppl    33.71 | acc     0.56 | train_ae_norm     1.00\n",
      "[41/200][699/4361] Loss_D: 0.01378170 (Loss_D_real: 0.00974723 Loss_D_fake: 0.00403447) Loss_G: 0.31286597 Loss_Enh_Dec: -1.34159017\n",
      "| epoch  41 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  3.56 | ppl    35.00 | acc     0.63 | train_ae_norm     1.00\n",
      "[41/200][799/4361] Loss_D: 0.01419495 (Loss_D_real: 0.00332337 Loss_D_fake: 0.01087157) Loss_G: 0.29133603 Loss_Enh_Dec: -1.51169646\n",
      "| epoch  41 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.76 | loss  3.52 | ppl    33.85 | acc     0.61 | train_ae_norm     1.00\n",
      "[41/200][899/4361] Loss_D: 0.02445013 (Loss_D_real: 0.00658911 Loss_D_fake: 0.01786102) Loss_G: 0.32395059 Loss_Enh_Dec: -1.33127809\n",
      "| epoch  41 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  3.54 | ppl    34.59 | acc     0.64 | train_ae_norm     1.00\n",
      "[41/200][999/4361] Loss_D: 0.01263378 (Loss_D_real: 0.00747112 Loss_D_fake: 0.00516266) Loss_G: 0.30420110 Loss_Enh_Dec: -1.47184122\n",
      "| epoch  41 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  3.53 | ppl    34.25 | acc     0.62 | train_ae_norm     1.00\n",
      "[41/200][1099/4361] Loss_D: 0.01812605 (Loss_D_real: 0.00898934 Loss_D_fake: 0.00913670) Loss_G: 0.32394609 Loss_Enh_Dec: -1.59135747\n",
      "| epoch  41 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  3.53 | ppl    34.23 | acc     0.59 | train_ae_norm     1.00\n",
      "[41/200][1199/4361] Loss_D: 0.00598562 (Loss_D_real: 0.00186944 Loss_D_fake: 0.00411618) Loss_G: 0.34884262 Loss_Enh_Dec: -1.38723648\n",
      "| epoch  41 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  3.55 | ppl    34.83 | acc     0.62 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41/200][1299/4361] Loss_D: 0.00755382 (Loss_D_real: 0.00171779 Loss_D_fake: 0.00583603) Loss_G: 0.30113909 Loss_Enh_Dec: -1.63520467\n",
      "| epoch  41 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  3.57 | ppl    35.57 | acc     0.60 | train_ae_norm     1.00\n",
      "[41/200][1399/4361] Loss_D: 0.01352487 (Loss_D_real: 0.00825998 Loss_D_fake: 0.00526488) Loss_G: 0.30636826 Loss_Enh_Dec: -1.77852499\n",
      "| epoch  41 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  3.57 | ppl    35.63 | acc     0.56 | train_ae_norm     1.00\n",
      "[41/200][1499/4361] Loss_D: 0.05702377 (Loss_D_real: 0.00690976 Loss_D_fake: 0.05011402) Loss_G: 0.48086295 Loss_Enh_Dec: -1.64305270\n",
      "| epoch  41 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  3.59 | ppl    36.30 | acc     0.59 | train_ae_norm     1.00\n",
      "[41/200][1599/4361] Loss_D: 0.03825755 (Loss_D_real: 0.02365303 Loss_D_fake: 0.01460451) Loss_G: 0.30412346 Loss_Enh_Dec: -1.71094632\n",
      "| epoch  41 |  1600/ 4361 batches | lr 0.000000 | ms/batch 402.19 | loss  3.55 | ppl    34.87 | acc     0.61 | train_ae_norm     1.00\n",
      "[41/200][1699/4361] Loss_D: 0.00812868 (Loss_D_real: 0.00256066 Loss_D_fake: 0.00556801) Loss_G: 0.30238655 Loss_Enh_Dec: -1.79140055\n",
      "| epoch  41 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.75 | loss  3.53 | ppl    34.00 | acc     0.60 | train_ae_norm     1.00\n",
      "[41/200][1799/4361] Loss_D: 0.03224418 (Loss_D_real: 0.00635132 Loss_D_fake: 0.02589286) Loss_G: 0.31891656 Loss_Enh_Dec: -1.82853031\n",
      "| epoch  41 |  1800/ 4361 batches | lr 0.000000 | ms/batch 402.13 | loss  3.50 | ppl    33.03 | acc     0.63 | train_ae_norm     1.00\n",
      "[41/200][1899/4361] Loss_D: 0.01377568 (Loss_D_real: 0.01259138 Loss_D_fake: 0.00118430) Loss_G: 0.59156513 Loss_Enh_Dec: -1.45403957\n",
      "| epoch  41 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  3.56 | ppl    35.33 | acc     0.62 | train_ae_norm     1.00\n",
      "[41/200][1999/4361] Loss_D: 0.01197248 (Loss_D_real: 0.00395852 Loss_D_fake: 0.00801396) Loss_G: 0.31426629 Loss_Enh_Dec: -1.06506789\n",
      "| epoch  41 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  3.51 | ppl    33.46 | acc     0.63 | train_ae_norm     1.00\n",
      "[41/200][2099/4361] Loss_D: 0.04271852 (Loss_D_real: 0.03119421 Loss_D_fake: 0.01152431) Loss_G: 0.29199591 Loss_Enh_Dec: -1.65574551\n",
      "| epoch  41 |  2100/ 4361 batches | lr 0.000000 | ms/batch 402.24 | loss  3.52 | ppl    33.82 | acc     0.61 | train_ae_norm     1.00\n",
      "[41/200][2199/4361] Loss_D: 0.01358863 (Loss_D_real: 0.00935050 Loss_D_fake: 0.00423813) Loss_G: 0.34693497 Loss_Enh_Dec: -1.64628017\n",
      "| epoch  41 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  3.51 | ppl    33.60 | acc     0.62 | train_ae_norm     1.00\n",
      "[41/200][2299/4361] Loss_D: 0.02080612 (Loss_D_real: 0.01572509 Loss_D_fake: 0.00508102) Loss_G: 0.34466362 Loss_Enh_Dec: -1.69819677\n",
      "| epoch  41 |  2300/ 4361 batches | lr 0.000000 | ms/batch 402.49 | loss  3.50 | ppl    33.03 | acc     0.61 | train_ae_norm     1.00\n",
      "[41/200][2399/4361] Loss_D: 0.01696195 (Loss_D_real: 0.01220875 Loss_D_fake: 0.00475320) Loss_G: 0.33385429 Loss_Enh_Dec: -1.55946791\n",
      "| epoch  41 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  3.51 | ppl    33.42 | acc     0.59 | train_ae_norm     1.00\n",
      "[41/200][2499/4361] Loss_D: 0.01801625 (Loss_D_real: 0.01316435 Loss_D_fake: 0.00485190) Loss_G: 0.33267429 Loss_Enh_Dec: -1.59817505\n",
      "| epoch  41 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  3.58 | ppl    35.74 | acc     0.61 | train_ae_norm     1.00\n",
      "[41/200][2599/4361] Loss_D: 0.03678964 (Loss_D_real: 0.03105008 Loss_D_fake: 0.00573956) Loss_G: 0.32384607 Loss_Enh_Dec: -1.63554323\n",
      "| epoch  41 |  2600/ 4361 batches | lr 0.000000 | ms/batch 402.08 | loss  3.53 | ppl    34.04 | acc     0.60 | train_ae_norm     1.00\n",
      "[41/200][2699/4361] Loss_D: 0.00618199 (Loss_D_real: 0.00091769 Loss_D_fake: 0.00526429) Loss_G: 0.33731601 Loss_Enh_Dec: -1.26189184\n",
      "| epoch  41 |  2700/ 4361 batches | lr 0.000000 | ms/batch 402.08 | loss  3.53 | ppl    34.20 | acc     0.60 | train_ae_norm     1.00\n",
      "[41/200][2799/4361] Loss_D: 0.05262215 (Loss_D_real: 0.04515618 Loss_D_fake: 0.00746598) Loss_G: 0.32793510 Loss_Enh_Dec: -1.66805267\n",
      "| epoch  41 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  3.47 | ppl    32.22 | acc     0.62 | train_ae_norm     1.00\n",
      "[41/200][2899/4361] Loss_D: 0.02271143 (Loss_D_real: 0.00901101 Loss_D_fake: 0.01370041) Loss_G: 0.34779727 Loss_Enh_Dec: -1.53906357\n",
      "| epoch  41 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  3.54 | ppl    34.52 | acc     0.60 | train_ae_norm     1.00\n",
      "[41/200][2999/4361] Loss_D: 0.01003309 (Loss_D_real: 0.00372373 Loss_D_fake: 0.00630937) Loss_G: 0.30533925 Loss_Enh_Dec: -1.75075817\n",
      "| epoch  41 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  3.52 | ppl    33.80 | acc     0.60 | train_ae_norm     1.00\n",
      "[41/200][3099/4361] Loss_D: 0.01267264 (Loss_D_real: 0.00777038 Loss_D_fake: 0.00490226) Loss_G: 0.32642481 Loss_Enh_Dec: -1.68191814\n",
      "| epoch  41 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  3.54 | ppl    34.46 | acc     0.59 | train_ae_norm     1.00\n",
      "[41/200][3199/4361] Loss_D: 0.02036694 (Loss_D_real: 0.01797468 Loss_D_fake: 0.00239226) Loss_G: 0.33577201 Loss_Enh_Dec: -1.56103826\n",
      "| epoch  41 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  3.57 | ppl    35.36 | acc     0.62 | train_ae_norm     1.00\n",
      "[41/200][3299/4361] Loss_D: 0.00978600 (Loss_D_real: 0.00267681 Loss_D_fake: 0.00710919) Loss_G: 0.35885867 Loss_Enh_Dec: -1.39456546\n",
      "| epoch  41 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  3.57 | ppl    35.63 | acc     0.57 | train_ae_norm     1.00\n",
      "[41/200][3399/4361] Loss_D: 0.01108591 (Loss_D_real: 0.00380931 Loss_D_fake: 0.00727660) Loss_G: 0.30565047 Loss_Enh_Dec: -1.37962568\n",
      "| epoch  41 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  3.56 | ppl    35.04 | acc     0.62 | train_ae_norm     1.00\n",
      "[41/200][3499/4361] Loss_D: 0.02352462 (Loss_D_real: 0.01770626 Loss_D_fake: 0.00581837) Loss_G: 0.31443483 Loss_Enh_Dec: -1.64995944\n",
      "| epoch  41 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.80 | loss  3.50 | ppl    33.16 | acc     0.62 | train_ae_norm     1.00\n",
      "[41/200][3599/4361] Loss_D: 0.00852309 (Loss_D_real: 0.00143567 Loss_D_fake: 0.00708741) Loss_G: 0.33603066 Loss_Enh_Dec: -1.71151316\n",
      "| epoch  41 |  3600/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  3.51 | ppl    33.57 | acc     0.62 | train_ae_norm     1.00\n",
      "[41/200][3699/4361] Loss_D: 0.02392130 (Loss_D_real: 0.01824379 Loss_D_fake: 0.00567752) Loss_G: 0.33595973 Loss_Enh_Dec: -1.49982321\n",
      "| epoch  41 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.72 | loss  3.54 | ppl    34.47 | acc     0.58 | train_ae_norm     1.00\n",
      "[41/200][3799/4361] Loss_D: 0.00821242 (Loss_D_real: 0.00383023 Loss_D_fake: 0.00438219) Loss_G: 0.37401709 Loss_Enh_Dec: -1.25394964\n",
      "| epoch  41 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  3.54 | ppl    34.51 | acc     0.66 | train_ae_norm     1.00\n",
      "[41/200][3899/4361] Loss_D: 0.00723671 (Loss_D_real: 0.00119267 Loss_D_fake: 0.00604404) Loss_G: 0.30786386 Loss_Enh_Dec: -1.50527608\n",
      "| epoch  41 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  3.56 | ppl    35.30 | acc     0.58 | train_ae_norm     1.00\n",
      "[41/200][3999/4361] Loss_D: 0.01786542 (Loss_D_real: 0.01171345 Loss_D_fake: 0.00615197) Loss_G: 0.33396384 Loss_Enh_Dec: -1.80711257\n",
      "| epoch  41 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  3.58 | ppl    35.91 | acc     0.60 | train_ae_norm     1.00\n",
      "[41/200][4099/4361] Loss_D: 0.01927518 (Loss_D_real: 0.01342062 Loss_D_fake: 0.00585456) Loss_G: 0.33825818 Loss_Enh_Dec: -1.47434878\n",
      "| epoch  41 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  3.52 | ppl    33.88 | acc     0.59 | train_ae_norm     1.00\n",
      "[41/200][4199/4361] Loss_D: 0.14448161 (Loss_D_real: 0.13569103 Loss_D_fake: 0.00879058) Loss_G: 0.30403650 Loss_Enh_Dec: -1.68340969\n",
      "| epoch  41 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  3.57 | ppl    35.64 | acc     0.64 | train_ae_norm     1.00\n",
      "[41/200][4299/4361] Loss_D: 0.00675122 (Loss_D_real: 0.00159563 Loss_D_fake: 0.00515559) Loss_G: 0.31275830 Loss_Enh_Dec: -1.47654188\n",
      "| epoch  41 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  3.52 | ppl    33.89 | acc     0.61 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| end of epoch  41 | time: 1852.54s | test loss  3.40 | test ppl 30.08 | acc 0.652\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 42 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.709\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.485\n",
      "  Test Loss: 3.858\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  42 |     0/ 4361 batches | lr 0.000000 | ms/batch 867.33 | loss  0.03 | ppl     1.03 | acc     0.64 | train_ae_norm     1.00\n",
      "[42/200][99/4361] Loss_D: 0.03030798 (Loss_D_real: 0.02064663 Loss_D_fake: 0.00966135) Loss_G: 0.28128538 Loss_Enh_Dec: -1.75490975\n",
      "| epoch  42 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  3.54 | ppl    34.63 | acc     0.59 | train_ae_norm     1.00\n",
      "[42/200][199/4361] Loss_D: 0.00911852 (Loss_D_real: 0.00206085 Loss_D_fake: 0.00705766) Loss_G: 0.30681065 Loss_Enh_Dec: -1.80874920\n",
      "| epoch  42 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  3.56 | ppl    35.23 | acc     0.62 | train_ae_norm     1.00\n",
      "[42/200][299/4361] Loss_D: 0.04958347 (Loss_D_real: 0.04201513 Loss_D_fake: 0.00756834) Loss_G: 0.31496397 Loss_Enh_Dec: -1.55460775\n",
      "| epoch  42 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  3.57 | ppl    35.38 | acc     0.60 | train_ae_norm     1.00\n",
      "[42/200][399/4361] Loss_D: 0.00966434 (Loss_D_real: 0.00284468 Loss_D_fake: 0.00681966) Loss_G: 0.32329831 Loss_Enh_Dec: -1.72101521\n",
      "| epoch  42 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  3.47 | ppl    32.24 | acc     0.61 | train_ae_norm     1.00\n",
      "[42/200][499/4361] Loss_D: 0.05156678 (Loss_D_real: 0.05027040 Loss_D_fake: 0.00129638) Loss_G: 0.41089806 Loss_Enh_Dec: -1.11349797\n",
      "| epoch  42 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  3.54 | ppl    34.64 | acc     0.62 | train_ae_norm     1.00\n",
      "[42/200][599/4361] Loss_D: 0.00549433 (Loss_D_real: 0.00153277 Loss_D_fake: 0.00396156) Loss_G: 0.31927848 Loss_Enh_Dec: -1.40856016\n",
      "| epoch  42 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  3.51 | ppl    33.53 | acc     0.61 | train_ae_norm     1.00\n",
      "[42/200][699/4361] Loss_D: 0.02559699 (Loss_D_real: 0.02052817 Loss_D_fake: 0.00506882) Loss_G: 0.35855466 Loss_Enh_Dec: -1.66738117\n",
      "| epoch  42 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.94 | loss  3.54 | ppl    34.47 | acc     0.62 | train_ae_norm     1.00\n",
      "[42/200][799/4361] Loss_D: 0.00881648 (Loss_D_real: 0.00313039 Loss_D_fake: 0.00568609) Loss_G: 0.30639774 Loss_Enh_Dec: -1.33330953\n",
      "| epoch  42 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  3.50 | ppl    33.14 | acc     0.61 | train_ae_norm     1.00\n",
      "[42/200][899/4361] Loss_D: 0.03562893 (Loss_D_real: 0.00424311 Loss_D_fake: 0.03138582) Loss_G: 0.30887821 Loss_Enh_Dec: -1.37384188\n",
      "| epoch  42 |   900/ 4361 batches | lr 0.000000 | ms/batch 402.03 | loss  3.52 | ppl    33.92 | acc     0.65 | train_ae_norm     1.00\n",
      "[42/200][999/4361] Loss_D: 0.03551305 (Loss_D_real: 0.00940255 Loss_D_fake: 0.02611050) Loss_G: 0.36365506 Loss_Enh_Dec: -1.45662618\n",
      "| epoch  42 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  3.51 | ppl    33.51 | acc     0.62 | train_ae_norm     1.00\n",
      "[42/200][1099/4361] Loss_D: 0.02509416 (Loss_D_real: 0.01711633 Loss_D_fake: 0.00797784) Loss_G: 0.30889252 Loss_Enh_Dec: -1.45350528\n",
      "| epoch  42 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  3.52 | ppl    33.82 | acc     0.58 | train_ae_norm     1.00\n",
      "[42/200][1199/4361] Loss_D: 0.02167171 (Loss_D_real: 0.00147914 Loss_D_fake: 0.02019257) Loss_G: 0.30167836 Loss_Enh_Dec: -1.46787548\n",
      "| epoch  42 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  3.52 | ppl    33.81 | acc     0.63 | train_ae_norm     1.00\n",
      "[42/200][1299/4361] Loss_D: 0.02380459 (Loss_D_real: 0.00108198 Loss_D_fake: 0.02272261) Loss_G: 0.35249186 Loss_Enh_Dec: -1.17842472\n",
      "| epoch  42 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  3.53 | ppl    34.23 | acc     0.61 | train_ae_norm     1.00\n",
      "[42/200][1399/4361] Loss_D: 0.01603874 (Loss_D_real: 0.00703569 Loss_D_fake: 0.00900305) Loss_G: 0.31944829 Loss_Enh_Dec: -1.44739532\n",
      "| epoch  42 |  1400/ 4361 batches | lr 0.000000 | ms/batch 402.33 | loss  3.54 | ppl    34.54 | acc     0.57 | train_ae_norm     1.00\n",
      "[42/200][1499/4361] Loss_D: 1.46472120 (Loss_D_real: 0.01988847 Loss_D_fake: 1.44483268) Loss_G: 0.59329790 Loss_Enh_Dec: -1.17363119\n",
      "| epoch  42 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  3.57 | ppl    35.48 | acc     0.60 | train_ae_norm     1.00\n",
      "[42/200][1599/4361] Loss_D: 0.00609033 (Loss_D_real: 0.00400923 Loss_D_fake: 0.00208110) Loss_G: 0.29248646 Loss_Enh_Dec: -1.38366497\n",
      "| epoch  42 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.99 | loss  3.53 | ppl    34.12 | acc     0.61 | train_ae_norm     1.00\n",
      "[42/200][1699/4361] Loss_D: 0.03510989 (Loss_D_real: 0.02743242 Loss_D_fake: 0.00767747) Loss_G: 0.33767220 Loss_Enh_Dec: -1.42761302\n",
      "| epoch  42 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.94 | loss  3.51 | ppl    33.48 | acc     0.60 | train_ae_norm     1.00\n",
      "[42/200][1799/4361] Loss_D: 0.03128583 (Loss_D_real: 0.02432438 Loss_D_fake: 0.00696145) Loss_G: 0.32645634 Loss_Enh_Dec: -1.05447447\n",
      "| epoch  42 |  1800/ 4361 batches | lr 0.000000 | ms/batch 402.29 | loss  3.48 | ppl    32.59 | acc     0.62 | train_ae_norm     1.00\n",
      "[42/200][1899/4361] Loss_D: 0.01952940 (Loss_D_real: 0.01170499 Loss_D_fake: 0.00782441) Loss_G: 0.28457603 Loss_Enh_Dec: -1.55767810\n",
      "| epoch  42 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  3.56 | ppl    35.01 | acc     0.64 | train_ae_norm     1.00\n",
      "[42/200][1999/4361] Loss_D: 0.02345059 (Loss_D_real: 0.01510270 Loss_D_fake: 0.00834788) Loss_G: 0.41919184 Loss_Enh_Dec: -1.49241316\n",
      "| epoch  42 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  3.49 | ppl    32.86 | acc     0.62 | train_ae_norm     1.00\n",
      "[42/200][2099/4361] Loss_D: 0.03126777 (Loss_D_real: 0.02135277 Loss_D_fake: 0.00991500) Loss_G: 0.29019353 Loss_Enh_Dec: -1.80096805\n",
      "| epoch  42 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  3.53 | ppl    34.11 | acc     0.61 | train_ae_norm     1.00\n",
      "[42/200][2199/4361] Loss_D: 0.06739539 (Loss_D_real: 0.05944875 Loss_D_fake: 0.00794664) Loss_G: 0.45872983 Loss_Enh_Dec: -1.35116410\n",
      "| epoch  42 |  2200/ 4361 batches | lr 0.000000 | ms/batch 402.05 | loss  3.51 | ppl    33.59 | acc     0.64 | train_ae_norm     1.00\n",
      "[42/200][2299/4361] Loss_D: 0.01211447 (Loss_D_real: 0.00536272 Loss_D_fake: 0.00675175) Loss_G: 0.33359191 Loss_Enh_Dec: -1.67229068\n",
      "| epoch  42 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  3.53 | ppl    33.98 | acc     0.62 | train_ae_norm     1.00\n",
      "[42/200][2399/4361] Loss_D: 0.02893297 (Loss_D_real: 0.00090507 Loss_D_fake: 0.02802790) Loss_G: 0.34476313 Loss_Enh_Dec: -1.63517559\n",
      "| epoch  42 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  3.53 | ppl    34.18 | acc     0.58 | train_ae_norm     1.00\n",
      "[42/200][2499/4361] Loss_D: 0.01465583 (Loss_D_real: 0.00580265 Loss_D_fake: 0.00885318) Loss_G: 0.33646190 Loss_Enh_Dec: -1.70180786\n",
      "| epoch  42 |  2500/ 4361 batches | lr 0.000000 | ms/batch 402.16 | loss  3.57 | ppl    35.34 | acc     0.63 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42/200][2599/4361] Loss_D: 0.01725084 (Loss_D_real: 0.00140796 Loss_D_fake: 0.01584288) Loss_G: 0.31070367 Loss_Enh_Dec: -1.91784596\n",
      "| epoch  42 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  3.55 | ppl    34.89 | acc     0.57 | train_ae_norm     1.00\n",
      "[42/200][2699/4361] Loss_D: 0.01373053 (Loss_D_real: 0.00818489 Loss_D_fake: 0.00554564) Loss_G: 0.35069224 Loss_Enh_Dec: -1.38767195\n",
      "| epoch  42 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  3.56 | ppl    35.21 | acc     0.60 | train_ae_norm     1.00\n",
      "[42/200][2799/4361] Loss_D: 0.08866606 (Loss_D_real: 0.07221626 Loss_D_fake: 0.01644980) Loss_G: 0.26185766 Loss_Enh_Dec: -1.41578138\n",
      "| epoch  42 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  3.47 | ppl    32.29 | acc     0.60 | train_ae_norm     1.00\n",
      "[42/200][2899/4361] Loss_D: 0.02749532 (Loss_D_real: 0.01878071 Loss_D_fake: 0.00871461) Loss_G: 0.28919226 Loss_Enh_Dec: -1.60324728\n",
      "| epoch  42 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  3.53 | ppl    34.19 | acc     0.63 | train_ae_norm     1.00\n",
      "[42/200][2999/4361] Loss_D: 0.00630899 (Loss_D_real: 0.00314638 Loss_D_fake: 0.00316262) Loss_G: 0.31169778 Loss_Enh_Dec: -1.29347420\n",
      "| epoch  42 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  3.54 | ppl    34.34 | acc     0.62 | train_ae_norm     1.00\n",
      "[42/200][3099/4361] Loss_D: 0.00739565 (Loss_D_real: 0.00037891 Loss_D_fake: 0.00701675) Loss_G: 0.30609185 Loss_Enh_Dec: -1.57947493\n",
      "| epoch  42 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  3.53 | ppl    34.18 | acc     0.60 | train_ae_norm     1.00\n",
      "[42/200][3199/4361] Loss_D: 0.00931791 (Loss_D_real: 0.00164423 Loss_D_fake: 0.00767369) Loss_G: 0.38835439 Loss_Enh_Dec: -0.90368193\n",
      "| epoch  42 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  3.55 | ppl    34.85 | acc     0.61 | train_ae_norm     1.00\n",
      "[42/200][3299/4361] Loss_D: 0.01903769 (Loss_D_real: 0.01319637 Loss_D_fake: 0.00584132) Loss_G: 0.31312168 Loss_Enh_Dec: -0.87934399\n",
      "| epoch  42 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  3.58 | ppl    35.77 | acc     0.60 | train_ae_norm     1.00\n",
      "[42/200][3399/4361] Loss_D: 0.00585217 (Loss_D_real: 0.00200219 Loss_D_fake: 0.00384999) Loss_G: 0.32777485 Loss_Enh_Dec: -1.17351794\n",
      "| epoch  42 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  3.52 | ppl    33.74 | acc     0.62 | train_ae_norm     1.00\n",
      "[42/200][3499/4361] Loss_D: 0.01722933 (Loss_D_real: 0.01063581 Loss_D_fake: 0.00659352) Loss_G: 0.31234398 Loss_Enh_Dec: -1.11169589\n",
      "| epoch  42 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.44 | loss  3.49 | ppl    32.76 | acc     0.62 | train_ae_norm     1.00\n",
      "[42/200][3599/4361] Loss_D: 0.00953497 (Loss_D_real: 0.00480477 Loss_D_fake: 0.00473020) Loss_G: 0.31041789 Loss_Enh_Dec: -1.51935637\n",
      "| epoch  42 |  3600/ 4361 batches | lr 0.000000 | ms/batch 400.73 | loss  3.51 | ppl    33.42 | acc     0.60 | train_ae_norm     1.00\n",
      "[42/200][3699/4361] Loss_D: 0.01195211 (Loss_D_real: 0.00160007 Loss_D_fake: 0.01035204) Loss_G: 0.30122003 Loss_Enh_Dec: -1.28998566\n",
      "| epoch  42 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  3.51 | ppl    33.29 | acc     0.60 | train_ae_norm     1.00\n",
      "[42/200][3799/4361] Loss_D: 0.00941999 (Loss_D_real: 0.00630488 Loss_D_fake: 0.00311511) Loss_G: 0.33418384 Loss_Enh_Dec: -1.09318960\n",
      "| epoch  42 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  3.53 | ppl    34.22 | acc     0.63 | train_ae_norm     1.00\n",
      "[42/200][3899/4361] Loss_D: 0.01164863 (Loss_D_real: 0.00589710 Loss_D_fake: 0.00575153) Loss_G: 0.31006783 Loss_Enh_Dec: -0.99520969\n",
      "| epoch  42 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  3.53 | ppl    34.09 | acc     0.58 | train_ae_norm     1.00\n",
      "[42/200][3999/4361] Loss_D: 0.01174650 (Loss_D_real: 0.00586143 Loss_D_fake: 0.00588507) Loss_G: 0.31983471 Loss_Enh_Dec: -0.94907409\n",
      "| epoch  42 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.90 | loss  3.53 | ppl    34.17 | acc     0.63 | train_ae_norm     1.00\n",
      "[42/200][4099/4361] Loss_D: 0.02823104 (Loss_D_real: 0.02264115 Loss_D_fake: 0.00558989) Loss_G: 0.27441102 Loss_Enh_Dec: -0.99135202\n",
      "| epoch  42 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.61 | loss  3.48 | ppl    32.30 | acc     0.60 | train_ae_norm     1.00\n",
      "[42/200][4199/4361] Loss_D: 0.01333364 (Loss_D_real: 0.00196478 Loss_D_fake: 0.01136885) Loss_G: 0.37699720 Loss_Enh_Dec: -1.13747859\n",
      "| epoch  42 |  4200/ 4361 batches | lr 0.000000 | ms/batch 400.79 | loss  3.52 | ppl    33.66 | acc     0.64 | train_ae_norm     1.00\n",
      "[42/200][4299/4361] Loss_D: 0.00851534 (Loss_D_real: 0.00695597 Loss_D_fake: 0.00155937) Loss_G: 0.47387487 Loss_Enh_Dec: -0.37973711\n",
      "| epoch  42 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.64 | loss  3.46 | ppl    31.66 | acc     0.64 | train_ae_norm     1.00\n",
      "| end of epoch  42 | time: 1853.32s | test loss  3.34 | test ppl 28.35 | acc 0.661\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 43 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.713\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.477\n",
      "  Test Loss: 3.921\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  43 |     0/ 4361 batches | lr 0.000000 | ms/batch 863.02 | loss  0.03 | ppl     1.03 | acc     0.65 | train_ae_norm     1.00\n",
      "[43/200][99/4361] Loss_D: 0.03121635 (Loss_D_real: 0.01871847 Loss_D_fake: 0.01249788) Loss_G: 0.40706465 Loss_Enh_Dec: -0.86328268\n",
      "| epoch  43 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.41 | loss  3.48 | ppl    32.42 | acc     0.61 | train_ae_norm     1.00\n",
      "[43/200][199/4361] Loss_D: 0.01275826 (Loss_D_real: 0.00628602 Loss_D_fake: 0.00647224) Loss_G: 0.30317527 Loss_Enh_Dec: -0.82379019\n",
      "| epoch  43 |   200/ 4361 batches | lr 0.000000 | ms/batch 400.73 | loss  3.51 | ppl    33.28 | acc     0.62 | train_ae_norm     1.00\n",
      "[43/200][299/4361] Loss_D: 0.01050298 (Loss_D_real: 0.00481866 Loss_D_fake: 0.00568432) Loss_G: 0.33335495 Loss_Enh_Dec: -1.55124414\n",
      "| epoch  43 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  3.52 | ppl    33.66 | acc     0.56 | train_ae_norm     1.00\n",
      "[43/200][399/4361] Loss_D: 0.01091587 (Loss_D_real: 0.00137943 Loss_D_fake: 0.00953644) Loss_G: 0.33628911 Loss_Enh_Dec: -1.70845222\n",
      "| epoch  43 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  3.42 | ppl    30.60 | acc     0.62 | train_ae_norm     1.00\n",
      "[43/200][499/4361] Loss_D: 0.02730268 (Loss_D_real: 0.02126024 Loss_D_fake: 0.00604244) Loss_G: 0.33432421 Loss_Enh_Dec: -1.06166112\n",
      "| epoch  43 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  3.50 | ppl    33.08 | acc     0.61 | train_ae_norm     1.00\n",
      "[43/200][599/4361] Loss_D: 0.01734450 (Loss_D_real: 0.01295353 Loss_D_fake: 0.00439097) Loss_G: 0.36483943 Loss_Enh_Dec: -1.02385044\n",
      "| epoch  43 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.40 | loss  3.45 | ppl    31.58 | acc     0.59 | train_ae_norm     1.00\n",
      "[43/200][699/4361] Loss_D: 0.02296791 (Loss_D_real: 0.01481668 Loss_D_fake: 0.00815122) Loss_G: 0.72165328 Loss_Enh_Dec: -0.85577118\n",
      "| epoch  43 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  3.50 | ppl    32.98 | acc     0.64 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43/200][799/4361] Loss_D: 0.01014770 (Loss_D_real: 0.00585361 Loss_D_fake: 0.00429409) Loss_G: 0.33420092 Loss_Enh_Dec: -0.93201697\n",
      "| epoch  43 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  3.47 | ppl    32.21 | acc     0.62 | train_ae_norm     1.00\n",
      "[43/200][899/4361] Loss_D: 0.00907449 (Loss_D_real: 0.00848429 Loss_D_fake: 0.00059019) Loss_G: 0.52486241 Loss_Enh_Dec: -0.69544047\n",
      "| epoch  43 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  3.50 | ppl    33.17 | acc     0.63 | train_ae_norm     1.00\n",
      "[43/200][999/4361] Loss_D: 0.00728136 (Loss_D_real: 0.00237951 Loss_D_fake: 0.00490185) Loss_G: 0.32755461 Loss_Enh_Dec: -1.02373970\n",
      "| epoch  43 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  3.49 | ppl    32.67 | acc     0.61 | train_ae_norm     1.00\n",
      "[43/200][1099/4361] Loss_D: 0.01434754 (Loss_D_real: 0.00535658 Loss_D_fake: 0.00899096) Loss_G: 0.39983127 Loss_Enh_Dec: -1.08559024\n",
      "| epoch  43 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  3.49 | ppl    32.85 | acc     0.58 | train_ae_norm     1.00\n",
      "[43/200][1199/4361] Loss_D: 0.01966738 (Loss_D_real: 0.01509529 Loss_D_fake: 0.00457208) Loss_G: 0.37441283 Loss_Enh_Dec: -1.18385983\n",
      "| epoch  43 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  3.51 | ppl    33.28 | acc     0.63 | train_ae_norm     1.00\n",
      "[43/200][1299/4361] Loss_D: 0.01198965 (Loss_D_real: 0.00751978 Loss_D_fake: 0.00446988) Loss_G: 0.32761118 Loss_Enh_Dec: -0.99091244\n",
      "| epoch  43 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  3.53 | ppl    33.99 | acc     0.60 | train_ae_norm     1.00\n",
      "[43/200][1399/4361] Loss_D: 0.00669444 (Loss_D_real: 0.00646029 Loss_D_fake: 0.00023415) Loss_G: 0.57530749 Loss_Enh_Dec: -0.86467409\n",
      "| epoch  43 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  3.52 | ppl    33.90 | acc     0.57 | train_ae_norm     1.00\n",
      "[43/200][1499/4361] Loss_D: 0.01677109 (Loss_D_real: 0.00195632 Loss_D_fake: 0.01481477) Loss_G: 0.34729201 Loss_Enh_Dec: -1.07619846\n",
      "| epoch  43 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  3.57 | ppl    35.49 | acc     0.59 | train_ae_norm     1.00\n",
      "[43/200][1599/4361] Loss_D: 0.03033006 (Loss_D_real: 0.02433569 Loss_D_fake: 0.00599438) Loss_G: 0.32756126 Loss_Enh_Dec: -1.18702376\n",
      "| epoch  43 |  1600/ 4361 batches | lr 0.000000 | ms/batch 402.45 | loss  3.53 | ppl    34.03 | acc     0.60 | train_ae_norm     1.00\n",
      "[43/200][1699/4361] Loss_D: 0.00668961 (Loss_D_real: 0.00119901 Loss_D_fake: 0.00549060) Loss_G: 0.33630687 Loss_Enh_Dec: -1.36676514\n",
      "| epoch  43 |  1700/ 4361 batches | lr 0.000000 | ms/batch 402.52 | loss  3.51 | ppl    33.59 | acc     0.58 | train_ae_norm     1.00\n",
      "[43/200][1799/4361] Loss_D: 0.01960992 (Loss_D_real: 0.01776080 Loss_D_fake: 0.00184911) Loss_G: 0.36324900 Loss_Enh_Dec: -1.40995240\n",
      "| epoch  43 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  3.50 | ppl    33.17 | acc     0.60 | train_ae_norm     1.00\n",
      "[43/200][1899/4361] Loss_D: 0.01437532 (Loss_D_real: 0.00890583 Loss_D_fake: 0.00546949) Loss_G: 0.36103168 Loss_Enh_Dec: -1.35405064\n",
      "| epoch  43 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  3.53 | ppl    34.15 | acc     0.62 | train_ae_norm     1.00\n",
      "[43/200][1999/4361] Loss_D: 0.01586387 (Loss_D_real: 0.01131255 Loss_D_fake: 0.00455132) Loss_G: 0.34287348 Loss_Enh_Dec: -1.32760656\n",
      "| epoch  43 |  2000/ 4361 batches | lr 0.000000 | ms/batch 402.03 | loss  3.47 | ppl    32.23 | acc     0.61 | train_ae_norm     1.00\n",
      "[43/200][2099/4361] Loss_D: 0.01296799 (Loss_D_real: 0.00569494 Loss_D_fake: 0.00727305) Loss_G: 0.31647289 Loss_Enh_Dec: -1.32988012\n",
      "| epoch  43 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  3.49 | ppl    32.80 | acc     0.61 | train_ae_norm     1.00\n",
      "[43/200][2199/4361] Loss_D: 0.00771565 (Loss_D_real: 0.00504655 Loss_D_fake: 0.00266910) Loss_G: 0.36530238 Loss_Enh_Dec: -1.59039724\n",
      "| epoch  43 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  3.49 | ppl    32.94 | acc     0.63 | train_ae_norm     1.00\n",
      "[43/200][2299/4361] Loss_D: 0.01525328 (Loss_D_real: 0.01419401 Loss_D_fake: 0.00105927) Loss_G: 0.47953224 Loss_Enh_Dec: -1.70026684\n",
      "| epoch  43 |  2300/ 4361 batches | lr 0.000000 | ms/batch 402.32 | loss  3.49 | ppl    32.76 | acc     0.66 | train_ae_norm     1.00\n",
      "[43/200][2399/4361] Loss_D: 0.03450538 (Loss_D_real: 0.02012771 Loss_D_fake: 0.01437767) Loss_G: 0.31488261 Loss_Enh_Dec: -1.87454891\n",
      "| epoch  43 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  3.49 | ppl    32.65 | acc     0.59 | train_ae_norm     1.00\n",
      "[43/200][2499/4361] Loss_D: 0.01550180 (Loss_D_real: 0.00206012 Loss_D_fake: 0.01344168) Loss_G: 0.53788161 Loss_Enh_Dec: -1.57086182\n",
      "| epoch  43 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  3.54 | ppl    34.47 | acc     0.61 | train_ae_norm     1.00\n",
      "[43/200][2599/4361] Loss_D: 0.01039663 (Loss_D_real: 0.00528525 Loss_D_fake: 0.00511138) Loss_G: 0.34411275 Loss_Enh_Dec: -1.74643195\n",
      "| epoch  43 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  3.49 | ppl    32.90 | acc     0.61 | train_ae_norm     1.00\n",
      "[43/200][2699/4361] Loss_D: 0.02472948 (Loss_D_real: 0.01782913 Loss_D_fake: 0.00690036) Loss_G: 0.36412153 Loss_Enh_Dec: -1.49105370\n",
      "| epoch  43 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  3.52 | ppl    33.84 | acc     0.60 | train_ae_norm     1.00\n",
      "[43/200][2799/4361] Loss_D: 0.00437274 (Loss_D_real: 0.00120505 Loss_D_fake: 0.00316769) Loss_G: 0.37461308 Loss_Enh_Dec: -1.42354000\n",
      "| epoch  43 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  3.52 | ppl    33.78 | acc     0.61 | train_ae_norm     1.00\n",
      "[43/200][2899/4361] Loss_D: 0.01549247 (Loss_D_real: 0.01043023 Loss_D_fake: 0.00506223) Loss_G: 0.73118562 Loss_Enh_Dec: -1.13040626\n",
      "| epoch  43 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  3.52 | ppl    33.78 | acc     0.63 | train_ae_norm     1.00\n",
      "[43/200][2999/4361] Loss_D: 0.01357191 (Loss_D_real: 0.00258385 Loss_D_fake: 0.01098805) Loss_G: 0.32906419 Loss_Enh_Dec: -1.38920116\n",
      "| epoch  43 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  3.48 | ppl    32.61 | acc     0.62 | train_ae_norm     1.00\n",
      "[43/200][3099/4361] Loss_D: 0.11700625 (Loss_D_real: 0.10494189 Loss_D_fake: 0.01206436) Loss_G: 0.33934322 Loss_Enh_Dec: -1.25676227\n",
      "| epoch  43 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  3.49 | ppl    32.64 | acc     0.62 | train_ae_norm     1.00\n",
      "[43/200][3199/4361] Loss_D: 0.01032270 (Loss_D_real: 0.00338879 Loss_D_fake: 0.00693390) Loss_G: 0.33526450 Loss_Enh_Dec: -1.27645576\n",
      "| epoch  43 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  3.52 | ppl    33.85 | acc     0.61 | train_ae_norm     1.00\n",
      "[43/200][3299/4361] Loss_D: 0.01087062 (Loss_D_real: 0.00312671 Loss_D_fake: 0.00774391) Loss_G: 0.40494075 Loss_Enh_Dec: -1.02985990\n",
      "| epoch  43 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  3.55 | ppl    34.92 | acc     0.60 | train_ae_norm     1.00\n",
      "[43/200][3399/4361] Loss_D: 0.01887316 (Loss_D_real: 0.01007375 Loss_D_fake: 0.00879940) Loss_G: 0.34453449 Loss_Enh_Dec: -1.52747917\n",
      "| epoch  43 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  3.53 | ppl    33.96 | acc     0.61 | train_ae_norm     1.00\n",
      "[43/200][3499/4361] Loss_D: 0.04649041 (Loss_D_real: 0.03772062 Loss_D_fake: 0.00876978) Loss_G: 0.36234388 Loss_Enh_Dec: -1.61547077\n",
      "| epoch  43 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.94 | loss  3.47 | ppl    32.01 | acc     0.62 | train_ae_norm     1.00\n",
      "[43/200][3599/4361] Loss_D: 0.02318052 (Loss_D_real: 0.01810710 Loss_D_fake: 0.00507342) Loss_G: 0.31855294 Loss_Enh_Dec: -1.43760610\n",
      "| epoch  43 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  3.49 | ppl    32.89 | acc     0.63 | train_ae_norm     1.00\n",
      "[43/200][3699/4361] Loss_D: 0.03709762 (Loss_D_real: 0.02739334 Loss_D_fake: 0.00970428) Loss_G: 0.31366929 Loss_Enh_Dec: -1.56064403\n",
      "| epoch  43 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  3.50 | ppl    33.08 | acc     0.60 | train_ae_norm     1.00\n",
      "[43/200][3799/4361] Loss_D: 0.00475437 (Loss_D_real: 0.00090084 Loss_D_fake: 0.00385353) Loss_G: 0.34379396 Loss_Enh_Dec: -1.48348367\n",
      "| epoch  43 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  3.54 | ppl    34.35 | acc     0.62 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43/200][3899/4361] Loss_D: 0.01060182 (Loss_D_real: 0.00277710 Loss_D_fake: 0.00782472) Loss_G: 0.32494551 Loss_Enh_Dec: -1.52386737\n",
      "| epoch  43 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  3.51 | ppl    33.45 | acc     0.59 | train_ae_norm     1.00\n",
      "[43/200][3999/4361] Loss_D: 0.00319557 (Loss_D_real: 0.00109593 Loss_D_fake: 0.00209965) Loss_G: 0.35919887 Loss_Enh_Dec: -1.63434970\n",
      "| epoch  43 |  4000/ 4361 batches | lr 0.000000 | ms/batch 402.63 | loss  3.54 | ppl    34.33 | acc     0.63 | train_ae_norm     1.00\n",
      "[43/200][4099/4361] Loss_D: 0.01278222 (Loss_D_real: 0.00383005 Loss_D_fake: 0.00895217) Loss_G: 0.35515234 Loss_Enh_Dec: -1.46977043\n",
      "| epoch  43 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  3.49 | ppl    32.65 | acc     0.60 | train_ae_norm     1.00\n",
      "[43/200][4199/4361] Loss_D: 0.02050543 (Loss_D_real: 0.00629660 Loss_D_fake: 0.01420883) Loss_G: 0.38932446 Loss_Enh_Dec: -1.64164507\n",
      "| epoch  43 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  3.54 | ppl    34.36 | acc     0.62 | train_ae_norm     1.00\n",
      "[43/200][4299/4361] Loss_D: 0.01686267 (Loss_D_real: 0.01055166 Loss_D_fake: 0.00631101) Loss_G: 0.32986227 Loss_Enh_Dec: -1.60114288\n",
      "| epoch  43 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  3.50 | ppl    33.02 | acc     0.62 | train_ae_norm     1.00\n",
      "| end of epoch  43 | time: 1852.99s | test loss  3.40 | test ppl 30.07 | acc 0.654\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 44 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.711\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.477\n",
      "  Test Loss: 3.987\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  44 |     0/ 4361 batches | lr 0.000000 | ms/batch 866.05 | loss  0.03 | ppl     1.03 | acc     0.62 | train_ae_norm     1.00\n",
      "[44/200][99/4361] Loss_D: 0.09436943 (Loss_D_real: 0.09041345 Loss_D_fake: 0.00395598) Loss_G: 0.33997792 Loss_Enh_Dec: -1.54744184\n",
      "| epoch  44 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.91 | loss  3.52 | ppl    33.74 | acc     0.60 | train_ae_norm     1.00\n",
      "[44/200][199/4361] Loss_D: 0.00653239 (Loss_D_real: 0.00396297 Loss_D_fake: 0.00256942) Loss_G: 0.40077034 Loss_Enh_Dec: -1.79652107\n",
      "| epoch  44 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  3.53 | ppl    34.11 | acc     0.64 | train_ae_norm     1.00\n",
      "[44/200][299/4361] Loss_D: 0.01207086 (Loss_D_real: 0.00730381 Loss_D_fake: 0.00476705) Loss_G: 0.37852761 Loss_Enh_Dec: -1.53332973\n",
      "| epoch  44 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  3.52 | ppl    33.78 | acc     0.57 | train_ae_norm     1.00\n",
      "[44/200][399/4361] Loss_D: 0.01318770 (Loss_D_real: 0.00626438 Loss_D_fake: 0.00692333) Loss_G: 0.32897383 Loss_Enh_Dec: -1.25374889\n",
      "| epoch  44 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  3.44 | ppl    31.28 | acc     0.62 | train_ae_norm     1.00\n",
      "[44/200][499/4361] Loss_D: 0.01011120 (Loss_D_real: 0.00228995 Loss_D_fake: 0.00782126) Loss_G: 0.35099551 Loss_Enh_Dec: -1.51982534\n",
      "| epoch  44 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  3.51 | ppl    33.36 | acc     0.65 | train_ae_norm     1.00\n",
      "[44/200][599/4361] Loss_D: 0.00359934 (Loss_D_real: 0.00216657 Loss_D_fake: 0.00143278) Loss_G: 0.38827237 Loss_Enh_Dec: -1.70289159\n",
      "| epoch  44 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  3.46 | ppl    31.80 | acc     0.58 | train_ae_norm     1.00\n",
      "[44/200][699/4361] Loss_D: 0.07741231 (Loss_D_real: 0.03909082 Loss_D_fake: 0.03832149) Loss_G: 0.35360986 Loss_Enh_Dec: -1.31138146\n",
      "| epoch  44 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  3.50 | ppl    33.22 | acc     0.63 | train_ae_norm     1.00\n",
      "[44/200][799/4361] Loss_D: 0.00714304 (Loss_D_real: 0.00292939 Loss_D_fake: 0.00421365) Loss_G: 0.37892026 Loss_Enh_Dec: -1.69442880\n",
      "| epoch  44 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  3.47 | ppl    32.28 | acc     0.62 | train_ae_norm     1.00\n",
      "[44/200][899/4361] Loss_D: 0.00628990 (Loss_D_real: 0.00101188 Loss_D_fake: 0.00527802) Loss_G: 0.34446591 Loss_Enh_Dec: -1.72869873\n",
      "| epoch  44 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  3.49 | ppl    32.84 | acc     0.65 | train_ae_norm     1.00\n",
      "[44/200][999/4361] Loss_D: 0.02616438 (Loss_D_real: 0.01770918 Loss_D_fake: 0.00845521) Loss_G: 0.33433589 Loss_Enh_Dec: -1.12337399\n",
      "| epoch  44 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  3.49 | ppl    32.84 | acc     0.63 | train_ae_norm     1.00\n",
      "[44/200][1099/4361] Loss_D: 0.00782516 (Loss_D_real: 0.00084795 Loss_D_fake: 0.00697721) Loss_G: 0.34526330 Loss_Enh_Dec: -1.43418229\n",
      "| epoch  44 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  3.51 | ppl    33.49 | acc     0.55 | train_ae_norm     1.00\n",
      "[44/200][1199/4361] Loss_D: 0.02680996 (Loss_D_real: 0.02270401 Loss_D_fake: 0.00410594) Loss_G: 0.40105087 Loss_Enh_Dec: -1.44482529\n",
      "| epoch  44 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.06 | loss  3.56 | ppl    35.08 | acc     0.62 | train_ae_norm     1.00\n",
      "[44/200][1299/4361] Loss_D: 0.00544231 (Loss_D_real: 0.00089942 Loss_D_fake: 0.00454290) Loss_G: 0.34147441 Loss_Enh_Dec: -1.47232044\n",
      "| epoch  44 |  1300/ 4361 batches | lr 0.000000 | ms/batch 402.18 | loss  3.53 | ppl    34.18 | acc     0.61 | train_ae_norm     1.00\n",
      "[44/200][1399/4361] Loss_D: 0.00350832 (Loss_D_real: 0.00114368 Loss_D_fake: 0.00236463) Loss_G: 0.43138990 Loss_Enh_Dec: -1.28843689\n",
      "| epoch  44 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  3.49 | ppl    32.64 | acc     0.58 | train_ae_norm     1.00\n",
      "[44/200][1499/4361] Loss_D: 0.03764430 (Loss_D_real: 0.02960842 Loss_D_fake: 0.00803588) Loss_G: 0.39571789 Loss_Enh_Dec: -0.86167222\n",
      "| epoch  44 |  1500/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  3.53 | ppl    34.08 | acc     0.58 | train_ae_norm     1.00\n",
      "[44/200][1599/4361] Loss_D: 0.01417441 (Loss_D_real: 0.00498940 Loss_D_fake: 0.00918501) Loss_G: 0.36373636 Loss_Enh_Dec: -1.16400254\n",
      "| epoch  44 |  1600/ 4361 batches | lr 0.000000 | ms/batch 402.23 | loss  3.47 | ppl    32.08 | acc     0.61 | train_ae_norm     1.00\n",
      "[44/200][1699/4361] Loss_D: 0.00455282 (Loss_D_real: 0.00090459 Loss_D_fake: 0.00364823) Loss_G: 0.36928317 Loss_Enh_Dec: -1.05631101\n",
      "| epoch  44 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  3.48 | ppl    32.35 | acc     0.59 | train_ae_norm     1.00\n",
      "[44/200][1799/4361] Loss_D: 0.00681293 (Loss_D_real: 0.00433421 Loss_D_fake: 0.00247872) Loss_G: 0.38079157 Loss_Enh_Dec: -1.44182301\n",
      "| epoch  44 |  1800/ 4361 batches | lr 0.000000 | ms/batch 402.08 | loss  3.44 | ppl    31.28 | acc     0.64 | train_ae_norm     1.00\n",
      "[44/200][1899/4361] Loss_D: 0.00466977 (Loss_D_real: 0.00178560 Loss_D_fake: 0.00288417) Loss_G: 0.31839880 Loss_Enh_Dec: -1.32222903\n",
      "| epoch  44 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  3.50 | ppl    33.22 | acc     0.66 | train_ae_norm     1.00\n",
      "[44/200][1999/4361] Loss_D: 0.00900813 (Loss_D_real: 0.00348853 Loss_D_fake: 0.00551960) Loss_G: 0.36783800 Loss_Enh_Dec: -1.40642345\n",
      "| epoch  44 |  2000/ 4361 batches | lr 0.000000 | ms/batch 402.07 | loss  3.44 | ppl    31.11 | acc     0.64 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44/200][2099/4361] Loss_D: 0.00597383 (Loss_D_real: 0.00378045 Loss_D_fake: 0.00219339) Loss_G: 0.36808288 Loss_Enh_Dec: -1.24489772\n",
      "| epoch  44 |  2100/ 4361 batches | lr 0.000000 | ms/batch 402.22 | loss  3.47 | ppl    32.08 | acc     0.60 | train_ae_norm     1.00\n",
      "[44/200][2199/4361] Loss_D: 0.17764829 (Loss_D_real: 0.00393462 Loss_D_fake: 0.17371367) Loss_G: 0.75028914 Loss_Enh_Dec: -1.32847893\n",
      "| epoch  44 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  3.46 | ppl    31.73 | acc     0.61 | train_ae_norm     1.00\n",
      "[44/200][2299/4361] Loss_D: 0.01422244 (Loss_D_real: 0.01155169 Loss_D_fake: 0.00267075) Loss_G: 0.34428063 Loss_Enh_Dec: -1.12864220\n",
      "| epoch  44 |  2300/ 4361 batches | lr 0.000000 | ms/batch 402.13 | loss  3.46 | ppl    31.73 | acc     0.61 | train_ae_norm     1.00\n",
      "[44/200][2399/4361] Loss_D: 0.00488342 (Loss_D_real: 0.00142479 Loss_D_fake: 0.00345863) Loss_G: 0.37440547 Loss_Enh_Dec: -0.95953369\n",
      "| epoch  44 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  3.44 | ppl    31.29 | acc     0.58 | train_ae_norm     1.00\n",
      "[44/200][2499/4361] Loss_D: 0.00587207 (Loss_D_real: 0.00199788 Loss_D_fake: 0.00387419) Loss_G: 0.35532644 Loss_Enh_Dec: -1.30584288\n",
      "| epoch  44 |  2500/ 4361 batches | lr 0.000000 | ms/batch 402.00 | loss  3.49 | ppl    32.66 | acc     0.63 | train_ae_norm     1.00\n",
      "[44/200][2599/4361] Loss_D: 0.00669724 (Loss_D_real: 0.00458250 Loss_D_fake: 0.00211474) Loss_G: 0.43092012 Loss_Enh_Dec: -1.15542126\n",
      "| epoch  44 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  3.46 | ppl    31.71 | acc     0.60 | train_ae_norm     1.00\n",
      "[44/200][2699/4361] Loss_D: 0.00690908 (Loss_D_real: 0.00474310 Loss_D_fake: 0.00216597) Loss_G: 0.35107571 Loss_Enh_Dec: -1.33584917\n",
      "| epoch  44 |  2700/ 4361 batches | lr 0.000000 | ms/batch 402.12 | loss  3.48 | ppl    32.33 | acc     0.60 | train_ae_norm     1.00\n",
      "[44/200][2799/4361] Loss_D: 0.00466078 (Loss_D_real: 0.00070996 Loss_D_fake: 0.00395082) Loss_G: 0.39748400 Loss_Enh_Dec: -1.39151001\n",
      "| epoch  44 |  2800/ 4361 batches | lr 0.000000 | ms/batch 402.04 | loss  3.40 | ppl    30.10 | acc     0.61 | train_ae_norm     1.00\n",
      "[44/200][2899/4361] Loss_D: 0.00698325 (Loss_D_real: 0.00331482 Loss_D_fake: 0.00366843) Loss_G: 0.39256468 Loss_Enh_Dec: -1.52068710\n",
      "| epoch  44 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.05 | loss  3.46 | ppl    31.69 | acc     0.62 | train_ae_norm     1.00\n",
      "[44/200][2999/4361] Loss_D: 0.00681482 (Loss_D_real: 0.00363963 Loss_D_fake: 0.00317519) Loss_G: 0.37659621 Loss_Enh_Dec: -1.11920762\n",
      "| epoch  44 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  3.45 | ppl    31.37 | acc     0.63 | train_ae_norm     1.00\n",
      "[44/200][3099/4361] Loss_D: 0.00504777 (Loss_D_real: 0.00178804 Loss_D_fake: 0.00325973) Loss_G: 0.37645274 Loss_Enh_Dec: -1.09621084\n",
      "| epoch  44 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  3.46 | ppl    31.91 | acc     0.60 | train_ae_norm     1.00\n",
      "[44/200][3199/4361] Loss_D: 0.00957874 (Loss_D_real: 0.00207145 Loss_D_fake: 0.00750729) Loss_G: 0.39070186 Loss_Enh_Dec: -0.61190557\n",
      "| epoch  44 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  3.49 | ppl    32.66 | acc     0.64 | train_ae_norm     1.00\n",
      "[44/200][3299/4361] Loss_D: 0.01854596 (Loss_D_real: 0.01291630 Loss_D_fake: 0.00562966) Loss_G: 0.40854865 Loss_Enh_Dec: -0.66741335\n",
      "| epoch  44 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  3.50 | ppl    32.96 | acc     0.63 | train_ae_norm     1.00\n",
      "[44/200][3399/4361] Loss_D: 0.01200490 (Loss_D_real: 0.00647077 Loss_D_fake: 0.00553413) Loss_G: 0.36824146 Loss_Enh_Dec: -0.93070889\n",
      "| epoch  44 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.65 | loss  3.47 | ppl    32.30 | acc     0.63 | train_ae_norm     1.00\n",
      "[44/200][3499/4361] Loss_D: 0.00620975 (Loss_D_real: 0.00205471 Loss_D_fake: 0.00415504) Loss_G: 0.37201405 Loss_Enh_Dec: -1.37252331\n",
      "| epoch  44 |  3500/ 4361 batches | lr 0.000000 | ms/batch 402.11 | loss  3.41 | ppl    30.38 | acc     0.62 | train_ae_norm     1.00\n",
      "[44/200][3599/4361] Loss_D: 0.01744621 (Loss_D_real: 0.01490552 Loss_D_fake: 0.00254069) Loss_G: 0.39495865 Loss_Enh_Dec: -1.03704345\n",
      "| epoch  44 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  3.42 | ppl    30.64 | acc     0.63 | train_ae_norm     1.00\n",
      "[44/200][3699/4361] Loss_D: 0.00500320 (Loss_D_real: 0.00245026 Loss_D_fake: 0.00255294) Loss_G: 0.38085660 Loss_Enh_Dec: -0.99805939\n",
      "| epoch  44 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.80 | loss  3.44 | ppl    31.29 | acc     0.61 | train_ae_norm     1.00\n",
      "[44/200][3799/4361] Loss_D: 0.01276777 (Loss_D_real: 0.00895898 Loss_D_fake: 0.00380879) Loss_G: 0.36888433 Loss_Enh_Dec: -1.30762434\n",
      "| epoch  44 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  3.46 | ppl    31.71 | acc     0.65 | train_ae_norm     1.00\n",
      "[44/200][3899/4361] Loss_D: 0.00890441 (Loss_D_real: 0.00355904 Loss_D_fake: 0.00534537) Loss_G: 0.36860609 Loss_Enh_Dec: -1.51107407\n",
      "| epoch  44 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  3.46 | ppl    31.83 | acc     0.60 | train_ae_norm     1.00\n",
      "[44/200][3999/4361] Loss_D: 0.00561101 (Loss_D_real: 0.00362926 Loss_D_fake: 0.00198175) Loss_G: 0.38240421 Loss_Enh_Dec: -1.56094968\n",
      "| epoch  44 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  3.47 | ppl    32.26 | acc     0.63 | train_ae_norm     1.00\n",
      "[44/200][4099/4361] Loss_D: 0.00688395 (Loss_D_real: 0.00301526 Loss_D_fake: 0.00386869) Loss_G: 0.39762202 Loss_Enh_Dec: -1.32743704\n",
      "| epoch  44 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  3.41 | ppl    30.33 | acc     0.62 | train_ae_norm     1.00\n",
      "[44/200][4199/4361] Loss_D: 0.00819397 (Loss_D_real: 0.00246065 Loss_D_fake: 0.00573332) Loss_G: 0.35973576 Loss_Enh_Dec: -1.10011756\n",
      "| epoch  44 |  4200/ 4361 batches | lr 0.000000 | ms/batch 400.91 | loss  3.47 | ppl    31.99 | acc     0.66 | train_ae_norm     1.00\n",
      "[44/200][4299/4361] Loss_D: 0.00252271 (Loss_D_real: 0.00096668 Loss_D_fake: 0.00155602) Loss_G: 0.38502008 Loss_Enh_Dec: -1.14675295\n",
      "| epoch  44 |  4300/ 4361 batches | lr 0.000000 | ms/batch 402.04 | loss  3.42 | ppl    30.49 | acc     0.63 | train_ae_norm     1.00\n",
      "| end of epoch  44 | time: 1853.61s | test loss  3.32 | test ppl 27.59 | acc 0.659\n",
      "bleu_self:  [2.12797619e-01 1.27700974e-01 1.23063519e-06 4.08715843e-09\n",
      " 1.43504430e-10]\n",
      "bleu_test:  [8.63095238e-01 5.20747650e-01 6.87936570e-02 1.08031292e-05\n",
      " 7.19718074e-08]\n",
      "bleu_self: [0.21279762,0.12770097,0.00000123,0.00000000,0.00000000]\n",
      "bleu_test: [0.86309524,0.52074765,0.06879366,0.00001080,0.00000007]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 45 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.710\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.482\n",
      "  Test Loss: 3.972\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  45 |     0/ 4361 batches | lr 0.000000 | ms/batch 864.77 | loss  0.03 | ppl     1.03 | acc     0.63 | train_ae_norm     1.00\n",
      "[45/200][99/4361] Loss_D: 1.50526857 (Loss_D_real: 0.00270666 Loss_D_fake: 1.50256193) Loss_G: 0.48500672 Loss_Enh_Dec: -0.64627296\n",
      "| epoch  45 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  3.46 | ppl    31.88 | acc     0.60 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45/200][199/4361] Loss_D: 0.00521786 (Loss_D_real: 0.00115269 Loss_D_fake: 0.00406517) Loss_G: 0.34319553 Loss_Enh_Dec: -1.14403796\n",
      "| epoch  45 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  3.48 | ppl    32.43 | acc     0.62 | train_ae_norm     1.00\n",
      "[45/200][299/4361] Loss_D: 0.00414928 (Loss_D_real: 0.00214252 Loss_D_fake: 0.00200676) Loss_G: 0.37601364 Loss_Enh_Dec: -1.24833250\n",
      "| epoch  45 |   300/ 4361 batches | lr 0.000000 | ms/batch 402.61 | loss  3.46 | ppl    31.90 | acc     0.59 | train_ae_norm     1.00\n",
      "[45/200][399/4361] Loss_D: 0.00561015 (Loss_D_real: 0.00254636 Loss_D_fake: 0.00306378) Loss_G: 0.35828367 Loss_Enh_Dec: -1.43245113\n",
      "| epoch  45 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  3.38 | ppl    29.49 | acc     0.63 | train_ae_norm     1.00\n",
      "[45/200][499/4361] Loss_D: 0.00497671 (Loss_D_real: 0.00078453 Loss_D_fake: 0.00419218) Loss_G: 0.41913730 Loss_Enh_Dec: -1.15417540\n",
      "| epoch  45 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  3.44 | ppl    31.23 | acc     0.65 | train_ae_norm     1.00\n",
      "[45/200][599/4361] Loss_D: 0.01476118 (Loss_D_real: 0.01086006 Loss_D_fake: 0.00390112) Loss_G: 0.38118744 Loss_Enh_Dec: -1.31252050\n",
      "| epoch  45 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  3.40 | ppl    29.95 | acc     0.61 | train_ae_norm     1.00\n",
      "[45/200][699/4361] Loss_D: 0.01296643 (Loss_D_real: 0.01018989 Loss_D_fake: 0.00277653) Loss_G: 0.35809562 Loss_Enh_Dec: -1.25645864\n",
      "| epoch  45 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  3.44 | ppl    31.26 | acc     0.66 | train_ae_norm     1.00\n",
      "[45/200][799/4361] Loss_D: 1.82554162 (Loss_D_real: 0.00783884 Loss_D_fake: 1.81770277) Loss_G: 0.81354105 Loss_Enh_Dec: -0.65877980\n",
      "| epoch  45 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  3.44 | ppl    31.12 | acc     0.63 | train_ae_norm     1.00\n",
      "[45/200][899/4361] Loss_D: 0.00933495 (Loss_D_real: 0.00168998 Loss_D_fake: 0.00764498) Loss_G: 0.36953464 Loss_Enh_Dec: -0.26528326\n",
      "| epoch  45 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.66 | loss  3.45 | ppl    31.53 | acc     0.65 | train_ae_norm     1.00\n",
      "[45/200][999/4361] Loss_D: 0.01899965 (Loss_D_real: 0.00612104 Loss_D_fake: 0.01287860) Loss_G: 0.35909680 Loss_Enh_Dec: -0.72567648\n",
      "| epoch  45 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  3.45 | ppl    31.42 | acc     0.62 | train_ae_norm     1.00\n",
      "[45/200][1099/4361] Loss_D: 0.00790673 (Loss_D_real: 0.00073844 Loss_D_fake: 0.00716828) Loss_G: 0.36190701 Loss_Enh_Dec: -0.88638192\n",
      "| epoch  45 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  3.43 | ppl    30.79 | acc     0.60 | train_ae_norm     1.00\n",
      "[45/200][1199/4361] Loss_D: 0.00428453 (Loss_D_real: 0.00213218 Loss_D_fake: 0.00215235) Loss_G: 0.43031803 Loss_Enh_Dec: -1.37335837\n",
      "| epoch  45 |  1200/ 4361 batches | lr 0.000000 | ms/batch 402.12 | loss  3.45 | ppl    31.60 | acc     0.63 | train_ae_norm     1.00\n",
      "[45/200][1299/4361] Loss_D: 0.00514971 (Loss_D_real: 0.00263266 Loss_D_fake: 0.00251705) Loss_G: 0.45891914 Loss_Enh_Dec: -1.02828205\n",
      "| epoch  45 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  3.45 | ppl    31.38 | acc     0.62 | train_ae_norm     1.00\n",
      "[45/200][1399/4361] Loss_D: 0.00612256 (Loss_D_real: 0.00339647 Loss_D_fake: 0.00272608) Loss_G: 0.38010022 Loss_Enh_Dec: -1.00642145\n",
      "| epoch  45 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  3.43 | ppl    30.92 | acc     0.56 | train_ae_norm     1.00\n",
      "[45/200][1499/4361] Loss_D: 3.09728813 (Loss_D_real: 0.00413557 Loss_D_fake: 3.09315252) Loss_G: 0.77233571 Loss_Enh_Dec: -1.01342762\n",
      "| epoch  45 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  3.49 | ppl    32.65 | acc     0.62 | train_ae_norm     1.00\n",
      "[45/200][1599/4361] Loss_D: 0.00617803 (Loss_D_real: 0.00343171 Loss_D_fake: 0.00274633) Loss_G: 0.35383448 Loss_Enh_Dec: -0.74388838\n",
      "| epoch  45 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  3.45 | ppl    31.64 | acc     0.61 | train_ae_norm     1.00\n",
      "[45/200][1699/4361] Loss_D: 0.01314787 (Loss_D_real: 0.00848143 Loss_D_fake: 0.00466644) Loss_G: 0.35112378 Loss_Enh_Dec: -0.77839500\n",
      "| epoch  45 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  3.45 | ppl    31.56 | acc     0.62 | train_ae_norm     1.00\n",
      "[45/200][1799/4361] Loss_D: 0.00685440 (Loss_D_real: 0.00222996 Loss_D_fake: 0.00462444) Loss_G: 0.36669445 Loss_Enh_Dec: -0.55677336\n",
      "| epoch  45 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  3.40 | ppl    30.04 | acc     0.63 | train_ae_norm     1.00\n",
      "[45/200][1899/4361] Loss_D: 0.00802017 (Loss_D_real: 0.00547852 Loss_D_fake: 0.00254166) Loss_G: 0.35629541 Loss_Enh_Dec: -0.45442876\n",
      "| epoch  45 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  3.47 | ppl    32.12 | acc     0.62 | train_ae_norm     1.00\n",
      "[45/200][1999/4361] Loss_D: 0.00529564 (Loss_D_real: 0.00142189 Loss_D_fake: 0.00387376) Loss_G: 0.33709106 Loss_Enh_Dec: -0.48076710\n",
      "| epoch  45 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  3.42 | ppl    30.48 | acc     0.63 | train_ae_norm     1.00\n",
      "[45/200][2099/4361] Loss_D: 0.00387299 (Loss_D_real: 0.00164339 Loss_D_fake: 0.00222959) Loss_G: 0.39962846 Loss_Enh_Dec: -0.27780896\n",
      "| epoch  45 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  3.43 | ppl    30.80 | acc     0.61 | train_ae_norm     1.00\n",
      "[45/200][2199/4361] Loss_D: 0.00797115 (Loss_D_real: 0.00352228 Loss_D_fake: 0.00444887) Loss_G: 0.36402613 Loss_Enh_Dec: -0.50772929\n",
      "| epoch  45 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  3.43 | ppl    30.91 | acc     0.64 | train_ae_norm     1.00\n",
      "[45/200][2299/4361] Loss_D: 0.01222344 (Loss_D_real: 0.00513544 Loss_D_fake: 0.00708800) Loss_G: 0.34909296 Loss_Enh_Dec: -0.62992316\n",
      "| epoch  45 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  3.40 | ppl    30.07 | acc     0.65 | train_ae_norm     1.00\n",
      "[45/200][2399/4361] Loss_D: 0.00550682 (Loss_D_real: 0.00131706 Loss_D_fake: 0.00418975) Loss_G: 0.34220466 Loss_Enh_Dec: -0.55215257\n",
      "| epoch  45 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  3.42 | ppl    30.52 | acc     0.58 | train_ae_norm     1.00\n",
      "[45/200][2499/4361] Loss_D: 0.00531790 (Loss_D_real: 0.00383777 Loss_D_fake: 0.00148013) Loss_G: 0.36426228 Loss_Enh_Dec: -0.79110849\n",
      "| epoch  45 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  3.44 | ppl    31.34 | acc     0.63 | train_ae_norm     1.00\n",
      "[45/200][2599/4361] Loss_D: 0.00379839 (Loss_D_real: 0.00324966 Loss_D_fake: 0.00054873) Loss_G: 0.63688278 Loss_Enh_Dec: -0.86275643\n",
      "| epoch  45 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  3.40 | ppl    29.98 | acc     0.60 | train_ae_norm     1.00\n",
      "[45/200][2699/4361] Loss_D: 0.00537113 (Loss_D_real: 0.00097053 Loss_D_fake: 0.00440060) Loss_G: 0.39325655 Loss_Enh_Dec: -0.87973845\n",
      "| epoch  45 |  2700/ 4361 batches | lr 0.000000 | ms/batch 402.07 | loss  3.42 | ppl    30.51 | acc     0.60 | train_ae_norm     1.00\n",
      "[45/200][2799/4361] Loss_D: 0.00718678 (Loss_D_real: 0.00463350 Loss_D_fake: 0.00255328) Loss_G: 0.39718398 Loss_Enh_Dec: -0.83597726\n",
      "| epoch  45 |  2800/ 4361 batches | lr 0.000000 | ms/batch 402.20 | loss  3.35 | ppl    28.51 | acc     0.62 | train_ae_norm     1.00\n",
      "[45/200][2899/4361] Loss_D: 0.00527533 (Loss_D_real: 0.00101662 Loss_D_fake: 0.00425871) Loss_G: 0.38944691 Loss_Enh_Dec: -1.22204328\n",
      "| epoch  45 |  2900/ 4361 batches | lr 0.000000 | ms/batch 402.15 | loss  3.40 | ppl    29.82 | acc     0.64 | train_ae_norm     1.00\n",
      "[45/200][2999/4361] Loss_D: 0.00374562 (Loss_D_real: 0.00185062 Loss_D_fake: 0.00189500) Loss_G: 0.37553301 Loss_Enh_Dec: -0.91484594\n",
      "| epoch  45 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  3.39 | ppl    29.81 | acc     0.64 | train_ae_norm     1.00\n",
      "[45/200][3099/4361] Loss_D: 0.00487374 (Loss_D_real: 0.00103906 Loss_D_fake: 0.00383468) Loss_G: 0.43796951 Loss_Enh_Dec: -1.06321025\n",
      "| epoch  45 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  3.40 | ppl    29.94 | acc     0.59 | train_ae_norm     1.00\n",
      "[45/200][3199/4361] Loss_D: 0.00436927 (Loss_D_real: 0.00241575 Loss_D_fake: 0.00195352) Loss_G: 0.45067063 Loss_Enh_Dec: -1.28432238\n",
      "| epoch  45 |  3200/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  3.42 | ppl    30.68 | acc     0.65 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45/200][3299/4361] Loss_D: 0.00242176 (Loss_D_real: 0.00104403 Loss_D_fake: 0.00137773) Loss_G: 0.46761948 Loss_Enh_Dec: -1.22525287\n",
      "| epoch  45 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  3.44 | ppl    31.29 | acc     0.63 | train_ae_norm     1.00\n",
      "[45/200][3399/4361] Loss_D: 0.00351009 (Loss_D_real: 0.00041027 Loss_D_fake: 0.00309983) Loss_G: 0.36581132 Loss_Enh_Dec: -1.44563019\n",
      "| epoch  45 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  3.41 | ppl    30.25 | acc     0.62 | train_ae_norm     1.00\n",
      "[45/200][3499/4361] Loss_D: 0.00420606 (Loss_D_real: 0.00107984 Loss_D_fake: 0.00312623) Loss_G: 0.36627936 Loss_Enh_Dec: -0.97053951\n",
      "| epoch  45 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  3.36 | ppl    28.74 | acc     0.64 | train_ae_norm     1.00\n",
      "[45/200][3599/4361] Loss_D: 0.00351581 (Loss_D_real: 0.00240470 Loss_D_fake: 0.00111111) Loss_G: 0.44689617 Loss_Enh_Dec: -0.77260035\n",
      "| epoch  45 |  3600/ 4361 batches | lr 0.000000 | ms/batch 402.33 | loss  3.37 | ppl    29.12 | acc     0.62 | train_ae_norm     1.00\n",
      "[45/200][3699/4361] Loss_D: 0.02335694 (Loss_D_real: 0.01854618 Loss_D_fake: 0.00481076) Loss_G: 0.38847569 Loss_Enh_Dec: -0.45785365\n",
      "| epoch  45 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  3.37 | ppl    29.18 | acc     0.58 | train_ae_norm     1.00\n",
      "[45/200][3799/4361] Loss_D: 0.00190386 (Loss_D_real: 0.00041598 Loss_D_fake: 0.00148789) Loss_G: 0.38110188 Loss_Enh_Dec: -1.25717759\n",
      "| epoch  45 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  3.41 | ppl    30.36 | acc     0.68 | train_ae_norm     1.00\n",
      "[45/200][3899/4361] Loss_D: 0.00278750 (Loss_D_real: 0.00077438 Loss_D_fake: 0.00201312) Loss_G: 0.40887704 Loss_Enh_Dec: -0.99736595\n",
      "| epoch  45 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  3.40 | ppl    29.84 | acc     0.62 | train_ae_norm     1.00\n",
      "[45/200][3999/4361] Loss_D: 0.05446338 (Loss_D_real: 0.05261692 Loss_D_fake: 0.00184645) Loss_G: 0.41223097 Loss_Enh_Dec: -1.02671421\n",
      "| epoch  45 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  3.41 | ppl    30.34 | acc     0.63 | train_ae_norm     1.00\n",
      "[45/200][4099/4361] Loss_D: 0.00299833 (Loss_D_real: 0.00024572 Loss_D_fake: 0.00275261) Loss_G: 0.44478795 Loss_Enh_Dec: -0.90488434\n",
      "| epoch  45 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  3.36 | ppl    28.83 | acc     0.62 | train_ae_norm     1.00\n",
      "[45/200][4199/4361] Loss_D: 0.00441380 (Loss_D_real: 0.00047965 Loss_D_fake: 0.00393416) Loss_G: 0.35170734 Loss_Enh_Dec: -1.28520429\n",
      "| epoch  45 |  4200/ 4361 batches | lr 0.000000 | ms/batch 402.30 | loss  3.42 | ppl    30.48 | acc     0.63 | train_ae_norm     1.00\n",
      "[45/200][4299/4361] Loss_D: 0.00971138 (Loss_D_real: 0.00502583 Loss_D_fake: 0.00468555) Loss_G: 0.43982717 Loss_Enh_Dec: -1.68836975\n",
      "| epoch  45 |  4300/ 4361 batches | lr 0.000000 | ms/batch 402.38 | loss  3.37 | ppl    28.98 | acc     0.65 | train_ae_norm     1.00\n",
      "| end of epoch  45 | time: 1853.58s | test loss  3.29 | test ppl 26.74 | acc 0.662\n",
      "bleu_self:  [4.00495338e-01 2.01400561e-01 5.06843456e-02 8.02079158e-06\n",
      " 4.33046005e-08]\n",
      "bleu_test:  [0.85871212 0.55280315 0.1907791  0.1026309  0.04045958]\n",
      "bleu_self: [0.40049534,0.20140056,0.05068435,0.00000802,0.00000004]\n",
      "bleu_test: [0.85871212,0.55280315,0.19077910,0.10263090,0.04045958]\n",
      "New saving model: epoch 045.\n",
      "Saving models to ./results/yahoo_merge_assigned_results\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 46 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:57.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:16.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.708\n",
      "  Training epcoh took: 0:01:27\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.490\n",
      "  Test Loss: 3.973\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  46 |     0/ 4361 batches | lr 0.000000 | ms/batch 870.48 | loss  0.03 | ppl     1.03 | acc     0.67 | train_ae_norm     1.00\n",
      "[46/200][99/4361] Loss_D: 0.00635524 (Loss_D_real: 0.00411398 Loss_D_fake: 0.00224126) Loss_G: 0.41107678 Loss_Enh_Dec: -1.50748813\n",
      "| epoch  46 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  3.37 | ppl    29.18 | acc     0.61 | train_ae_norm     1.00\n",
      "[46/200][199/4361] Loss_D: 0.00388683 (Loss_D_real: 0.00100638 Loss_D_fake: 0.00288045) Loss_G: 0.34910819 Loss_Enh_Dec: -1.60764849\n",
      "| epoch  46 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  3.43 | ppl    30.99 | acc     0.64 | train_ae_norm     1.00\n",
      "[46/200][299/4361] Loss_D: 0.00645175 (Loss_D_real: 0.00446088 Loss_D_fake: 0.00199088) Loss_G: 0.37710151 Loss_Enh_Dec: -1.61062276\n",
      "| epoch  46 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  3.45 | ppl    31.44 | acc     0.59 | train_ae_norm     1.00\n",
      "[46/200][399/4361] Loss_D: 0.03758813 (Loss_D_real: 0.03460005 Loss_D_fake: 0.00298809) Loss_G: 0.41420889 Loss_Enh_Dec: -1.50970745\n",
      "| epoch  46 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  3.35 | ppl    28.45 | acc     0.63 | train_ae_norm     1.00\n",
      "[46/200][499/4361] Loss_D: 0.00634017 (Loss_D_real: 0.00165221 Loss_D_fake: 0.00468795) Loss_G: 0.36701614 Loss_Enh_Dec: -1.40654945\n",
      "| epoch  46 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  3.41 | ppl    30.21 | acc     0.65 | train_ae_norm     1.00\n",
      "[46/200][599/4361] Loss_D: 0.00651156 (Loss_D_real: 0.00357726 Loss_D_fake: 0.00293430) Loss_G: 0.37620044 Loss_Enh_Dec: -1.30546176\n",
      "| epoch  46 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  3.36 | ppl    28.84 | acc     0.58 | train_ae_norm     1.00\n",
      "[46/200][699/4361] Loss_D: 0.00414380 (Loss_D_real: 0.00330081 Loss_D_fake: 0.00084299) Loss_G: 0.41453394 Loss_Enh_Dec: -1.22429848\n",
      "| epoch  46 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  3.40 | ppl    29.90 | acc     0.64 | train_ae_norm     1.00\n",
      "[46/200][799/4361] Loss_D: 0.00633285 (Loss_D_real: 0.00359465 Loss_D_fake: 0.00273820) Loss_G: 0.40938264 Loss_Enh_Dec: -1.25963366\n",
      "| epoch  46 |   800/ 4361 batches | lr 0.000000 | ms/batch 402.18 | loss  3.37 | ppl    29.21 | acc     0.64 | train_ae_norm     1.00\n",
      "[46/200][899/4361] Loss_D: 0.01351330 (Loss_D_real: 0.00474712 Loss_D_fake: 0.00876618) Loss_G: 0.37367988 Loss_Enh_Dec: -0.94512099\n",
      "| epoch  46 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  3.40 | ppl    29.96 | acc     0.66 | train_ae_norm     1.00\n",
      "[46/200][999/4361] Loss_D: 0.07138573 (Loss_D_real: 0.06251249 Loss_D_fake: 0.00887324) Loss_G: 0.37363836 Loss_Enh_Dec: -1.15958560\n",
      "| epoch  46 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  3.39 | ppl    29.76 | acc     0.65 | train_ae_norm     1.00\n",
      "[46/200][1099/4361] Loss_D: 0.00538968 (Loss_D_real: 0.00287194 Loss_D_fake: 0.00251774) Loss_G: 0.40436742 Loss_Enh_Dec: -0.97957844\n",
      "| epoch  46 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  3.38 | ppl    29.31 | acc     0.61 | train_ae_norm     1.00\n",
      "[46/200][1199/4361] Loss_D: 0.01339684 (Loss_D_real: 0.01143309 Loss_D_fake: 0.00196374) Loss_G: 0.50626785 Loss_Enh_Dec: -1.55784059\n",
      "| epoch  46 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  3.39 | ppl    29.79 | acc     0.64 | train_ae_norm     1.00\n",
      "[46/200][1299/4361] Loss_D: 0.00903678 (Loss_D_real: 0.00663728 Loss_D_fake: 0.00239951) Loss_G: 0.40303120 Loss_Enh_Dec: -1.25051069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  46 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  3.42 | ppl    30.61 | acc     0.65 | train_ae_norm     1.00\n",
      "[46/200][1399/4361] Loss_D: 0.00418100 (Loss_D_real: 0.00045204 Loss_D_fake: 0.00372896) Loss_G: 0.32184201 Loss_Enh_Dec: -1.29192436\n",
      "| epoch  46 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  3.40 | ppl    29.87 | acc     0.57 | train_ae_norm     1.00\n",
      "[46/200][1499/4361] Loss_D: 0.00526015 (Loss_D_real: 0.00158424 Loss_D_fake: 0.00367591) Loss_G: 0.36807746 Loss_Enh_Dec: -1.37732697\n",
      "| epoch  46 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  3.43 | ppl    30.97 | acc     0.61 | train_ae_norm     1.00\n",
      "[46/200][1599/4361] Loss_D: 0.01121724 (Loss_D_real: 0.00678818 Loss_D_fake: 0.00442906) Loss_G: 0.40884933 Loss_Enh_Dec: -1.23800755\n",
      "| epoch  46 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  3.40 | ppl    29.82 | acc     0.61 | train_ae_norm     1.00\n",
      "[46/200][1699/4361] Loss_D: 0.01592374 (Loss_D_real: 0.01313826 Loss_D_fake: 0.00278548) Loss_G: 0.33899289 Loss_Enh_Dec: -1.52757549\n",
      "| epoch  46 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  3.39 | ppl    29.63 | acc     0.62 | train_ae_norm     1.00\n",
      "[46/200][1799/4361] Loss_D: 0.05056916 (Loss_D_real: 0.04833713 Loss_D_fake: 0.00223203) Loss_G: 0.36028084 Loss_Enh_Dec: -1.69831204\n",
      "| epoch  46 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  3.36 | ppl    28.78 | acc     0.63 | train_ae_norm     1.00\n",
      "[46/200][1899/4361] Loss_D: 0.00334965 (Loss_D_real: 0.00181573 Loss_D_fake: 0.00153392) Loss_G: 0.41152069 Loss_Enh_Dec: -1.47748137\n",
      "| epoch  46 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  3.44 | ppl    31.12 | acc     0.63 | train_ae_norm     1.00\n",
      "[46/200][1999/4361] Loss_D: 0.01409583 (Loss_D_real: 0.01183117 Loss_D_fake: 0.00226466) Loss_G: 0.38283882 Loss_Enh_Dec: -1.42557085\n",
      "| epoch  46 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  3.37 | ppl    29.06 | acc     0.62 | train_ae_norm     1.00\n",
      "[46/200][2099/4361] Loss_D: 0.00260755 (Loss_D_real: 0.00074445 Loss_D_fake: 0.00186311) Loss_G: 0.35186997 Loss_Enh_Dec: -1.27677488\n",
      "| epoch  46 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  3.41 | ppl    30.13 | acc     0.63 | train_ae_norm     1.00\n",
      "[46/200][2199/4361] Loss_D: 0.00514940 (Loss_D_real: 0.00112037 Loss_D_fake: 0.00402903) Loss_G: 0.35994431 Loss_Enh_Dec: -1.21620023\n",
      "| epoch  46 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  3.40 | ppl    29.89 | acc     0.64 | train_ae_norm     1.00\n",
      "[46/200][2299/4361] Loss_D: 0.00536374 (Loss_D_real: 0.00186607 Loss_D_fake: 0.00349767) Loss_G: 0.39663455 Loss_Enh_Dec: -1.07060170\n",
      "| epoch  46 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  3.38 | ppl    29.36 | acc     0.64 | train_ae_norm     1.00\n",
      "[46/200][2399/4361] Loss_D: 0.00464624 (Loss_D_real: 0.00159593 Loss_D_fake: 0.00305031) Loss_G: 0.37996262 Loss_Enh_Dec: -1.48053288\n",
      "| epoch  46 |  2400/ 4361 batches | lr 0.000000 | ms/batch 402.47 | loss  3.40 | ppl    29.91 | acc     0.59 | train_ae_norm     1.00\n",
      "[46/200][2499/4361] Loss_D: 0.00887997 (Loss_D_real: 0.00218747 Loss_D_fake: 0.00669250) Loss_G: 0.35798767 Loss_Enh_Dec: -1.48352945\n",
      "| epoch  46 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  3.42 | ppl    30.61 | acc     0.62 | train_ae_norm     1.00\n",
      "[46/200][2599/4361] Loss_D: 0.01775656 (Loss_D_real: 0.00963925 Loss_D_fake: 0.00811731) Loss_G: 0.33556840 Loss_Enh_Dec: -1.03132451\n",
      "| epoch  46 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  3.42 | ppl    30.47 | acc     0.60 | train_ae_norm     1.00\n",
      "[46/200][2699/4361] Loss_D: 0.02233492 (Loss_D_real: 0.01884425 Loss_D_fake: 0.00349067) Loss_G: 0.47430047 Loss_Enh_Dec: -1.21989357\n",
      "| epoch  46 |  2700/ 4361 batches | lr 0.000000 | ms/batch 402.03 | loss  3.48 | ppl    32.37 | acc     0.63 | train_ae_norm     1.00\n",
      "[46/200][2799/4361] Loss_D: 0.00415012 (Loss_D_real: 0.00221939 Loss_D_fake: 0.00193073) Loss_G: 0.42836475 Loss_Enh_Dec: -1.41955781\n",
      "| epoch  46 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  3.39 | ppl    29.67 | acc     0.62 | train_ae_norm     1.00\n",
      "[46/200][2899/4361] Loss_D: 0.00483435 (Loss_D_real: 0.00234872 Loss_D_fake: 0.00248563) Loss_G: 0.34254977 Loss_Enh_Dec: -1.30501008\n",
      "| epoch  46 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  3.44 | ppl    31.05 | acc     0.63 | train_ae_norm     1.00\n",
      "[46/200][2999/4361] Loss_D: 0.00648497 (Loss_D_real: 0.00164003 Loss_D_fake: 0.00484495) Loss_G: 0.40620375 Loss_Enh_Dec: -1.30614495\n",
      "| epoch  46 |  3000/ 4361 batches | lr 0.000000 | ms/batch 402.67 | loss  3.46 | ppl    31.73 | acc     0.62 | train_ae_norm     1.00\n",
      "[46/200][3099/4361] Loss_D: 2.70692110 (Loss_D_real: 0.00864082 Loss_D_fake: 2.69828033) Loss_G: 0.68953973 Loss_Enh_Dec: -1.48403537\n",
      "| epoch  46 |  3100/ 4361 batches | lr 0.000000 | ms/batch 402.43 | loss  3.65 | ppl    38.34 | acc     0.59 | train_ae_norm     1.00\n",
      "[46/200][3199/4361] Loss_D: 0.00877561 (Loss_D_real: 0.00265398 Loss_D_fake: 0.00612163) Loss_G: 0.42479035 Loss_Enh_Dec: -1.17495275\n",
      "| epoch  46 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  3.50 | ppl    33.13 | acc     0.65 | train_ae_norm     1.00\n",
      "[46/200][3299/4361] Loss_D: 0.00420098 (Loss_D_real: 0.00322597 Loss_D_fake: 0.00097500) Loss_G: 0.39106065 Loss_Enh_Dec: -1.62826347\n",
      "| epoch  46 |  3300/ 4361 batches | lr 0.000000 | ms/batch 402.12 | loss  3.44 | ppl    31.10 | acc     0.64 | train_ae_norm     1.00\n",
      "[46/200][3399/4361] Loss_D: 0.00315563 (Loss_D_real: 0.00160278 Loss_D_fake: 0.00155285) Loss_G: 0.37053296 Loss_Enh_Dec: -1.29659688\n",
      "| epoch  46 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  3.41 | ppl    30.23 | acc     0.63 | train_ae_norm     1.00\n",
      "[46/200][3499/4361] Loss_D: 0.00274871 (Loss_D_real: 0.00076118 Loss_D_fake: 0.00198754) Loss_G: 0.34840116 Loss_Enh_Dec: -1.28717899\n",
      "| epoch  46 |  3500/ 4361 batches | lr 0.000000 | ms/batch 402.45 | loss  3.36 | ppl    28.86 | acc     0.64 | train_ae_norm     1.00\n",
      "[46/200][3599/4361] Loss_D: 0.01743283 (Loss_D_real: 0.00971560 Loss_D_fake: 0.00771724) Loss_G: 0.34396121 Loss_Enh_Dec: -1.35165489\n",
      "| epoch  46 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  3.36 | ppl    28.67 | acc     0.65 | train_ae_norm     1.00\n",
      "[46/200][3699/4361] Loss_D: 0.00841767 (Loss_D_real: 0.00494604 Loss_D_fake: 0.00347163) Loss_G: 0.33775932 Loss_Enh_Dec: -1.46870339\n",
      "| epoch  46 |  3700/ 4361 batches | lr 0.000000 | ms/batch 402.13 | loss  3.37 | ppl    29.01 | acc     0.61 | train_ae_norm     1.00\n",
      "[46/200][3799/4361] Loss_D: 0.00916866 (Loss_D_real: 0.00506755 Loss_D_fake: 0.00410111) Loss_G: 0.38873413 Loss_Enh_Dec: -0.99268848\n",
      "| epoch  46 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  3.39 | ppl    29.77 | acc     0.67 | train_ae_norm     1.00\n",
      "[46/200][3899/4361] Loss_D: 0.00393875 (Loss_D_real: 0.00190722 Loss_D_fake: 0.00203153) Loss_G: 0.39986721 Loss_Enh_Dec: -1.05112302\n",
      "| epoch  46 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  3.38 | ppl    29.36 | acc     0.61 | train_ae_norm     1.00\n",
      "[46/200][3999/4361] Loss_D: 0.00611444 (Loss_D_real: 0.00349161 Loss_D_fake: 0.00262283) Loss_G: 0.37325302 Loss_Enh_Dec: -0.84552449\n",
      "| epoch  46 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  3.40 | ppl    29.95 | acc     0.64 | train_ae_norm     1.00\n",
      "[46/200][4099/4361] Loss_D: 0.00639245 (Loss_D_real: 0.00368923 Loss_D_fake: 0.00270322) Loss_G: 0.37920704 Loss_Enh_Dec: -1.02906072\n",
      "| epoch  46 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  3.34 | ppl    28.35 | acc     0.64 | train_ae_norm     1.00\n",
      "[46/200][4199/4361] Loss_D: 0.01036570 (Loss_D_real: 0.00770244 Loss_D_fake: 0.00266326) Loss_G: 0.65933275 Loss_Enh_Dec: -0.66199392\n",
      "| epoch  46 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  3.40 | ppl    29.98 | acc     0.65 | train_ae_norm     1.00\n",
      "[46/200][4299/4361] Loss_D: 0.01116071 (Loss_D_real: 0.00163659 Loss_D_fake: 0.00952412) Loss_G: 0.41286451 Loss_Enh_Dec: -0.79875213\n",
      "| epoch  46 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  3.34 | ppl    28.29 | acc     0.64 | train_ae_norm     1.00\n",
      "| end of epoch  46 | time: 1854.60s | test loss  3.26 | test ppl 26.17 | acc 0.667\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 47 / 200 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.716\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.480\n",
      "  Test Loss: 3.929\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  47 |     0/ 4361 batches | lr 0.000000 | ms/batch 863.88 | loss  0.03 | ppl     1.03 | acc     0.68 | train_ae_norm     1.00\n",
      "[47/200][99/4361] Loss_D: 0.01164127 (Loss_D_real: 0.00103794 Loss_D_fake: 0.01060333) Loss_G: 0.38002136 Loss_Enh_Dec: -0.48264486\n",
      "| epoch  47 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  3.36 | ppl    28.77 | acc     0.59 | train_ae_norm     1.00\n",
      "[47/200][199/4361] Loss_D: 0.00808082 (Loss_D_real: 0.00330010 Loss_D_fake: 0.00478072) Loss_G: 0.39349335 Loss_Enh_Dec: -0.72680092\n",
      "| epoch  47 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  3.39 | ppl    29.60 | acc     0.64 | train_ae_norm     1.00\n",
      "[47/200][299/4361] Loss_D: 0.00599794 (Loss_D_real: 0.00181494 Loss_D_fake: 0.00418300) Loss_G: 0.45791107 Loss_Enh_Dec: -0.86066550\n",
      "| epoch  47 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  3.39 | ppl    29.53 | acc     0.60 | train_ae_norm     1.00\n",
      "[47/200][399/4361] Loss_D: 0.03313088 (Loss_D_real: 0.02059491 Loss_D_fake: 0.01253597) Loss_G: 0.76926917 Loss_Enh_Dec: -0.95826304\n",
      "| epoch  47 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  3.31 | ppl    27.43 | acc     0.63 | train_ae_norm     1.00\n",
      "[47/200][499/4361] Loss_D: 0.00416885 (Loss_D_real: 0.00140371 Loss_D_fake: 0.00276514) Loss_G: 0.37666869 Loss_Enh_Dec: -1.01679122\n",
      "| epoch  47 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  3.40 | ppl    29.91 | acc     0.65 | train_ae_norm     1.00\n",
      "[47/200][599/4361] Loss_D: 0.01219974 (Loss_D_real: 0.01049554 Loss_D_fake: 0.00170420) Loss_G: 0.31851131 Loss_Enh_Dec: -1.15488708\n",
      "| epoch  47 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  3.34 | ppl    28.15 | acc     0.60 | train_ae_norm     1.00\n",
      "[47/200][699/4361] Loss_D: 0.00681071 (Loss_D_real: 0.00351053 Loss_D_fake: 0.00330018) Loss_G: 0.37940994 Loss_Enh_Dec: -0.92110789\n",
      "| epoch  47 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  3.38 | ppl    29.50 | acc     0.66 | train_ae_norm     1.00\n",
      "[47/200][799/4361] Loss_D: 0.00388163 (Loss_D_real: 0.00108789 Loss_D_fake: 0.00279374) Loss_G: 0.41752592 Loss_Enh_Dec: -1.03461099\n",
      "| epoch  47 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  3.34 | ppl    28.35 | acc     0.65 | train_ae_norm     1.00\n",
      "[47/200][899/4361] Loss_D: 0.00714076 (Loss_D_real: 0.00373452 Loss_D_fake: 0.00340624) Loss_G: 0.36556116 Loss_Enh_Dec: -1.15748906\n",
      "| epoch  47 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  3.36 | ppl    28.79 | acc     0.65 | train_ae_norm     1.00\n",
      "[47/200][999/4361] Loss_D: 0.01158399 (Loss_D_real: 0.00907520 Loss_D_fake: 0.00250879) Loss_G: 0.37567440 Loss_Enh_Dec: -0.95088559\n",
      "| epoch  47 |  1000/ 4361 batches | lr 0.000000 | ms/batch 402.05 | loss  3.34 | ppl    28.31 | acc     0.63 | train_ae_norm     1.00\n",
      "[47/200][1099/4361] Loss_D: 0.00713913 (Loss_D_real: 0.00334689 Loss_D_fake: 0.00379224) Loss_G: 0.41685867 Loss_Enh_Dec: -0.98137236\n",
      "| epoch  47 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  3.35 | ppl    28.43 | acc     0.61 | train_ae_norm     1.00\n",
      "[47/200][1199/4361] Loss_D: 0.00522217 (Loss_D_real: 0.00255077 Loss_D_fake: 0.00267140) Loss_G: 0.37552738 Loss_Enh_Dec: -1.33999276\n",
      "| epoch  47 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  3.36 | ppl    28.69 | acc     0.65 | train_ae_norm     1.00\n",
      "[47/200][1299/4361] Loss_D: 0.00839745 (Loss_D_real: 0.00499281 Loss_D_fake: 0.00340464) Loss_G: 0.38443479 Loss_Enh_Dec: -1.12812603\n",
      "| epoch  47 |  1300/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  3.38 | ppl    29.50 | acc     0.64 | train_ae_norm     1.00\n",
      "[47/200][1399/4361] Loss_D: 0.01203918 (Loss_D_real: 0.00163944 Loss_D_fake: 0.01039974) Loss_G: 0.41498104 Loss_Enh_Dec: -1.23705351\n",
      "| epoch  47 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.90 | loss  3.38 | ppl    29.28 | acc     0.61 | train_ae_norm     1.00\n",
      "[47/200][1499/4361] Loss_D: 0.00531786 (Loss_D_real: 0.00146513 Loss_D_fake: 0.00385274) Loss_G: 0.37646094 Loss_Enh_Dec: -1.31880891\n",
      "| epoch  47 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  3.41 | ppl    30.37 | acc     0.61 | train_ae_norm     1.00\n",
      "[47/200][1599/4361] Loss_D: 0.00420606 (Loss_D_real: 0.00168376 Loss_D_fake: 0.00252230) Loss_G: 0.34126744 Loss_Enh_Dec: -1.23453748\n",
      "| epoch  47 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  3.38 | ppl    29.33 | acc     0.63 | train_ae_norm     1.00\n",
      "[47/200][1699/4361] Loss_D: 0.00559841 (Loss_D_real: 0.00065069 Loss_D_fake: 0.00494772) Loss_G: 0.36780357 Loss_Enh_Dec: -1.35195279\n",
      "| epoch  47 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  3.35 | ppl    28.59 | acc     0.63 | train_ae_norm     1.00\n",
      "[47/200][1799/4361] Loss_D: 0.97371209 (Loss_D_real: 0.08629244 Loss_D_fake: 0.88741964) Loss_G: 0.72154784 Loss_Enh_Dec: -1.21382666\n",
      "| epoch  47 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  3.32 | ppl    27.57 | acc     0.65 | train_ae_norm     1.00\n",
      "[47/200][1899/4361] Loss_D: 0.01378370 (Loss_D_real: 0.01272197 Loss_D_fake: 0.00106174) Loss_G: 0.59412307 Loss_Enh_Dec: -0.70002955\n",
      "| epoch  47 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.84 | loss  3.39 | ppl    29.71 | acc     0.65 | train_ae_norm     1.00\n",
      "[47/200][1999/4361] Loss_D: 0.00425886 (Loss_D_real: 0.00238066 Loss_D_fake: 0.00187820) Loss_G: 0.37899968 Loss_Enh_Dec: -1.26574349\n",
      "| epoch  47 |  2000/ 4361 batches | lr 0.000000 | ms/batch 402.07 | loss  3.33 | ppl    27.92 | acc     0.66 | train_ae_norm     1.00\n",
      "[47/200][2099/4361] Loss_D: 0.00318807 (Loss_D_real: 0.00171788 Loss_D_fake: 0.00147019) Loss_G: 0.37902555 Loss_Enh_Dec: -0.67273659\n",
      "| epoch  47 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  3.34 | ppl    28.18 | acc     0.64 | train_ae_norm     1.00\n",
      "[47/200][2199/4361] Loss_D: 0.00679728 (Loss_D_real: 0.00442924 Loss_D_fake: 0.00236804) Loss_G: 0.42294416 Loss_Enh_Dec: -1.22781134\n",
      "| epoch  47 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  3.34 | ppl    28.35 | acc     0.64 | train_ae_norm     1.00\n",
      "[47/200][2299/4361] Loss_D: 0.00248798 (Loss_D_real: 0.00082639 Loss_D_fake: 0.00166159) Loss_G: 0.37125465 Loss_Enh_Dec: -1.28417695\n",
      "| epoch  47 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  3.31 | ppl    27.31 | acc     0.64 | train_ae_norm     1.00\n",
      "[47/200][2399/4361] Loss_D: 0.00132934 (Loss_D_real: 0.00037719 Loss_D_fake: 0.00095215) Loss_G: 0.39258501 Loss_Enh_Dec: -1.48914862\n",
      "| epoch  47 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  3.32 | ppl    27.76 | acc     0.61 | train_ae_norm     1.00\n",
      "[47/200][2499/4361] Loss_D: 0.00271970 (Loss_D_real: 0.00068013 Loss_D_fake: 0.00203957) Loss_G: 0.40539894 Loss_Enh_Dec: -1.17300880\n",
      "| epoch  47 |  2500/ 4361 batches | lr 0.000000 | ms/batch 400.72 | loss  3.36 | ppl    28.78 | acc     0.64 | train_ae_norm     1.00\n",
      "[47/200][2599/4361] Loss_D: 0.00579934 (Loss_D_real: 0.00057560 Loss_D_fake: 0.00522374) Loss_G: 0.37107897 Loss_Enh_Dec: -1.38190770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  47 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.82 | loss  3.35 | ppl    28.42 | acc     0.62 | train_ae_norm     1.00\n",
      "[47/200][2699/4361] Loss_D: 0.01076593 (Loss_D_real: 0.00735053 Loss_D_fake: 0.00341540) Loss_G: 0.39697808 Loss_Enh_Dec: -0.98631305\n",
      "| epoch  47 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  3.37 | ppl    29.06 | acc     0.62 | train_ae_norm     1.00\n",
      "[47/200][2799/4361] Loss_D: 0.00563834 (Loss_D_real: 0.00183115 Loss_D_fake: 0.00380719) Loss_G: 0.42489576 Loss_Enh_Dec: -1.33200228\n",
      "| epoch  47 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.77 | loss  3.31 | ppl    27.49 | acc     0.61 | train_ae_norm     1.00\n",
      "[47/200][2899/4361] Loss_D: 0.00836711 (Loss_D_real: 0.00771957 Loss_D_fake: 0.00064755) Loss_G: 0.47590119 Loss_Enh_Dec: -1.30071139\n",
      "| epoch  47 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  3.36 | ppl    28.80 | acc     0.63 | train_ae_norm     1.00\n",
      "[47/200][2999/4361] Loss_D: 0.00596034 (Loss_D_real: 0.00119816 Loss_D_fake: 0.00476217) Loss_G: 0.40618244 Loss_Enh_Dec: -1.65513325\n",
      "| epoch  47 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.18 | loss  3.34 | ppl    28.18 | acc     0.64 | train_ae_norm     1.00\n",
      "[47/200][3099/4361] Loss_D: 0.00559590 (Loss_D_real: 0.00111741 Loss_D_fake: 0.00447848) Loss_G: 0.60814989 Loss_Enh_Dec: -1.19626987\n",
      "| epoch  47 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.93 | loss  3.36 | ppl    28.92 | acc     0.62 | train_ae_norm     1.00\n",
      "[47/200][3199/4361] Loss_D: 0.00940014 (Loss_D_real: 0.00742036 Loss_D_fake: 0.00197978) Loss_G: 0.40050673 Loss_Enh_Dec: -1.06959593\n",
      "| epoch  47 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.86 | loss  3.39 | ppl    29.75 | acc     0.63 | train_ae_norm     1.00\n",
      "[47/200][3299/4361] Loss_D: 0.01859733 (Loss_D_real: 0.00431274 Loss_D_fake: 0.01428459) Loss_G: 0.38176462 Loss_Enh_Dec: -1.42437398\n",
      "| epoch  47 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  3.38 | ppl    29.50 | acc     0.65 | train_ae_norm     1.00\n",
      "[47/200][3399/4361] Loss_D: 0.00207592 (Loss_D_real: 0.00119902 Loss_D_fake: 0.00087690) Loss_G: 0.48917723 Loss_Enh_Dec: -1.25276721\n",
      "| epoch  47 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.86 | loss  3.34 | ppl    28.14 | acc     0.61 | train_ae_norm     1.00\n",
      "[47/200][3499/4361] Loss_D: 0.00678339 (Loss_D_real: 0.00181508 Loss_D_fake: 0.00496831) Loss_G: 0.41923323 Loss_Enh_Dec: -1.23269570\n",
      "| epoch  47 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  3.40 | ppl    29.87 | acc     0.64 | train_ae_norm     1.00\n",
      "[47/200][3599/4361] Loss_D: 0.00796174 (Loss_D_real: 0.00595056 Loss_D_fake: 0.00201118) Loss_G: 0.38943759 Loss_Enh_Dec: -1.40036607\n",
      "| epoch  47 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  3.32 | ppl    27.65 | acc     0.65 | train_ae_norm     1.00\n",
      "[47/200][3699/4361] Loss_D: 0.00268071 (Loss_D_real: 0.00129653 Loss_D_fake: 0.00138417) Loss_G: 0.34222057 Loss_Enh_Dec: -1.14351869\n",
      "| epoch  47 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  3.32 | ppl    27.65 | acc     0.62 | train_ae_norm     1.00\n",
      "[47/200][3799/4361] Loss_D: 0.00872838 (Loss_D_real: 0.00670326 Loss_D_fake: 0.00202513) Loss_G: 0.38137355 Loss_Enh_Dec: -1.38191259\n",
      "| epoch  47 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  3.35 | ppl    28.36 | acc     0.68 | train_ae_norm     1.00\n",
      "[47/200][3899/4361] Loss_D: 0.00495876 (Loss_D_real: 0.00158904 Loss_D_fake: 0.00336973) Loss_G: 0.46113071 Loss_Enh_Dec: -1.19644082\n",
      "| epoch  47 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  3.35 | ppl    28.40 | acc     0.60 | train_ae_norm     1.00\n",
      "[47/200][3999/4361] Loss_D: 0.00332229 (Loss_D_real: 0.00073591 Loss_D_fake: 0.00258638) Loss_G: 0.39051780 Loss_Enh_Dec: -1.24917364\n",
      "| epoch  47 |  4000/ 4361 batches | lr 0.000000 | ms/batch 402.05 | loss  3.35 | ppl    28.59 | acc     0.64 | train_ae_norm     1.00\n",
      "[47/200][4099/4361] Loss_D: 0.00285900 (Loss_D_real: 0.00075985 Loss_D_fake: 0.00209915) Loss_G: 0.39275432 Loss_Enh_Dec: -1.20931923\n",
      "| epoch  47 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  3.33 | ppl    27.85 | acc     0.63 | train_ae_norm     1.00\n",
      "[47/200][4199/4361] Loss_D: 0.00282327 (Loss_D_real: 0.00158500 Loss_D_fake: 0.00123827) Loss_G: 0.41060787 Loss_Enh_Dec: -1.27790213\n",
      "| epoch  47 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  3.35 | ppl    28.47 | acc     0.65 | train_ae_norm     1.00\n",
      "[47/200][4299/4361] Loss_D: 0.01019329 (Loss_D_real: 0.00833931 Loss_D_fake: 0.00185399) Loss_G: 0.43184805 Loss_Enh_Dec: -1.19913387\n",
      "| epoch  47 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  3.31 | ppl    27.27 | acc     0.65 | train_ae_norm     1.00\n",
      "| end of epoch  47 | time: 1852.54s | test loss  3.22 | test ppl 25.05 | acc 0.670\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 48 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.708\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.480\n",
      "  Test Loss: 4.003\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  48 |     0/ 4361 batches | lr 0.000000 | ms/batch 864.14 | loss  0.03 | ppl     1.03 | acc     0.67 | train_ae_norm     1.00\n",
      "[48/200][99/4361] Loss_D: 0.01930714 (Loss_D_real: 0.01697079 Loss_D_fake: 0.00233635) Loss_G: 0.36602506 Loss_Enh_Dec: -1.24698019\n",
      "| epoch  48 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  3.31 | ppl    27.49 | acc     0.60 | train_ae_norm     1.00\n",
      "[48/200][199/4361] Loss_D: 0.00302005 (Loss_D_real: 0.00067302 Loss_D_fake: 0.00234703) Loss_G: 0.44622651 Loss_Enh_Dec: -1.22574258\n",
      "| epoch  48 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  3.35 | ppl    28.39 | acc     0.65 | train_ae_norm     1.00\n",
      "[48/200][299/4361] Loss_D: 0.00788688 (Loss_D_real: 0.00187139 Loss_D_fake: 0.00601549) Loss_G: 0.49698171 Loss_Enh_Dec: -1.41567159\n",
      "| epoch  48 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  3.34 | ppl    28.30 | acc     0.60 | train_ae_norm     1.00\n",
      "[48/200][399/4361] Loss_D: 0.01479990 (Loss_D_real: 0.01396532 Loss_D_fake: 0.00083459) Loss_G: 0.81317347 Loss_Enh_Dec: -1.19429040\n",
      "| epoch  48 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  3.28 | ppl    26.67 | acc     0.64 | train_ae_norm     1.00\n",
      "[48/200][499/4361] Loss_D: 0.00360859 (Loss_D_real: 0.00168332 Loss_D_fake: 0.00192527) Loss_G: 0.39939976 Loss_Enh_Dec: -1.44602430\n",
      "| epoch  48 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  3.36 | ppl    28.90 | acc     0.65 | train_ae_norm     1.00\n",
      "[48/200][599/4361] Loss_D: 0.00419850 (Loss_D_real: 0.00174112 Loss_D_fake: 0.00245738) Loss_G: 0.43203568 Loss_Enh_Dec: -1.66546082\n",
      "| epoch  48 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  3.31 | ppl    27.42 | acc     0.60 | train_ae_norm     1.00\n",
      "[48/200][699/4361] Loss_D: 0.00517238 (Loss_D_real: 0.00377719 Loss_D_fake: 0.00139519) Loss_G: 0.39454433 Loss_Enh_Dec: -1.70869720\n",
      "| epoch  48 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  3.35 | ppl    28.46 | acc     0.65 | train_ae_norm     1.00\n",
      "[48/200][799/4361] Loss_D: 0.00544009 (Loss_D_real: 0.00081523 Loss_D_fake: 0.00462486) Loss_G: 0.42177865 Loss_Enh_Dec: -1.80482006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  48 |   800/ 4361 batches | lr 0.000000 | ms/batch 402.13 | loss  3.33 | ppl    27.83 | acc     0.65 | train_ae_norm     1.00\n",
      "[48/200][899/4361] Loss_D: 0.00638588 (Loss_D_real: 0.00083143 Loss_D_fake: 0.00555444) Loss_G: 0.40195996 Loss_Enh_Dec: -1.79579473\n",
      "| epoch  48 |   900/ 4361 batches | lr 0.000000 | ms/batch 402.05 | loss  3.35 | ppl    28.46 | acc     0.64 | train_ae_norm     1.00\n",
      "[48/200][999/4361] Loss_D: 0.00788849 (Loss_D_real: 0.00083251 Loss_D_fake: 0.00705598) Loss_G: 0.34060523 Loss_Enh_Dec: -1.56326854\n",
      "| epoch  48 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  3.32 | ppl    27.73 | acc     0.64 | train_ae_norm     1.00\n",
      "[48/200][1099/4361] Loss_D: 0.00917333 (Loss_D_real: 0.00694149 Loss_D_fake: 0.00223184) Loss_G: 0.45135686 Loss_Enh_Dec: -1.56686735\n",
      "| epoch  48 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  3.32 | ppl    27.78 | acc     0.61 | train_ae_norm     1.00\n",
      "[48/200][1199/4361] Loss_D: 0.05595355 (Loss_D_real: 0.05251697 Loss_D_fake: 0.00343658) Loss_G: 0.37687373 Loss_Enh_Dec: -1.81093049\n",
      "| epoch  48 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  3.34 | ppl    28.16 | acc     0.65 | train_ae_norm     1.00\n",
      "[48/200][1299/4361] Loss_D: 0.00358987 (Loss_D_real: 0.00239245 Loss_D_fake: 0.00119743) Loss_G: 0.46167371 Loss_Enh_Dec: -1.43487513\n",
      "| epoch  48 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  3.35 | ppl    28.54 | acc     0.63 | train_ae_norm     1.00\n",
      "[48/200][1399/4361] Loss_D: 0.00301148 (Loss_D_real: 0.00071399 Loss_D_fake: 0.00229749) Loss_G: 0.35586768 Loss_Enh_Dec: -1.69153082\n",
      "| epoch  48 |  1400/ 4361 batches | lr 0.000000 | ms/batch 402.31 | loss  3.36 | ppl    28.76 | acc     0.58 | train_ae_norm     1.00\n",
      "[48/200][1499/4361] Loss_D: 0.00644612 (Loss_D_real: 0.00182566 Loss_D_fake: 0.00462047) Loss_G: 0.58628696 Loss_Enh_Dec: -1.47298801\n",
      "| epoch  48 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  3.39 | ppl    29.59 | acc     0.61 | train_ae_norm     1.00\n",
      "[48/200][1599/4361] Loss_D: 0.01947841 (Loss_D_real: 0.00543025 Loss_D_fake: 0.01404816) Loss_G: 0.60759443 Loss_Enh_Dec: -1.60522449\n",
      "| epoch  48 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  3.35 | ppl    28.56 | acc     0.62 | train_ae_norm     1.00\n",
      "[48/200][1699/4361] Loss_D: 0.00930312 (Loss_D_real: 0.00216050 Loss_D_fake: 0.00714262) Loss_G: 0.34385404 Loss_Enh_Dec: -1.41726863\n",
      "| epoch  48 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  3.33 | ppl    27.89 | acc     0.63 | train_ae_norm     1.00\n",
      "[48/200][1799/4361] Loss_D: 0.00660263 (Loss_D_real: 0.00347452 Loss_D_fake: 0.00312811) Loss_G: 0.38108736 Loss_Enh_Dec: -1.30978429\n",
      "| epoch  48 |  1800/ 4361 batches | lr 0.000000 | ms/batch 402.13 | loss  3.29 | ppl    26.93 | acc     0.66 | train_ae_norm     1.00\n",
      "[48/200][1899/4361] Loss_D: 0.27708238 (Loss_D_real: 0.00214040 Loss_D_fake: 0.27494198) Loss_G: 0.36504298 Loss_Enh_Dec: -1.22055304\n",
      "| epoch  48 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  3.36 | ppl    28.86 | acc     0.67 | train_ae_norm     1.00\n",
      "[48/200][1999/4361] Loss_D: 0.01463152 (Loss_D_real: 0.00878882 Loss_D_fake: 0.00584270) Loss_G: 0.37461489 Loss_Enh_Dec: -0.82441205\n",
      "| epoch  48 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  3.31 | ppl    27.39 | acc     0.65 | train_ae_norm     1.00\n",
      "[48/200][2099/4361] Loss_D: 0.00459846 (Loss_D_real: 0.00098735 Loss_D_fake: 0.00361111) Loss_G: 0.34513685 Loss_Enh_Dec: -1.10413492\n",
      "| epoch  48 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  3.33 | ppl    28.01 | acc     0.63 | train_ae_norm     1.00\n",
      "[48/200][2199/4361] Loss_D: 0.00700421 (Loss_D_real: 0.00137120 Loss_D_fake: 0.00563301) Loss_G: 0.41079974 Loss_Enh_Dec: -0.92945766\n",
      "| epoch  48 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  3.32 | ppl    27.60 | acc     0.64 | train_ae_norm     1.00\n",
      "[48/200][2299/4361] Loss_D: 0.09011818 (Loss_D_real: 0.08401325 Loss_D_fake: 0.00610494) Loss_G: 0.41712031 Loss_Enh_Dec: -0.99830133\n",
      "| epoch  48 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  3.30 | ppl    27.00 | acc     0.65 | train_ae_norm     1.00\n",
      "[48/200][2399/4361] Loss_D: 0.00706709 (Loss_D_real: 0.00517720 Loss_D_fake: 0.00188990) Loss_G: 0.40750179 Loss_Enh_Dec: -0.97771722\n",
      "| epoch  48 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  3.31 | ppl    27.29 | acc     0.62 | train_ae_norm     1.00\n",
      "[48/200][2499/4361] Loss_D: 0.00575363 (Loss_D_real: 0.00342300 Loss_D_fake: 0.00233063) Loss_G: 0.39008752 Loss_Enh_Dec: -1.07631588\n",
      "| epoch  48 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  3.36 | ppl    28.73 | acc     0.64 | train_ae_norm     1.00\n",
      "[48/200][2599/4361] Loss_D: 0.00493451 (Loss_D_real: 0.00182295 Loss_D_fake: 0.00311156) Loss_G: 0.39348298 Loss_Enh_Dec: -1.02894199\n",
      "| epoch  48 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  3.35 | ppl    28.40 | acc     0.63 | train_ae_norm     1.00\n",
      "[48/200][2699/4361] Loss_D: 0.00765037 (Loss_D_real: 0.00519393 Loss_D_fake: 0.00245644) Loss_G: 0.34874904 Loss_Enh_Dec: -1.27788579\n",
      "| epoch  48 |  2700/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  3.34 | ppl    28.23 | acc     0.63 | train_ae_norm     1.00\n",
      "[48/200][2799/4361] Loss_D: 0.01079043 (Loss_D_real: 0.00834138 Loss_D_fake: 0.00244905) Loss_G: 0.35621414 Loss_Enh_Dec: -1.52102983\n",
      "| epoch  48 |  2800/ 4361 batches | lr 0.000000 | ms/batch 402.19 | loss  3.28 | ppl    26.69 | acc     0.62 | train_ae_norm     1.00\n",
      "[48/200][2899/4361] Loss_D: 0.01358685 (Loss_D_real: 0.00826863 Loss_D_fake: 0.00531822) Loss_G: 0.34987137 Loss_Enh_Dec: -1.41315937\n",
      "| epoch  48 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  3.32 | ppl    27.64 | acc     0.66 | train_ae_norm     1.00\n",
      "[48/200][2999/4361] Loss_D: 0.00786217 (Loss_D_real: 0.00325794 Loss_D_fake: 0.00460423) Loss_G: 0.37044182 Loss_Enh_Dec: -1.56310964\n",
      "| epoch  48 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  3.31 | ppl    27.41 | acc     0.65 | train_ae_norm     1.00\n",
      "[48/200][3099/4361] Loss_D: 0.00516978 (Loss_D_real: 0.00207297 Loss_D_fake: 0.00309682) Loss_G: 0.45368081 Loss_Enh_Dec: -1.68415928\n",
      "| epoch  48 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.67 | loss  3.33 | ppl    28.04 | acc     0.63 | train_ae_norm     1.00\n",
      "[48/200][3199/4361] Loss_D: 0.01053443 (Loss_D_real: 0.00222505 Loss_D_fake: 0.00830938) Loss_G: 0.42003208 Loss_Enh_Dec: -1.30732715\n",
      "| epoch  48 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  3.37 | ppl    29.06 | acc     0.65 | train_ae_norm     1.00\n",
      "[48/200][3299/4361] Loss_D: 0.00508282 (Loss_D_real: 0.00305117 Loss_D_fake: 0.00203165) Loss_G: 0.38998133 Loss_Enh_Dec: -1.67717171\n",
      "| epoch  48 |  3300/ 4361 batches | lr 0.000000 | ms/batch 400.89 | loss  3.36 | ppl    28.88 | acc     0.64 | train_ae_norm     1.00\n",
      "[48/200][3399/4361] Loss_D: 0.00759643 (Loss_D_real: 0.00495439 Loss_D_fake: 0.00264204) Loss_G: 0.50647044 Loss_Enh_Dec: -1.17318761\n",
      "| epoch  48 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  3.33 | ppl    27.97 | acc     0.63 | train_ae_norm     1.00\n",
      "[48/200][3499/4361] Loss_D: 0.00436702 (Loss_D_real: 0.00182481 Loss_D_fake: 0.00254221) Loss_G: 0.37609670 Loss_Enh_Dec: -1.46230423\n",
      "| epoch  48 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  3.29 | ppl    26.72 | acc     0.64 | train_ae_norm     1.00\n",
      "[48/200][3599/4361] Loss_D: 0.00315373 (Loss_D_real: 0.00167223 Loss_D_fake: 0.00148150) Loss_G: 0.41607305 Loss_Enh_Dec: -1.54827249\n",
      "| epoch  48 |  3600/ 4361 batches | lr 0.000000 | ms/batch 402.27 | loss  3.29 | ppl    26.95 | acc     0.67 | train_ae_norm     1.00\n",
      "[48/200][3699/4361] Loss_D: 0.00437694 (Loss_D_real: 0.00238892 Loss_D_fake: 0.00198801) Loss_G: 0.41816416 Loss_Enh_Dec: -1.50757408\n",
      "| epoch  48 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  3.31 | ppl    27.30 | acc     0.62 | train_ae_norm     1.00\n",
      "[48/200][3799/4361] Loss_D: 0.00676943 (Loss_D_real: 0.00345083 Loss_D_fake: 0.00331860) Loss_G: 0.38521561 Loss_Enh_Dec: -1.12105596\n",
      "| epoch  48 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  3.34 | ppl    28.27 | acc     0.69 | train_ae_norm     1.00\n",
      "[48/200][3899/4361] Loss_D: 0.00418376 (Loss_D_real: 0.00159528 Loss_D_fake: 0.00258849) Loss_G: 0.38030443 Loss_Enh_Dec: -1.12536454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  48 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  3.31 | ppl    27.52 | acc     0.62 | train_ae_norm     1.00\n",
      "[48/200][3999/4361] Loss_D: 0.00859984 (Loss_D_real: 0.00066044 Loss_D_fake: 0.00793940) Loss_G: 0.40607759 Loss_Enh_Dec: -0.99760765\n",
      "| epoch  48 |  4000/ 4361 batches | lr 0.000000 | ms/batch 402.05 | loss  3.33 | ppl    27.88 | acc     0.65 | train_ae_norm     1.00\n",
      "[48/200][4099/4361] Loss_D: 0.00406724 (Loss_D_real: 0.00050194 Loss_D_fake: 0.00356530) Loss_G: 0.38799569 Loss_Enh_Dec: -0.86514515\n",
      "| epoch  48 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.68 | loss  3.28 | ppl    26.56 | acc     0.63 | train_ae_norm     1.00\n",
      "[48/200][4199/4361] Loss_D: 0.00639723 (Loss_D_real: 0.00076673 Loss_D_fake: 0.00563050) Loss_G: 0.36780375 Loss_Enh_Dec: -1.04585016\n",
      "| epoch  48 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  3.33 | ppl    27.94 | acc     0.68 | train_ae_norm     1.00\n",
      "[48/200][4299/4361] Loss_D: 0.00410527 (Loss_D_real: 0.00198582 Loss_D_fake: 0.00211945) Loss_G: 0.41353664 Loss_Enh_Dec: -0.94406939\n",
      "| epoch  48 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  3.28 | ppl    26.52 | acc     0.67 | train_ae_norm     1.00\n",
      "| end of epoch  48 | time: 1853.21s | test loss  3.20 | test ppl 24.57 | acc 0.670\n",
      "bleu_self:  [3.40505059e-01 6.78408809e-09 1.95718669e-11 1.11337841e-12\n",
      " 2.13297595e-13]\n",
      "bleu_test:  [8.55303030e-01 4.03841171e-01 5.06874475e-02 3.77671994e-02\n",
      " 3.25130060e-05]\n",
      "bleu_self: [0.34050506,0.00000001,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.85530303,0.40384117,0.05068745,0.03776720,0.00003251]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 49 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.705\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.487\n",
      "  Test Loss: 3.847\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  49 |     0/ 4361 batches | lr 0.000000 | ms/batch 868.03 | loss  0.03 | ppl     1.03 | acc     0.68 | train_ae_norm     1.00\n",
      "[49/200][99/4361] Loss_D: 0.00482180 (Loss_D_real: 0.00198526 Loss_D_fake: 0.00283654) Loss_G: 0.37896949 Loss_Enh_Dec: -1.00344181\n",
      "| epoch  49 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.13 | loss  3.31 | ppl    27.39 | acc     0.61 | train_ae_norm     1.00\n",
      "[49/200][199/4361] Loss_D: 0.00118649 (Loss_D_real: 0.00067438 Loss_D_fake: 0.00051211) Loss_G: 0.52239001 Loss_Enh_Dec: -1.15201747\n",
      "| epoch  49 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  3.33 | ppl    27.86 | acc     0.66 | train_ae_norm     1.00\n",
      "[49/200][299/4361] Loss_D: 0.00648649 (Loss_D_real: 0.00095874 Loss_D_fake: 0.00552775) Loss_G: 0.39749992 Loss_Enh_Dec: -1.29764462\n",
      "| epoch  49 |   300/ 4361 batches | lr 0.000000 | ms/batch 402.19 | loss  3.33 | ppl    27.83 | acc     0.59 | train_ae_norm     1.00\n",
      "[49/200][399/4361] Loss_D: 0.00269798 (Loss_D_real: 0.00175842 Loss_D_fake: 0.00093956) Loss_G: 0.33762965 Loss_Enh_Dec: -1.11622655\n",
      "| epoch  49 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  3.26 | ppl    25.99 | acc     0.66 | train_ae_norm     1.00\n",
      "[49/200][499/4361] Loss_D: 0.00795313 (Loss_D_real: 0.00314454 Loss_D_fake: 0.00480859) Loss_G: 0.38624266 Loss_Enh_Dec: -1.11494815\n",
      "| epoch  49 |   500/ 4361 batches | lr 0.000000 | ms/batch 402.30 | loss  3.32 | ppl    27.76 | acc     0.65 | train_ae_norm     1.00\n",
      "[49/200][599/4361] Loss_D: 0.00800215 (Loss_D_real: 0.00378391 Loss_D_fake: 0.00421823) Loss_G: 0.39795360 Loss_Enh_Dec: -1.16430414\n",
      "| epoch  49 |   600/ 4361 batches | lr 0.000000 | ms/batch 402.09 | loss  3.27 | ppl    26.42 | acc     0.61 | train_ae_norm     1.00\n",
      "[49/200][699/4361] Loss_D: 0.00460119 (Loss_D_real: 0.00279631 Loss_D_fake: 0.00180489) Loss_G: 0.36633626 Loss_Enh_Dec: -1.17564857\n",
      "| epoch  49 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  3.33 | ppl    27.81 | acc     0.67 | train_ae_norm     1.00\n",
      "[49/200][799/4361] Loss_D: 0.00809235 (Loss_D_real: 0.00319284 Loss_D_fake: 0.00489950) Loss_G: 0.37965116 Loss_Enh_Dec: -1.47630441\n",
      "| epoch  49 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  3.29 | ppl    26.88 | acc     0.64 | train_ae_norm     1.00\n",
      "[49/200][899/4361] Loss_D: 0.00378927 (Loss_D_real: 0.00112057 Loss_D_fake: 0.00266870) Loss_G: 0.66248846 Loss_Enh_Dec: -0.71179229\n",
      "| epoch  49 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  3.30 | ppl    27.10 | acc     0.66 | train_ae_norm     1.00\n",
      "[49/200][999/4361] Loss_D: 0.01157981 (Loss_D_real: 0.00583142 Loss_D_fake: 0.00574839) Loss_G: 0.36479214 Loss_Enh_Dec: -1.08030772\n",
      "| epoch  49 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  3.28 | ppl    26.64 | acc     0.64 | train_ae_norm     1.00\n",
      "[49/200][1099/4361] Loss_D: 0.00687339 (Loss_D_real: 0.00499660 Loss_D_fake: 0.00187679) Loss_G: 0.41199484 Loss_Enh_Dec: -0.57624888\n",
      "| epoch  49 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  3.28 | ppl    26.64 | acc     0.64 | train_ae_norm     1.00\n",
      "[49/200][1199/4361] Loss_D: 0.00523207 (Loss_D_real: 0.00369623 Loss_D_fake: 0.00153584) Loss_G: 0.42108297 Loss_Enh_Dec: -1.06265020\n",
      "| epoch  49 |  1200/ 4361 batches | lr 0.000000 | ms/batch 402.20 | loss  3.28 | ppl    26.51 | acc     0.65 | train_ae_norm     1.00\n",
      "[49/200][1299/4361] Loss_D: 0.00340671 (Loss_D_real: 0.00125919 Loss_D_fake: 0.00214752) Loss_G: 0.39124581 Loss_Enh_Dec: -0.71647209\n",
      "| epoch  49 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  3.29 | ppl    26.90 | acc     0.64 | train_ae_norm     1.00\n",
      "[49/200][1399/4361] Loss_D: 0.00419445 (Loss_D_real: 0.00313878 Loss_D_fake: 0.00105568) Loss_G: 0.44440684 Loss_Enh_Dec: -0.83551359\n",
      "| epoch  49 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  3.30 | ppl    27.12 | acc     0.59 | train_ae_norm     1.00\n",
      "[49/200][1499/4361] Loss_D: 0.02918528 (Loss_D_real: 0.02741837 Loss_D_fake: 0.00176691) Loss_G: 0.41396433 Loss_Enh_Dec: -1.44594371\n",
      "| epoch  49 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  3.34 | ppl    28.34 | acc     0.63 | train_ae_norm     1.00\n",
      "[49/200][1599/4361] Loss_D: 0.01030256 (Loss_D_real: 0.00691678 Loss_D_fake: 0.00338578) Loss_G: 0.38876382 Loss_Enh_Dec: -1.11545527\n",
      "| epoch  49 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  3.33 | ppl    27.83 | acc     0.61 | train_ae_norm     1.00\n",
      "[49/200][1699/4361] Loss_D: 0.01181462 (Loss_D_real: 0.00391770 Loss_D_fake: 0.00789692) Loss_G: 0.37065011 Loss_Enh_Dec: -1.23545969\n",
      "| epoch  49 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.77 | loss  3.30 | ppl    27.22 | acc     0.62 | train_ae_norm     1.00\n",
      "[49/200][1799/4361] Loss_D: 0.00203279 (Loss_D_real: 0.00063170 Loss_D_fake: 0.00140109) Loss_G: 0.44504672 Loss_Enh_Dec: -1.06503856\n",
      "| epoch  49 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  3.28 | ppl    26.57 | acc     0.63 | train_ae_norm     1.00\n",
      "[49/200][1899/4361] Loss_D: 0.00575766 (Loss_D_real: 0.00436495 Loss_D_fake: 0.00139271) Loss_G: 0.38386735 Loss_Enh_Dec: -0.57085198\n",
      "| epoch  49 |  1900/ 4361 batches | lr 0.000000 | ms/batch 402.12 | loss  3.37 | ppl    29.21 | acc     0.66 | train_ae_norm     1.00\n",
      "[49/200][1999/4361] Loss_D: 0.00613435 (Loss_D_real: 0.00464999 Loss_D_fake: 0.00148436) Loss_G: 0.38382307 Loss_Enh_Dec: -0.82432574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  49 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.06 | loss  3.30 | ppl    27.00 | acc     0.61 | train_ae_norm     1.00\n",
      "[49/200][2099/4361] Loss_D: 0.00389405 (Loss_D_real: 0.00107445 Loss_D_fake: 0.00281960) Loss_G: 0.42133495 Loss_Enh_Dec: -0.35821959\n",
      "| epoch  49 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  3.32 | ppl    27.76 | acc     0.64 | train_ae_norm     1.00\n",
      "[49/200][2199/4361] Loss_D: 0.00260316 (Loss_D_real: 0.00127760 Loss_D_fake: 0.00132556) Loss_G: 0.45389181 Loss_Enh_Dec: -0.83464509\n",
      "| epoch  49 |  2200/ 4361 batches | lr 0.000000 | ms/batch 402.16 | loss  3.32 | ppl    27.56 | acc     0.64 | train_ae_norm     1.00\n",
      "[49/200][2299/4361] Loss_D: 0.00476018 (Loss_D_real: 0.00268089 Loss_D_fake: 0.00207929) Loss_G: 0.44418979 Loss_Enh_Dec: -0.96355057\n",
      "| epoch  49 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  3.30 | ppl    27.00 | acc     0.66 | train_ae_norm     1.00\n",
      "[49/200][2399/4361] Loss_D: 0.00373625 (Loss_D_real: 0.00077062 Loss_D_fake: 0.00296563) Loss_G: 0.38715103 Loss_Enh_Dec: -0.65483171\n",
      "| epoch  49 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  3.30 | ppl    27.07 | acc     0.60 | train_ae_norm     1.00\n",
      "[49/200][2499/4361] Loss_D: 0.00287472 (Loss_D_real: 0.00046430 Loss_D_fake: 0.00241042) Loss_G: 0.40646592 Loss_Enh_Dec: -0.66503078\n",
      "| epoch  49 |  2500/ 4361 batches | lr 0.000000 | ms/batch 402.41 | loss  3.33 | ppl    27.88 | acc     0.64 | train_ae_norm     1.00\n",
      "[49/200][2599/4361] Loss_D: 0.00780275 (Loss_D_real: 0.00053146 Loss_D_fake: 0.00727130) Loss_G: 0.39796755 Loss_Enh_Dec: -0.70912200\n",
      "| epoch  49 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  3.30 | ppl    27.10 | acc     0.63 | train_ae_norm     1.00\n",
      "[49/200][2699/4361] Loss_D: 0.02182882 (Loss_D_real: 0.01761010 Loss_D_fake: 0.00421872) Loss_G: 0.38204607 Loss_Enh_Dec: -0.33168936\n",
      "| epoch  49 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  3.33 | ppl    28.02 | acc     0.61 | train_ae_norm     1.00\n",
      "[49/200][2799/4361] Loss_D: 0.00661837 (Loss_D_real: 0.00083736 Loss_D_fake: 0.00578101) Loss_G: 0.36887178 Loss_Enh_Dec: -0.89351887\n",
      "| epoch  49 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  3.27 | ppl    26.20 | acc     0.61 | train_ae_norm     1.00\n",
      "[49/200][2899/4361] Loss_D: 0.00530395 (Loss_D_real: 0.00229332 Loss_D_fake: 0.00301063) Loss_G: 0.38272625 Loss_Enh_Dec: -0.97602147\n",
      "| epoch  49 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  3.32 | ppl    27.58 | acc     0.64 | train_ae_norm     1.00\n",
      "[49/200][2999/4361] Loss_D: 0.01359245 (Loss_D_real: 0.01331360 Loss_D_fake: 0.00027885) Loss_G: 0.73047602 Loss_Enh_Dec: -0.48577318\n",
      "| epoch  49 |  3000/ 4361 batches | lr 0.000000 | ms/batch 402.11 | loss  3.31 | ppl    27.28 | acc     0.63 | train_ae_norm     1.00\n",
      "[49/200][3099/4361] Loss_D: 0.00362264 (Loss_D_real: 0.00242250 Loss_D_fake: 0.00120014) Loss_G: 0.41076460 Loss_Enh_Dec: -0.43741438\n",
      "| epoch  49 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  3.33 | ppl    27.85 | acc     0.61 | train_ae_norm     1.00\n",
      "[49/200][3199/4361] Loss_D: 0.00365993 (Loss_D_real: 0.00267631 Loss_D_fake: 0.00098363) Loss_G: 0.46565935 Loss_Enh_Dec: -1.07342637\n",
      "| epoch  49 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  3.34 | ppl    28.17 | acc     0.64 | train_ae_norm     1.00\n",
      "[49/200][3299/4361] Loss_D: 0.00238193 (Loss_D_real: 0.00055745 Loss_D_fake: 0.00182447) Loss_G: 0.38760087 Loss_Enh_Dec: -0.98647356\n",
      "| epoch  49 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  3.34 | ppl    28.18 | acc     0.64 | train_ae_norm     1.00\n",
      "[49/200][3399/4361] Loss_D: 0.00502843 (Loss_D_real: 0.00087774 Loss_D_fake: 0.00415069) Loss_G: 0.39603940 Loss_Enh_Dec: -0.66996634\n",
      "| epoch  49 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  3.31 | ppl    27.49 | acc     0.62 | train_ae_norm     1.00\n",
      "[49/200][3499/4361] Loss_D: 0.00558485 (Loss_D_real: 0.00209415 Loss_D_fake: 0.00349069) Loss_G: 0.37773320 Loss_Enh_Dec: -0.96634787\n",
      "| epoch  49 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  3.26 | ppl    25.94 | acc     0.64 | train_ae_norm     1.00\n",
      "[49/200][3599/4361] Loss_D: 0.00242853 (Loss_D_real: 0.00173370 Loss_D_fake: 0.00069483) Loss_G: 0.42587623 Loss_Enh_Dec: -0.80861825\n",
      "| epoch  49 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  3.26 | ppl    26.09 | acc     0.65 | train_ae_norm     1.00\n",
      "[49/200][3699/4361] Loss_D: 0.00538396 (Loss_D_real: 0.00455150 Loss_D_fake: 0.00083246) Loss_G: 0.66595912 Loss_Enh_Dec: -0.94432306\n",
      "| epoch  49 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  3.28 | ppl    26.48 | acc     0.61 | train_ae_norm     1.00\n",
      "[49/200][3799/4361] Loss_D: 0.00627850 (Loss_D_real: 0.00443148 Loss_D_fake: 0.00184702) Loss_G: 0.40754041 Loss_Enh_Dec: -0.98790437\n",
      "| epoch  49 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  3.30 | ppl    27.03 | acc     0.66 | train_ae_norm     1.00\n",
      "[49/200][3899/4361] Loss_D: 0.05557143 (Loss_D_real: 0.05198326 Loss_D_fake: 0.00358817) Loss_G: 0.39613748 Loss_Enh_Dec: -1.02431130\n",
      "| epoch  49 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  3.29 | ppl    26.81 | acc     0.63 | train_ae_norm     1.00\n",
      "[49/200][3999/4361] Loss_D: 0.01077685 (Loss_D_real: 0.00637354 Loss_D_fake: 0.00440332) Loss_G: 0.42505309 Loss_Enh_Dec: -0.95516104\n",
      "| epoch  49 |  4000/ 4361 batches | lr 0.000000 | ms/batch 402.04 | loss  3.29 | ppl    26.93 | acc     0.66 | train_ae_norm     1.00\n",
      "[49/200][4099/4361] Loss_D: 0.00330152 (Loss_D_real: 0.00085609 Loss_D_fake: 0.00244543) Loss_G: 0.39595956 Loss_Enh_Dec: -1.10857427\n",
      "| epoch  49 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  3.24 | ppl    25.56 | acc     0.64 | train_ae_norm     1.00\n",
      "[49/200][4199/4361] Loss_D: 0.00280366 (Loss_D_real: 0.00126040 Loss_D_fake: 0.00154326) Loss_G: 0.47168493 Loss_Enh_Dec: -1.24095976\n",
      "| epoch  49 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  3.31 | ppl    27.42 | acc     0.67 | train_ae_norm     1.00\n",
      "[49/200][4299/4361] Loss_D: 0.00153655 (Loss_D_real: 0.00033814 Loss_D_fake: 0.00119841) Loss_G: 0.37948361 Loss_Enh_Dec: -1.31607974\n",
      "| epoch  49 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  3.25 | ppl    25.90 | acc     0.66 | train_ae_norm     1.00\n",
      "| end of epoch  49 | time: 1854.35s | test loss  3.18 | test ppl 24.11 | acc 0.672\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 50 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:57.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.709\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.485\n",
      "  Test Loss: 3.923\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  50 |     0/ 4361 batches | lr 0.000000 | ms/batch 868.41 | loss  0.03 | ppl     1.03 | acc     0.68 | train_ae_norm     1.00\n",
      "[50/200][99/4361] Loss_D: 0.00527144 (Loss_D_real: 0.00077796 Loss_D_fake: 0.00449348) Loss_G: 0.38633981 Loss_Enh_Dec: -0.40739629\n",
      "| epoch  50 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  3.28 | ppl    26.46 | acc     0.61 | train_ae_norm     1.00\n",
      "[50/200][199/4361] Loss_D: 0.01288968 (Loss_D_real: 0.00728853 Loss_D_fake: 0.00560115) Loss_G: 0.37197712 Loss_Enh_Dec: -1.01731431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  50 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  3.29 | ppl    26.72 | acc     0.65 | train_ae_norm     1.00\n",
      "[50/200][299/4361] Loss_D: 0.00194309 (Loss_D_real: 0.00049453 Loss_D_fake: 0.00144856) Loss_G: 0.40789166 Loss_Enh_Dec: -1.27744663\n",
      "| epoch  50 |   300/ 4361 batches | lr 0.000000 | ms/batch 402.12 | loss  3.30 | ppl    27.10 | acc     0.59 | train_ae_norm     1.00\n",
      "[50/200][399/4361] Loss_D: 0.01510574 (Loss_D_real: 0.01324595 Loss_D_fake: 0.00185979) Loss_G: 0.42447239 Loss_Enh_Dec: -1.28898621\n",
      "| epoch  50 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  3.20 | ppl    24.61 | acc     0.65 | train_ae_norm     1.00\n",
      "[50/200][499/4361] Loss_D: 0.00176614 (Loss_D_real: 0.00032063 Loss_D_fake: 0.00144551) Loss_G: 0.39614922 Loss_Enh_Dec: -1.23617935\n",
      "| epoch  50 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  3.28 | ppl    26.49 | acc     0.67 | train_ae_norm     1.00\n",
      "[50/200][599/4361] Loss_D: 0.00349467 (Loss_D_real: 0.00093409 Loss_D_fake: 0.00256057) Loss_G: 0.54135150 Loss_Enh_Dec: -1.27755260\n",
      "| epoch  50 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  3.24 | ppl    25.43 | acc     0.62 | train_ae_norm     1.00\n",
      "[50/200][699/4361] Loss_D: 0.00238875 (Loss_D_real: 0.00097672 Loss_D_fake: 0.00141203) Loss_G: 0.41210413 Loss_Enh_Dec: -0.66025579\n",
      "| epoch  50 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  3.26 | ppl    26.12 | acc     0.65 | train_ae_norm     1.00\n",
      "[50/200][799/4361] Loss_D: 0.00920504 (Loss_D_real: 0.00806956 Loss_D_fake: 0.00113548) Loss_G: 0.40685138 Loss_Enh_Dec: -1.25777006\n",
      "| epoch  50 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  3.26 | ppl    25.94 | acc     0.64 | train_ae_norm     1.00\n",
      "[50/200][899/4361] Loss_D: 0.00753614 (Loss_D_real: 0.00606927 Loss_D_fake: 0.00146687) Loss_G: 0.47077069 Loss_Enh_Dec: -1.29945707\n",
      "| epoch  50 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  3.27 | ppl    26.21 | acc     0.65 | train_ae_norm     1.00\n",
      "[50/200][999/4361] Loss_D: 0.00984166 (Loss_D_real: 0.00031928 Loss_D_fake: 0.00952238) Loss_G: 0.39974952 Loss_Enh_Dec: -1.36576414\n",
      "| epoch  50 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  3.25 | ppl    25.80 | acc     0.65 | train_ae_norm     1.00\n",
      "[50/200][1099/4361] Loss_D: 0.00314481 (Loss_D_real: 0.00078285 Loss_D_fake: 0.00236197) Loss_G: 0.39699385 Loss_Enh_Dec: -1.00621951\n",
      "| epoch  50 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  3.27 | ppl    26.33 | acc     0.62 | train_ae_norm     1.00\n",
      "[50/200][1199/4361] Loss_D: 0.00253204 (Loss_D_real: 0.00108929 Loss_D_fake: 0.00144275) Loss_G: 0.38878375 Loss_Enh_Dec: -1.26207006\n",
      "| epoch  50 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  3.26 | ppl    25.98 | acc     0.65 | train_ae_norm     1.00\n",
      "[50/200][1299/4361] Loss_D: 0.00209922 (Loss_D_real: 0.00099342 Loss_D_fake: 0.00110580) Loss_G: 0.38876647 Loss_Enh_Dec: -1.10141027\n",
      "| epoch  50 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  3.28 | ppl    26.70 | acc     0.63 | train_ae_norm     1.00\n",
      "[50/200][1399/4361] Loss_D: 0.00582512 (Loss_D_real: 0.00442479 Loss_D_fake: 0.00140033) Loss_G: 0.40982971 Loss_Enh_Dec: -1.33517110\n",
      "| epoch  50 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  3.29 | ppl    26.90 | acc     0.59 | train_ae_norm     1.00\n",
      "[50/200][1499/4361] Loss_D: 0.00811087 (Loss_D_real: 0.00144753 Loss_D_fake: 0.00666334) Loss_G: 0.45741662 Loss_Enh_Dec: -1.29749322\n",
      "| epoch  50 |  1500/ 4361 batches | lr 0.000000 | ms/batch 402.11 | loss  3.32 | ppl    27.78 | acc     0.62 | train_ae_norm     1.00\n",
      "[50/200][1599/4361] Loss_D: 0.00166207 (Loss_D_real: 0.00097117 Loss_D_fake: 0.00069090) Loss_G: 0.43915063 Loss_Enh_Dec: -1.36792409\n",
      "| epoch  50 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  3.28 | ppl    26.64 | acc     0.65 | train_ae_norm     1.00\n",
      "[50/200][1699/4361] Loss_D: 0.00359945 (Loss_D_real: 0.00127401 Loss_D_fake: 0.00232544) Loss_G: 0.40622988 Loss_Enh_Dec: -0.84602398\n",
      "| epoch  50 |  1700/ 4361 batches | lr 0.000000 | ms/batch 402.09 | loss  3.25 | ppl    25.86 | acc     0.65 | train_ae_norm     1.00\n",
      "[50/200][1799/4361] Loss_D: 0.00145152 (Loss_D_real: 0.00032433 Loss_D_fake: 0.00112719) Loss_G: 0.42141038 Loss_Enh_Dec: -1.19238961\n",
      "| epoch  50 |  1800/ 4361 batches | lr 0.000000 | ms/batch 402.26 | loss  3.23 | ppl    25.29 | acc     0.65 | train_ae_norm     1.00\n",
      "[50/200][1899/4361] Loss_D: 0.00504600 (Loss_D_real: 0.00213873 Loss_D_fake: 0.00290726) Loss_G: 0.38530549 Loss_Enh_Dec: -1.17847955\n",
      "| epoch  50 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  3.29 | ppl    26.92 | acc     0.66 | train_ae_norm     1.00\n",
      "[50/200][1999/4361] Loss_D: 0.00205076 (Loss_D_real: 0.00047323 Loss_D_fake: 0.00157753) Loss_G: 0.42912135 Loss_Enh_Dec: -1.44484484\n",
      "| epoch  50 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.91 | loss  3.25 | ppl    25.85 | acc     0.65 | train_ae_norm     1.00\n",
      "[50/200][2099/4361] Loss_D: 0.00664977 (Loss_D_real: 0.00250790 Loss_D_fake: 0.00414187) Loss_G: 0.45100036 Loss_Enh_Dec: -1.29775512\n",
      "| epoch  50 |  2100/ 4361 batches | lr 0.000000 | ms/batch 402.23 | loss  3.29 | ppl    26.90 | acc     0.62 | train_ae_norm     1.00\n",
      "[50/200][2199/4361] Loss_D: 0.02151415 (Loss_D_real: 0.02025078 Loss_D_fake: 0.00126338) Loss_G: 0.38267609 Loss_Enh_Dec: -1.26843536\n",
      "| epoch  50 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  3.30 | ppl    27.04 | acc     0.64 | train_ae_norm     1.00\n",
      "[50/200][2299/4361] Loss_D: 0.00247103 (Loss_D_real: 0.00133143 Loss_D_fake: 0.00113961) Loss_G: 0.50243694 Loss_Enh_Dec: -1.16464698\n",
      "| epoch  50 |  2300/ 4361 batches | lr 0.000000 | ms/batch 402.12 | loss  3.26 | ppl    25.99 | acc     0.68 | train_ae_norm     1.00\n",
      "[50/200][2399/4361] Loss_D: 0.00506761 (Loss_D_real: 0.00170862 Loss_D_fake: 0.00335899) Loss_G: 0.36841270 Loss_Enh_Dec: -0.71318752\n",
      "| epoch  50 |  2400/ 4361 batches | lr 0.000000 | ms/batch 402.14 | loss  3.26 | ppl    26.10 | acc     0.62 | train_ae_norm     1.00\n",
      "[50/200][2499/4361] Loss_D: 0.00510181 (Loss_D_real: 0.00113593 Loss_D_fake: 0.00396588) Loss_G: 0.44586912 Loss_Enh_Dec: -1.37010980\n",
      "| epoch  50 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  3.30 | ppl    27.02 | acc     0.64 | train_ae_norm     1.00\n",
      "[50/200][2599/4361] Loss_D: 0.00481771 (Loss_D_real: 0.00203716 Loss_D_fake: 0.00278054) Loss_G: 0.46006814 Loss_Enh_Dec: -1.19092846\n",
      "| epoch  50 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  3.26 | ppl    26.13 | acc     0.64 | train_ae_norm     1.00\n",
      "[50/200][2699/4361] Loss_D: 0.00197599 (Loss_D_real: 0.00035973 Loss_D_fake: 0.00161626) Loss_G: 0.40798113 Loss_Enh_Dec: -1.07317412\n",
      "| epoch  50 |  2700/ 4361 batches | lr 0.000000 | ms/batch 402.14 | loss  3.28 | ppl    26.51 | acc     0.63 | train_ae_norm     1.00\n",
      "[50/200][2799/4361] Loss_D: 0.00169374 (Loss_D_real: 0.00081250 Loss_D_fake: 0.00088124) Loss_G: 0.43729803 Loss_Enh_Dec: -1.15210378\n",
      "| epoch  50 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  3.22 | ppl    25.10 | acc     0.62 | train_ae_norm     1.00\n",
      "[50/200][2899/4361] Loss_D: 0.00973881 (Loss_D_real: 0.00268916 Loss_D_fake: 0.00704964) Loss_G: 0.34586734 Loss_Enh_Dec: -1.29760849\n",
      "| epoch  50 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  3.26 | ppl    26.15 | acc     0.64 | train_ae_norm     1.00\n",
      "[50/200][2999/4361] Loss_D: 0.00273782 (Loss_D_real: 0.00065847 Loss_D_fake: 0.00207935) Loss_G: 0.36881495 Loss_Enh_Dec: -1.05488884\n",
      "| epoch  50 |  3000/ 4361 batches | lr 0.000000 | ms/batch 402.25 | loss  3.26 | ppl    26.07 | acc     0.65 | train_ae_norm     1.00\n",
      "[50/200][3099/4361] Loss_D: 0.00315720 (Loss_D_real: 0.00187669 Loss_D_fake: 0.00128051) Loss_G: 0.42008758 Loss_Enh_Dec: -0.89653558\n",
      "| epoch  50 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  3.26 | ppl    26.12 | acc     0.63 | train_ae_norm     1.00\n",
      "[50/200][3199/4361] Loss_D: 0.00313814 (Loss_D_real: 0.00228999 Loss_D_fake: 0.00084815) Loss_G: 0.48573777 Loss_Enh_Dec: -0.39164093\n",
      "| epoch  50 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  3.29 | ppl    26.81 | acc     0.66 | train_ae_norm     1.00\n",
      "[50/200][3299/4361] Loss_D: 0.00474197 (Loss_D_real: 0.00093806 Loss_D_fake: 0.00380391) Loss_G: 0.42010233 Loss_Enh_Dec: -0.94046801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  50 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  3.29 | ppl    26.72 | acc     0.64 | train_ae_norm     1.00\n",
      "[50/200][3399/4361] Loss_D: 0.01936329 (Loss_D_real: 0.01786020 Loss_D_fake: 0.00150309) Loss_G: 0.39855775 Loss_Enh_Dec: -0.84263676\n",
      "| epoch  50 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  3.28 | ppl    26.70 | acc     0.63 | train_ae_norm     1.00\n",
      "[50/200][3499/4361] Loss_D: 0.00234851 (Loss_D_real: 0.00106535 Loss_D_fake: 0.00128316) Loss_G: 0.41436529 Loss_Enh_Dec: -0.83222675\n",
      "| epoch  50 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  3.24 | ppl    25.58 | acc     0.63 | train_ae_norm     1.00\n",
      "[50/200][3599/4361] Loss_D: 0.00133702 (Loss_D_real: 0.00069754 Loss_D_fake: 0.00063948) Loss_G: 0.41236255 Loss_Enh_Dec: -0.88187999\n",
      "| epoch  50 |  3600/ 4361 batches | lr 0.000000 | ms/batch 402.37 | loss  3.23 | ppl    25.30 | acc     0.64 | train_ae_norm     1.00\n",
      "[50/200][3699/4361] Loss_D: 0.00742541 (Loss_D_real: 0.00181468 Loss_D_fake: 0.00561073) Loss_G: 0.40450245 Loss_Enh_Dec: -1.21798658\n",
      "| epoch  50 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  3.25 | ppl    25.91 | acc     0.61 | train_ae_norm     1.00\n",
      "[50/200][3799/4361] Loss_D: 0.00110118 (Loss_D_real: 0.00041496 Loss_D_fake: 0.00068622) Loss_G: 0.40946808 Loss_Enh_Dec: -1.27442932\n",
      "| epoch  50 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  3.27 | ppl    26.27 | acc     0.67 | train_ae_norm     1.00\n",
      "[50/200][3899/4361] Loss_D: 0.00191738 (Loss_D_real: 0.00084224 Loss_D_fake: 0.00107514) Loss_G: 0.40901026 Loss_Enh_Dec: -1.25263906\n",
      "| epoch  50 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  3.26 | ppl    26.06 | acc     0.61 | train_ae_norm     1.00\n",
      "[50/200][3999/4361] Loss_D: 0.00228465 (Loss_D_real: 0.00086097 Loss_D_fake: 0.00142368) Loss_G: 0.40304229 Loss_Enh_Dec: -1.18904400\n",
      "| epoch  50 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  3.27 | ppl    26.40 | acc     0.67 | train_ae_norm     1.00\n",
      "[50/200][4099/4361] Loss_D: 0.00475178 (Loss_D_real: 0.00262489 Loss_D_fake: 0.00212689) Loss_G: 0.43194896 Loss_Enh_Dec: -0.88749218\n",
      "| epoch  50 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  3.23 | ppl    25.26 | acc     0.64 | train_ae_norm     1.00\n",
      "[50/200][4199/4361] Loss_D: 0.00380251 (Loss_D_real: 0.00153795 Loss_D_fake: 0.00226456) Loss_G: 0.41496000 Loss_Enh_Dec: -1.07463348\n",
      "| epoch  50 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  3.28 | ppl    26.57 | acc     0.67 | train_ae_norm     1.00\n",
      "[50/200][4299/4361] Loss_D: 0.00294994 (Loss_D_real: 0.00053488 Loss_D_fake: 0.00241507) Loss_G: 0.40832430 Loss_Enh_Dec: -0.91531438\n",
      "| epoch  50 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  3.24 | ppl    25.50 | acc     0.66 | train_ae_norm     1.00\n",
      "| end of epoch  50 | time: 1854.77s | test loss  3.18 | test ppl 24.03 | acc 0.674\n",
      "bleu_self:  [2.04924242e-01 7.30343794e-09 2.85125585e-11 1.62762514e-10\n",
      " 7.20531424e-10]\n",
      "bleu_test:  [9.09659090e-01 1.09201893e-01 8.59826555e-07 2.50148202e-08\n",
      " 3.23646669e-08]\n",
      "bleu_self: [0.20492424,0.00000001,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.90965909,0.10920189,0.00000086,0.00000003,0.00000003]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 51 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.710\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.497\n",
      "  Test Loss: 3.898\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  51 |     0/ 4361 batches | lr 0.000000 | ms/batch 862.20 | loss  0.03 | ppl     1.03 | acc     0.68 | train_ae_norm     1.00\n",
      "[51/200][99/4361] Loss_D: 0.00311468 (Loss_D_real: 0.00092230 Loss_D_fake: 0.00219238) Loss_G: 0.45130682 Loss_Enh_Dec: -0.90780526\n",
      "| epoch  51 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.28 | loss  3.27 | ppl    26.22 | acc     0.63 | train_ae_norm     1.00\n",
      "[51/200][199/4361] Loss_D: 0.00493426 (Loss_D_real: 0.00290395 Loss_D_fake: 0.00203031) Loss_G: 0.41895685 Loss_Enh_Dec: -1.01236236\n",
      "| epoch  51 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  3.28 | ppl    26.63 | acc     0.64 | train_ae_norm     1.00\n",
      "[51/200][299/4361] Loss_D: 0.00462795 (Loss_D_real: 0.00301732 Loss_D_fake: 0.00161063) Loss_G: 0.38453028 Loss_Enh_Dec: -1.07369912\n",
      "| epoch  51 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  3.29 | ppl    26.89 | acc     0.59 | train_ae_norm     1.00\n",
      "[51/200][399/4361] Loss_D: 0.00667680 (Loss_D_real: 0.00527641 Loss_D_fake: 0.00140040) Loss_G: 0.41052815 Loss_Enh_Dec: -0.01650444\n",
      "| epoch  51 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  3.20 | ppl    24.62 | acc     0.65 | train_ae_norm     1.00\n",
      "[51/200][499/4361] Loss_D: 0.00221281 (Loss_D_real: 0.00125476 Loss_D_fake: 0.00095805) Loss_G: 0.41524544 Loss_Enh_Dec: 0.03044223\n",
      "| epoch  51 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  3.27 | ppl    26.28 | acc     0.64 | train_ae_norm     1.00\n",
      "[51/200][599/4361] Loss_D: 0.01165046 (Loss_D_real: 0.00898123 Loss_D_fake: 0.00266923) Loss_G: 0.56092280 Loss_Enh_Dec: 0.13196515\n",
      "| epoch  51 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  3.23 | ppl    25.30 | acc     0.63 | train_ae_norm     1.00\n",
      "[51/200][699/4361] Loss_D: 0.00469330 (Loss_D_real: 0.00121968 Loss_D_fake: 0.00347362) Loss_G: 0.43203783 Loss_Enh_Dec: 0.06640283\n",
      "| epoch  51 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  3.27 | ppl    26.38 | acc     0.68 | train_ae_norm     1.00\n",
      "[51/200][799/4361] Loss_D: 0.00478382 (Loss_D_real: 0.00250692 Loss_D_fake: 0.00227689) Loss_G: 0.38305399 Loss_Enh_Dec: 0.06605642\n",
      "| epoch  51 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  3.25 | ppl    25.74 | acc     0.66 | train_ae_norm     1.00\n",
      "[51/200][899/4361] Loss_D: 0.00771820 (Loss_D_real: 0.00589820 Loss_D_fake: 0.00182000) Loss_G: 0.44017702 Loss_Enh_Dec: 0.08423176\n",
      "| epoch  51 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  3.24 | ppl    25.47 | acc     0.66 | train_ae_norm     1.00\n",
      "[51/200][999/4361] Loss_D: 0.00647053 (Loss_D_real: 0.00158598 Loss_D_fake: 0.00488455) Loss_G: 0.34363207 Loss_Enh_Dec: -0.34168130\n",
      "| epoch  51 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  3.22 | ppl    25.14 | acc     0.66 | train_ae_norm     1.00\n",
      "[51/200][1099/4361] Loss_D: 0.00357848 (Loss_D_real: 0.00126192 Loss_D_fake: 0.00231656) Loss_G: 0.38655874 Loss_Enh_Dec: -0.27924487\n",
      "| epoch  51 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.58 | loss  3.23 | ppl    25.18 | acc     0.62 | train_ae_norm     1.00\n",
      "[51/200][1199/4361] Loss_D: 0.00726787 (Loss_D_real: 0.00283497 Loss_D_fake: 0.00443290) Loss_G: 0.60706824 Loss_Enh_Dec: -0.17346595\n",
      "| epoch  51 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.50 | loss  3.23 | ppl    25.31 | acc     0.64 | train_ae_norm     1.00\n",
      "[51/200][1299/4361] Loss_D: 0.01152856 (Loss_D_real: 0.00738922 Loss_D_fake: 0.00413934) Loss_G: 0.40132886 Loss_Enh_Dec: -0.08298139\n",
      "| epoch  51 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.29 | loss  3.25 | ppl    25.78 | acc     0.65 | train_ae_norm     1.00\n",
      "[51/200][1399/4361] Loss_D: 0.00710572 (Loss_D_real: 0.00498034 Loss_D_fake: 0.00212538) Loss_G: 0.37606561 Loss_Enh_Dec: -0.06410141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  51 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.48 | loss  3.24 | ppl    25.65 | acc     0.60 | train_ae_norm     1.00\n",
      "[51/200][1499/4361] Loss_D: 0.00643648 (Loss_D_real: 0.00403745 Loss_D_fake: 0.00239903) Loss_G: 0.53821486 Loss_Enh_Dec: 0.01308637\n",
      "| epoch  51 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  3.29 | ppl    26.87 | acc     0.64 | train_ae_norm     1.00\n",
      "[51/200][1599/4361] Loss_D: 0.01872446 (Loss_D_real: 0.01658557 Loss_D_fake: 0.00213889) Loss_G: 0.39306545 Loss_Enh_Dec: 0.12151837\n",
      "| epoch  51 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  3.26 | ppl    25.96 | acc     0.65 | train_ae_norm     1.00\n",
      "[51/200][1699/4361] Loss_D: 0.00742829 (Loss_D_real: 0.00181661 Loss_D_fake: 0.00561169) Loss_G: 0.53678274 Loss_Enh_Dec: 0.11990204\n",
      "| epoch  51 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.91 | loss  3.23 | ppl    25.38 | acc     0.63 | train_ae_norm     1.00\n",
      "[51/200][1799/4361] Loss_D: 0.01752323 (Loss_D_real: 0.01571866 Loss_D_fake: 0.00180458) Loss_G: 0.39565173 Loss_Enh_Dec: 0.08852482\n",
      "| epoch  51 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  3.23 | ppl    25.15 | acc     0.66 | train_ae_norm     1.00\n",
      "[51/200][1899/4361] Loss_D: 0.07654931 (Loss_D_real: 0.07436489 Loss_D_fake: 0.00218442) Loss_G: 0.41015765 Loss_Enh_Dec: -0.33465868\n",
      "| epoch  51 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.82 | loss  3.28 | ppl    26.66 | acc     0.66 | train_ae_norm     1.00\n",
      "[51/200][1999/4361] Loss_D: 0.00441225 (Loss_D_real: 0.00047586 Loss_D_fake: 0.00393639) Loss_G: 0.39331743 Loss_Enh_Dec: 0.09685642\n",
      "| epoch  51 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  3.23 | ppl    25.19 | acc     0.65 | train_ae_norm     1.00\n",
      "[51/200][2099/4361] Loss_D: 0.00289852 (Loss_D_real: 0.00060373 Loss_D_fake: 0.00229479) Loss_G: 0.43342242 Loss_Enh_Dec: -0.18103945\n",
      "| epoch  51 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.77 | loss  3.26 | ppl    26.07 | acc     0.64 | train_ae_norm     1.00\n",
      "[51/200][2199/4361] Loss_D: 0.00257358 (Loss_D_real: 0.00089020 Loss_D_fake: 0.00168338) Loss_G: 0.40964967 Loss_Enh_Dec: -0.32040450\n",
      "| epoch  51 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  3.40 | ppl    29.99 | acc     0.66 | train_ae_norm     1.00\n",
      "[51/200][2299/4361] Loss_D: 0.00951007 (Loss_D_real: 0.00702184 Loss_D_fake: 0.00248823) Loss_G: 0.39594761 Loss_Enh_Dec: -0.15535095\n",
      "| epoch  51 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  3.23 | ppl    25.34 | acc     0.65 | train_ae_norm     1.00\n",
      "[51/200][2399/4361] Loss_D: 0.00487142 (Loss_D_real: 0.00361966 Loss_D_fake: 0.00125176) Loss_G: 0.41941237 Loss_Enh_Dec: 0.03548834\n",
      "| epoch  51 |  2400/ 4361 batches | lr 0.000000 | ms/batch 400.63 | loss  3.26 | ppl    26.03 | acc     0.61 | train_ae_norm     1.00\n",
      "[51/200][2499/4361] Loss_D: 0.00271881 (Loss_D_real: 0.00046582 Loss_D_fake: 0.00225299) Loss_G: 0.44826880 Loss_Enh_Dec: -0.12224644\n",
      "| epoch  51 |  2500/ 4361 batches | lr 0.000000 | ms/batch 400.40 | loss  3.31 | ppl    27.26 | acc     0.65 | train_ae_norm     1.00\n",
      "[51/200][2599/4361] Loss_D: 0.00313284 (Loss_D_real: 0.00047732 Loss_D_fake: 0.00265552) Loss_G: 0.40377402 Loss_Enh_Dec: -0.04608826\n",
      "| epoch  51 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.73 | loss  3.28 | ppl    26.46 | acc     0.64 | train_ae_norm     1.00\n",
      "[51/200][2699/4361] Loss_D: 0.00223110 (Loss_D_real: 0.00059151 Loss_D_fake: 0.00163958) Loss_G: 0.40567085 Loss_Enh_Dec: -0.24456480\n",
      "| epoch  51 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  3.30 | ppl    27.21 | acc     0.61 | train_ae_norm     1.00\n",
      "[51/200][2799/4361] Loss_D: 0.00870080 (Loss_D_real: 0.00086222 Loss_D_fake: 0.00783858) Loss_G: 0.39837241 Loss_Enh_Dec: -0.15492611\n",
      "| epoch  51 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.56 | loss  3.25 | ppl    25.88 | acc     0.64 | train_ae_norm     1.00\n",
      "[51/200][2899/4361] Loss_D: 0.01684228 (Loss_D_real: 0.00169807 Loss_D_fake: 0.01514420) Loss_G: 0.75788385 Loss_Enh_Dec: -0.44159013\n",
      "| epoch  51 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  3.29 | ppl    26.93 | acc     0.65 | train_ae_norm     1.00\n",
      "[51/200][2999/4361] Loss_D: 0.00477110 (Loss_D_real: 0.00207651 Loss_D_fake: 0.00269459) Loss_G: 0.41054249 Loss_Enh_Dec: -0.26304483\n",
      "| epoch  51 |  3000/ 4361 batches | lr 0.000000 | ms/batch 402.38 | loss  3.30 | ppl    27.06 | acc     0.64 | train_ae_norm     1.00\n",
      "[51/200][3099/4361] Loss_D: 0.10244623 (Loss_D_real: 0.02114090 Loss_D_fake: 0.08130533) Loss_G: 0.85013258 Loss_Enh_Dec: -0.68986201\n",
      "| epoch  51 |  3100/ 4361 batches | lr 0.000000 | ms/batch 402.12 | loss  3.30 | ppl    27.15 | acc     0.62 | train_ae_norm     1.00\n",
      "[51/200][3199/4361] Loss_D: 0.00372901 (Loss_D_real: 0.00126988 Loss_D_fake: 0.00245914) Loss_G: 0.41999951 Loss_Enh_Dec: -1.24637032\n",
      "| epoch  51 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.73 | loss  3.33 | ppl    28.00 | acc     0.64 | train_ae_norm     1.00\n",
      "[51/200][3299/4361] Loss_D: 0.04882003 (Loss_D_real: 0.00259994 Loss_D_fake: 0.04622009) Loss_G: 0.39226642 Loss_Enh_Dec: -0.51102442\n",
      "| epoch  51 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  3.32 | ppl    27.59 | acc     0.64 | train_ae_norm     1.00\n",
      "[51/200][3399/4361] Loss_D: 0.00578860 (Loss_D_real: 0.00395009 Loss_D_fake: 0.00183851) Loss_G: 0.47647905 Loss_Enh_Dec: -1.35939205\n",
      "| epoch  51 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  3.28 | ppl    26.64 | acc     0.65 | train_ae_norm     1.00\n",
      "[51/200][3499/4361] Loss_D: 0.02134748 (Loss_D_real: 0.01709909 Loss_D_fake: 0.00424839) Loss_G: 0.38958290 Loss_Enh_Dec: -1.47743559\n",
      "| epoch  51 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  3.25 | ppl    25.75 | acc     0.63 | train_ae_norm     1.00\n",
      "[51/200][3599/4361] Loss_D: 0.00434704 (Loss_D_real: 0.00083803 Loss_D_fake: 0.00350902) Loss_G: 0.40746814 Loss_Enh_Dec: -1.18762875\n",
      "| epoch  51 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  3.25 | ppl    25.90 | acc     0.66 | train_ae_norm     1.00\n",
      "[51/200][3699/4361] Loss_D: 0.00689570 (Loss_D_real: 0.00533000 Loss_D_fake: 0.00156570) Loss_G: 0.38604188 Loss_Enh_Dec: -1.34235454\n",
      "| epoch  51 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  3.25 | ppl    25.80 | acc     0.61 | train_ae_norm     1.00\n",
      "[51/200][3799/4361] Loss_D: 0.00390321 (Loss_D_real: 0.00094978 Loss_D_fake: 0.00295342) Loss_G: 0.37672290 Loss_Enh_Dec: -1.07184279\n",
      "| epoch  51 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  3.29 | ppl    26.80 | acc     0.66 | train_ae_norm     1.00\n",
      "[51/200][3899/4361] Loss_D: 0.00738736 (Loss_D_real: 0.00576944 Loss_D_fake: 0.00161792) Loss_G: 0.41908717 Loss_Enh_Dec: -1.08349597\n",
      "| epoch  51 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  3.27 | ppl    26.18 | acc     0.61 | train_ae_norm     1.00\n",
      "[51/200][3999/4361] Loss_D: 0.00345125 (Loss_D_real: 0.00043449 Loss_D_fake: 0.00301675) Loss_G: 0.45559669 Loss_Enh_Dec: -0.93730986\n",
      "| epoch  51 |  4000/ 4361 batches | lr 0.000000 | ms/batch 402.34 | loss  3.28 | ppl    26.59 | acc     0.65 | train_ae_norm     1.00\n",
      "[51/200][4099/4361] Loss_D: 0.00881548 (Loss_D_real: 0.00685369 Loss_D_fake: 0.00196179) Loss_G: 0.42826089 Loss_Enh_Dec: -0.92971593\n",
      "| epoch  51 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  3.24 | ppl    25.63 | acc     0.65 | train_ae_norm     1.00\n",
      "[51/200][4199/4361] Loss_D: 0.00682741 (Loss_D_real: 0.00367671 Loss_D_fake: 0.00315069) Loss_G: 0.46139929 Loss_Enh_Dec: -1.22419238\n",
      "| epoch  51 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  3.28 | ppl    26.52 | acc     0.66 | train_ae_norm     1.00\n",
      "[51/200][4299/4361] Loss_D: 0.02372893 (Loss_D_real: 0.01829374 Loss_D_fake: 0.00543519) Loss_G: 0.38098884 Loss_Enh_Dec: -0.85556906\n",
      "| epoch  51 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  3.22 | ppl    25.11 | acc     0.64 | train_ae_norm     1.00\n",
      "| end of epoch  51 | time: 1851.75s | test loss  3.16 | test ppl 23.53 | acc 0.676\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 52 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.705\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.497\n",
      "  Test Loss: 3.898\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  52 |     0/ 4361 batches | lr 0.000000 | ms/batch 865.03 | loss  0.03 | ppl     1.03 | acc     0.67 | train_ae_norm     1.00\n",
      "[52/200][99/4361] Loss_D: 0.00252362 (Loss_D_real: 0.00076508 Loss_D_fake: 0.00175854) Loss_G: 0.41933781 Loss_Enh_Dec: -1.20773733\n",
      "| epoch  52 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  3.23 | ppl    25.22 | acc     0.62 | train_ae_norm     1.00\n",
      "[52/200][199/4361] Loss_D: 0.00766839 (Loss_D_real: 0.00285533 Loss_D_fake: 0.00481306) Loss_G: 0.39111069 Loss_Enh_Dec: -1.16077733\n",
      "| epoch  52 |   200/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  3.26 | ppl    26.17 | acc     0.65 | train_ae_norm     1.00\n",
      "[52/200][299/4361] Loss_D: 0.00371009 (Loss_D_real: 0.00246978 Loss_D_fake: 0.00124032) Loss_G: 0.36652145 Loss_Enh_Dec: -1.31443214\n",
      "| epoch  52 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  3.28 | ppl    26.57 | acc     0.61 | train_ae_norm     1.00\n",
      "[52/200][399/4361] Loss_D: 0.00408418 (Loss_D_real: 0.00138549 Loss_D_fake: 0.00269869) Loss_G: 0.41108170 Loss_Enh_Dec: -1.45693743\n",
      "| epoch  52 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  3.19 | ppl    24.36 | acc     0.65 | train_ae_norm     1.00\n",
      "[52/200][499/4361] Loss_D: 0.00240150 (Loss_D_real: 0.00025781 Loss_D_fake: 0.00214369) Loss_G: 0.38231346 Loss_Enh_Dec: -1.42500007\n",
      "| epoch  52 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  3.26 | ppl    26.04 | acc     0.68 | train_ae_norm     1.00\n",
      "[52/200][599/4361] Loss_D: 0.01168779 (Loss_D_real: 0.00462209 Loss_D_fake: 0.00706570) Loss_G: 0.39749351 Loss_Enh_Dec: -1.21996975\n",
      "| epoch  52 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.81 | loss  3.22 | ppl    25.07 | acc     0.62 | train_ae_norm     1.00\n",
      "[52/200][699/4361] Loss_D: 0.00247909 (Loss_D_real: 0.00035933 Loss_D_fake: 0.00211976) Loss_G: 0.43928844 Loss_Enh_Dec: -1.08633864\n",
      "| epoch  52 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  3.28 | ppl    26.47 | acc     0.65 | train_ae_norm     1.00\n",
      "[52/200][799/4361] Loss_D: 0.00238495 (Loss_D_real: 0.00121952 Loss_D_fake: 0.00116543) Loss_G: 0.38163078 Loss_Enh_Dec: -1.57808566\n",
      "| epoch  52 |   800/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  3.27 | ppl    26.32 | acc     0.66 | train_ae_norm     1.00\n",
      "[52/200][899/4361] Loss_D: 0.00445516 (Loss_D_real: 0.00134917 Loss_D_fake: 0.00310600) Loss_G: 0.42865640 Loss_Enh_Dec: -0.97460616\n",
      "| epoch  52 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  3.29 | ppl    26.76 | acc     0.67 | train_ae_norm     1.00\n",
      "[52/200][999/4361] Loss_D: 0.00530288 (Loss_D_real: 0.00088034 Loss_D_fake: 0.00442254) Loss_G: 0.38353878 Loss_Enh_Dec: -1.17184198\n",
      "| epoch  52 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  3.26 | ppl    26.15 | acc     0.66 | train_ae_norm     1.00\n",
      "[52/200][1099/4361] Loss_D: 0.00306315 (Loss_D_real: 0.00147679 Loss_D_fake: 0.00158636) Loss_G: 0.39112639 Loss_Enh_Dec: -1.15643418\n",
      "| epoch  52 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  3.25 | ppl    25.74 | acc     0.63 | train_ae_norm     1.00\n",
      "[52/200][1199/4361] Loss_D: 0.00422699 (Loss_D_real: 0.00216801 Loss_D_fake: 0.00205898) Loss_G: 0.40141708 Loss_Enh_Dec: -1.38338912\n",
      "| epoch  52 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  3.24 | ppl    25.65 | acc     0.66 | train_ae_norm     1.00\n",
      "[52/200][1299/4361] Loss_D: 0.00424713 (Loss_D_real: 0.00250589 Loss_D_fake: 0.00174124) Loss_G: 0.43862420 Loss_Enh_Dec: -1.45642626\n",
      "| epoch  52 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.93 | loss  3.27 | ppl    26.42 | acc     0.65 | train_ae_norm     1.00\n",
      "[52/200][1399/4361] Loss_D: 0.00279323 (Loss_D_real: 0.00150316 Loss_D_fake: 0.00129007) Loss_G: 0.62641120 Loss_Enh_Dec: -1.17254949\n",
      "| epoch  52 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  3.29 | ppl    26.77 | acc     0.60 | train_ae_norm     1.00\n",
      "[52/200][1499/4361] Loss_D: 0.00247000 (Loss_D_real: 0.00104031 Loss_D_fake: 0.00142969) Loss_G: 0.40549326 Loss_Enh_Dec: -1.56004596\n",
      "| epoch  52 |  1500/ 4361 batches | lr 0.000000 | ms/batch 400.90 | loss  3.30 | ppl    27.00 | acc     0.63 | train_ae_norm     1.00\n",
      "[52/200][1599/4361] Loss_D: 0.00799571 (Loss_D_real: 0.00610792 Loss_D_fake: 0.00188779) Loss_G: 0.35929415 Loss_Enh_Dec: -1.25397873\n",
      "| epoch  52 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  3.26 | ppl    26.12 | acc     0.63 | train_ae_norm     1.00\n",
      "[52/200][1699/4361] Loss_D: 0.00434327 (Loss_D_real: 0.00293376 Loss_D_fake: 0.00140952) Loss_G: 0.41779682 Loss_Enh_Dec: -0.98226494\n",
      "| epoch  52 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  3.24 | ppl    25.60 | acc     0.62 | train_ae_norm     1.00\n",
      "[52/200][1799/4361] Loss_D: 0.02999719 (Loss_D_real: 0.02898093 Loss_D_fake: 0.00101626) Loss_G: 0.38182208 Loss_Enh_Dec: -1.35723460\n",
      "| epoch  52 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  3.22 | ppl    25.06 | acc     0.64 | train_ae_norm     1.00\n",
      "[52/200][1899/4361] Loss_D: 0.00434824 (Loss_D_real: 0.00326895 Loss_D_fake: 0.00107930) Loss_G: 0.49886122 Loss_Enh_Dec: -1.34935331\n",
      "| epoch  52 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.81 | loss  3.30 | ppl    27.22 | acc     0.64 | train_ae_norm     1.00\n",
      "[52/200][1999/4361] Loss_D: 0.03167094 (Loss_D_real: 0.02738499 Loss_D_fake: 0.00428595) Loss_G: 0.38191637 Loss_Enh_Dec: -1.14148772\n",
      "| epoch  52 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  3.24 | ppl    25.46 | acc     0.63 | train_ae_norm     1.00\n",
      "[52/200][2099/4361] Loss_D: 0.00582686 (Loss_D_real: 0.00273707 Loss_D_fake: 0.00308979) Loss_G: 0.41193506 Loss_Enh_Dec: -1.11035526\n",
      "| epoch  52 |  2100/ 4361 batches | lr 0.000000 | ms/batch 402.04 | loss  3.26 | ppl    25.94 | acc     0.63 | train_ae_norm     1.00\n",
      "[52/200][2199/4361] Loss_D: 0.01209080 (Loss_D_real: 0.00834981 Loss_D_fake: 0.00374099) Loss_G: 0.38880658 Loss_Enh_Dec: -1.78096926\n",
      "| epoch  52 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.91 | loss  3.23 | ppl    25.33 | acc     0.65 | train_ae_norm     1.00\n",
      "[52/200][2299/4361] Loss_D: 0.00250292 (Loss_D_real: 0.00100276 Loss_D_fake: 0.00150016) Loss_G: 0.38236481 Loss_Enh_Dec: -1.66519034\n",
      "| epoch  52 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  3.23 | ppl    25.28 | acc     0.67 | train_ae_norm     1.00\n",
      "[52/200][2399/4361] Loss_D: 0.00342303 (Loss_D_real: 0.00177869 Loss_D_fake: 0.00164434) Loss_G: 0.41732150 Loss_Enh_Dec: -1.13256967\n",
      "| epoch  52 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  3.23 | ppl    25.28 | acc     0.62 | train_ae_norm     1.00\n",
      "[52/200][2499/4361] Loss_D: 0.01012680 (Loss_D_real: 0.00834034 Loss_D_fake: 0.00178647) Loss_G: 0.39197198 Loss_Enh_Dec: -1.30252075\n",
      "| epoch  52 |  2500/ 4361 batches | lr 0.000000 | ms/batch 400.86 | loss  3.28 | ppl    26.68 | acc     0.64 | train_ae_norm     1.00\n",
      "[52/200][2599/4361] Loss_D: 0.00700062 (Loss_D_real: 0.00203520 Loss_D_fake: 0.00496542) Loss_G: 0.39757273 Loss_Enh_Dec: -0.97435057\n",
      "| epoch  52 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  3.25 | ppl    25.77 | acc     0.62 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52/200][2699/4361] Loss_D: 0.00429599 (Loss_D_real: 0.00182323 Loss_D_fake: 0.00247276) Loss_G: 0.39282814 Loss_Enh_Dec: -1.72062337\n",
      "| epoch  52 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  3.27 | ppl    26.27 | acc     0.63 | train_ae_norm     1.00\n",
      "[52/200][2799/4361] Loss_D: 0.00739164 (Loss_D_real: 0.00495200 Loss_D_fake: 0.00243964) Loss_G: 0.37300774 Loss_Enh_Dec: -0.91606158\n",
      "| epoch  52 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  3.24 | ppl    25.49 | acc     0.64 | train_ae_norm     1.00\n",
      "[52/200][2899/4361] Loss_D: 0.01272603 (Loss_D_real: 0.01097981 Loss_D_fake: 0.00174622) Loss_G: 0.39592788 Loss_Enh_Dec: -1.32500494\n",
      "| epoch  52 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  3.26 | ppl    26.03 | acc     0.64 | train_ae_norm     1.00\n",
      "[52/200][2999/4361] Loss_D: 0.00700819 (Loss_D_real: 0.00494319 Loss_D_fake: 0.00206500) Loss_G: 0.41224805 Loss_Enh_Dec: -1.01802158\n",
      "| epoch  52 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  3.28 | ppl    26.60 | acc     0.65 | train_ae_norm     1.00\n",
      "[52/200][3099/4361] Loss_D: 0.01870201 (Loss_D_real: 0.00272800 Loss_D_fake: 0.01597401) Loss_G: 0.38095167 Loss_Enh_Dec: -1.18532336\n",
      "| epoch  52 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  3.27 | ppl    26.22 | acc     0.62 | train_ae_norm     1.00\n",
      "[52/200][3199/4361] Loss_D: 0.00579562 (Loss_D_real: 0.00476987 Loss_D_fake: 0.00102575) Loss_G: 0.44465819 Loss_Enh_Dec: -1.36470640\n",
      "| epoch  52 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  3.30 | ppl    27.08 | acc     0.65 | train_ae_norm     1.00\n",
      "[52/200][3299/4361] Loss_D: 0.00432662 (Loss_D_real: 0.00084383 Loss_D_fake: 0.00348280) Loss_G: 0.41097885 Loss_Enh_Dec: -1.42608929\n",
      "| epoch  52 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  3.32 | ppl    27.65 | acc     0.66 | train_ae_norm     1.00\n",
      "[52/200][3399/4361] Loss_D: 0.00286790 (Loss_D_real: 0.00131906 Loss_D_fake: 0.00154884) Loss_G: 0.43236953 Loss_Enh_Dec: -1.54280841\n",
      "| epoch  52 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  3.28 | ppl    26.56 | acc     0.64 | train_ae_norm     1.00\n",
      "[52/200][3499/4361] Loss_D: 0.00303033 (Loss_D_real: 0.00078419 Loss_D_fake: 0.00224614) Loss_G: 0.48236951 Loss_Enh_Dec: -1.02581787\n",
      "| epoch  52 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  3.24 | ppl    25.55 | acc     0.65 | train_ae_norm     1.00\n",
      "[52/200][3599/4361] Loss_D: 0.01128601 (Loss_D_real: 0.00561476 Loss_D_fake: 0.00567125) Loss_G: 0.42262584 Loss_Enh_Dec: -1.41727352\n",
      "| epoch  52 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  3.27 | ppl    26.22 | acc     0.64 | train_ae_norm     1.00\n",
      "[52/200][3699/4361] Loss_D: 0.00322687 (Loss_D_real: 0.00187404 Loss_D_fake: 0.00135283) Loss_G: 0.42074805 Loss_Enh_Dec: -1.29030263\n",
      "| epoch  52 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  3.28 | ppl    26.50 | acc     0.61 | train_ae_norm     1.00\n",
      "[52/200][3799/4361] Loss_D: 0.00618157 (Loss_D_real: 0.00137723 Loss_D_fake: 0.00480434) Loss_G: 0.39901352 Loss_Enh_Dec: -0.99135745\n",
      "| epoch  52 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  3.30 | ppl    27.01 | acc     0.68 | train_ae_norm     1.00\n",
      "[52/200][3899/4361] Loss_D: 0.00332108 (Loss_D_real: 0.00087531 Loss_D_fake: 0.00244576) Loss_G: 0.42032573 Loss_Enh_Dec: -1.31283987\n",
      "| epoch  52 |  3900/ 4361 batches | lr 0.000000 | ms/batch 402.47 | loss  3.26 | ppl    25.96 | acc     0.58 | train_ae_norm     1.00\n",
      "[52/200][3999/4361] Loss_D: 0.01707344 (Loss_D_real: 0.01648490 Loss_D_fake: 0.00058854) Loss_G: 0.46445557 Loss_Enh_Dec: -1.28344452\n",
      "| epoch  52 |  4000/ 4361 batches | lr 0.000000 | ms/batch 402.15 | loss  3.29 | ppl    26.86 | acc     0.63 | train_ae_norm     1.00\n",
      "[52/200][4099/4361] Loss_D: 0.00693467 (Loss_D_real: 0.00477462 Loss_D_fake: 0.00216005) Loss_G: 0.40625173 Loss_Enh_Dec: -1.13774633\n",
      "| epoch  52 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  3.23 | ppl    25.25 | acc     0.63 | train_ae_norm     1.00\n",
      "[52/200][4199/4361] Loss_D: 0.00315806 (Loss_D_real: 0.00166786 Loss_D_fake: 0.00149020) Loss_G: 0.41801366 Loss_Enh_Dec: -1.06583941\n",
      "| epoch  52 |  4200/ 4361 batches | lr 0.000000 | ms/batch 402.18 | loss  3.29 | ppl    26.84 | acc     0.66 | train_ae_norm     1.00\n",
      "[52/200][4299/4361] Loss_D: 0.00122027 (Loss_D_real: 0.00017820 Loss_D_fake: 0.00104207) Loss_G: 0.40055257 Loss_Enh_Dec: -1.44683897\n",
      "| epoch  52 |  4300/ 4361 batches | lr 0.000000 | ms/batch 402.62 | loss  3.22 | ppl    25.11 | acc     0.66 | train_ae_norm     1.00\n",
      "| end of epoch  52 | time: 1853.23s | test loss  3.17 | test ppl 23.73 | acc 0.675\n",
      "bleu_self:  [1.29340278e-01 3.66646568e-09 1.83558689e-11 3.57631855e-11\n",
      " 1.02894648e-10]\n",
      "bleu_test:  [6.70691288e-01 1.95605671e-01 1.38379406e-06 4.51466692e-09\n",
      " 2.10073390e-09]\n",
      "bleu_self: [0.12934028,0.00000000,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.67069129,0.19560567,0.00000138,0.00000000,0.00000000]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 53 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:57.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:16.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.709\n",
      "  Training epcoh took: 0:01:27\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.492\n",
      "  Test Loss: 3.885\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  53 |     0/ 4361 batches | lr 0.000000 | ms/batch 870.15 | loss  0.03 | ppl     1.03 | acc     0.69 | train_ae_norm     1.00\n",
      "[53/200][99/4361] Loss_D: 0.00254986 (Loss_D_real: 0.00114218 Loss_D_fake: 0.00140768) Loss_G: 0.42311963 Loss_Enh_Dec: -1.71112621\n",
      "| epoch  53 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.61 | loss  3.27 | ppl    26.30 | acc     0.62 | train_ae_norm     1.00\n",
      "[53/200][199/4361] Loss_D: 0.00708631 (Loss_D_real: 0.00461250 Loss_D_fake: 0.00247381) Loss_G: 0.42357883 Loss_Enh_Dec: -1.50421703\n",
      "| epoch  53 |   200/ 4361 batches | lr 0.000000 | ms/batch 402.03 | loss  3.32 | ppl    27.77 | acc     0.64 | train_ae_norm     1.00\n",
      "[53/200][299/4361] Loss_D: 0.00259027 (Loss_D_real: 0.00146304 Loss_D_fake: 0.00112723) Loss_G: 0.39299330 Loss_Enh_Dec: -1.73267519\n",
      "| epoch  53 |   300/ 4361 batches | lr 0.000000 | ms/batch 402.44 | loss  3.28 | ppl    26.60 | acc     0.62 | train_ae_norm     1.00\n",
      "[53/200][399/4361] Loss_D: 0.00180418 (Loss_D_real: 0.00067917 Loss_D_fake: 0.00112500) Loss_G: 0.40189224 Loss_Enh_Dec: -1.38466573\n",
      "| epoch  53 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  3.19 | ppl    24.25 | acc     0.65 | train_ae_norm     1.00\n",
      "[53/200][499/4361] Loss_D: 0.00555920 (Loss_D_real: 0.00020070 Loss_D_fake: 0.00535850) Loss_G: 0.41210890 Loss_Enh_Dec: -1.53558278\n",
      "| epoch  53 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  3.26 | ppl    26.09 | acc     0.67 | train_ae_norm     1.00\n",
      "[53/200][599/4361] Loss_D: 0.00399808 (Loss_D_real: 0.00246463 Loss_D_fake: 0.00153345) Loss_G: 0.51800853 Loss_Enh_Dec: -1.14798009\n",
      "| epoch  53 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  3.23 | ppl    25.21 | acc     0.62 | train_ae_norm     1.00\n",
      "[53/200][699/4361] Loss_D: 0.00995787 (Loss_D_real: 0.00312816 Loss_D_fake: 0.00682971) Loss_G: 0.46717793 Loss_Enh_Dec: -1.09942758\n",
      "| epoch  53 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  3.26 | ppl    26.04 | acc     0.64 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53/200][799/4361] Loss_D: 0.01524338 (Loss_D_real: 0.01483469 Loss_D_fake: 0.00040868) Loss_G: 0.57671136 Loss_Enh_Dec: -1.01983643\n",
      "| epoch  53 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  3.24 | ppl    25.65 | acc     0.65 | train_ae_norm     1.00\n",
      "[53/200][899/4361] Loss_D: 0.00475585 (Loss_D_real: 0.00266510 Loss_D_fake: 0.00209075) Loss_G: 0.39431027 Loss_Enh_Dec: -1.37637961\n",
      "| epoch  53 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  3.26 | ppl    25.96 | acc     0.66 | train_ae_norm     1.00\n",
      "[53/200][999/4361] Loss_D: 0.00206047 (Loss_D_real: 0.00111900 Loss_D_fake: 0.00094148) Loss_G: 0.38776454 Loss_Enh_Dec: -1.33906817\n",
      "| epoch  53 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  3.22 | ppl    25.09 | acc     0.66 | train_ae_norm     1.00\n",
      "[53/200][1099/4361] Loss_D: 0.00793754 (Loss_D_real: 0.00633879 Loss_D_fake: 0.00159875) Loss_G: 0.63079357 Loss_Enh_Dec: -1.31738007\n",
      "| epoch  53 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  3.22 | ppl    24.91 | acc     0.62 | train_ae_norm     1.00\n",
      "[53/200][1199/4361] Loss_D: 0.01020782 (Loss_D_real: 0.00121405 Loss_D_fake: 0.00899376) Loss_G: 0.46331865 Loss_Enh_Dec: -1.12546873\n",
      "| epoch  53 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  3.24 | ppl    25.62 | acc     0.66 | train_ae_norm     1.00\n",
      "[53/200][1299/4361] Loss_D: 0.00516559 (Loss_D_real: 0.00124376 Loss_D_fake: 0.00392183) Loss_G: 0.48394853 Loss_Enh_Dec: -1.33737183\n",
      "| epoch  53 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  3.25 | ppl    25.80 | acc     0.63 | train_ae_norm     1.00\n",
      "[53/200][1399/4361] Loss_D: 0.00322137 (Loss_D_real: 0.00181825 Loss_D_fake: 0.00140313) Loss_G: 0.44389626 Loss_Enh_Dec: -1.45202208\n",
      "| epoch  53 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  3.23 | ppl    25.29 | acc     0.60 | train_ae_norm     1.00\n",
      "[53/200][1499/4361] Loss_D: 0.00371678 (Loss_D_real: 0.00080041 Loss_D_fake: 0.00291638) Loss_G: 0.44933882 Loss_Enh_Dec: -1.33243740\n",
      "| epoch  53 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  3.28 | ppl    26.65 | acc     0.64 | train_ae_norm     1.00\n",
      "[53/200][1599/4361] Loss_D: 0.00321699 (Loss_D_real: 0.00108858 Loss_D_fake: 0.00212841) Loss_G: 0.42071658 Loss_Enh_Dec: -1.08933628\n",
      "| epoch  53 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  3.25 | ppl    25.67 | acc     0.64 | train_ae_norm     1.00\n",
      "[53/200][1699/4361] Loss_D: 0.00195475 (Loss_D_real: 0.00108603 Loss_D_fake: 0.00086872) Loss_G: 0.42631194 Loss_Enh_Dec: -1.44038105\n",
      "| epoch  53 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  3.23 | ppl    25.16 | acc     0.66 | train_ae_norm     1.00\n",
      "[53/200][1799/4361] Loss_D: 0.00834843 (Loss_D_real: 0.00320411 Loss_D_fake: 0.00514432) Loss_G: 0.39397693 Loss_Enh_Dec: -1.17596495\n",
      "| epoch  53 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  3.20 | ppl    24.61 | acc     0.66 | train_ae_norm     1.00\n",
      "[53/200][1899/4361] Loss_D: 0.00270181 (Loss_D_real: 0.00154700 Loss_D_fake: 0.00115482) Loss_G: 0.47779790 Loss_Enh_Dec: -1.19018686\n",
      "| epoch  53 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  3.29 | ppl    26.74 | acc     0.64 | train_ae_norm     1.00\n",
      "[53/200][1999/4361] Loss_D: 0.00185396 (Loss_D_real: 0.00104897 Loss_D_fake: 0.00080499) Loss_G: 0.42726222 Loss_Enh_Dec: -1.31925273\n",
      "| epoch  53 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  3.24 | ppl    25.62 | acc     0.66 | train_ae_norm     1.00\n",
      "[53/200][2099/4361] Loss_D: 0.00211978 (Loss_D_real: 0.00042209 Loss_D_fake: 0.00169769) Loss_G: 0.45216829 Loss_Enh_Dec: -1.02141905\n",
      "| epoch  53 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  3.25 | ppl    25.85 | acc     0.65 | train_ae_norm     1.00\n",
      "[53/200][2199/4361] Loss_D: 0.00202406 (Loss_D_real: 0.00114083 Loss_D_fake: 0.00088322) Loss_G: 0.48432237 Loss_Enh_Dec: -1.54408765\n",
      "| epoch  53 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  3.24 | ppl    25.52 | acc     0.67 | train_ae_norm     1.00\n",
      "[53/200][2299/4361] Loss_D: 0.00144875 (Loss_D_real: 0.00073142 Loss_D_fake: 0.00071733) Loss_G: 0.39930600 Loss_Enh_Dec: -1.37981069\n",
      "| epoch  53 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  3.22 | ppl    25.01 | acc     0.67 | train_ae_norm     1.00\n",
      "[53/200][2399/4361] Loss_D: 0.00177431 (Loss_D_real: 0.00052730 Loss_D_fake: 0.00124701) Loss_G: 0.42912626 Loss_Enh_Dec: -1.63430107\n",
      "| epoch  53 |  2400/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  3.24 | ppl    25.44 | acc     0.61 | train_ae_norm     1.00\n",
      "[53/200][2499/4361] Loss_D: 0.00810505 (Loss_D_real: 0.00641911 Loss_D_fake: 0.00168594) Loss_G: 0.40619573 Loss_Enh_Dec: -1.43285453\n",
      "| epoch  53 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  3.27 | ppl    26.42 | acc     0.64 | train_ae_norm     1.00\n",
      "[53/200][2599/4361] Loss_D: 0.00234580 (Loss_D_real: 0.00135990 Loss_D_fake: 0.00098591) Loss_G: 0.46221718 Loss_Enh_Dec: -1.40208590\n",
      "| epoch  53 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  3.24 | ppl    25.61 | acc     0.62 | train_ae_norm     1.00\n",
      "[53/200][2699/4361] Loss_D: 0.00490751 (Loss_D_real: 0.00029620 Loss_D_fake: 0.00461131) Loss_G: 0.43886995 Loss_Enh_Dec: -1.23730659\n",
      "| epoch  53 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  3.26 | ppl    25.97 | acc     0.66 | train_ae_norm     1.00\n",
      "[53/200][2799/4361] Loss_D: 0.00300643 (Loss_D_real: 0.00064942 Loss_D_fake: 0.00235700) Loss_G: 0.41812426 Loss_Enh_Dec: -1.10378826\n",
      "| epoch  53 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  3.22 | ppl    24.97 | acc     0.63 | train_ae_norm     1.00\n",
      "[53/200][2899/4361] Loss_D: 0.00248473 (Loss_D_real: 0.00053225 Loss_D_fake: 0.00195248) Loss_G: 0.45918486 Loss_Enh_Dec: -1.55650699\n",
      "| epoch  53 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  3.25 | ppl    25.81 | acc     0.63 | train_ae_norm     1.00\n",
      "[53/200][2999/4361] Loss_D: 0.00169309 (Loss_D_real: 0.00094126 Loss_D_fake: 0.00075183) Loss_G: 0.41436586 Loss_Enh_Dec: -1.77326202\n",
      "| epoch  53 |  3000/ 4361 batches | lr 0.000000 | ms/batch 417.88 | loss  3.26 | ppl    26.00 | acc     0.66 | train_ae_norm     1.00\n",
      "[53/200][3099/4361] Loss_D: 0.00538281 (Loss_D_real: 0.00454238 Loss_D_fake: 0.00084043) Loss_G: 0.43856469 Loss_Enh_Dec: -1.43123758\n",
      "| epoch  53 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  3.25 | ppl    25.69 | acc     0.64 | train_ae_norm     1.00\n",
      "[53/200][3199/4361] Loss_D: 0.00240242 (Loss_D_real: 0.00031904 Loss_D_fake: 0.00208338) Loss_G: 0.41093332 Loss_Enh_Dec: -1.77758873\n",
      "| epoch  53 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  3.27 | ppl    26.43 | acc     0.67 | train_ae_norm     1.00\n",
      "[53/200][3299/4361] Loss_D: 0.00613326 (Loss_D_real: 0.00392473 Loss_D_fake: 0.00220853) Loss_G: 0.41797182 Loss_Enh_Dec: -1.19731510\n",
      "| epoch  53 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  3.29 | ppl    26.75 | acc     0.63 | train_ae_norm     1.00\n",
      "[53/200][3399/4361] Loss_D: 0.00710369 (Loss_D_real: 0.00577106 Loss_D_fake: 0.00133263) Loss_G: 0.47492859 Loss_Enh_Dec: -1.39085877\n",
      "| epoch  53 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  3.25 | ppl    25.82 | acc     0.64 | train_ae_norm     1.00\n",
      "[53/200][3499/4361] Loss_D: 0.00931289 (Loss_D_real: 0.00276343 Loss_D_fake: 0.00654946) Loss_G: 0.44020367 Loss_Enh_Dec: -1.74422133\n",
      "| epoch  53 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  3.22 | ppl    25.08 | acc     0.64 | train_ae_norm     1.00\n",
      "[53/200][3599/4361] Loss_D: 0.00222776 (Loss_D_real: 0.00064528 Loss_D_fake: 0.00158249) Loss_G: 0.54390287 Loss_Enh_Dec: -1.70834196\n",
      "| epoch  53 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  3.23 | ppl    25.37 | acc     0.67 | train_ae_norm     1.00\n",
      "[53/200][3699/4361] Loss_D: 0.00556855 (Loss_D_real: 0.00338402 Loss_D_fake: 0.00218452) Loss_G: 0.45232794 Loss_Enh_Dec: -1.73932481\n",
      "| epoch  53 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  3.25 | ppl    25.89 | acc     0.62 | train_ae_norm     1.00\n",
      "[53/200][3799/4361] Loss_D: 0.02975113 (Loss_D_real: 0.00699826 Loss_D_fake: 0.02275287) Loss_G: 0.20479870 Loss_Enh_Dec: -1.11777043\n",
      "| epoch  53 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  3.30 | ppl    27.15 | acc     0.69 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53/200][3899/4361] Loss_D: 0.01735785 (Loss_D_real: 0.00644887 Loss_D_fake: 0.01090898) Loss_G: 0.22853903 Loss_Enh_Dec: -1.03782594\n",
      "| epoch  53 |  3900/ 4361 batches | lr 0.000000 | ms/batch 402.16 | loss  3.30 | ppl    27.02 | acc     0.61 | train_ae_norm     1.00\n",
      "[53/200][3999/4361] Loss_D: 0.01377275 (Loss_D_real: 0.00626285 Loss_D_fake: 0.00750990) Loss_G: 0.24679565 Loss_Enh_Dec: -0.90859145\n",
      "| epoch  53 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  3.36 | ppl    28.67 | acc     0.63 | train_ae_norm     1.00\n",
      "[53/200][4099/4361] Loss_D: 0.01399714 (Loss_D_real: 0.00527855 Loss_D_fake: 0.00871859) Loss_G: 0.24372044 Loss_Enh_Dec: -0.84208411\n",
      "| epoch  53 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  3.24 | ppl    25.51 | acc     0.63 | train_ae_norm     1.00\n",
      "[53/200][4199/4361] Loss_D: 0.01152775 (Loss_D_real: 0.00384109 Loss_D_fake: 0.00768667) Loss_G: 0.24554686 Loss_Enh_Dec: -1.02188480\n",
      "| epoch  53 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  3.27 | ppl    26.25 | acc     0.66 | train_ae_norm     1.00\n",
      "[53/200][4299/4361] Loss_D: 0.00787291 (Loss_D_real: 0.00187229 Loss_D_fake: 0.00600062) Loss_G: 0.26227114 Loss_Enh_Dec: -1.33700454\n",
      "| epoch  53 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  3.22 | ppl    24.97 | acc     0.66 | train_ae_norm     1.00\n",
      "| end of epoch  53 | time: 1855.69s | test loss  3.22 | test ppl 24.99 | acc 0.665\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 54 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:57.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.704\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.487\n",
      "  Test Loss: 3.934\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  54 |     0/ 4361 batches | lr 0.000000 | ms/batch 868.36 | loss  0.03 | ppl     1.03 | acc     0.66 | train_ae_norm     1.00\n",
      "[54/200][99/4361] Loss_D: 0.00964554 (Loss_D_real: 0.00442612 Loss_D_fake: 0.00521942) Loss_G: 0.26273116 Loss_Enh_Dec: -0.74308628\n",
      "| epoch  54 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.56 | loss  3.25 | ppl    25.90 | acc     0.61 | train_ae_norm     1.00\n",
      "[54/200][199/4361] Loss_D: 0.01643783 (Loss_D_real: 0.00120633 Loss_D_fake: 0.01523151) Loss_G: 0.26971921 Loss_Enh_Dec: -1.08746743\n",
      "| epoch  54 |   200/ 4361 batches | lr 0.000000 | ms/batch 402.23 | loss  3.26 | ppl    25.96 | acc     0.66 | train_ae_norm     1.00\n",
      "[54/200][299/4361] Loss_D: 0.01715685 (Loss_D_real: 0.01097786 Loss_D_fake: 0.00617899) Loss_G: 0.27186474 Loss_Enh_Dec: -1.47140431\n",
      "| epoch  54 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  3.26 | ppl    26.02 | acc     0.63 | train_ae_norm     1.00\n",
      "[54/200][399/4361] Loss_D: 0.00636140 (Loss_D_real: 0.00093307 Loss_D_fake: 0.00542833) Loss_G: 0.27110347 Loss_Enh_Dec: -1.31768024\n",
      "| epoch  54 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  3.16 | ppl    23.47 | acc     0.65 | train_ae_norm     1.00\n",
      "[54/200][499/4361] Loss_D: 0.00585963 (Loss_D_real: 0.00112461 Loss_D_fake: 0.00473502) Loss_G: 0.27132964 Loss_Enh_Dec: -1.09577298\n",
      "| epoch  54 |   500/ 4361 batches | lr 0.000000 | ms/batch 402.48 | loss  3.24 | ppl    25.44 | acc     0.67 | train_ae_norm     1.00\n",
      "[54/200][599/4361] Loss_D: 0.00839308 (Loss_D_real: 0.00376693 Loss_D_fake: 0.00462615) Loss_G: 0.27222145 Loss_Enh_Dec: -1.52499855\n",
      "| epoch  54 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  3.20 | ppl    24.43 | acc     0.63 | train_ae_norm     1.00\n",
      "[54/200][699/4361] Loss_D: 0.00728431 (Loss_D_real: 0.00300923 Loss_D_fake: 0.00427508) Loss_G: 0.27784505 Loss_Enh_Dec: -1.31156826\n",
      "| epoch  54 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  3.24 | ppl    25.46 | acc     0.65 | train_ae_norm     1.00\n",
      "[54/200][799/4361] Loss_D: 0.02887948 (Loss_D_real: 0.02523867 Loss_D_fake: 0.00364082) Loss_G: 0.32670972 Loss_Enh_Dec: -1.39072096\n",
      "| epoch  54 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  3.21 | ppl    24.71 | acc     0.66 | train_ae_norm     1.00\n",
      "[54/200][899/4361] Loss_D: 0.01414850 (Loss_D_real: 0.00946281 Loss_D_fake: 0.00468568) Loss_G: 0.29945976 Loss_Enh_Dec: -1.51386786\n",
      "| epoch  54 |   900/ 4361 batches | lr 0.000000 | ms/batch 402.17 | loss  3.23 | ppl    25.28 | acc     0.65 | train_ae_norm     1.00\n",
      "[54/200][999/4361] Loss_D: 0.00408338 (Loss_D_real: 0.00140190 Loss_D_fake: 0.00268147) Loss_G: 0.31710601 Loss_Enh_Dec: -1.49875927\n",
      "| epoch  54 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  3.21 | ppl    24.71 | acc     0.65 | train_ae_norm     1.00\n",
      "[54/200][1099/4361] Loss_D: 0.00472843 (Loss_D_real: 0.00266282 Loss_D_fake: 0.00206561) Loss_G: 0.29811507 Loss_Enh_Dec: -1.54940450\n",
      "| epoch  54 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  3.21 | ppl    24.69 | acc     0.63 | train_ae_norm     1.00\n",
      "[54/200][1199/4361] Loss_D: 0.00857114 (Loss_D_real: 0.00634099 Loss_D_fake: 0.00223015) Loss_G: 0.32982787 Loss_Enh_Dec: -1.47468626\n",
      "| epoch  54 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  3.22 | ppl    25.11 | acc     0.65 | train_ae_norm     1.00\n",
      "[54/200][1299/4361] Loss_D: 0.01241748 (Loss_D_real: 0.00994048 Loss_D_fake: 0.00247700) Loss_G: 0.30434370 Loss_Enh_Dec: -1.70456505\n",
      "| epoch  54 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  3.23 | ppl    25.27 | acc     0.64 | train_ae_norm     1.00\n",
      "[54/200][1399/4361] Loss_D: 0.00471096 (Loss_D_real: 0.00105607 Loss_D_fake: 0.00365490) Loss_G: 0.29891434 Loss_Enh_Dec: -1.50720632\n",
      "| epoch  54 |  1400/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  3.22 | ppl    25.04 | acc     0.61 | train_ae_norm     1.00\n",
      "[54/200][1499/4361] Loss_D: 0.00531683 (Loss_D_real: 0.00269327 Loss_D_fake: 0.00262356) Loss_G: 0.30316046 Loss_Enh_Dec: -1.68956745\n",
      "| epoch  54 |  1500/ 4361 batches | lr 0.000000 | ms/batch 402.47 | loss  3.27 | ppl    26.20 | acc     0.64 | train_ae_norm     1.00\n",
      "[54/200][1599/4361] Loss_D: 0.00625513 (Loss_D_real: 0.00370964 Loss_D_fake: 0.00254548) Loss_G: 0.33101842 Loss_Enh_Dec: -1.24573851\n",
      "| epoch  54 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  3.24 | ppl    25.52 | acc     0.63 | train_ae_norm     1.00\n",
      "[54/200][1699/4361] Loss_D: 0.00320610 (Loss_D_real: 0.00064541 Loss_D_fake: 0.00256068) Loss_G: 0.30349454 Loss_Enh_Dec: -1.29191005\n",
      "| epoch  54 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  3.22 | ppl    25.14 | acc     0.63 | train_ae_norm     1.00\n",
      "[54/200][1799/4361] Loss_D: 0.00938790 (Loss_D_real: 0.00560442 Loss_D_fake: 0.00378347) Loss_G: 0.30548543 Loss_Enh_Dec: -1.50314474\n",
      "| epoch  54 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  3.22 | ppl    25.04 | acc     0.66 | train_ae_norm     1.00\n",
      "[54/200][1899/4361] Loss_D: 0.00670337 (Loss_D_real: 0.00406140 Loss_D_fake: 0.00264198) Loss_G: 0.32745847 Loss_Enh_Dec: -1.57066000\n",
      "| epoch  54 |  1900/ 4361 batches | lr 0.000000 | ms/batch 402.54 | loss  3.29 | ppl    26.79 | acc     0.66 | train_ae_norm     1.00\n",
      "[54/200][1999/4361] Loss_D: 0.00381793 (Loss_D_real: 0.00156150 Loss_D_fake: 0.00225643) Loss_G: 0.32882297 Loss_Enh_Dec: -1.69373512\n",
      "| epoch  54 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  3.22 | ppl    25.08 | acc     0.66 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54/200][2099/4361] Loss_D: 0.00695276 (Loss_D_real: 0.00433384 Loss_D_fake: 0.00261892) Loss_G: 0.31013256 Loss_Enh_Dec: -1.23764932\n",
      "| epoch  54 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  3.24 | ppl    25.50 | acc     0.64 | train_ae_norm     1.00\n",
      "[54/200][2199/4361] Loss_D: 0.00627827 (Loss_D_real: 0.00393956 Loss_D_fake: 0.00233870) Loss_G: 0.32326332 Loss_Enh_Dec: -1.32910037\n",
      "| epoch  54 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  3.22 | ppl    25.01 | acc     0.65 | train_ae_norm     1.00\n",
      "[54/200][2299/4361] Loss_D: 0.00579194 (Loss_D_real: 0.00291272 Loss_D_fake: 0.00287922) Loss_G: 0.31503335 Loss_Enh_Dec: -1.41293013\n",
      "| epoch  54 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  3.20 | ppl    24.56 | acc     0.66 | train_ae_norm     1.00\n",
      "[54/200][2399/4361] Loss_D: 0.00801863 (Loss_D_real: 0.00450868 Loss_D_fake: 0.00350995) Loss_G: 0.29598260 Loss_Enh_Dec: -1.71651733\n",
      "| epoch  54 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  3.24 | ppl    25.50 | acc     0.62 | train_ae_norm     1.00\n",
      "[54/200][2499/4361] Loss_D: 0.00482266 (Loss_D_real: 0.00136700 Loss_D_fake: 0.00345566) Loss_G: 0.30115336 Loss_Enh_Dec: -1.65586364\n",
      "| epoch  54 |  2500/ 4361 batches | lr 0.000000 | ms/batch 402.39 | loss  3.27 | ppl    26.34 | acc     0.65 | train_ae_norm     1.00\n",
      "[54/200][2599/4361] Loss_D: 0.00424294 (Loss_D_real: 0.00122662 Loss_D_fake: 0.00301632) Loss_G: 0.30911335 Loss_Enh_Dec: -1.59104979\n",
      "| epoch  54 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  3.24 | ppl    25.64 | acc     0.62 | train_ae_norm     1.00\n",
      "[54/200][2699/4361] Loss_D: 0.00458095 (Loss_D_real: 0.00059079 Loss_D_fake: 0.00399017) Loss_G: 0.31558400 Loss_Enh_Dec: -1.34790266\n",
      "| epoch  54 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  3.27 | ppl    26.32 | acc     0.62 | train_ae_norm     1.00\n",
      "[54/200][2799/4361] Loss_D: 0.01650476 (Loss_D_real: 0.01365590 Loss_D_fake: 0.00284887) Loss_G: 0.30241567 Loss_Enh_Dec: -1.53348672\n",
      "| epoch  54 |  2800/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  3.20 | ppl    24.63 | acc     0.61 | train_ae_norm     1.00\n",
      "[54/200][2899/4361] Loss_D: 0.00333659 (Loss_D_real: 0.00029666 Loss_D_fake: 0.00303993) Loss_G: 0.30734611 Loss_Enh_Dec: -1.59865057\n",
      "| epoch  54 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  3.24 | ppl    25.47 | acc     0.66 | train_ae_norm     1.00\n",
      "[54/200][2999/4361] Loss_D: 0.01410304 (Loss_D_real: 0.01176563 Loss_D_fake: 0.00233741) Loss_G: 0.31079942 Loss_Enh_Dec: -1.56304204\n",
      "| epoch  54 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  3.24 | ppl    25.44 | acc     0.66 | train_ae_norm     1.00\n",
      "[54/200][3099/4361] Loss_D: 0.00332620 (Loss_D_real: 0.00067733 Loss_D_fake: 0.00264887) Loss_G: 0.30011597 Loss_Enh_Dec: -1.42672765\n",
      "| epoch  54 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  3.24 | ppl    25.60 | acc     0.64 | train_ae_norm     1.00\n",
      "[54/200][3199/4361] Loss_D: 0.00307964 (Loss_D_real: 0.00057062 Loss_D_fake: 0.00250902) Loss_G: 0.30263272 Loss_Enh_Dec: -1.59979928\n",
      "| epoch  54 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.80 | loss  3.29 | ppl    26.82 | acc     0.66 | train_ae_norm     1.00\n",
      "[54/200][3299/4361] Loss_D: 0.00486512 (Loss_D_real: 0.00209486 Loss_D_fake: 0.00277026) Loss_G: 0.29884052 Loss_Enh_Dec: -1.62572765\n",
      "| epoch  54 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  3.29 | ppl    26.84 | acc     0.65 | train_ae_norm     1.00\n",
      "[54/200][3399/4361] Loss_D: 0.00853293 (Loss_D_real: 0.00612736 Loss_D_fake: 0.00240557) Loss_G: 0.30511093 Loss_Enh_Dec: -1.79373705\n",
      "| epoch  54 |  3400/ 4361 batches | lr 0.000000 | ms/batch 402.05 | loss  3.26 | ppl    26.07 | acc     0.64 | train_ae_norm     1.00\n",
      "[54/200][3499/4361] Loss_D: 0.00386636 (Loss_D_real: 0.00089701 Loss_D_fake: 0.00296934) Loss_G: 0.29572269 Loss_Enh_Dec: -1.95652962\n",
      "| epoch  54 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  3.22 | ppl    24.93 | acc     0.64 | train_ae_norm     1.00\n",
      "[54/200][3599/4361] Loss_D: 0.00501798 (Loss_D_real: 0.00252844 Loss_D_fake: 0.00248954) Loss_G: 0.30679062 Loss_Enh_Dec: -1.58326364\n",
      "| epoch  54 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  3.23 | ppl    25.20 | acc     0.66 | train_ae_norm     1.00\n",
      "[54/200][3699/4361] Loss_D: 0.00433774 (Loss_D_real: 0.00107529 Loss_D_fake: 0.00326245) Loss_G: 0.30208659 Loss_Enh_Dec: -1.79550445\n",
      "| epoch  54 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  3.29 | ppl    26.93 | acc     0.61 | train_ae_norm     1.00\n",
      "[54/200][3799/4361] Loss_D: 0.00773995 (Loss_D_real: 0.00362583 Loss_D_fake: 0.00411412) Loss_G: 0.28078482 Loss_Enh_Dec: -1.98364830\n",
      "| epoch  54 |  3800/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  3.27 | ppl    26.38 | acc     0.68 | train_ae_norm     1.00\n",
      "[54/200][3899/4361] Loss_D: 0.03268129 (Loss_D_real: 0.02948167 Loss_D_fake: 0.00319962) Loss_G: 0.30602580 Loss_Enh_Dec: -2.05353045\n",
      "| epoch  54 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  3.27 | ppl    26.20 | acc     0.60 | train_ae_norm     1.00\n",
      "[54/200][3999/4361] Loss_D: 0.02197664 (Loss_D_real: 0.01633283 Loss_D_fake: 0.00564381) Loss_G: 0.26652074 Loss_Enh_Dec: -2.11466575\n",
      "| epoch  54 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  3.29 | ppl    26.83 | acc     0.65 | train_ae_norm     1.00\n",
      "[54/200][4099/4361] Loss_D: 0.08817672 (Loss_D_real: 0.06581169 Loss_D_fake: 0.02236503) Loss_G: 0.64229691 Loss_Enh_Dec: -1.98584902\n",
      "| epoch  54 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  3.25 | ppl    25.87 | acc     0.62 | train_ae_norm     1.00\n",
      "[54/200][4199/4361] Loss_D: 0.00713678 (Loss_D_real: 0.00281088 Loss_D_fake: 0.00432589) Loss_G: 0.27864227 Loss_Enh_Dec: -2.04595327\n",
      "| epoch  54 |  4200/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  3.31 | ppl    27.48 | acc     0.65 | train_ae_norm     1.00\n",
      "[54/200][4299/4361] Loss_D: 0.07315572 (Loss_D_real: 0.06822210 Loss_D_fake: 0.00493362) Loss_G: 0.26916951 Loss_Enh_Dec: -2.12427950\n",
      "| epoch  54 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  3.28 | ppl    26.57 | acc     0.64 | train_ae_norm     1.00\n",
      "| end of epoch  54 | time: 1854.30s | test loss  3.24 | test ppl 25.42 | acc 0.668\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 55 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.703\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.477\n",
      "  Test Loss: 4.009\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  55 |     0/ 4361 batches | lr 0.000000 | ms/batch 866.57 | loss  0.03 | ppl     1.03 | acc     0.68 | train_ae_norm     1.00\n",
      "[55/200][99/4361] Loss_D: 0.02701479 (Loss_D_real: 0.02146452 Loss_D_fake: 0.00555026) Loss_G: 0.25867778 Loss_Enh_Dec: -2.22180223\n",
      "| epoch  55 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.22 | loss  3.30 | ppl    27.19 | acc     0.62 | train_ae_norm     1.00\n",
      "[55/200][199/4361] Loss_D: 0.06844149 (Loss_D_real: 0.06317972 Loss_D_fake: 0.00526177) Loss_G: 0.26611215 Loss_Enh_Dec: -2.13780189\n",
      "| epoch  55 |   200/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  3.31 | ppl    27.34 | acc     0.65 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55/200][299/4361] Loss_D: 0.00830219 (Loss_D_real: 0.00328940 Loss_D_fake: 0.00501279) Loss_G: 0.29296330 Loss_Enh_Dec: -2.06446123\n",
      "| epoch  55 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  3.32 | ppl    27.77 | acc     0.60 | train_ae_norm     1.00\n",
      "[55/200][399/4361] Loss_D: 0.01665466 (Loss_D_real: 0.01240304 Loss_D_fake: 0.00425162) Loss_G: 0.29650480 Loss_Enh_Dec: -2.05287457\n",
      "| epoch  55 |   400/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  3.23 | ppl    25.16 | acc     0.67 | train_ae_norm     1.00\n",
      "[55/200][499/4361] Loss_D: 0.00407955 (Loss_D_real: 0.00178681 Loss_D_fake: 0.00229274) Loss_G: 0.38055396 Loss_Enh_Dec: -1.78203094\n",
      "| epoch  55 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.71 | loss  3.32 | ppl    27.71 | acc     0.66 | train_ae_norm     1.00\n",
      "[55/200][599/4361] Loss_D: 0.00910487 (Loss_D_real: 0.00150962 Loss_D_fake: 0.00759525) Loss_G: 0.42771870 Loss_Enh_Dec: -1.92111003\n",
      "| epoch  55 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  3.27 | ppl    26.35 | acc     0.60 | train_ae_norm     1.00\n",
      "[55/200][699/4361] Loss_D: 0.13316184 (Loss_D_real: 0.00096019 Loss_D_fake: 0.13220166) Loss_G: 0.37313756 Loss_Enh_Dec: -1.83158767\n",
      "| epoch  55 |   700/ 4361 batches | lr 0.000000 | ms/batch 402.36 | loss  3.31 | ppl    27.37 | acc     0.66 | train_ae_norm     1.00\n",
      "[55/200][799/4361] Loss_D: 0.21536630 (Loss_D_real: 0.00170292 Loss_D_fake: 0.21366338) Loss_G: 0.40393838 Loss_Enh_Dec: -1.33349216\n",
      "| epoch  55 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  3.28 | ppl    26.50 | acc     0.64 | train_ae_norm     1.00\n",
      "[55/200][899/4361] Loss_D: 0.00587210 (Loss_D_real: 0.00189531 Loss_D_fake: 0.00397679) Loss_G: 0.31960756 Loss_Enh_Dec: -1.44216502\n",
      "| epoch  55 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  3.29 | ppl    26.89 | acc     0.65 | train_ae_norm     1.00\n",
      "[55/200][999/4361] Loss_D: 0.00923212 (Loss_D_real: 0.00051512 Loss_D_fake: 0.00871700) Loss_G: 0.37575474 Loss_Enh_Dec: -1.46781194\n",
      "| epoch  55 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  3.28 | ppl    26.64 | acc     0.62 | train_ae_norm     1.00\n",
      "[55/200][1099/4361] Loss_D: 0.00451677 (Loss_D_real: 0.00124161 Loss_D_fake: 0.00327516) Loss_G: 0.37196797 Loss_Enh_Dec: -1.47998989\n",
      "| epoch  55 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  3.28 | ppl    26.47 | acc     0.62 | train_ae_norm     1.00\n",
      "[55/200][1199/4361] Loss_D: 0.00536944 (Loss_D_real: 0.00290732 Loss_D_fake: 0.00246212) Loss_G: 0.35225704 Loss_Enh_Dec: -1.37257195\n",
      "| epoch  55 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  3.30 | ppl    26.98 | acc     0.65 | train_ae_norm     1.00\n",
      "[55/200][1299/4361] Loss_D: 0.02273155 (Loss_D_real: 0.01833251 Loss_D_fake: 0.00439904) Loss_G: 0.38694581 Loss_Enh_Dec: -1.49452543\n",
      "| epoch  55 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  3.33 | ppl    27.96 | acc     0.63 | train_ae_norm     1.00\n",
      "[55/200][1399/4361] Loss_D: 0.00271437 (Loss_D_real: 0.00099363 Loss_D_fake: 0.00172074) Loss_G: 0.37354690 Loss_Enh_Dec: -1.18687022\n",
      "| epoch  55 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  3.32 | ppl    27.53 | acc     0.60 | train_ae_norm     1.00\n",
      "[55/200][1499/4361] Loss_D: 0.07195237 (Loss_D_real: 0.00081262 Loss_D_fake: 0.07113975) Loss_G: 0.52503961 Loss_Enh_Dec: -1.46830785\n",
      "| epoch  55 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  3.35 | ppl    28.46 | acc     0.62 | train_ae_norm     1.00\n",
      "[55/200][1599/4361] Loss_D: 0.00706514 (Loss_D_real: 0.00143561 Loss_D_fake: 0.00562952) Loss_G: 0.36868641 Loss_Enh_Dec: -1.27336729\n",
      "| epoch  55 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  3.29 | ppl    26.93 | acc     0.63 | train_ae_norm     1.00\n",
      "[55/200][1699/4361] Loss_D: 0.00928085 (Loss_D_real: 0.00503706 Loss_D_fake: 0.00424378) Loss_G: 0.36113945 Loss_Enh_Dec: -1.33420312\n",
      "| epoch  55 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  3.29 | ppl    26.77 | acc     0.65 | train_ae_norm     1.00\n",
      "[55/200][1799/4361] Loss_D: 0.00396189 (Loss_D_real: 0.00039771 Loss_D_fake: 0.00356418) Loss_G: 0.38679600 Loss_Enh_Dec: -1.38481557\n",
      "| epoch  55 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  3.25 | ppl    25.89 | acc     0.63 | train_ae_norm     1.00\n",
      "[55/200][1899/4361] Loss_D: 0.00795157 (Loss_D_real: 0.00272025 Loss_D_fake: 0.00523132) Loss_G: 0.39427337 Loss_Enh_Dec: -1.51141918\n",
      "| epoch  55 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  3.32 | ppl    27.71 | acc     0.64 | train_ae_norm     1.00\n",
      "[55/200][1999/4361] Loss_D: 0.00590895 (Loss_D_real: 0.00368133 Loss_D_fake: 0.00222763) Loss_G: 0.37056142 Loss_Enh_Dec: -1.48632133\n",
      "| epoch  55 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  3.26 | ppl    25.99 | acc     0.65 | train_ae_norm     1.00\n",
      "[55/200][2099/4361] Loss_D: 0.00309830 (Loss_D_real: 0.00035932 Loss_D_fake: 0.00273898) Loss_G: 0.37599587 Loss_Enh_Dec: -1.14936531\n",
      "| epoch  55 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  3.28 | ppl    26.51 | acc     0.66 | train_ae_norm     1.00\n",
      "[55/200][2199/4361] Loss_D: 0.00916587 (Loss_D_real: 0.00521850 Loss_D_fake: 0.00394737) Loss_G: 0.33266863 Loss_Enh_Dec: -1.48170686\n",
      "| epoch  55 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  3.26 | ppl    26.09 | acc     0.66 | train_ae_norm     1.00\n",
      "[55/200][2299/4361] Loss_D: 0.00767518 (Loss_D_real: 0.00218885 Loss_D_fake: 0.00548633) Loss_G: 0.35402051 Loss_Enh_Dec: -1.17665195\n",
      "| epoch  55 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  3.24 | ppl    25.44 | acc     0.66 | train_ae_norm     1.00\n",
      "[55/200][2399/4361] Loss_D: 0.01032420 (Loss_D_real: 0.00662724 Loss_D_fake: 0.00369696) Loss_G: 0.39764318 Loss_Enh_Dec: -1.45851827\n",
      "| epoch  55 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  3.25 | ppl    25.74 | acc     0.62 | train_ae_norm     1.00\n",
      "[55/200][2499/4361] Loss_D: 0.00716736 (Loss_D_real: 0.00529570 Loss_D_fake: 0.00187167) Loss_G: 0.39978954 Loss_Enh_Dec: -1.54396808\n",
      "| epoch  55 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  3.28 | ppl    26.60 | acc     0.65 | train_ae_norm     1.00\n",
      "[55/200][2599/4361] Loss_D: 0.01931310 (Loss_D_real: 0.01849176 Loss_D_fake: 0.00082134) Loss_G: 0.63520080 Loss_Enh_Dec: -1.17998636\n",
      "| epoch  55 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  3.24 | ppl    25.61 | acc     0.62 | train_ae_norm     1.00\n",
      "[55/200][2699/4361] Loss_D: 0.00485614 (Loss_D_real: 0.00206192 Loss_D_fake: 0.00279422) Loss_G: 0.36047944 Loss_Enh_Dec: -1.14401495\n",
      "| epoch  55 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  3.24 | ppl    25.65 | acc     0.65 | train_ae_norm     1.00\n",
      "[55/200][2799/4361] Loss_D: 0.02026426 (Loss_D_real: 0.01813790 Loss_D_fake: 0.00212636) Loss_G: 0.36379260 Loss_Enh_Dec: -1.57032526\n",
      "| epoch  55 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  3.19 | ppl    24.33 | acc     0.66 | train_ae_norm     1.00\n",
      "[55/200][2899/4361] Loss_D: 0.00300328 (Loss_D_real: 0.00054760 Loss_D_fake: 0.00245567) Loss_G: 0.38017708 Loss_Enh_Dec: -1.59507787\n",
      "| epoch  55 |  2900/ 4361 batches | lr 0.000000 | ms/batch 402.45 | loss  3.23 | ppl    25.39 | acc     0.66 | train_ae_norm     1.00\n",
      "[55/200][2999/4361] Loss_D: 0.00999422 (Loss_D_real: 0.00126818 Loss_D_fake: 0.00872604) Loss_G: 0.39806539 Loss_Enh_Dec: -1.42541957\n",
      "| epoch  55 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  3.23 | ppl    25.38 | acc     0.64 | train_ae_norm     1.00\n",
      "[55/200][3099/4361] Loss_D: 0.03105118 (Loss_D_real: 0.02574642 Loss_D_fake: 0.00530476) Loss_G: 0.37445554 Loss_Enh_Dec: -1.40760100\n",
      "| epoch  55 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  3.24 | ppl    25.43 | acc     0.65 | train_ae_norm     1.00\n",
      "[55/200][3199/4361] Loss_D: 0.06614721 (Loss_D_real: 0.01865841 Loss_D_fake: 0.04748879) Loss_G: 0.59844208 Loss_Enh_Dec: -1.26813281\n",
      "| epoch  55 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  3.26 | ppl    26.16 | acc     0.65 | train_ae_norm     1.00\n",
      "[55/200][3299/4361] Loss_D: 0.00984709 (Loss_D_real: 0.00518726 Loss_D_fake: 0.00465983) Loss_G: 0.38935077 Loss_Enh_Dec: -1.48080206\n",
      "| epoch  55 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  3.27 | ppl    26.34 | acc     0.64 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55/200][3399/4361] Loss_D: 0.00134505 (Loss_D_real: 0.00059887 Loss_D_fake: 0.00074618) Loss_G: 0.44123307 Loss_Enh_Dec: -1.25390053\n",
      "| epoch  55 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.05 | loss  3.25 | ppl    25.68 | acc     0.64 | train_ae_norm     1.00\n",
      "[55/200][3499/4361] Loss_D: 0.00898224 (Loss_D_real: 0.00380854 Loss_D_fake: 0.00517371) Loss_G: 0.36357614 Loss_Enh_Dec: -1.04535854\n",
      "| epoch  55 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  3.20 | ppl    24.50 | acc     0.64 | train_ae_norm     1.00\n",
      "[55/200][3599/4361] Loss_D: 0.00246490 (Loss_D_real: 0.00069469 Loss_D_fake: 0.00177021) Loss_G: 0.36364490 Loss_Enh_Dec: -1.35787892\n",
      "| epoch  55 |  3600/ 4361 batches | lr 0.000000 | ms/batch 402.16 | loss  3.28 | ppl    26.52 | acc     0.62 | train_ae_norm     1.00\n",
      "[55/200][3699/4361] Loss_D: 0.00213578 (Loss_D_real: 0.00103304 Loss_D_fake: 0.00110274) Loss_G: 0.51039541 Loss_Enh_Dec: -1.27340019\n",
      "| epoch  55 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  3.28 | ppl    26.61 | acc     0.63 | train_ae_norm     1.00\n",
      "[55/200][3799/4361] Loss_D: 0.00282298 (Loss_D_real: 0.00121130 Loss_D_fake: 0.00161168) Loss_G: 0.44028559 Loss_Enh_Dec: -1.55086398\n",
      "| epoch  55 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  3.26 | ppl    26.16 | acc     0.68 | train_ae_norm     1.00\n",
      "[55/200][3899/4361] Loss_D: 0.01239474 (Loss_D_real: 0.00063090 Loss_D_fake: 0.01176384) Loss_G: 0.47298986 Loss_Enh_Dec: -1.32107759\n",
      "| epoch  55 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  3.24 | ppl    25.59 | acc     0.62 | train_ae_norm     1.00\n",
      "[55/200][3999/4361] Loss_D: 0.01180614 (Loss_D_real: 0.01028198 Loss_D_fake: 0.00152416) Loss_G: 0.42363071 Loss_Enh_Dec: -1.39240026\n",
      "| epoch  55 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  3.26 | ppl    25.95 | acc     0.65 | train_ae_norm     1.00\n",
      "[55/200][4099/4361] Loss_D: 0.01403726 (Loss_D_real: 0.01067129 Loss_D_fake: 0.00336597) Loss_G: 0.38485479 Loss_Enh_Dec: -1.03830814\n",
      "| epoch  55 |  4100/ 4361 batches | lr 0.000000 | ms/batch 402.18 | loss  3.23 | ppl    25.38 | acc     0.62 | train_ae_norm     1.00\n",
      "[55/200][4199/4361] Loss_D: 0.00950492 (Loss_D_real: 0.00797422 Loss_D_fake: 0.00153070) Loss_G: 0.39082754 Loss_Enh_Dec: -1.29976726\n",
      "| epoch  55 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  3.27 | ppl    26.28 | acc     0.66 | train_ae_norm     1.00\n",
      "[55/200][4299/4361] Loss_D: 0.00544145 (Loss_D_real: 0.00081973 Loss_D_fake: 0.00462171) Loss_G: 0.37873265 Loss_Enh_Dec: -1.51734459\n",
      "| epoch  55 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  3.23 | ppl    25.19 | acc     0.65 | train_ae_norm     1.00\n",
      "| end of epoch  55 | time: 1853.77s | test loss  3.16 | test ppl 23.49 | acc 0.675\n",
      "bleu_self:  [2.38321404e-01 5.34567711e-09 1.60252295e-11 9.21615794e-13\n",
      " 1.74548472e-13]\n",
      "bleu_test:  [8.16875832e-01 3.19770513e-01 2.15686066e-06 5.86144174e-09\n",
      " 1.76474420e-10]\n",
      "bleu_self: [0.23832140,0.00000001,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.81687583,0.31977051,0.00000216,0.00000001,0.00000000]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 56 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.709\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.487\n",
      "  Test Loss: 3.907\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  56 |     0/ 4361 batches | lr 0.000000 | ms/batch 867.79 | loss  0.03 | ppl     1.03 | acc     0.65 | train_ae_norm     1.00\n",
      "[56/200][99/4361] Loss_D: 0.00225564 (Loss_D_real: 0.00043510 Loss_D_fake: 0.00182054) Loss_G: 0.37407598 Loss_Enh_Dec: -1.28715253\n",
      "| epoch  56 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  3.26 | ppl    26.11 | acc     0.60 | train_ae_norm     1.00\n",
      "[56/200][199/4361] Loss_D: 0.00740829 (Loss_D_real: 0.00258498 Loss_D_fake: 0.00482331) Loss_G: 0.39014575 Loss_Enh_Dec: -1.29091704\n",
      "| epoch  56 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  3.26 | ppl    26.03 | acc     0.64 | train_ae_norm     1.00\n",
      "[56/200][299/4361] Loss_D: 0.01153714 (Loss_D_real: 0.00990219 Loss_D_fake: 0.00163496) Loss_G: 0.37292790 Loss_Enh_Dec: -1.25449646\n",
      "| epoch  56 |   300/ 4361 batches | lr 0.000000 | ms/batch 419.19 | loss  3.28 | ppl    26.55 | acc     0.62 | train_ae_norm     1.00\n",
      "[56/200][399/4361] Loss_D: 0.00389933 (Loss_D_real: 0.00281533 Loss_D_fake: 0.00108399) Loss_G: 0.38178462 Loss_Enh_Dec: -1.17731464\n",
      "| epoch  56 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  3.20 | ppl    24.54 | acc     0.64 | train_ae_norm     1.00\n",
      "[56/200][499/4361] Loss_D: 0.03170964 (Loss_D_real: 0.02981067 Loss_D_fake: 0.00189897) Loss_G: 0.36028048 Loss_Enh_Dec: -1.21544957\n",
      "| epoch  56 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  3.28 | ppl    26.61 | acc     0.66 | train_ae_norm     1.00\n",
      "[56/200][599/4361] Loss_D: 0.00436073 (Loss_D_real: 0.00227889 Loss_D_fake: 0.00208185) Loss_G: 0.42794657 Loss_Enh_Dec: -1.07581198\n",
      "| epoch  56 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.63 | loss  3.23 | ppl    25.37 | acc     0.62 | train_ae_norm     1.00\n",
      "[56/200][699/4361] Loss_D: 0.00210101 (Loss_D_real: 0.00169409 Loss_D_fake: 0.00040692) Loss_G: 0.50578946 Loss_Enh_Dec: -1.02770424\n",
      "| epoch  56 |   700/ 4361 batches | lr 0.000000 | ms/batch 402.36 | loss  3.28 | ppl    26.65 | acc     0.65 | train_ae_norm     1.00\n",
      "[56/200][799/4361] Loss_D: 0.00484561 (Loss_D_real: 0.00363648 Loss_D_fake: 0.00120913) Loss_G: 0.39793396 Loss_Enh_Dec: -1.02976573\n",
      "| epoch  56 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  3.23 | ppl    25.23 | acc     0.64 | train_ae_norm     1.00\n",
      "[56/200][899/4361] Loss_D: 0.00468748 (Loss_D_real: 0.00342907 Loss_D_fake: 0.00125840) Loss_G: 0.38109690 Loss_Enh_Dec: -1.09805930\n",
      "| epoch  56 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  3.25 | ppl    25.89 | acc     0.66 | train_ae_norm     1.00\n",
      "[56/200][999/4361] Loss_D: 0.04289835 (Loss_D_real: 0.04117219 Loss_D_fake: 0.00172616) Loss_G: 0.40380803 Loss_Enh_Dec: -1.21937835\n",
      "| epoch  56 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.99 | loss  3.23 | ppl    25.31 | acc     0.65 | train_ae_norm     1.00\n",
      "[56/200][1099/4361] Loss_D: 0.00810580 (Loss_D_real: 0.00456857 Loss_D_fake: 0.00353724) Loss_G: 0.39677092 Loss_Enh_Dec: -1.46427298\n",
      "| epoch  56 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  3.24 | ppl    25.64 | acc     0.63 | train_ae_norm     1.00\n",
      "[56/200][1199/4361] Loss_D: 0.00333008 (Loss_D_real: 0.00054717 Loss_D_fake: 0.00278291) Loss_G: 0.39978161 Loss_Enh_Dec: -1.47658622\n",
      "| epoch  56 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  3.25 | ppl    25.73 | acc     0.66 | train_ae_norm     1.00\n",
      "[56/200][1299/4361] Loss_D: 0.00307583 (Loss_D_real: 0.00112887 Loss_D_fake: 0.00194696) Loss_G: 0.38060012 Loss_Enh_Dec: -1.44339490\n",
      "| epoch  56 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  3.26 | ppl    25.94 | acc     0.63 | train_ae_norm     1.00\n",
      "[56/200][1399/4361] Loss_D: 0.00835383 (Loss_D_real: 0.00647005 Loss_D_fake: 0.00188379) Loss_G: 0.37149009 Loss_Enh_Dec: -1.38321960\n",
      "| epoch  56 |  1400/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  3.27 | ppl    26.19 | acc     0.60 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56/200][1499/4361] Loss_D: 0.05252388 (Loss_D_real: 0.04950312 Loss_D_fake: 0.00302076) Loss_G: 0.37229416 Loss_Enh_Dec: -1.17072058\n",
      "| epoch  56 |  1500/ 4361 batches | lr 0.000000 | ms/batch 402.03 | loss  3.29 | ppl    26.88 | acc     0.62 | train_ae_norm     1.00\n",
      "[56/200][1599/4361] Loss_D: 0.00384848 (Loss_D_real: 0.00222045 Loss_D_fake: 0.00162803) Loss_G: 0.37203348 Loss_Enh_Dec: -1.44894028\n",
      "| epoch  56 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  3.26 | ppl    26.03 | acc     0.63 | train_ae_norm     1.00\n",
      "[56/200][1699/4361] Loss_D: 0.01956739 (Loss_D_real: 0.01824028 Loss_D_fake: 0.00132711) Loss_G: 0.38207570 Loss_Enh_Dec: -1.93027937\n",
      "| epoch  56 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  3.26 | ppl    25.96 | acc     0.62 | train_ae_norm     1.00\n",
      "[56/200][1799/4361] Loss_D: 0.00290493 (Loss_D_real: 0.00097826 Loss_D_fake: 0.00192667) Loss_G: 0.36060750 Loss_Enh_Dec: -1.75982726\n",
      "| epoch  56 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  3.26 | ppl    26.00 | acc     0.65 | train_ae_norm     1.00\n",
      "[56/200][1899/4361] Loss_D: 0.00265456 (Loss_D_real: 0.00013871 Loss_D_fake: 0.00251585) Loss_G: 0.36906889 Loss_Enh_Dec: -2.16125083\n",
      "| epoch  56 |  1900/ 4361 batches | lr 0.000000 | ms/batch 402.07 | loss  3.31 | ppl    27.43 | acc     0.67 | train_ae_norm     1.00\n",
      "[56/200][1999/4361] Loss_D: 0.04726749 (Loss_D_real: 0.04550238 Loss_D_fake: 0.00176511) Loss_G: 0.39638737 Loss_Enh_Dec: -1.62496817\n",
      "| epoch  56 |  2000/ 4361 batches | lr 0.000000 | ms/batch 402.52 | loss  3.24 | ppl    25.60 | acc     0.64 | train_ae_norm     1.00\n",
      "[56/200][2099/4361] Loss_D: 0.00926660 (Loss_D_real: 0.00392296 Loss_D_fake: 0.00534364) Loss_G: 0.40361857 Loss_Enh_Dec: -1.89780045\n",
      "| epoch  56 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  3.28 | ppl    26.51 | acc     0.65 | train_ae_norm     1.00\n",
      "[56/200][2199/4361] Loss_D: 0.00159397 (Loss_D_real: 0.00062711 Loss_D_fake: 0.00096686) Loss_G: 0.40776354 Loss_Enh_Dec: -1.78262901\n",
      "| epoch  56 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  3.28 | ppl    26.65 | acc     0.64 | train_ae_norm     1.00\n",
      "[56/200][2299/4361] Loss_D: 0.87962842 (Loss_D_real: 0.00614228 Loss_D_fake: 0.87348616) Loss_G: 0.58318555 Loss_Enh_Dec: -1.67577863\n",
      "| epoch  56 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  3.25 | ppl    25.84 | acc     0.65 | train_ae_norm     1.00\n",
      "[56/200][2399/4361] Loss_D: 0.00310145 (Loss_D_real: 0.00110670 Loss_D_fake: 0.00199475) Loss_G: 0.41383108 Loss_Enh_Dec: -1.64506114\n",
      "| epoch  56 |  2400/ 4361 batches | lr 0.000000 | ms/batch 402.35 | loss  3.27 | ppl    26.34 | acc     0.61 | train_ae_norm     1.00\n",
      "[56/200][2499/4361] Loss_D: 0.00236206 (Loss_D_real: 0.00080013 Loss_D_fake: 0.00156194) Loss_G: 0.36497295 Loss_Enh_Dec: -1.82585037\n",
      "| epoch  56 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  3.32 | ppl    27.70 | acc     0.63 | train_ae_norm     1.00\n",
      "[56/200][2599/4361] Loss_D: 0.02399725 (Loss_D_real: 0.01483814 Loss_D_fake: 0.00915912) Loss_G: 0.35714483 Loss_Enh_Dec: -1.66638553\n",
      "| epoch  56 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  3.27 | ppl    26.32 | acc     0.63 | train_ae_norm     1.00\n",
      "[56/200][2699/4361] Loss_D: 0.00258310 (Loss_D_real: 0.00147333 Loss_D_fake: 0.00110977) Loss_G: 0.40077233 Loss_Enh_Dec: -1.98063695\n",
      "| epoch  56 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  3.30 | ppl    27.23 | acc     0.64 | train_ae_norm     1.00\n",
      "[56/200][2799/4361] Loss_D: 0.00181335 (Loss_D_real: 0.00038707 Loss_D_fake: 0.00142628) Loss_G: 0.38189790 Loss_Enh_Dec: -1.78385437\n",
      "| epoch  56 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  3.23 | ppl    25.39 | acc     0.61 | train_ae_norm     1.00\n",
      "[56/200][2899/4361] Loss_D: 0.00598504 (Loss_D_real: 0.00486678 Loss_D_fake: 0.00111826) Loss_G: 0.38027641 Loss_Enh_Dec: -1.51616228\n",
      "| epoch  56 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  3.26 | ppl    26.11 | acc     0.66 | train_ae_norm     1.00\n",
      "[56/200][2999/4361] Loss_D: 0.00193048 (Loss_D_real: 0.00050933 Loss_D_fake: 0.00142115) Loss_G: 0.39077672 Loss_Enh_Dec: -1.45756567\n",
      "| epoch  56 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  3.28 | ppl    26.50 | acc     0.63 | train_ae_norm     1.00\n",
      "[56/200][3099/4361] Loss_D: 0.00187552 (Loss_D_real: 0.00042681 Loss_D_fake: 0.00144871) Loss_G: 0.35459018 Loss_Enh_Dec: -1.39899909\n",
      "| epoch  56 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  3.29 | ppl    26.75 | acc     0.62 | train_ae_norm     1.00\n",
      "[56/200][3199/4361] Loss_D: 0.00404818 (Loss_D_real: 0.00033690 Loss_D_fake: 0.00371127) Loss_G: 0.37612772 Loss_Enh_Dec: -1.51209867\n",
      "| epoch  56 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  3.31 | ppl    27.49 | acc     0.63 | train_ae_norm     1.00\n",
      "[56/200][3299/4361] Loss_D: 0.03427156 (Loss_D_real: 0.03414545 Loss_D_fake: 0.00012610) Loss_G: 0.78936094 Loss_Enh_Dec: -1.56674123\n",
      "| epoch  56 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  3.32 | ppl    27.70 | acc     0.63 | train_ae_norm     1.00\n",
      "[56/200][3399/4361] Loss_D: 0.00257984 (Loss_D_real: 0.00108756 Loss_D_fake: 0.00149227) Loss_G: 0.36836573 Loss_Enh_Dec: -1.57276154\n",
      "| epoch  56 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  3.29 | ppl    26.88 | acc     0.62 | train_ae_norm     1.00\n",
      "[56/200][3499/4361] Loss_D: 0.00135459 (Loss_D_real: 0.00037617 Loss_D_fake: 0.00097842) Loss_G: 0.39599147 Loss_Enh_Dec: -1.41586387\n",
      "| epoch  56 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  3.25 | ppl    25.75 | acc     0.63 | train_ae_norm     1.00\n",
      "[56/200][3599/4361] Loss_D: 0.01114361 (Loss_D_real: 0.00973016 Loss_D_fake: 0.00141345) Loss_G: 0.39585987 Loss_Enh_Dec: -1.60667801\n",
      "| epoch  56 |  3600/ 4361 batches | lr 0.000000 | ms/batch 402.05 | loss  3.25 | ppl    25.91 | acc     0.65 | train_ae_norm     1.00\n",
      "[56/200][3699/4361] Loss_D: 0.00539219 (Loss_D_real: 0.00074691 Loss_D_fake: 0.00464528) Loss_G: 0.34167013 Loss_Enh_Dec: -1.38772607\n",
      "| epoch  56 |  3700/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  3.28 | ppl    26.63 | acc     0.61 | train_ae_norm     1.00\n",
      "[56/200][3799/4361] Loss_D: 0.00443774 (Loss_D_real: 0.00188937 Loss_D_fake: 0.00254837) Loss_G: 0.40604639 Loss_Enh_Dec: -1.34279406\n",
      "| epoch  56 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  3.31 | ppl    27.47 | acc     0.66 | train_ae_norm     1.00\n",
      "[56/200][3899/4361] Loss_D: 0.00669997 (Loss_D_real: 0.00085643 Loss_D_fake: 0.00584355) Loss_G: 0.36398771 Loss_Enh_Dec: -1.20451355\n",
      "| epoch  56 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  3.29 | ppl    26.73 | acc     0.61 | train_ae_norm     1.00\n",
      "[56/200][3999/4361] Loss_D: 0.01148669 (Loss_D_real: 0.00539526 Loss_D_fake: 0.00609143) Loss_G: 0.36284271 Loss_Enh_Dec: -1.73682213\n",
      "| epoch  56 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  3.30 | ppl    27.05 | acc     0.68 | train_ae_norm     1.00\n",
      "[56/200][4099/4361] Loss_D: 0.00430292 (Loss_D_real: 0.00294791 Loss_D_fake: 0.00135501) Loss_G: 0.49572012 Loss_Enh_Dec: -1.44743574\n",
      "| epoch  56 |  4100/ 4361 batches | lr 0.000000 | ms/batch 402.14 | loss  3.26 | ppl    26.06 | acc     0.64 | train_ae_norm     1.00\n",
      "[56/200][4199/4361] Loss_D: 0.10961848 (Loss_D_real: 0.10771724 Loss_D_fake: 0.00190124) Loss_G: 0.40229771 Loss_Enh_Dec: -1.60525286\n",
      "| epoch  56 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  3.30 | ppl    27.04 | acc     0.66 | train_ae_norm     1.00\n",
      "[56/200][4299/4361] Loss_D: 0.02978613 (Loss_D_real: 0.02119824 Loss_D_fake: 0.00858789) Loss_G: 0.41514635 Loss_Enh_Dec: -1.34387672\n",
      "| epoch  56 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  3.27 | ppl    26.23 | acc     0.65 | train_ae_norm     1.00\n",
      "| end of epoch  56 | time: 1855.96s | test loss  3.18 | test ppl 24.10 | acc 0.674\n",
      "bleu_self:  [2.92618615e-01 1.18687837e-01 1.08262876e-06 3.57789950e-09\n",
      " 3.04303940e-10]\n",
      "bleu_test:  [7.93948412e-01 3.19076579e-01 5.46006972e-02 7.63672669e-06\n",
      " 3.91068928e-08]\n",
      "bleu_self: [0.29261861,0.11868784,0.00000108,0.00000000,0.00000000]\n",
      "bleu_test: [0.79394841,0.31907658,0.05460070,0.00000764,0.00000004]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 57 / 200 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.707\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.495\n",
      "  Test Loss: 4.006\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  57 |     0/ 4361 batches | lr 0.000000 | ms/batch 867.07 | loss  0.03 | ppl     1.03 | acc     0.68 | train_ae_norm     1.00\n",
      "[57/200][99/4361] Loss_D: 0.01185246 (Loss_D_real: 0.00823244 Loss_D_fake: 0.00362002) Loss_G: 0.38578245 Loss_Enh_Dec: -1.17473972\n",
      "| epoch  57 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  3.28 | ppl    26.53 | acc     0.60 | train_ae_norm     1.00\n",
      "[57/200][199/4361] Loss_D: 0.00279815 (Loss_D_real: 0.00071552 Loss_D_fake: 0.00208263) Loss_G: 0.37764168 Loss_Enh_Dec: -1.30359936\n",
      "| epoch  57 |   200/ 4361 batches | lr 0.000000 | ms/batch 402.52 | loss  3.29 | ppl    26.95 | acc     0.65 | train_ae_norm     1.00\n",
      "[57/200][299/4361] Loss_D: 0.02504975 (Loss_D_real: 0.02381831 Loss_D_fake: 0.00123144) Loss_G: 0.37509114 Loss_Enh_Dec: -1.75483835\n",
      "| epoch  57 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  3.31 | ppl    27.52 | acc     0.61 | train_ae_norm     1.00\n",
      "[57/200][399/4361] Loss_D: 0.00542683 (Loss_D_real: 0.00219233 Loss_D_fake: 0.00323450) Loss_G: 0.33380014 Loss_Enh_Dec: -1.73204410\n",
      "| epoch  57 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  3.21 | ppl    24.83 | acc     0.65 | train_ae_norm     1.00\n",
      "[57/200][499/4361] Loss_D: 0.00456966 (Loss_D_real: 0.00170623 Loss_D_fake: 0.00286343) Loss_G: 0.40108886 Loss_Enh_Dec: -1.02859104\n",
      "| epoch  57 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  3.30 | ppl    27.08 | acc     0.66 | train_ae_norm     1.00\n",
      "[57/200][599/4361] Loss_D: 0.00249117 (Loss_D_real: 0.00059148 Loss_D_fake: 0.00189970) Loss_G: 0.40840021 Loss_Enh_Dec: -1.82998562\n",
      "| epoch  57 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  3.24 | ppl    25.56 | acc     0.59 | train_ae_norm     1.00\n",
      "[57/200][699/4361] Loss_D: 0.00231647 (Loss_D_real: 0.00126336 Loss_D_fake: 0.00105312) Loss_G: 0.38398796 Loss_Enh_Dec: -1.60227191\n",
      "| epoch  57 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  3.31 | ppl    27.31 | acc     0.66 | train_ae_norm     1.00\n",
      "[57/200][799/4361] Loss_D: 0.01206328 (Loss_D_real: 0.00843381 Loss_D_fake: 0.00362947) Loss_G: 0.36641598 Loss_Enh_Dec: -1.15469801\n",
      "| epoch  57 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  3.27 | ppl    26.39 | acc     0.65 | train_ae_norm     1.00\n",
      "[57/200][899/4361] Loss_D: 0.00977733 (Loss_D_real: 0.00838081 Loss_D_fake: 0.00139653) Loss_G: 0.36461493 Loss_Enh_Dec: -1.47230482\n",
      "| epoch  57 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  3.27 | ppl    26.36 | acc     0.64 | train_ae_norm     1.00\n",
      "[57/200][999/4361] Loss_D: 0.00419726 (Loss_D_real: 0.00046112 Loss_D_fake: 0.00373614) Loss_G: 0.33626920 Loss_Enh_Dec: -1.10561740\n",
      "| epoch  57 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  3.26 | ppl    26.16 | acc     0.64 | train_ae_norm     1.00\n",
      "[57/200][1099/4361] Loss_D: 0.02633200 (Loss_D_real: 0.02040765 Loss_D_fake: 0.00592435) Loss_G: 0.38617474 Loss_Enh_Dec: -0.65017283\n",
      "| epoch  57 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  3.27 | ppl    26.23 | acc     0.62 | train_ae_norm     1.00\n",
      "[57/200][1199/4361] Loss_D: 0.00346906 (Loss_D_real: 0.00107457 Loss_D_fake: 0.00239450) Loss_G: 0.39624938 Loss_Enh_Dec: -0.90635031\n",
      "| epoch  57 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  3.28 | ppl    26.69 | acc     0.68 | train_ae_norm     1.00\n",
      "[57/200][1299/4361] Loss_D: 0.00527440 (Loss_D_real: 0.00346766 Loss_D_fake: 0.00180674) Loss_G: 0.36164615 Loss_Enh_Dec: -1.07644570\n",
      "| epoch  57 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  3.28 | ppl    26.48 | acc     0.63 | train_ae_norm     1.00\n",
      "[57/200][1399/4361] Loss_D: 0.00891164 (Loss_D_real: 0.00677977 Loss_D_fake: 0.00213187) Loss_G: 0.38530603 Loss_Enh_Dec: -1.21651995\n",
      "| epoch  57 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  3.27 | ppl    26.36 | acc     0.59 | train_ae_norm     1.00\n",
      "[57/200][1499/4361] Loss_D: 0.09289025 (Loss_D_real: 0.08955675 Loss_D_fake: 0.00333350) Loss_G: 0.38169259 Loss_Enh_Dec: -1.23413694\n",
      "| epoch  57 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  3.31 | ppl    27.30 | acc     0.64 | train_ae_norm     1.00\n",
      "[57/200][1599/4361] Loss_D: 0.13193606 (Loss_D_real: 0.13135926 Loss_D_fake: 0.00057679) Loss_G: 0.45700693 Loss_Enh_Dec: -0.99132156\n",
      "| epoch  57 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  3.27 | ppl    26.20 | acc     0.64 | train_ae_norm     1.00\n",
      "[57/200][1699/4361] Loss_D: 0.00903286 (Loss_D_real: 0.00057912 Loss_D_fake: 0.00845374) Loss_G: 0.35653815 Loss_Enh_Dec: -1.38168085\n",
      "| epoch  57 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  3.25 | ppl    25.84 | acc     0.64 | train_ae_norm     1.00\n",
      "[57/200][1799/4361] Loss_D: 0.00948765 (Loss_D_real: 0.00727391 Loss_D_fake: 0.00221375) Loss_G: 0.39511400 Loss_Enh_Dec: -1.02143657\n",
      "| epoch  57 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  3.24 | ppl    25.50 | acc     0.64 | train_ae_norm     1.00\n",
      "[57/200][1899/4361] Loss_D: 0.00362759 (Loss_D_real: 0.00180541 Loss_D_fake: 0.00182218) Loss_G: 0.55309862 Loss_Enh_Dec: -1.23918343\n",
      "| epoch  57 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  3.30 | ppl    27.05 | acc     0.66 | train_ae_norm     1.00\n",
      "[57/200][1999/4361] Loss_D: 0.02699868 (Loss_D_real: 0.01618560 Loss_D_fake: 0.01081308) Loss_G: 0.35184771 Loss_Enh_Dec: -1.30676532\n",
      "| epoch  57 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  3.25 | ppl    25.73 | acc     0.64 | train_ae_norm     1.00\n",
      "[57/200][2099/4361] Loss_D: 0.00666872 (Loss_D_real: 0.00285725 Loss_D_fake: 0.00381147) Loss_G: 0.38663092 Loss_Enh_Dec: -1.08159471\n",
      "| epoch  57 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  3.27 | ppl    26.35 | acc     0.66 | train_ae_norm     1.00\n",
      "[57/200][2199/4361] Loss_D: 0.00867241 (Loss_D_real: 0.00455644 Loss_D_fake: 0.00411597) Loss_G: 0.35509419 Loss_Enh_Dec: -0.84802926\n",
      "| epoch  57 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  3.27 | ppl    26.31 | acc     0.63 | train_ae_norm     1.00\n",
      "[57/200][2299/4361] Loss_D: 0.04172674 (Loss_D_real: 0.03851413 Loss_D_fake: 0.00321261) Loss_G: 0.37184379 Loss_Enh_Dec: -1.13739896\n",
      "| epoch  57 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  3.25 | ppl    25.84 | acc     0.64 | train_ae_norm     1.00\n",
      "[57/200][2399/4361] Loss_D: 0.00753117 (Loss_D_real: 0.00590760 Loss_D_fake: 0.00162358) Loss_G: 0.35716915 Loss_Enh_Dec: -1.31210315\n",
      "| epoch  57 |  2400/ 4361 batches | lr 0.000000 | ms/batch 402.15 | loss  3.26 | ppl    26.02 | acc     0.59 | train_ae_norm     1.00\n",
      "[57/200][2499/4361] Loss_D: 0.01193683 (Loss_D_real: 0.00782292 Loss_D_fake: 0.00411391) Loss_G: 0.33660921 Loss_Enh_Dec: -1.03201318\n",
      "| epoch  57 |  2500/ 4361 batches | lr 0.000000 | ms/batch 402.39 | loss  3.29 | ppl    26.73 | acc     0.65 | train_ae_norm     1.00\n",
      "[57/200][2599/4361] Loss_D: 0.06071968 (Loss_D_real: 0.05821110 Loss_D_fake: 0.00250858) Loss_G: 0.31040141 Loss_Enh_Dec: -1.47684860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  57 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  3.26 | ppl    26.03 | acc     0.64 | train_ae_norm     1.00\n",
      "[57/200][2699/4361] Loss_D: 0.00649997 (Loss_D_real: 0.00056922 Loss_D_fake: 0.00593075) Loss_G: 0.32245958 Loss_Enh_Dec: -1.31383669\n",
      "| epoch  57 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  3.28 | ppl    26.68 | acc     0.62 | train_ae_norm     1.00\n",
      "[57/200][2799/4361] Loss_D: 0.00371596 (Loss_D_real: 0.00126209 Loss_D_fake: 0.00245387) Loss_G: 0.35437080 Loss_Enh_Dec: -1.43750656\n",
      "| epoch  57 |  2800/ 4361 batches | lr 0.000000 | ms/batch 402.00 | loss  3.24 | ppl    25.59 | acc     0.62 | train_ae_norm     1.00\n",
      "[57/200][2899/4361] Loss_D: 0.00319566 (Loss_D_real: 0.00090829 Loss_D_fake: 0.00228737) Loss_G: 0.33469963 Loss_Enh_Dec: -1.66477811\n",
      "| epoch  57 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  3.25 | ppl    25.92 | acc     0.64 | train_ae_norm     1.00\n",
      "[57/200][2999/4361] Loss_D: 0.00522018 (Loss_D_real: 0.00049791 Loss_D_fake: 0.00472227) Loss_G: 0.34137025 Loss_Enh_Dec: -1.31827772\n",
      "| epoch  57 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  3.23 | ppl    25.37 | acc     0.66 | train_ae_norm     1.00\n",
      "[57/200][3099/4361] Loss_D: 0.00238230 (Loss_D_real: 0.00031513 Loss_D_fake: 0.00206717) Loss_G: 0.35283974 Loss_Enh_Dec: -1.45504892\n",
      "| epoch  57 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  3.25 | ppl    25.90 | acc     0.64 | train_ae_norm     1.00\n",
      "[57/200][3199/4361] Loss_D: 0.00526009 (Loss_D_real: 0.00144458 Loss_D_fake: 0.00381551) Loss_G: 0.37074906 Loss_Enh_Dec: -1.68534362\n",
      "| epoch  57 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  3.30 | ppl    26.99 | acc     0.66 | train_ae_norm     1.00\n",
      "[57/200][3299/4361] Loss_D: 0.00397703 (Loss_D_real: 0.00055453 Loss_D_fake: 0.00342251) Loss_G: 0.37070400 Loss_Enh_Dec: -1.36889839\n",
      "| epoch  57 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  3.28 | ppl    26.57 | acc     0.64 | train_ae_norm     1.00\n",
      "[57/200][3399/4361] Loss_D: 0.00120084 (Loss_D_real: 0.00033747 Loss_D_fake: 0.00086336) Loss_G: 0.41262513 Loss_Enh_Dec: -1.15065253\n",
      "| epoch  57 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  3.26 | ppl    26.10 | acc     0.64 | train_ae_norm     1.00\n",
      "[57/200][3499/4361] Loss_D: 0.00233467 (Loss_D_real: 0.00079629 Loss_D_fake: 0.00153839) Loss_G: 0.36654368 Loss_Enh_Dec: -1.11677623\n",
      "| epoch  57 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  3.20 | ppl    24.58 | acc     0.64 | train_ae_norm     1.00\n",
      "[57/200][3599/4361] Loss_D: 0.00323459 (Loss_D_real: 0.00104466 Loss_D_fake: 0.00218992) Loss_G: 0.39052677 Loss_Enh_Dec: -1.43492079\n",
      "| epoch  57 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  3.22 | ppl    24.91 | acc     0.67 | train_ae_norm     1.00\n",
      "[57/200][3699/4361] Loss_D: 0.00255935 (Loss_D_real: 0.00036207 Loss_D_fake: 0.00219728) Loss_G: 0.38753963 Loss_Enh_Dec: -1.34982669\n",
      "| epoch  57 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  3.23 | ppl    25.20 | acc     0.63 | train_ae_norm     1.00\n",
      "[57/200][3799/4361] Loss_D: 0.00365492 (Loss_D_real: 0.00125549 Loss_D_fake: 0.00239943) Loss_G: 0.35429883 Loss_Enh_Dec: -1.44536769\n",
      "| epoch  57 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  3.24 | ppl    25.61 | acc     0.68 | train_ae_norm     1.00\n",
      "[57/200][3899/4361] Loss_D: 0.11557317 (Loss_D_real: 0.01067028 Loss_D_fake: 0.10490289) Loss_G: 0.72146684 Loss_Enh_Dec: -1.05608809\n",
      "| epoch  57 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.63 | loss  3.24 | ppl    25.53 | acc     0.60 | train_ae_norm     1.00\n",
      "[57/200][3999/4361] Loss_D: 0.00571538 (Loss_D_real: 0.00227471 Loss_D_fake: 0.00344066) Loss_G: 0.38613373 Loss_Enh_Dec: -0.81325597\n",
      "| epoch  57 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  3.24 | ppl    25.63 | acc     0.65 | train_ae_norm     1.00\n",
      "[57/200][4099/4361] Loss_D: 0.00638300 (Loss_D_real: 0.00371337 Loss_D_fake: 0.00266963) Loss_G: 0.36315334 Loss_Enh_Dec: -0.68686593\n",
      "| epoch  57 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  3.20 | ppl    24.45 | acc     0.61 | train_ae_norm     1.00\n",
      "[57/200][4199/4361] Loss_D: 0.00524171 (Loss_D_real: 0.00318627 Loss_D_fake: 0.00205543) Loss_G: 0.34921309 Loss_Enh_Dec: -0.74946874\n",
      "| epoch  57 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  3.24 | ppl    25.62 | acc     0.67 | train_ae_norm     1.00\n",
      "[57/200][4299/4361] Loss_D: 0.00359811 (Loss_D_real: 0.00084243 Loss_D_fake: 0.00275568) Loss_G: 0.34225574 Loss_Enh_Dec: -0.77745944\n",
      "| epoch  57 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.46 | loss  3.23 | ppl    25.26 | acc     0.64 | train_ae_norm     1.00\n",
      "| end of epoch  57 | time: 1853.70s | test loss  3.14 | test ppl 23.17 | acc 0.676\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 58 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.710\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.492\n",
      "  Test Loss: 3.982\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  58 |     0/ 4361 batches | lr 0.000000 | ms/batch 863.15 | loss  0.03 | ppl     1.03 | acc     0.67 | train_ae_norm     1.00\n",
      "[58/200][99/4361] Loss_D: 0.00460148 (Loss_D_real: 0.00187684 Loss_D_fake: 0.00272465) Loss_G: 0.42838240 Loss_Enh_Dec: -1.27470100\n",
      "| epoch  58 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  3.24 | ppl    25.59 | acc     0.62 | train_ae_norm     1.00\n",
      "[58/200][199/4361] Loss_D: 0.00484309 (Loss_D_real: 0.00116953 Loss_D_fake: 0.00367356) Loss_G: 0.36282194 Loss_Enh_Dec: -1.00622261\n",
      "| epoch  58 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  3.27 | ppl    26.38 | acc     0.65 | train_ae_norm     1.00\n",
      "[58/200][299/4361] Loss_D: 0.01457688 (Loss_D_real: 0.01260826 Loss_D_fake: 0.00196862) Loss_G: 0.42010623 Loss_Enh_Dec: -0.96435899\n",
      "| epoch  58 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.78 | loss  3.26 | ppl    26.16 | acc     0.60 | train_ae_norm     1.00\n",
      "[58/200][399/4361] Loss_D: 0.00471403 (Loss_D_real: 0.00217804 Loss_D_fake: 0.00253599) Loss_G: 0.39453793 Loss_Enh_Dec: -0.84701979\n",
      "| epoch  58 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  3.19 | ppl    24.29 | acc     0.65 | train_ae_norm     1.00\n",
      "[58/200][499/4361] Loss_D: 0.00711064 (Loss_D_real: 0.00309457 Loss_D_fake: 0.00401606) Loss_G: 0.40287375 Loss_Enh_Dec: -1.15118015\n",
      "| epoch  58 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  3.25 | ppl    25.69 | acc     0.68 | train_ae_norm     1.00\n",
      "[58/200][599/4361] Loss_D: 0.02583221 (Loss_D_real: 0.02360230 Loss_D_fake: 0.00222991) Loss_G: 0.31452042 Loss_Enh_Dec: -0.94719774\n",
      "| epoch  58 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.73 | loss  3.19 | ppl    24.38 | acc     0.60 | train_ae_norm     1.00\n",
      "[58/200][699/4361] Loss_D: 0.01368309 (Loss_D_real: 0.00449324 Loss_D_fake: 0.00918985) Loss_G: 0.35595521 Loss_Enh_Dec: -1.17261827\n",
      "| epoch  58 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.86 | loss  3.24 | ppl    25.65 | acc     0.68 | train_ae_norm     1.00\n",
      "[58/200][799/4361] Loss_D: 0.01566490 (Loss_D_real: 0.01139446 Loss_D_fake: 0.00427044) Loss_G: 0.35280237 Loss_Enh_Dec: -0.70247096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  58 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.72 | loss  3.22 | ppl    24.94 | acc     0.66 | train_ae_norm     1.00\n",
      "[58/200][899/4361] Loss_D: 0.00677973 (Loss_D_real: 0.00413477 Loss_D_fake: 0.00264496) Loss_G: 0.38011989 Loss_Enh_Dec: -1.23437953\n",
      "| epoch  58 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  3.23 | ppl    25.39 | acc     0.67 | train_ae_norm     1.00\n",
      "[58/200][999/4361] Loss_D: 0.05782839 (Loss_D_real: 0.05464501 Loss_D_fake: 0.00318338) Loss_G: 0.48219118 Loss_Enh_Dec: -1.15826368\n",
      "| epoch  58 |  1000/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  3.23 | ppl    25.40 | acc     0.65 | train_ae_norm     1.00\n",
      "[58/200][1099/4361] Loss_D: 0.00250953 (Loss_D_real: 0.00017665 Loss_D_fake: 0.00233288) Loss_G: 0.42102310 Loss_Enh_Dec: -1.27063203\n",
      "| epoch  58 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  3.22 | ppl    25.11 | acc     0.61 | train_ae_norm     1.00\n",
      "[58/200][1199/4361] Loss_D: 0.00399340 (Loss_D_real: 0.00129055 Loss_D_fake: 0.00270286) Loss_G: 0.33780199 Loss_Enh_Dec: -1.58622491\n",
      "| epoch  58 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  3.30 | ppl    27.21 | acc     0.65 | train_ae_norm     1.00\n",
      "[58/200][1299/4361] Loss_D: 0.00545672 (Loss_D_real: 0.00141710 Loss_D_fake: 0.00403962) Loss_G: 0.38352001 Loss_Enh_Dec: -0.66100460\n",
      "| epoch  58 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  3.25 | ppl    25.75 | acc     0.65 | train_ae_norm     1.00\n",
      "[58/200][1399/4361] Loss_D: 0.00685831 (Loss_D_real: 0.00302997 Loss_D_fake: 0.00382834) Loss_G: 0.34392872 Loss_Enh_Dec: -1.38816297\n",
      "| epoch  58 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  3.25 | ppl    25.78 | acc     0.61 | train_ae_norm     1.00\n",
      "[58/200][1499/4361] Loss_D: 0.00604724 (Loss_D_real: 0.00168753 Loss_D_fake: 0.00435970) Loss_G: 0.36678186 Loss_Enh_Dec: -0.90053809\n",
      "| epoch  58 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  3.30 | ppl    27.20 | acc     0.63 | train_ae_norm     1.00\n",
      "[58/200][1599/4361] Loss_D: 0.01430326 (Loss_D_real: 0.01002138 Loss_D_fake: 0.00428189) Loss_G: 0.37539431 Loss_Enh_Dec: -1.04864025\n",
      "| epoch  58 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  3.27 | ppl    26.39 | acc     0.65 | train_ae_norm     1.00\n",
      "[58/200][1699/4361] Loss_D: 0.06232205 (Loss_D_real: 0.00150080 Loss_D_fake: 0.06082125) Loss_G: 0.50189924 Loss_Enh_Dec: -1.03213966\n",
      "| epoch  58 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  3.25 | ppl    25.77 | acc     0.62 | train_ae_norm     1.00\n",
      "[58/200][1799/4361] Loss_D: 0.00117580 (Loss_D_real: 0.00088855 Loss_D_fake: 0.00028725) Loss_G: 0.78669775 Loss_Enh_Dec: -0.89177126\n",
      "| epoch  58 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  3.23 | ppl    25.22 | acc     0.64 | train_ae_norm     1.00\n",
      "[58/200][1899/4361] Loss_D: 0.01335345 (Loss_D_real: 0.00987102 Loss_D_fake: 0.00348244) Loss_G: 0.36893097 Loss_Enh_Dec: -1.17125738\n",
      "| epoch  58 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.25 | loss  3.29 | ppl    26.83 | acc     0.67 | train_ae_norm     1.00\n",
      "[58/200][1999/4361] Loss_D: 0.00438218 (Loss_D_real: 0.00058775 Loss_D_fake: 0.00379443) Loss_G: 0.36364195 Loss_Enh_Dec: -0.96592158\n",
      "| epoch  58 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.67 | loss  3.21 | ppl    24.78 | acc     0.66 | train_ae_norm     1.00\n",
      "[58/200][2099/4361] Loss_D: 0.00736285 (Loss_D_real: 0.00169876 Loss_D_fake: 0.00566409) Loss_G: 0.37921968 Loss_Enh_Dec: -0.95564264\n",
      "| epoch  58 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.49 | loss  3.24 | ppl    25.55 | acc     0.66 | train_ae_norm     1.00\n",
      "[58/200][2199/4361] Loss_D: 0.00430775 (Loss_D_real: 0.00212586 Loss_D_fake: 0.00218189) Loss_G: 0.39356476 Loss_Enh_Dec: -0.93226683\n",
      "| epoch  58 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  3.24 | ppl    25.44 | acc     0.65 | train_ae_norm     1.00\n",
      "[58/200][2299/4361] Loss_D: 0.00911333 (Loss_D_real: 0.00199270 Loss_D_fake: 0.00712063) Loss_G: 0.35970715 Loss_Enh_Dec: -1.37727797\n",
      "| epoch  58 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  3.23 | ppl    25.33 | acc     0.65 | train_ae_norm     1.00\n",
      "[58/200][2399/4361] Loss_D: 0.00395472 (Loss_D_real: 0.00104273 Loss_D_fake: 0.00291198) Loss_G: 0.39282426 Loss_Enh_Dec: -1.24401081\n",
      "| epoch  58 |  2400/ 4361 batches | lr 0.000000 | ms/batch 400.82 | loss  3.23 | ppl    25.24 | acc     0.62 | train_ae_norm     1.00\n",
      "[58/200][2499/4361] Loss_D: 0.00571986 (Loss_D_real: 0.00153282 Loss_D_fake: 0.00418704) Loss_G: 0.38091752 Loss_Enh_Dec: -1.17449522\n",
      "| epoch  58 |  2500/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  3.27 | ppl    26.43 | acc     0.63 | train_ae_norm     1.00\n",
      "[58/200][2599/4361] Loss_D: 0.00982425 (Loss_D_real: 0.00548059 Loss_D_fake: 0.00434366) Loss_G: 0.39010715 Loss_Enh_Dec: -1.39153850\n",
      "| epoch  58 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.67 | loss  3.27 | ppl    26.27 | acc     0.62 | train_ae_norm     1.00\n",
      "[58/200][2699/4361] Loss_D: 0.01195616 (Loss_D_real: 0.00397736 Loss_D_fake: 0.00797881) Loss_G: 0.35863981 Loss_Enh_Dec: -1.33335054\n",
      "| epoch  58 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  3.25 | ppl    25.87 | acc     0.63 | train_ae_norm     1.00\n",
      "[58/200][2799/4361] Loss_D: 0.00397407 (Loss_D_real: 0.00302817 Loss_D_fake: 0.00094590) Loss_G: 0.55939859 Loss_Enh_Dec: -1.07237709\n",
      "| epoch  58 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  3.20 | ppl    24.52 | acc     0.64 | train_ae_norm     1.00\n",
      "[58/200][2899/4361] Loss_D: 0.00288765 (Loss_D_real: 0.00074433 Loss_D_fake: 0.00214332) Loss_G: 0.36206594 Loss_Enh_Dec: -0.81757337\n",
      "| epoch  58 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  3.23 | ppl    25.31 | acc     0.63 | train_ae_norm     1.00\n",
      "[58/200][2999/4361] Loss_D: 0.00758491 (Loss_D_real: 0.00155779 Loss_D_fake: 0.00602712) Loss_G: 0.40767413 Loss_Enh_Dec: -1.22966039\n",
      "| epoch  58 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.63 | loss  3.30 | ppl    27.24 | acc     0.63 | train_ae_norm     1.00\n",
      "[58/200][3099/4361] Loss_D: 0.02534458 (Loss_D_real: 0.02110949 Loss_D_fake: 0.00423509) Loss_G: 0.35189492 Loss_Enh_Dec: -1.04035759\n",
      "| epoch  58 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  3.29 | ppl    26.72 | acc     0.62 | train_ae_norm     1.00\n",
      "[58/200][3199/4361] Loss_D: 0.00241978 (Loss_D_real: 0.00032415 Loss_D_fake: 0.00209562) Loss_G: 0.35900146 Loss_Enh_Dec: -1.22235191\n",
      "| epoch  58 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  3.33 | ppl    27.95 | acc     0.63 | train_ae_norm     1.00\n",
      "[58/200][3299/4361] Loss_D: 0.00346552 (Loss_D_real: 0.00136941 Loss_D_fake: 0.00209611) Loss_G: 0.45375976 Loss_Enh_Dec: -1.31427944\n",
      "| epoch  58 |  3300/ 4361 batches | lr 0.000000 | ms/batch 400.80 | loss  3.30 | ppl    27.10 | acc     0.64 | train_ae_norm     1.00\n",
      "[58/200][3399/4361] Loss_D: 0.05895551 (Loss_D_real: 0.00457409 Loss_D_fake: 0.05438141) Loss_G: 0.61646050 Loss_Enh_Dec: -1.17741299\n",
      "| epoch  58 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  3.27 | ppl    26.36 | acc     0.63 | train_ae_norm     1.00\n",
      "[58/200][3499/4361] Loss_D: 0.00671201 (Loss_D_real: 0.00296287 Loss_D_fake: 0.00374914) Loss_G: 0.36566621 Loss_Enh_Dec: -1.39972138\n",
      "| epoch  58 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  3.23 | ppl    25.22 | acc     0.65 | train_ae_norm     1.00\n",
      "[58/200][3599/4361] Loss_D: 0.01926782 (Loss_D_real: 0.00024496 Loss_D_fake: 0.01902286) Loss_G: 0.40780887 Loss_Enh_Dec: -1.63686943\n",
      "| epoch  58 |  3600/ 4361 batches | lr 0.000000 | ms/batch 402.27 | loss  3.24 | ppl    25.41 | acc     0.63 | train_ae_norm     1.00\n",
      "[58/200][3699/4361] Loss_D: 0.01714886 (Loss_D_real: 0.01475008 Loss_D_fake: 0.00239878) Loss_G: 0.55406636 Loss_Enh_Dec: -1.27427578\n",
      "| epoch  58 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  3.25 | ppl    25.77 | acc     0.60 | train_ae_norm     1.00\n",
      "[58/200][3799/4361] Loss_D: 0.00606217 (Loss_D_real: 0.00310696 Loss_D_fake: 0.00295521) Loss_G: 0.49152976 Loss_Enh_Dec: -1.08726394\n",
      "| epoch  58 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  3.26 | ppl    26.07 | acc     0.70 | train_ae_norm     1.00\n",
      "[58/200][3899/4361] Loss_D: 0.03905730 (Loss_D_real: 0.01144147 Loss_D_fake: 0.02761583) Loss_G: 0.41885719 Loss_Enh_Dec: -1.34258306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  58 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.56 | loss  3.24 | ppl    25.43 | acc     0.61 | train_ae_norm     1.00\n",
      "[58/200][3999/4361] Loss_D: 0.03197069 (Loss_D_real: 0.01868466 Loss_D_fake: 0.01328603) Loss_G: 0.42469129 Loss_Enh_Dec: -1.15960491\n",
      "| epoch  58 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  3.23 | ppl    25.28 | acc     0.65 | train_ae_norm     1.00\n",
      "[58/200][4099/4361] Loss_D: 0.00349918 (Loss_D_real: 0.00125786 Loss_D_fake: 0.00224132) Loss_G: 0.38894916 Loss_Enh_Dec: -1.42492771\n",
      "| epoch  58 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  3.19 | ppl    24.33 | acc     0.64 | train_ae_norm     1.00\n",
      "[58/200][4199/4361] Loss_D: 0.01557316 (Loss_D_real: 0.01076570 Loss_D_fake: 0.00480746) Loss_G: 0.39713582 Loss_Enh_Dec: -1.47531605\n",
      "| epoch  58 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  3.23 | ppl    25.36 | acc     0.65 | train_ae_norm     1.00\n",
      "[58/200][4299/4361] Loss_D: 0.00717033 (Loss_D_real: 0.00031775 Loss_D_fake: 0.00685258) Loss_G: 0.42249790 Loss_Enh_Dec: -1.27281189\n",
      "| epoch  58 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  3.19 | ppl    24.41 | acc     0.64 | train_ae_norm     1.00\n",
      "| end of epoch  58 | time: 1851.61s | test loss  3.11 | test ppl 22.34 | acc 0.679\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 59 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.705\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.497\n",
      "  Test Loss: 4.061\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  59 |     0/ 4361 batches | lr 0.000000 | ms/batch 863.31 | loss  0.03 | ppl     1.03 | acc     0.68 | train_ae_norm     1.00\n",
      "[59/200][99/4361] Loss_D: 0.00190621 (Loss_D_real: 0.00081794 Loss_D_fake: 0.00108827) Loss_G: 0.67021245 Loss_Enh_Dec: -0.84459418\n",
      "| epoch  59 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  3.22 | ppl    24.96 | acc     0.61 | train_ae_norm     1.00\n",
      "[59/200][199/4361] Loss_D: 0.00218003 (Loss_D_real: 0.00046783 Loss_D_fake: 0.00171220) Loss_G: 0.33857191 Loss_Enh_Dec: -1.15771854\n",
      "| epoch  59 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.06 | loss  3.24 | ppl    25.41 | acc     0.67 | train_ae_norm     1.00\n",
      "[59/200][299/4361] Loss_D: 0.01560358 (Loss_D_real: 0.01460480 Loss_D_fake: 0.00099877) Loss_G: 0.63543922 Loss_Enh_Dec: -0.90032494\n",
      "| epoch  59 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  3.24 | ppl    25.41 | acc     0.61 | train_ae_norm     1.00\n",
      "[59/200][399/4361] Loss_D: 0.02410137 (Loss_D_real: 0.02147163 Loss_D_fake: 0.00262973) Loss_G: 0.33892250 Loss_Enh_Dec: -1.20337832\n",
      "| epoch  59 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  3.17 | ppl    23.76 | acc     0.65 | train_ae_norm     1.00\n",
      "[59/200][499/4361] Loss_D: 0.00709064 (Loss_D_real: 0.00108183 Loss_D_fake: 0.00600881) Loss_G: 0.35326260 Loss_Enh_Dec: -1.04102671\n",
      "| epoch  59 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  3.23 | ppl    25.32 | acc     0.66 | train_ae_norm     1.00\n",
      "[59/200][599/4361] Loss_D: 0.00558345 (Loss_D_real: 0.00083130 Loss_D_fake: 0.00475215) Loss_G: 0.32620665 Loss_Enh_Dec: -1.24983740\n",
      "| epoch  59 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  3.17 | ppl    23.76 | acc     0.60 | train_ae_norm     1.00\n",
      "[59/200][699/4361] Loss_D: 0.00999780 (Loss_D_real: 0.00676011 Loss_D_fake: 0.00323769) Loss_G: 0.37491581 Loss_Enh_Dec: -1.50889337\n",
      "| epoch  59 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  3.22 | ppl    25.14 | acc     0.67 | train_ae_norm     1.00\n",
      "[59/200][799/4361] Loss_D: 0.00825716 (Loss_D_real: 0.00595202 Loss_D_fake: 0.00230514) Loss_G: 0.43882045 Loss_Enh_Dec: -1.54602706\n",
      "| epoch  59 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  3.21 | ppl    24.67 | acc     0.63 | train_ae_norm     1.00\n",
      "[59/200][899/4361] Loss_D: 0.00454273 (Loss_D_real: 0.00104553 Loss_D_fake: 0.00349720) Loss_G: 0.38705757 Loss_Enh_Dec: -1.40651965\n",
      "| epoch  59 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  3.22 | ppl    24.95 | acc     0.68 | train_ae_norm     1.00\n",
      "[59/200][999/4361] Loss_D: 0.00718171 (Loss_D_real: 0.00055096 Loss_D_fake: 0.00663075) Loss_G: 0.33888435 Loss_Enh_Dec: -1.21447551\n",
      "| epoch  59 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  3.22 | ppl    25.02 | acc     0.66 | train_ae_norm     1.00\n",
      "[59/200][1099/4361] Loss_D: 0.00335462 (Loss_D_real: 0.00047578 Loss_D_fake: 0.00287884) Loss_G: 0.34724355 Loss_Enh_Dec: -1.44509721\n",
      "| epoch  59 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.99 | loss  3.23 | ppl    25.20 | acc     0.62 | train_ae_norm     1.00\n",
      "[59/200][1199/4361] Loss_D: 0.00471059 (Loss_D_real: 0.00073951 Loss_D_fake: 0.00397107) Loss_G: 0.33584294 Loss_Enh_Dec: -1.31261969\n",
      "| epoch  59 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  3.24 | ppl    25.55 | acc     0.65 | train_ae_norm     1.00\n",
      "[59/200][1299/4361] Loss_D: 0.00943104 (Loss_D_real: 0.00892842 Loss_D_fake: 0.00050262) Loss_G: 0.44866171 Loss_Enh_Dec: -1.25671947\n",
      "| epoch  59 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.74 | loss  3.26 | ppl    26.10 | acc     0.64 | train_ae_norm     1.00\n",
      "[59/200][1399/4361] Loss_D: 0.00283872 (Loss_D_real: 0.00203387 Loss_D_fake: 0.00080485) Loss_G: 0.38148567 Loss_Enh_Dec: -1.04409242\n",
      "| epoch  59 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  3.22 | ppl    25.04 | acc     0.61 | train_ae_norm     1.00\n",
      "[59/200][1499/4361] Loss_D: 0.04577340 (Loss_D_real: 0.04366825 Loss_D_fake: 0.00210515) Loss_G: 0.41622248 Loss_Enh_Dec: -1.12522531\n",
      "| epoch  59 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  3.28 | ppl    26.49 | acc     0.63 | train_ae_norm     1.00\n",
      "[59/200][1599/4361] Loss_D: 0.00198464 (Loss_D_real: 0.00037378 Loss_D_fake: 0.00161087) Loss_G: 0.41087332 Loss_Enh_Dec: -1.35427177\n",
      "| epoch  59 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  3.24 | ppl    25.49 | acc     0.63 | train_ae_norm     1.00\n",
      "[59/200][1699/4361] Loss_D: 0.00948092 (Loss_D_real: 0.00516695 Loss_D_fake: 0.00431397) Loss_G: 0.37130675 Loss_Enh_Dec: -1.38650537\n",
      "| epoch  59 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  3.21 | ppl    24.85 | acc     0.62 | train_ae_norm     1.00\n",
      "[59/200][1999/4361] Loss_D: 0.00279747 (Loss_D_real: 0.00071588 Loss_D_fake: 0.00208159) Loss_G: 0.41904172 Loss_Enh_Dec: -1.86180401\n",
      "| epoch  59 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  3.19 | ppl    24.18 | acc     0.66 | train_ae_norm     1.00\n",
      "[59/200][2099/4361] Loss_D: 0.00407546 (Loss_D_real: 0.00064237 Loss_D_fake: 0.00343309) Loss_G: 0.38467440 Loss_Enh_Dec: -1.19219053\n",
      "| epoch  59 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  3.23 | ppl    25.36 | acc     0.65 | train_ae_norm     1.00\n",
      "[59/200][2199/4361] Loss_D: 0.01080084 (Loss_D_real: 0.00400665 Loss_D_fake: 0.00679420) Loss_G: 0.38861796 Loss_Enh_Dec: -1.37560356\n",
      "| epoch  59 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  3.20 | ppl    24.61 | acc     0.63 | train_ae_norm     1.00\n",
      "[59/200][2299/4361] Loss_D: 0.00389882 (Loss_D_real: 0.00050850 Loss_D_fake: 0.00339033) Loss_G: 0.34740630 Loss_Enh_Dec: -1.39779818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  59 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  3.17 | ppl    23.90 | acc     0.64 | train_ae_norm     1.00\n",
      "[59/200][2399/4361] Loss_D: 0.00406872 (Loss_D_real: 0.00371902 Loss_D_fake: 0.00034970) Loss_G: 0.58022797 Loss_Enh_Dec: -1.33958173\n",
      "| epoch  59 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  3.19 | ppl    24.34 | acc     0.62 | train_ae_norm     1.00\n",
      "[59/200][2499/4361] Loss_D: 0.00479215 (Loss_D_real: 0.00197181 Loss_D_fake: 0.00282033) Loss_G: 0.35935766 Loss_Enh_Dec: -1.68697953\n",
      "| epoch  59 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  3.23 | ppl    25.32 | acc     0.64 | train_ae_norm     1.00\n",
      "[59/200][2599/4361] Loss_D: 0.00481003 (Loss_D_real: 0.00303352 Loss_D_fake: 0.00177651) Loss_G: 0.38221818 Loss_Enh_Dec: -1.58689344\n",
      "| epoch  59 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  3.20 | ppl    24.47 | acc     0.63 | train_ae_norm     1.00\n",
      "[59/200][2699/4361] Loss_D: 0.00240931 (Loss_D_real: 0.00170312 Loss_D_fake: 0.00070619) Loss_G: 0.47206792 Loss_Enh_Dec: -1.11027253\n",
      "| epoch  59 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  3.20 | ppl    24.55 | acc     0.63 | train_ae_norm     1.00\n",
      "[59/200][2799/4361] Loss_D: 0.00380961 (Loss_D_real: 0.00148031 Loss_D_fake: 0.00232930) Loss_G: 0.39831063 Loss_Enh_Dec: -1.29697526\n",
      "| epoch  59 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  3.14 | ppl    23.13 | acc     0.64 | train_ae_norm     1.00\n",
      "[59/200][2899/4361] Loss_D: 0.01504701 (Loss_D_real: 0.00051944 Loss_D_fake: 0.01452757) Loss_G: 0.37873352 Loss_Enh_Dec: -1.33774471\n",
      "| epoch  59 |  2900/ 4361 batches | lr 0.000000 | ms/batch 402.00 | loss  3.19 | ppl    24.22 | acc     0.62 | train_ae_norm     1.00\n",
      "[59/200][2999/4361] Loss_D: 0.01221282 (Loss_D_real: 0.00662644 Loss_D_fake: 0.00558638) Loss_G: 0.47925907 Loss_Enh_Dec: -1.59359920\n",
      "| epoch  59 |  3000/ 4361 batches | lr 0.000000 | ms/batch 402.17 | loss  3.19 | ppl    24.33 | acc     0.65 | train_ae_norm     1.00\n",
      "[59/200][3099/4361] Loss_D: 0.00634757 (Loss_D_real: 0.00288578 Loss_D_fake: 0.00346179) Loss_G: 0.35779637 Loss_Enh_Dec: -1.38297498\n",
      "| epoch  59 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  3.20 | ppl    24.63 | acc     0.61 | train_ae_norm     1.00\n",
      "[59/200][3199/4361] Loss_D: 0.01573358 (Loss_D_real: 0.00130871 Loss_D_fake: 0.01442487) Loss_G: 0.42649385 Loss_Enh_Dec: -1.50100267\n",
      "| epoch  59 |  3200/ 4361 batches | lr 0.000000 | ms/batch 402.00 | loss  3.23 | ppl    25.36 | acc     0.63 | train_ae_norm     1.00\n",
      "[59/200][3299/4361] Loss_D: 0.01814849 (Loss_D_real: 0.01586690 Loss_D_fake: 0.00228159) Loss_G: 0.33055568 Loss_Enh_Dec: -1.58676720\n",
      "| epoch  59 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  3.22 | ppl    25.15 | acc     0.64 | train_ae_norm     1.00\n",
      "[59/200][3399/4361] Loss_D: 0.00863716 (Loss_D_real: 0.00589357 Loss_D_fake: 0.00274359) Loss_G: 0.38254166 Loss_Enh_Dec: -1.38707769\n",
      "| epoch  59 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  3.20 | ppl    24.50 | acc     0.64 | train_ae_norm     1.00\n",
      "[59/200][3499/4361] Loss_D: 0.01059622 (Loss_D_real: 0.00270171 Loss_D_fake: 0.00789450) Loss_G: 0.36349878 Loss_Enh_Dec: -1.33063972\n",
      "| epoch  59 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  3.15 | ppl    23.22 | acc     0.66 | train_ae_norm     1.00\n",
      "[59/200][3599/4361] Loss_D: 0.03090924 (Loss_D_real: 0.02943674 Loss_D_fake: 0.00147249) Loss_G: 0.50946015 Loss_Enh_Dec: -0.92897719\n",
      "| epoch  59 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  3.16 | ppl    23.56 | acc     0.67 | train_ae_norm     1.00\n",
      "[59/200][3699/4361] Loss_D: 0.00546598 (Loss_D_real: 0.00159475 Loss_D_fake: 0.00387123) Loss_G: 0.37947416 Loss_Enh_Dec: -1.45743656\n",
      "| epoch  59 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.94 | loss  3.17 | ppl    23.70 | acc     0.62 | train_ae_norm     1.00\n",
      "[59/200][3799/4361] Loss_D: 0.01094368 (Loss_D_real: 0.00494503 Loss_D_fake: 0.00599865) Loss_G: 0.34266993 Loss_Enh_Dec: -0.98261750\n",
      "| epoch  59 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  3.19 | ppl    24.34 | acc     0.70 | train_ae_norm     1.00\n",
      "[59/200][3899/4361] Loss_D: 0.00891053 (Loss_D_real: 0.00105395 Loss_D_fake: 0.00785658) Loss_G: 0.39709696 Loss_Enh_Dec: -0.87328905\n",
      "| epoch  59 |  3900/ 4361 batches | lr 0.000000 | ms/batch 402.07 | loss  3.19 | ppl    24.18 | acc     0.63 | train_ae_norm     1.00\n",
      "[59/200][3999/4361] Loss_D: 0.00368965 (Loss_D_real: 0.00207529 Loss_D_fake: 0.00161436) Loss_G: 0.34803197 Loss_Enh_Dec: -1.12368774\n",
      "| epoch  59 |  4000/ 4361 batches | lr 0.000000 | ms/batch 402.33 | loss  3.20 | ppl    24.54 | acc     0.64 | train_ae_norm     1.00\n",
      "[59/200][4099/4361] Loss_D: 0.00276096 (Loss_D_real: 0.00047895 Loss_D_fake: 0.00228201) Loss_G: 0.41521642 Loss_Enh_Dec: -1.20662522\n",
      "| epoch  59 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  3.15 | ppl    23.32 | acc     0.65 | train_ae_norm     1.00\n",
      "[59/200][4199/4361] Loss_D: 0.01117510 (Loss_D_real: 0.00711757 Loss_D_fake: 0.00405754) Loss_G: 0.38367629 Loss_Enh_Dec: -1.08241308\n",
      "| epoch  59 |  4200/ 4361 batches | lr 0.000000 | ms/batch 402.32 | loss  3.19 | ppl    24.37 | acc     0.70 | train_ae_norm     1.00\n",
      "[59/200][4299/4361] Loss_D: 0.00118267 (Loss_D_real: 0.00036774 Loss_D_fake: 0.00081493) Loss_G: 0.37364358 Loss_Enh_Dec: -0.78387731\n",
      "| epoch  59 |  4300/ 4361 batches | lr 0.000000 | ms/batch 402.16 | loss  3.13 | ppl    22.86 | acc     0.67 | train_ae_norm     1.00\n",
      "| end of epoch  59 | time: 1853.54s | test loss  3.07 | test ppl 21.50 | acc 0.682\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 60 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:57.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:16.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.704\n",
      "  Training epcoh took: 0:01:27\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.495\n",
      "  Test Loss: 4.046\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  60 |     0/ 4361 batches | lr 0.000000 | ms/batch 869.91 | loss  0.03 | ppl     1.03 | acc     0.68 | train_ae_norm     1.00\n",
      "[60/200][99/4361] Loss_D: 0.00536980 (Loss_D_real: 0.00207629 Loss_D_fake: 0.00329352) Loss_G: 0.47892857 Loss_Enh_Dec: -1.42500365\n",
      "| epoch  60 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.39 | loss  3.16 | ppl    23.48 | acc     0.61 | train_ae_norm     1.00\n",
      "[60/200][199/4361] Loss_D: 0.01388993 (Loss_D_real: 0.00274357 Loss_D_fake: 0.01114636) Loss_G: 0.43542534 Loss_Enh_Dec: -0.95874119\n",
      "| epoch  60 |   200/ 4361 batches | lr 0.000000 | ms/batch 402.04 | loss  3.19 | ppl    24.31 | acc     0.65 | train_ae_norm     1.00\n",
      "[60/200][299/4361] Loss_D: 0.00599177 (Loss_D_real: 0.00229408 Loss_D_fake: 0.00369769) Loss_G: 0.37734768 Loss_Enh_Dec: -1.03882074\n",
      "| epoch  60 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  3.18 | ppl    24.05 | acc     0.62 | train_ae_norm     1.00\n",
      "[60/200][399/4361] Loss_D: 0.03534286 (Loss_D_real: 0.02420921 Loss_D_fake: 0.01113365) Loss_G: 0.37659952 Loss_Enh_Dec: -0.99150592\n",
      "| epoch  60 |   400/ 4361 batches | lr 0.000000 | ms/batch 402.32 | loss  3.11 | ppl    22.35 | acc     0.66 | train_ae_norm     1.00\n",
      "[60/200][499/4361] Loss_D: 0.00191158 (Loss_D_real: 0.00045588 Loss_D_fake: 0.00145570) Loss_G: 0.33798003 Loss_Enh_Dec: -0.92014611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  60 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.69 | loss  3.15 | ppl    23.35 | acc     0.68 | train_ae_norm     1.00\n",
      "[60/200][599/4361] Loss_D: 0.00313499 (Loss_D_real: 0.00205455 Loss_D_fake: 0.00108044) Loss_G: 0.40966174 Loss_Enh_Dec: -0.93832606\n",
      "| epoch  60 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  3.11 | ppl    22.53 | acc     0.64 | train_ae_norm     1.00\n",
      "[60/200][699/4361] Loss_D: 0.00462023 (Loss_D_real: 0.00242666 Loss_D_fake: 0.00219357) Loss_G: 0.42349535 Loss_Enh_Dec: -1.09378040\n",
      "| epoch  60 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  3.16 | ppl    23.62 | acc     0.66 | train_ae_norm     1.00\n",
      "[60/200][799/4361] Loss_D: 0.00377317 (Loss_D_real: 0.00150994 Loss_D_fake: 0.00226323) Loss_G: 0.41914639 Loss_Enh_Dec: -0.95570594\n",
      "| epoch  60 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  3.15 | ppl    23.36 | acc     0.64 | train_ae_norm     1.00\n",
      "[60/200][899/4361] Loss_D: 0.00924767 (Loss_D_real: 0.00850443 Loss_D_fake: 0.00074325) Loss_G: 0.44329023 Loss_Enh_Dec: -0.97984165\n",
      "| epoch  60 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  3.15 | ppl    23.36 | acc     0.66 | train_ae_norm     1.00\n",
      "[60/200][999/4361] Loss_D: 0.00417828 (Loss_D_real: 0.00112802 Loss_D_fake: 0.00305027) Loss_G: 0.37592259 Loss_Enh_Dec: -1.13608074\n",
      "| epoch  60 |  1000/ 4361 batches | lr 0.000000 | ms/batch 402.08 | loss  3.15 | ppl    23.33 | acc     0.66 | train_ae_norm     1.00\n",
      "[60/200][1099/4361] Loss_D: 0.00907806 (Loss_D_real: 0.00600440 Loss_D_fake: 0.00307367) Loss_G: 0.36931178 Loss_Enh_Dec: -1.15988982\n",
      "| epoch  60 |  1100/ 4361 batches | lr 0.000000 | ms/batch 402.30 | loss  3.15 | ppl    23.31 | acc     0.62 | train_ae_norm     1.00\n",
      "[60/200][1199/4361] Loss_D: 0.00683207 (Loss_D_real: 0.00187233 Loss_D_fake: 0.00495974) Loss_G: 0.37038001 Loss_Enh_Dec: -1.31725419\n",
      "| epoch  60 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  3.16 | ppl    23.46 | acc     0.66 | train_ae_norm     1.00\n",
      "[60/200][1299/4361] Loss_D: 0.00998831 (Loss_D_real: 0.00154159 Loss_D_fake: 0.00844672) Loss_G: 0.40726995 Loss_Enh_Dec: -1.20274997\n",
      "| epoch  60 |  1300/ 4361 batches | lr 0.000000 | ms/batch 402.45 | loss  3.17 | ppl    23.90 | acc     0.65 | train_ae_norm     1.00\n",
      "[60/200][1399/4361] Loss_D: 0.00504137 (Loss_D_real: 0.00414502 Loss_D_fake: 0.00089634) Loss_G: 0.64122790 Loss_Enh_Dec: -1.31185329\n",
      "| epoch  60 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  3.16 | ppl    23.55 | acc     0.61 | train_ae_norm     1.00\n",
      "[60/200][1499/4361] Loss_D: 0.00332504 (Loss_D_real: 0.00190471 Loss_D_fake: 0.00142032) Loss_G: 0.39614755 Loss_Enh_Dec: -1.04955614\n",
      "| epoch  60 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  3.21 | ppl    24.76 | acc     0.63 | train_ae_norm     1.00\n",
      "[60/200][1599/4361] Loss_D: 0.00292733 (Loss_D_real: 0.00047901 Loss_D_fake: 0.00244832) Loss_G: 0.36264247 Loss_Enh_Dec: -1.63036942\n",
      "| epoch  60 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  3.17 | ppl    23.79 | acc     0.67 | train_ae_norm     1.00\n",
      "[60/200][1699/4361] Loss_D: 0.01246993 (Loss_D_real: 0.01105026 Loss_D_fake: 0.00141967) Loss_G: 0.40453020 Loss_Enh_Dec: -1.67387450\n",
      "| epoch  60 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  3.13 | ppl    22.82 | acc     0.66 | train_ae_norm     1.00\n",
      "[60/200][1799/4361] Loss_D: 0.00518920 (Loss_D_real: 0.00088916 Loss_D_fake: 0.00430004) Loss_G: 0.36206350 Loss_Enh_Dec: -1.23639166\n",
      "| epoch  60 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  3.11 | ppl    22.49 | acc     0.67 | train_ae_norm     1.00\n",
      "[60/200][1899/4361] Loss_D: 0.00484848 (Loss_D_real: 0.00452711 Loss_D_fake: 0.00032138) Loss_G: 0.75033849 Loss_Enh_Dec: -1.79996097\n",
      "| epoch  60 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  3.18 | ppl    24.06 | acc     0.68 | train_ae_norm     1.00\n",
      "[60/200][1999/4361] Loss_D: 0.00849175 (Loss_D_real: 0.00460894 Loss_D_fake: 0.00388282) Loss_G: 0.40679336 Loss_Enh_Dec: -1.15999496\n",
      "| epoch  60 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  3.12 | ppl    22.57 | acc     0.68 | train_ae_norm     1.00\n",
      "[60/200][2099/4361] Loss_D: 0.00205104 (Loss_D_real: 0.00010760 Loss_D_fake: 0.00194344) Loss_G: 0.37529972 Loss_Enh_Dec: -1.63142812\n",
      "| epoch  60 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  3.15 | ppl    23.26 | acc     0.66 | train_ae_norm     1.00\n",
      "[60/200][2199/4361] Loss_D: 0.00543660 (Loss_D_real: 0.00087318 Loss_D_fake: 0.00456342) Loss_G: 0.39129576 Loss_Enh_Dec: -1.22651315\n",
      "| epoch  60 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  3.13 | ppl    22.88 | acc     0.65 | train_ae_norm     1.00\n",
      "[60/200][2299/4361] Loss_D: 0.00363356 (Loss_D_real: 0.00070477 Loss_D_fake: 0.00292879) Loss_G: 0.40147611 Loss_Enh_Dec: -1.92933500\n",
      "| epoch  60 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  3.12 | ppl    22.62 | acc     0.68 | train_ae_norm     1.00\n",
      "[60/200][2399/4361] Loss_D: 0.00515788 (Loss_D_real: 0.00289678 Loss_D_fake: 0.00226110) Loss_G: 0.39201432 Loss_Enh_Dec: -1.17159832\n",
      "| epoch  60 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  3.13 | ppl    22.97 | acc     0.61 | train_ae_norm     1.00\n",
      "[60/200][2499/4361] Loss_D: 0.00891128 (Loss_D_real: 0.00439548 Loss_D_fake: 0.00451581) Loss_G: 0.34427413 Loss_Enh_Dec: -1.62215841\n",
      "| epoch  60 |  2500/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  3.18 | ppl    24.04 | acc     0.65 | train_ae_norm     1.00\n",
      "[60/200][2599/4361] Loss_D: 0.00680707 (Loss_D_real: 0.00365292 Loss_D_fake: 0.00315415) Loss_G: 0.43474227 Loss_Enh_Dec: -1.31832969\n",
      "| epoch  60 |  2600/ 4361 batches | lr 0.000000 | ms/batch 402.20 | loss  3.14 | ppl    23.13 | acc     0.62 | train_ae_norm     1.00\n",
      "[60/200][2699/4361] Loss_D: 0.02923329 (Loss_D_real: 0.02728060 Loss_D_fake: 0.00195268) Loss_G: 0.38234115 Loss_Enh_Dec: -1.35110474\n",
      "| epoch  60 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  3.15 | ppl    23.44 | acc     0.63 | train_ae_norm     1.00\n",
      "[60/200][2799/4361] Loss_D: 0.05216151 (Loss_D_real: 0.00046059 Loss_D_fake: 0.05170092) Loss_G: 0.43612871 Loss_Enh_Dec: -1.65707242\n",
      "| epoch  60 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  3.09 | ppl    22.01 | acc     0.68 | train_ae_norm     1.00\n",
      "[60/200][2899/4361] Loss_D: 0.00348639 (Loss_D_real: 0.00070845 Loss_D_fake: 0.00277794) Loss_G: 0.37682459 Loss_Enh_Dec: -1.50468016\n",
      "| epoch  60 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  3.14 | ppl    23.15 | acc     0.65 | train_ae_norm     1.00\n",
      "[60/200][2999/4361] Loss_D: 0.00676061 (Loss_D_real: 0.00235539 Loss_D_fake: 0.00440521) Loss_G: 0.45222473 Loss_Enh_Dec: -1.34061801\n",
      "| epoch  60 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  3.15 | ppl    23.39 | acc     0.66 | train_ae_norm     1.00\n",
      "[60/200][3099/4361] Loss_D: 0.00514286 (Loss_D_real: 0.00110781 Loss_D_fake: 0.00403505) Loss_G: 0.37706691 Loss_Enh_Dec: -1.10784912\n",
      "| epoch  60 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  3.16 | ppl    23.47 | acc     0.62 | train_ae_norm     1.00\n",
      "[60/200][3199/4361] Loss_D: 0.00337417 (Loss_D_real: 0.00094545 Loss_D_fake: 0.00242872) Loss_G: 0.36777851 Loss_Enh_Dec: -1.47465396\n",
      "| epoch  60 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  3.19 | ppl    24.40 | acc     0.66 | train_ae_norm     1.00\n",
      "[60/200][3299/4361] Loss_D: 0.02695100 (Loss_D_real: 0.02409341 Loss_D_fake: 0.00285759) Loss_G: 0.41091591 Loss_Enh_Dec: -1.12687838\n",
      "| epoch  60 |  3300/ 4361 batches | lr 0.000000 | ms/batch 402.42 | loss  3.20 | ppl    24.61 | acc     0.66 | train_ae_norm     1.00\n",
      "[60/200][3399/4361] Loss_D: 0.00492057 (Loss_D_real: 0.00125405 Loss_D_fake: 0.00366652) Loss_G: 0.40602168 Loss_Enh_Dec: -1.03489876\n",
      "| epoch  60 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  3.17 | ppl    23.83 | acc     0.66 | train_ae_norm     1.00\n",
      "[60/200][3499/4361] Loss_D: 0.00846995 (Loss_D_real: 0.00123880 Loss_D_fake: 0.00723115) Loss_G: 0.36973140 Loss_Enh_Dec: -1.13268697\n",
      "| epoch  60 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  3.11 | ppl    22.46 | acc     0.64 | train_ae_norm     1.00\n",
      "[60/200][3599/4361] Loss_D: 0.00431929 (Loss_D_real: 0.00159667 Loss_D_fake: 0.00272262) Loss_G: 0.44566399 Loss_Enh_Dec: -1.45892596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  60 |  3600/ 4361 batches | lr 0.000000 | ms/batch 402.11 | loss  3.13 | ppl    22.87 | acc     0.67 | train_ae_norm     1.00\n",
      "[60/200][3699/4361] Loss_D: 0.00134069 (Loss_D_real: 0.00050384 Loss_D_fake: 0.00083685) Loss_G: 0.41261521 Loss_Enh_Dec: -1.74431419\n",
      "| epoch  60 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  3.16 | ppl    23.48 | acc     0.61 | train_ae_norm     1.00\n",
      "[60/200][3799/4361] Loss_D: 0.00822140 (Loss_D_real: 0.00392141 Loss_D_fake: 0.00429999) Loss_G: 0.42806378 Loss_Enh_Dec: -1.20488513\n",
      "| epoch  60 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  3.18 | ppl    23.97 | acc     0.69 | train_ae_norm     1.00\n",
      "[60/200][3899/4361] Loss_D: 0.00355101 (Loss_D_real: 0.00037698 Loss_D_fake: 0.00317403) Loss_G: 0.41240293 Loss_Enh_Dec: -1.87762916\n",
      "| epoch  60 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  3.19 | ppl    24.34 | acc     0.61 | train_ae_norm     1.00\n",
      "[60/200][3999/4361] Loss_D: 0.00319482 (Loss_D_real: 0.00094919 Loss_D_fake: 0.00224563) Loss_G: 0.38375720 Loss_Enh_Dec: -1.77336562\n",
      "| epoch  60 |  4000/ 4361 batches | lr 0.000000 | ms/batch 402.56 | loss  3.20 | ppl    24.43 | acc     0.67 | train_ae_norm     1.00\n",
      "[60/200][4099/4361] Loss_D: 0.00850771 (Loss_D_real: 0.00680411 Loss_D_fake: 0.00170360) Loss_G: 0.40423656 Loss_Enh_Dec: -2.24011493\n",
      "| epoch  60 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  3.13 | ppl    22.83 | acc     0.64 | train_ae_norm     1.00\n",
      "[60/200][4199/4361] Loss_D: 0.00248605 (Loss_D_real: 0.00042740 Loss_D_fake: 0.00205865) Loss_G: 0.37769553 Loss_Enh_Dec: -1.72007787\n",
      "| epoch  60 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  3.19 | ppl    24.18 | acc     0.68 | train_ae_norm     1.00\n",
      "[60/200][4299/4361] Loss_D: 0.00381938 (Loss_D_real: 0.00079617 Loss_D_fake: 0.00302321) Loss_G: 0.38770843 Loss_Enh_Dec: -1.97930396\n",
      "| epoch  60 |  4300/ 4361 batches | lr 0.000000 | ms/batch 402.23 | loss  3.14 | ppl    23.17 | acc     0.66 | train_ae_norm     1.00\n",
      "| end of epoch  60 | time: 1854.46s | test loss  3.06 | test ppl 21.30 | acc 0.684\n",
      "bleu_self:  [5.72916667e-02 1.92576410e-09 6.70866279e-12 7.04207589e-12\n",
      " 5.47443150e-11]\n",
      "bleu_test:  [7.08581349e-01 1.38417267e-01 1.07877998e-06 4.39916004e-08\n",
      " 5.88296029e-08]\n",
      "bleu_self: [0.05729167,0.00000000,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.70858135,0.13841727,0.00000108,0.00000004,0.00000006]\n",
      "New saving model: epoch 060.\n",
      "Saving models to ./results/yahoo_merge_assigned_results\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 61 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.706\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.482\n",
      "  Test Loss: 4.073\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  61 |     0/ 4361 batches | lr 0.000000 | ms/batch 866.66 | loss  0.03 | ppl     1.03 | acc     0.69 | train_ae_norm     1.00\n",
      "[61/200][99/4361] Loss_D: 0.00186693 (Loss_D_real: 0.00181849 Loss_D_fake: 0.00004844) Loss_G: 0.71436113 Loss_Enh_Dec: -1.00539935\n",
      "| epoch  61 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  3.16 | ppl    23.68 | acc     0.61 | train_ae_norm     1.00\n",
      "[61/200][199/4361] Loss_D: 0.00546970 (Loss_D_real: 0.00022478 Loss_D_fake: 0.00524492) Loss_G: 0.38759822 Loss_Enh_Dec: -1.69588113\n",
      "| epoch  61 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  3.19 | ppl    24.23 | acc     0.66 | train_ae_norm     1.00\n",
      "[61/200][299/4361] Loss_D: 0.00573784 (Loss_D_real: 0.00247331 Loss_D_fake: 0.00326453) Loss_G: 0.38247404 Loss_Enh_Dec: -1.72583699\n",
      "| epoch  61 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  3.18 | ppl    24.12 | acc     0.61 | train_ae_norm     1.00\n",
      "[61/200][399/4361] Loss_D: 0.02952706 (Loss_D_real: 0.02834386 Loss_D_fake: 0.00118320) Loss_G: 0.37343702 Loss_Enh_Dec: -1.16744566\n",
      "| epoch  61 |   400/ 4361 batches | lr 0.000000 | ms/batch 402.22 | loss  3.09 | ppl    22.08 | acc     0.66 | train_ae_norm     1.00\n",
      "[61/200][499/4361] Loss_D: 0.01503281 (Loss_D_real: 0.00959926 Loss_D_fake: 0.00543355) Loss_G: 0.42152300 Loss_Enh_Dec: -1.40747285\n",
      "| epoch  61 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  3.16 | ppl    23.60 | acc     0.68 | train_ae_norm     1.00\n",
      "[61/200][599/4361] Loss_D: 0.00170138 (Loss_D_real: 0.00060361 Loss_D_fake: 0.00109777) Loss_G: 0.36228740 Loss_Enh_Dec: -1.60635412\n",
      "| epoch  61 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  3.13 | ppl    22.89 | acc     0.63 | train_ae_norm     1.00\n",
      "[61/200][699/4361] Loss_D: 0.00541822 (Loss_D_real: 0.00244839 Loss_D_fake: 0.00296983) Loss_G: 0.46993238 Loss_Enh_Dec: -1.44759333\n",
      "| epoch  61 |   700/ 4361 batches | lr 0.000000 | ms/batch 402.19 | loss  3.17 | ppl    23.91 | acc     0.67 | train_ae_norm     1.00\n",
      "[61/200][799/4361] Loss_D: 0.02851375 (Loss_D_real: 0.00336739 Loss_D_fake: 0.02514637) Loss_G: 0.47145757 Loss_Enh_Dec: -1.11993062\n",
      "| epoch  61 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  3.15 | ppl    23.32 | acc     0.65 | train_ae_norm     1.00\n",
      "[61/200][899/4361] Loss_D: 0.00575319 (Loss_D_real: 0.00214345 Loss_D_fake: 0.00360974) Loss_G: 0.42421457 Loss_Enh_Dec: -1.15564728\n",
      "| epoch  61 |   900/ 4361 batches | lr 0.000000 | ms/batch 402.53 | loss  3.16 | ppl    23.67 | acc     0.68 | train_ae_norm     1.00\n",
      "[61/200][999/4361] Loss_D: 0.00615807 (Loss_D_real: 0.00423689 Loss_D_fake: 0.00192118) Loss_G: 0.42313758 Loss_Enh_Dec: -1.29705381\n",
      "| epoch  61 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  3.15 | ppl    23.28 | acc     0.65 | train_ae_norm     1.00\n",
      "[61/200][1099/4361] Loss_D: 0.00422520 (Loss_D_real: 0.00071865 Loss_D_fake: 0.00350655) Loss_G: 0.38660929 Loss_Enh_Dec: -1.56748104\n",
      "| epoch  61 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  3.16 | ppl    23.48 | acc     0.63 | train_ae_norm     1.00\n",
      "[61/200][1199/4361] Loss_D: 0.00230641 (Loss_D_real: 0.00018264 Loss_D_fake: 0.00212377) Loss_G: 0.42385674 Loss_Enh_Dec: -1.70165288\n",
      "| epoch  61 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.57 | loss  3.17 | ppl    23.76 | acc     0.65 | train_ae_norm     1.00\n",
      "[61/200][1299/4361] Loss_D: 0.00424600 (Loss_D_real: 0.00246703 Loss_D_fake: 0.00177897) Loss_G: 0.44508335 Loss_Enh_Dec: -1.70083678\n",
      "| epoch  61 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  3.19 | ppl    24.33 | acc     0.63 | train_ae_norm     1.00\n",
      "[61/200][1399/4361] Loss_D: 0.00180172 (Loss_D_real: 0.00029772 Loss_D_fake: 0.00150400) Loss_G: 0.44261861 Loss_Enh_Dec: -1.22576022\n",
      "| epoch  61 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.57 | loss  3.19 | ppl    24.24 | acc     0.61 | train_ae_norm     1.00\n",
      "[61/200][1499/4361] Loss_D: 0.00256734 (Loss_D_real: 0.00107584 Loss_D_fake: 0.00149150) Loss_G: 0.43213746 Loss_Enh_Dec: -1.41403043\n",
      "| epoch  61 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  3.21 | ppl    24.71 | acc     0.63 | train_ae_norm     1.00\n",
      "[61/200][1599/4361] Loss_D: 0.00728444 (Loss_D_real: 0.00223708 Loss_D_fake: 0.00504736) Loss_G: 0.36337298 Loss_Enh_Dec: -1.63083613\n",
      "| epoch  61 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  3.17 | ppl    23.88 | acc     0.64 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61/200][1699/4361] Loss_D: 0.00745903 (Loss_D_real: 0.00378644 Loss_D_fake: 0.00367259) Loss_G: 0.37160277 Loss_Enh_Dec: -1.40818405\n",
      "| epoch  61 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  3.16 | ppl    23.50 | acc     0.64 | train_ae_norm     1.00\n",
      "[61/200][1799/4361] Loss_D: 0.05193255 (Loss_D_real: 0.04939127 Loss_D_fake: 0.00254129) Loss_G: 0.55609733 Loss_Enh_Dec: -1.54685366\n",
      "| epoch  61 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  3.14 | ppl    23.16 | acc     0.66 | train_ae_norm     1.00\n",
      "[61/200][1899/4361] Loss_D: 0.00151252 (Loss_D_real: 0.00064394 Loss_D_fake: 0.00086858) Loss_G: 0.52325487 Loss_Enh_Dec: -1.08309042\n",
      "| epoch  61 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  3.19 | ppl    24.29 | acc     0.67 | train_ae_norm     1.00\n",
      "[61/200][1999/4361] Loss_D: 0.00554042 (Loss_D_real: 0.00469482 Loss_D_fake: 0.00084560) Loss_G: 0.40608701 Loss_Enh_Dec: -1.63648117\n",
      "| epoch  61 |  2000/ 4361 batches | lr 0.000000 | ms/batch 402.30 | loss  3.13 | ppl    22.90 | acc     0.64 | train_ae_norm     1.00\n",
      "[61/200][2099/4361] Loss_D: 0.00580258 (Loss_D_real: 0.00441762 Loss_D_fake: 0.00138496) Loss_G: 0.37975338 Loss_Enh_Dec: -0.98456585\n",
      "| epoch  61 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  3.16 | ppl    23.61 | acc     0.66 | train_ae_norm     1.00\n",
      "[61/200][2199/4361] Loss_D: 0.00299471 (Loss_D_real: 0.00143088 Loss_D_fake: 0.00156382) Loss_G: 0.39983732 Loss_Enh_Dec: -1.49753189\n",
      "| epoch  61 |  2200/ 4361 batches | lr 0.000000 | ms/batch 402.08 | loss  3.15 | ppl    23.39 | acc     0.65 | train_ae_norm     1.00\n",
      "[61/200][2299/4361] Loss_D: 0.02055454 (Loss_D_real: 0.01059266 Loss_D_fake: 0.00996188) Loss_G: 0.35737923 Loss_Enh_Dec: -1.02100015\n",
      "| epoch  61 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  3.13 | ppl    22.95 | acc     0.67 | train_ae_norm     1.00\n",
      "[61/200][2399/4361] Loss_D: 0.00469408 (Loss_D_real: 0.00090634 Loss_D_fake: 0.00378774) Loss_G: 0.49131805 Loss_Enh_Dec: -1.16988122\n",
      "| epoch  61 |  2400/ 4361 batches | lr 0.000000 | ms/batch 402.39 | loss  3.14 | ppl    23.21 | acc     0.63 | train_ae_norm     1.00\n",
      "[61/200][2499/4361] Loss_D: 0.00121718 (Loss_D_real: 0.00016876 Loss_D_fake: 0.00104842) Loss_G: 0.36798713 Loss_Enh_Dec: -1.24688280\n",
      "| epoch  61 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  3.18 | ppl    24.07 | acc     0.65 | train_ae_norm     1.00\n",
      "[61/200][2599/4361] Loss_D: 0.00350726 (Loss_D_real: 0.00084644 Loss_D_fake: 0.00266082) Loss_G: 0.46563926 Loss_Enh_Dec: -1.47401083\n",
      "| epoch  61 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  3.14 | ppl    23.20 | acc     0.61 | train_ae_norm     1.00\n",
      "[61/200][2699/4361] Loss_D: 0.00156802 (Loss_D_real: 0.00046630 Loss_D_fake: 0.00110172) Loss_G: 0.38450465 Loss_Enh_Dec: -0.94709301\n",
      "| epoch  61 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  3.15 | ppl    23.43 | acc     0.66 | train_ae_norm     1.00\n",
      "[61/200][2799/4361] Loss_D: 0.00408000 (Loss_D_real: 0.00092904 Loss_D_fake: 0.00315096) Loss_G: 0.39375484 Loss_Enh_Dec: -0.87157249\n",
      "| epoch  61 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  3.10 | ppl    22.14 | acc     0.66 | train_ae_norm     1.00\n",
      "[61/200][2899/4361] Loss_D: 0.00788118 (Loss_D_real: 0.00453354 Loss_D_fake: 0.00334764) Loss_G: 0.36511552 Loss_Enh_Dec: -1.03957772\n",
      "| epoch  61 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  3.14 | ppl    23.15 | acc     0.66 | train_ae_norm     1.00\n",
      "[61/200][2999/4361] Loss_D: 0.01696179 (Loss_D_real: 0.01560923 Loss_D_fake: 0.00135256) Loss_G: 0.44666091 Loss_Enh_Dec: -1.38582289\n",
      "| epoch  61 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  3.14 | ppl    23.09 | acc     0.65 | train_ae_norm     1.00\n",
      "[61/200][3099/4361] Loss_D: 0.00530944 (Loss_D_real: 0.00415070 Loss_D_fake: 0.00115874) Loss_G: 0.40705052 Loss_Enh_Dec: -1.34509790\n",
      "| epoch  61 |  3100/ 4361 batches | lr 0.000000 | ms/batch 402.07 | loss  3.15 | ppl    23.29 | acc     0.63 | train_ae_norm     1.00\n",
      "[61/200][3199/4361] Loss_D: 0.00695598 (Loss_D_real: 0.00579115 Loss_D_fake: 0.00116483) Loss_G: 0.41771683 Loss_Enh_Dec: -1.42195511\n",
      "| epoch  61 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  3.18 | ppl    23.97 | acc     0.65 | train_ae_norm     1.00\n",
      "[61/200][3299/4361] Loss_D: 0.00789845 (Loss_D_real: 0.00120719 Loss_D_fake: 0.00669126) Loss_G: 0.38601127 Loss_Enh_Dec: -1.38335490\n",
      "| epoch  61 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  3.19 | ppl    24.29 | acc     0.64 | train_ae_norm     1.00\n",
      "[61/200][3399/4361] Loss_D: 0.00476560 (Loss_D_real: 0.00361750 Loss_D_fake: 0.00114809) Loss_G: 0.76297468 Loss_Enh_Dec: -1.04732776\n",
      "| epoch  61 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  3.17 | ppl    23.78 | acc     0.64 | train_ae_norm     1.00\n",
      "[61/200][3499/4361] Loss_D: 0.04315317 (Loss_D_real: 0.04186002 Loss_D_fake: 0.00129315) Loss_G: 0.42699614 Loss_Enh_Dec: -1.83478665\n",
      "| epoch  61 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.83 | loss  3.12 | ppl    22.57 | acc     0.66 | train_ae_norm     1.00\n",
      "[61/200][3599/4361] Loss_D: 0.01117521 (Loss_D_real: 0.00924755 Loss_D_fake: 0.00192766) Loss_G: 0.38480976 Loss_Enh_Dec: -1.66978574\n",
      "| epoch  61 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  3.14 | ppl    23.07 | acc     0.67 | train_ae_norm     1.00\n",
      "[61/200][3699/4361] Loss_D: 0.00208207 (Loss_D_real: 0.00049956 Loss_D_fake: 0.00158251) Loss_G: 0.38726231 Loss_Enh_Dec: -1.35690594\n",
      "| epoch  61 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  3.17 | ppl    23.77 | acc     0.61 | train_ae_norm     1.00\n",
      "[61/200][3799/4361] Loss_D: 0.00442208 (Loss_D_real: 0.00080235 Loss_D_fake: 0.00361973) Loss_G: 0.38280654 Loss_Enh_Dec: -1.50599408\n",
      "| epoch  61 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  3.18 | ppl    23.96 | acc     0.68 | train_ae_norm     1.00\n",
      "[61/200][3899/4361] Loss_D: 0.00382572 (Loss_D_real: 0.00230695 Loss_D_fake: 0.00151876) Loss_G: 0.38674617 Loss_Enh_Dec: -1.39118099\n",
      "| epoch  61 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  3.16 | ppl    23.56 | acc     0.61 | train_ae_norm     1.00\n",
      "[61/200][3999/4361] Loss_D: 0.00443765 (Loss_D_real: 0.00077856 Loss_D_fake: 0.00365908) Loss_G: 0.40605092 Loss_Enh_Dec: -1.38226414\n",
      "| epoch  61 |  4000/ 4361 batches | lr 0.000000 | ms/batch 402.47 | loss  3.16 | ppl    23.68 | acc     0.65 | train_ae_norm     1.00\n",
      "[61/200][4099/4361] Loss_D: 0.00546660 (Loss_D_real: 0.00072276 Loss_D_fake: 0.00474383) Loss_G: 0.38843772 Loss_Enh_Dec: -1.46171570\n",
      "| epoch  61 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  3.12 | ppl    22.64 | acc     0.66 | train_ae_norm     1.00\n",
      "[61/200][4199/4361] Loss_D: 0.00902983 (Loss_D_real: 0.00383833 Loss_D_fake: 0.00519150) Loss_G: 0.39060768 Loss_Enh_Dec: -2.08126187\n",
      "| epoch  61 |  4200/ 4361 batches | lr 0.000000 | ms/batch 402.30 | loss  3.17 | ppl    23.89 | acc     0.67 | train_ae_norm     1.00\n",
      "[61/200][4299/4361] Loss_D: 0.00687247 (Loss_D_real: 0.00090155 Loss_D_fake: 0.00597092) Loss_G: 0.43578273 Loss_Enh_Dec: -1.56958568\n",
      "| epoch  61 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  3.11 | ppl    22.42 | acc     0.66 | train_ae_norm     1.00\n",
      "| end of epoch  61 | time: 1854.03s | test loss  3.08 | test ppl 21.69 | acc 0.684\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 62 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.706\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.490\n",
      "  Test Loss: 4.021\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  62 |     0/ 4361 batches | lr 0.000000 | ms/batch 867.13 | loss  0.03 | ppl     1.03 | acc     0.67 | train_ae_norm     1.00\n",
      "[62/200][99/4361] Loss_D: 0.00450474 (Loss_D_real: 0.00087855 Loss_D_fake: 0.00362619) Loss_G: 0.43284950 Loss_Enh_Dec: -1.75719869\n",
      "| epoch  62 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.99 | loss  3.15 | ppl    23.41 | acc     0.62 | train_ae_norm     1.00\n",
      "[62/200][199/4361] Loss_D: 0.00509033 (Loss_D_real: 0.00303605 Loss_D_fake: 0.00205428) Loss_G: 0.43867618 Loss_Enh_Dec: -1.49184692\n",
      "| epoch  62 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  3.19 | ppl    24.21 | acc     0.67 | train_ae_norm     1.00\n",
      "[62/200][299/4361] Loss_D: 0.00424926 (Loss_D_real: 0.00206130 Loss_D_fake: 0.00218796) Loss_G: 0.37200680 Loss_Enh_Dec: -1.67447758\n",
      "| epoch  62 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  3.18 | ppl    24.00 | acc     0.62 | train_ae_norm     1.00\n",
      "[62/200][399/4361] Loss_D: 0.00181640 (Loss_D_real: 0.00051443 Loss_D_fake: 0.00130198) Loss_G: 0.41844067 Loss_Enh_Dec: -1.82368147\n",
      "| epoch  62 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  3.10 | ppl    22.25 | acc     0.66 | train_ae_norm     1.00\n",
      "[62/200][499/4361] Loss_D: 0.03448212 (Loss_D_real: 0.00058165 Loss_D_fake: 0.03390047) Loss_G: 0.44213220 Loss_Enh_Dec: -1.89514887\n",
      "| epoch  62 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  3.15 | ppl    23.40 | acc     0.67 | train_ae_norm     1.00\n",
      "[62/200][599/4361] Loss_D: 0.00111643 (Loss_D_real: 0.00007906 Loss_D_fake: 0.00103737) Loss_G: 0.40463313 Loss_Enh_Dec: -1.62403715\n",
      "| epoch  62 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  3.11 | ppl    22.44 | acc     0.62 | train_ae_norm     1.00\n",
      "[62/200][699/4361] Loss_D: 0.00630678 (Loss_D_real: 0.00096601 Loss_D_fake: 0.00534077) Loss_G: 0.47351280 Loss_Enh_Dec: -2.14868164\n",
      "| epoch  62 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  3.16 | ppl    23.47 | acc     0.68 | train_ae_norm     1.00\n",
      "[62/200][799/4361] Loss_D: 0.00362179 (Loss_D_real: 0.00083114 Loss_D_fake: 0.00279065) Loss_G: 0.41526085 Loss_Enh_Dec: -1.99652135\n",
      "| epoch  62 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  3.13 | ppl    22.76 | acc     0.64 | train_ae_norm     1.00\n",
      "[62/200][899/4361] Loss_D: 0.00191447 (Loss_D_real: 0.00069505 Loss_D_fake: 0.00121943) Loss_G: 0.58442032 Loss_Enh_Dec: -1.30297506\n",
      "| epoch  62 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.86 | loss  3.14 | ppl    23.01 | acc     0.67 | train_ae_norm     1.00\n",
      "[62/200][999/4361] Loss_D: 0.00359677 (Loss_D_real: 0.00218349 Loss_D_fake: 0.00141327) Loss_G: 0.40740177 Loss_Enh_Dec: -1.95345306\n",
      "| epoch  62 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  3.12 | ppl    22.66 | acc     0.67 | train_ae_norm     1.00\n",
      "[62/200][1099/4361] Loss_D: 0.00235874 (Loss_D_real: 0.00079906 Loss_D_fake: 0.00155968) Loss_G: 0.38162258 Loss_Enh_Dec: -1.92596436\n",
      "| epoch  62 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  3.11 | ppl    22.44 | acc     0.63 | train_ae_norm     1.00\n",
      "[62/200][1199/4361] Loss_D: 0.02785147 (Loss_D_real: 0.02434110 Loss_D_fake: 0.00351037) Loss_G: 0.41566154 Loss_Enh_Dec: -1.34816587\n",
      "| epoch  62 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  3.13 | ppl    22.97 | acc     0.66 | train_ae_norm     1.00\n",
      "[62/200][1299/4361] Loss_D: 0.00367760 (Loss_D_real: 0.00044388 Loss_D_fake: 0.00323372) Loss_G: 0.43375930 Loss_Enh_Dec: -1.25703967\n",
      "| epoch  62 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  3.14 | ppl    23.14 | acc     0.67 | train_ae_norm     1.00\n",
      "[62/200][1399/4361] Loss_D: 0.00154302 (Loss_D_real: 0.00037082 Loss_D_fake: 0.00117219) Loss_G: 0.39597398 Loss_Enh_Dec: -1.67774856\n",
      "| epoch  62 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  3.14 | ppl    23.17 | acc     0.63 | train_ae_norm     1.00\n",
      "[62/200][1499/4361] Loss_D: 0.00379803 (Loss_D_real: 0.00149291 Loss_D_fake: 0.00230512) Loss_G: 0.39347515 Loss_Enh_Dec: -1.35294402\n",
      "| epoch  62 |  1500/ 4361 batches | lr 0.000000 | ms/batch 400.71 | loss  3.18 | ppl    24.01 | acc     0.64 | train_ae_norm     1.00\n",
      "[62/200][1599/4361] Loss_D: 0.00452042 (Loss_D_real: 0.00201584 Loss_D_fake: 0.00250458) Loss_G: 0.35963011 Loss_Enh_Dec: -1.23171568\n",
      "| epoch  62 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  3.15 | ppl    23.26 | acc     0.65 | train_ae_norm     1.00\n",
      "[62/200][1699/4361] Loss_D: 0.00695722 (Loss_D_real: 0.00297856 Loss_D_fake: 0.00397865) Loss_G: 0.37331772 Loss_Enh_Dec: -1.29146659\n",
      "| epoch  62 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  3.11 | ppl    22.33 | acc     0.65 | train_ae_norm     1.00\n",
      "[62/200][1799/4361] Loss_D: 0.00287170 (Loss_D_real: 0.00088148 Loss_D_fake: 0.00199023) Loss_G: 0.38280475 Loss_Enh_Dec: -1.47086108\n",
      "| epoch  62 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  3.09 | ppl    22.02 | acc     0.67 | train_ae_norm     1.00\n",
      "[62/200][1899/4361] Loss_D: 0.00284924 (Loss_D_real: 0.00159008 Loss_D_fake: 0.00125916) Loss_G: 0.38247648 Loss_Enh_Dec: -1.48557508\n",
      "| epoch  62 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  3.17 | ppl    23.72 | acc     0.67 | train_ae_norm     1.00\n",
      "[62/200][1999/4361] Loss_D: 0.00266875 (Loss_D_real: 0.00143716 Loss_D_fake: 0.00123159) Loss_G: 0.42626444 Loss_Enh_Dec: -1.35178173\n",
      "| epoch  62 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  3.10 | ppl    22.27 | acc     0.64 | train_ae_norm     1.00\n",
      "[62/200][2099/4361] Loss_D: 0.00264028 (Loss_D_real: 0.00095892 Loss_D_fake: 0.00168136) Loss_G: 0.38897306 Loss_Enh_Dec: -1.67415106\n",
      "| epoch  62 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  3.13 | ppl    22.98 | acc     0.66 | train_ae_norm     1.00\n",
      "[62/200][2199/4361] Loss_D: 0.00400726 (Loss_D_real: 0.00063063 Loss_D_fake: 0.00337663) Loss_G: 0.42664108 Loss_Enh_Dec: -1.92678189\n",
      "| epoch  62 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  3.14 | ppl    23.14 | acc     0.66 | train_ae_norm     1.00\n",
      "[62/200][2299/4361] Loss_D: 0.03946164 (Loss_D_real: 0.03021834 Loss_D_fake: 0.00924330) Loss_G: 0.47757062 Loss_Enh_Dec: -1.92320883\n",
      "| epoch  62 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  3.12 | ppl    22.72 | acc     0.67 | train_ae_norm     1.00\n",
      "[62/200][2399/4361] Loss_D: 0.00434498 (Loss_D_real: 0.00124804 Loss_D_fake: 0.00309694) Loss_G: 0.42934275 Loss_Enh_Dec: -1.93359184\n",
      "| epoch  62 |  2400/ 4361 batches | lr 0.000000 | ms/batch 402.05 | loss  3.13 | ppl    22.77 | acc     0.64 | train_ae_norm     1.00\n",
      "[62/200][2499/4361] Loss_D: 0.00196946 (Loss_D_real: 0.00010745 Loss_D_fake: 0.00186201) Loss_G: 0.40556499 Loss_Enh_Dec: -1.98552513\n",
      "| epoch  62 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  3.16 | ppl    23.58 | acc     0.65 | train_ae_norm     1.00\n",
      "[62/200][2599/4361] Loss_D: 0.00362351 (Loss_D_real: 0.00113923 Loss_D_fake: 0.00248428) Loss_G: 0.41582093 Loss_Enh_Dec: -1.80606067\n",
      "| epoch  62 |  2600/ 4361 batches | lr 0.000000 | ms/batch 402.07 | loss  3.14 | ppl    23.03 | acc     0.64 | train_ae_norm     1.00\n",
      "[62/200][2699/4361] Loss_D: 0.00237835 (Loss_D_real: 0.00089221 Loss_D_fake: 0.00148614) Loss_G: 0.41564590 Loss_Enh_Dec: -2.13654494\n",
      "| epoch  62 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  3.15 | ppl    23.24 | acc     0.64 | train_ae_norm     1.00\n",
      "[62/200][2799/4361] Loss_D: 0.00138354 (Loss_D_real: 0.00036613 Loss_D_fake: 0.00101741) Loss_G: 0.48602095 Loss_Enh_Dec: -2.08444166\n",
      "| epoch  62 |  2800/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  3.09 | ppl    21.95 | acc     0.65 | train_ae_norm     1.00\n",
      "[62/200][2899/4361] Loss_D: 0.00609254 (Loss_D_real: 0.00528511 Loss_D_fake: 0.00080744) Loss_G: 0.43098506 Loss_Enh_Dec: -2.12418556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  62 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  3.13 | ppl    22.78 | acc     0.64 | train_ae_norm     1.00\n",
      "[62/200][2999/4361] Loss_D: 0.02797361 (Loss_D_real: 0.02655159 Loss_D_fake: 0.00142202) Loss_G: 0.43915382 Loss_Enh_Dec: -1.85785675\n",
      "| epoch  62 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  3.14 | ppl    22.99 | acc     0.65 | train_ae_norm     1.00\n",
      "[62/200][3099/4361] Loss_D: 0.00213063 (Loss_D_real: 0.00121384 Loss_D_fake: 0.00091680) Loss_G: 0.43429556 Loss_Enh_Dec: -1.96607339\n",
      "| epoch  62 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  3.15 | ppl    23.28 | acc     0.62 | train_ae_norm     1.00\n",
      "[62/200][3199/4361] Loss_D: 0.00291585 (Loss_D_real: 0.00122044 Loss_D_fake: 0.00169541) Loss_G: 0.41621199 Loss_Enh_Dec: -2.04264522\n",
      "| epoch  62 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  3.17 | ppl    23.74 | acc     0.65 | train_ae_norm     1.00\n",
      "[62/200][3299/4361] Loss_D: 0.00202226 (Loss_D_real: 0.00023361 Loss_D_fake: 0.00178865) Loss_G: 0.49061868 Loss_Enh_Dec: -1.83011186\n",
      "| epoch  62 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  3.19 | ppl    24.20 | acc     0.67 | train_ae_norm     1.00\n",
      "[62/200][3399/4361] Loss_D: 0.00283182 (Loss_D_real: 0.00044825 Loss_D_fake: 0.00238357) Loss_G: 0.43342406 Loss_Enh_Dec: -1.96757340\n",
      "| epoch  62 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  3.16 | ppl    23.48 | acc     0.66 | train_ae_norm     1.00\n",
      "[62/200][3499/4361] Loss_D: 0.00076980 (Loss_D_real: 0.00007344 Loss_D_fake: 0.00069636) Loss_G: 0.40763149 Loss_Enh_Dec: -2.02928138\n",
      "| epoch  62 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  3.09 | ppl    21.87 | acc     0.68 | train_ae_norm     1.00\n",
      "[62/200][3599/4361] Loss_D: 0.00260028 (Loss_D_real: 0.00048456 Loss_D_fake: 0.00211573) Loss_G: 0.47441515 Loss_Enh_Dec: -1.66019690\n",
      "| epoch  62 |  3600/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  3.11 | ppl    22.38 | acc     0.65 | train_ae_norm     1.00\n",
      "[62/200][3699/4361] Loss_D: 0.00358700 (Loss_D_real: 0.00153940 Loss_D_fake: 0.00204761) Loss_G: 0.40039682 Loss_Enh_Dec: -1.56857908\n",
      "| epoch  62 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  3.12 | ppl    22.61 | acc     0.61 | train_ae_norm     1.00\n",
      "[62/200][3799/4361] Loss_D: 0.00547269 (Loss_D_real: 0.00351910 Loss_D_fake: 0.00195359) Loss_G: 0.51415604 Loss_Enh_Dec: -1.48356271\n",
      "| epoch  62 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  3.13 | ppl    22.94 | acc     0.69 | train_ae_norm     1.00\n",
      "[62/200][3899/4361] Loss_D: 0.00289289 (Loss_D_real: 0.00202003 Loss_D_fake: 0.00087286) Loss_G: 0.44672808 Loss_Enh_Dec: -1.64177442\n",
      "| epoch  62 |  3900/ 4361 batches | lr 0.000000 | ms/batch 402.27 | loss  3.11 | ppl    22.53 | acc     0.62 | train_ae_norm     1.00\n",
      "[62/200][3999/4361] Loss_D: 0.02253829 (Loss_D_real: 0.01987139 Loss_D_fake: 0.00266690) Loss_G: 0.46029934 Loss_Enh_Dec: -1.14830434\n",
      "| epoch  62 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  3.14 | ppl    23.05 | acc     0.66 | train_ae_norm     1.00\n",
      "[62/200][4099/4361] Loss_D: 0.00261417 (Loss_D_real: 0.00105928 Loss_D_fake: 0.00155490) Loss_G: 0.46691599 Loss_Enh_Dec: -1.86576843\n",
      "| epoch  62 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  3.09 | ppl    21.88 | acc     0.66 | train_ae_norm     1.00\n",
      "[62/200][4199/4361] Loss_D: 0.00203833 (Loss_D_real: 0.00022553 Loss_D_fake: 0.00181280) Loss_G: 0.42611179 Loss_Enh_Dec: -1.81224859\n",
      "| epoch  62 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  3.14 | ppl    23.14 | acc     0.68 | train_ae_norm     1.00\n",
      "[62/200][4299/4361] Loss_D: 0.00347092 (Loss_D_real: 0.00301745 Loss_D_fake: 0.00045346) Loss_G: 0.38768154 Loss_Enh_Dec: -2.01591873\n",
      "| epoch  62 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  3.10 | ppl    22.24 | acc     0.66 | train_ae_norm     1.00\n",
      "| end of epoch  62 | time: 1853.25s | test loss  3.06 | test ppl 21.31 | acc 0.687\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 63 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.707\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.497\n",
      "  Test Loss: 4.014\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  63 |     0/ 4361 batches | lr 0.000000 | ms/batch 864.66 | loss  0.03 | ppl     1.03 | acc     0.68 | train_ae_norm     1.00\n",
      "[63/200][99/4361] Loss_D: 0.01828828 (Loss_D_real: 0.01433505 Loss_D_fake: 0.00395323) Loss_G: 0.40263301 Loss_Enh_Dec: -1.65875232\n",
      "| epoch  63 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  3.12 | ppl    22.75 | acc     0.61 | train_ae_norm     1.00\n",
      "[63/200][199/4361] Loss_D: 0.40211427 (Loss_D_real: 0.00100404 Loss_D_fake: 0.40111023) Loss_G: 0.60565639 Loss_Enh_Dec: -1.38893771\n",
      "| epoch  63 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  3.14 | ppl    23.18 | acc     0.65 | train_ae_norm     1.00\n",
      "[63/200][299/4361] Loss_D: 0.00491360 (Loss_D_real: 0.00367416 Loss_D_fake: 0.00123944) Loss_G: 0.39932248 Loss_Enh_Dec: -1.36951399\n",
      "| epoch  63 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  3.16 | ppl    23.48 | acc     0.62 | train_ae_norm     1.00\n",
      "[63/200][399/4361] Loss_D: 0.00514885 (Loss_D_real: 0.00387206 Loss_D_fake: 0.00127679) Loss_G: 0.38518018 Loss_Enh_Dec: -1.77205431\n",
      "| epoch  63 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  3.08 | ppl    21.85 | acc     0.67 | train_ae_norm     1.00\n",
      "[63/200][499/4361] Loss_D: 0.00166287 (Loss_D_real: 0.00014624 Loss_D_fake: 0.00151664) Loss_G: 0.42396697 Loss_Enh_Dec: -1.73963249\n",
      "| epoch  63 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  3.14 | ppl    23.01 | acc     0.66 | train_ae_norm     1.00\n",
      "[63/200][599/4361] Loss_D: 0.04871119 (Loss_D_real: 0.00306892 Loss_D_fake: 0.04564227) Loss_G: 0.47962436 Loss_Enh_Dec: -1.73864257\n",
      "| epoch  63 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.79 | loss  3.09 | ppl    21.98 | acc     0.62 | train_ae_norm     1.00\n",
      "[63/200][699/4361] Loss_D: 0.00353166 (Loss_D_real: 0.00138907 Loss_D_fake: 0.00214259) Loss_G: 0.42599869 Loss_Enh_Dec: -1.79107094\n",
      "| epoch  63 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  3.14 | ppl    22.99 | acc     0.67 | train_ae_norm     1.00\n",
      "[63/200][799/4361] Loss_D: 0.00162118 (Loss_D_real: 0.00097179 Loss_D_fake: 0.00064939) Loss_G: 0.41750604 Loss_Enh_Dec: -1.67032242\n",
      "| epoch  63 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  3.10 | ppl    22.24 | acc     0.65 | train_ae_norm     1.00\n",
      "[63/200][899/4361] Loss_D: 0.01794971 (Loss_D_real: 0.01636310 Loss_D_fake: 0.00158661) Loss_G: 0.42513347 Loss_Enh_Dec: -1.98937309\n",
      "| epoch  63 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  3.12 | ppl    22.55 | acc     0.69 | train_ae_norm     1.00\n",
      "[63/200][999/4361] Loss_D: 0.00403928 (Loss_D_real: 0.00020904 Loss_D_fake: 0.00383024) Loss_G: 0.41488239 Loss_Enh_Dec: -1.87388420\n",
      "| epoch  63 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  3.09 | ppl    21.93 | acc     0.66 | train_ae_norm     1.00\n",
      "[63/200][1099/4361] Loss_D: 0.00146177 (Loss_D_real: 0.00018610 Loss_D_fake: 0.00127567) Loss_G: 0.47853288 Loss_Enh_Dec: -1.48308361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  63 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  3.11 | ppl    22.40 | acc     0.65 | train_ae_norm     1.00\n",
      "[63/200][1199/4361] Loss_D: 0.00424427 (Loss_D_real: 0.00264333 Loss_D_fake: 0.00160094) Loss_G: 0.51231968 Loss_Enh_Dec: -1.45831645\n",
      "| epoch  63 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  3.11 | ppl    22.38 | acc     0.68 | train_ae_norm     1.00\n",
      "[63/200][1299/4361] Loss_D: 0.00255373 (Loss_D_real: 0.00123899 Loss_D_fake: 0.00131475) Loss_G: 0.43357155 Loss_Enh_Dec: -1.18479538\n",
      "| epoch  63 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  3.12 | ppl    22.63 | acc     0.66 | train_ae_norm     1.00\n",
      "[63/200][1399/4361] Loss_D: 0.00272050 (Loss_D_real: 0.00153365 Loss_D_fake: 0.00118685) Loss_G: 0.48295030 Loss_Enh_Dec: -1.35541260\n",
      "| epoch  63 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  3.12 | ppl    22.69 | acc     0.61 | train_ae_norm     1.00\n",
      "[63/200][1499/4361] Loss_D: 0.00186543 (Loss_D_real: 0.00082182 Loss_D_fake: 0.00104362) Loss_G: 0.49582395 Loss_Enh_Dec: -0.84078467\n",
      "| epoch  63 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  3.18 | ppl    23.95 | acc     0.63 | train_ae_norm     1.00\n",
      "[63/200][1599/4361] Loss_D: 0.00444370 (Loss_D_real: 0.00057515 Loss_D_fake: 0.00386855) Loss_G: 0.43457270 Loss_Enh_Dec: -1.41974008\n",
      "| epoch  63 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  3.14 | ppl    23.08 | acc     0.64 | train_ae_norm     1.00\n",
      "[63/200][1699/4361] Loss_D: 0.00406767 (Loss_D_real: 0.00060927 Loss_D_fake: 0.00345840) Loss_G: 0.43934527 Loss_Enh_Dec: -1.93944418\n",
      "| epoch  63 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  3.11 | ppl    22.36 | acc     0.61 | train_ae_norm     1.00\n",
      "[63/200][1799/4361] Loss_D: 0.00194167 (Loss_D_real: 0.00011423 Loss_D_fake: 0.00182744) Loss_G: 0.40416861 Loss_Enh_Dec: -1.92619646\n",
      "| epoch  63 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  3.10 | ppl    22.17 | acc     0.65 | train_ae_norm     1.00\n",
      "[63/200][1899/4361] Loss_D: 0.00339307 (Loss_D_real: 0.00269957 Loss_D_fake: 0.00069351) Loss_G: 0.60050279 Loss_Enh_Dec: -1.81705475\n",
      "| epoch  63 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  3.16 | ppl    23.46 | acc     0.66 | train_ae_norm     1.00\n",
      "[63/200][1999/4361] Loss_D: 0.00304937 (Loss_D_real: 0.00134857 Loss_D_fake: 0.00170080) Loss_G: 0.54637069 Loss_Enh_Dec: -1.64692724\n",
      "| epoch  63 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  3.10 | ppl    22.10 | acc     0.64 | train_ae_norm     1.00\n",
      "[63/200][2099/4361] Loss_D: 0.00798475 (Loss_D_real: 0.00617663 Loss_D_fake: 0.00180812) Loss_G: 0.45428583 Loss_Enh_Dec: -1.49188888\n",
      "| epoch  63 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  3.13 | ppl    22.85 | acc     0.66 | train_ae_norm     1.00\n",
      "[63/200][2199/4361] Loss_D: 0.00444733 (Loss_D_real: 0.00157109 Loss_D_fake: 0.00287624) Loss_G: 0.47723657 Loss_Enh_Dec: -1.35120428\n",
      "| epoch  63 |  2200/ 4361 batches | lr 0.000000 | ms/batch 402.05 | loss  3.13 | ppl    22.88 | acc     0.65 | train_ae_norm     1.00\n",
      "[63/200][2299/4361] Loss_D: 0.00171984 (Loss_D_real: 0.00061372 Loss_D_fake: 0.00110612) Loss_G: 0.37823331 Loss_Enh_Dec: -1.72479558\n",
      "| epoch  63 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  3.09 | ppl    21.93 | acc     0.67 | train_ae_norm     1.00\n",
      "[63/200][2399/4361] Loss_D: 0.03356387 (Loss_D_real: 0.03264455 Loss_D_fake: 0.00091932) Loss_G: 0.51814646 Loss_Enh_Dec: -1.69992816\n",
      "| epoch  63 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  3.11 | ppl    22.35 | acc     0.64 | train_ae_norm     1.00\n",
      "[63/200][2499/4361] Loss_D: 0.00507538 (Loss_D_real: 0.00377100 Loss_D_fake: 0.00130439) Loss_G: 0.44332314 Loss_Enh_Dec: -1.83976293\n",
      "| epoch  63 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  3.14 | ppl    23.05 | acc     0.65 | train_ae_norm     1.00\n",
      "[63/200][2599/4361] Loss_D: 0.00449223 (Loss_D_real: 0.00247424 Loss_D_fake: 0.00201799) Loss_G: 0.41176304 Loss_Enh_Dec: -1.70964324\n",
      "| epoch  63 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  3.11 | ppl    22.43 | acc     0.62 | train_ae_norm     1.00\n",
      "[63/200][2699/4361] Loss_D: 0.00196377 (Loss_D_real: 0.00033209 Loss_D_fake: 0.00163168) Loss_G: 0.39549622 Loss_Enh_Dec: -1.98307896\n",
      "| epoch  63 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  3.13 | ppl    22.94 | acc     0.67 | train_ae_norm     1.00\n",
      "[63/200][2799/4361] Loss_D: 0.00652582 (Loss_D_real: 0.00320224 Loss_D_fake: 0.00332358) Loss_G: 0.39498520 Loss_Enh_Dec: -1.54430294\n",
      "| epoch  63 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  3.09 | ppl    21.93 | acc     0.65 | train_ae_norm     1.00\n",
      "[63/200][2899/4361] Loss_D: 0.00811595 (Loss_D_real: 0.00539123 Loss_D_fake: 0.00272472) Loss_G: 0.38394338 Loss_Enh_Dec: -1.84569681\n",
      "| epoch  63 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  3.11 | ppl    22.45 | acc     0.66 | train_ae_norm     1.00\n",
      "[63/200][2999/4361] Loss_D: 0.00425532 (Loss_D_real: 0.00173939 Loss_D_fake: 0.00251592) Loss_G: 0.35486966 Loss_Enh_Dec: -1.86649311\n",
      "| epoch  63 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.99 | loss  3.12 | ppl    22.53 | acc     0.67 | train_ae_norm     1.00\n",
      "[63/200][3099/4361] Loss_D: 0.00654406 (Loss_D_real: 0.00391506 Loss_D_fake: 0.00262900) Loss_G: 0.38745660 Loss_Enh_Dec: -2.25604844\n",
      "| epoch  63 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  3.14 | ppl    23.00 | acc     0.64 | train_ae_norm     1.00\n",
      "[63/200][3199/4361] Loss_D: 0.00267446 (Loss_D_real: 0.00051099 Loss_D_fake: 0.00216347) Loss_G: 0.44014239 Loss_Enh_Dec: -1.66839790\n",
      "| epoch  63 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  3.19 | ppl    24.27 | acc     0.64 | train_ae_norm     1.00\n",
      "[63/200][3299/4361] Loss_D: 0.00390809 (Loss_D_real: 0.00243857 Loss_D_fake: 0.00146953) Loss_G: 0.43932602 Loss_Enh_Dec: -1.71054876\n",
      "| epoch  63 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  3.20 | ppl    24.47 | acc     0.66 | train_ae_norm     1.00\n",
      "[63/200][3399/4361] Loss_D: 0.00210791 (Loss_D_real: 0.00072927 Loss_D_fake: 0.00137864) Loss_G: 0.38711032 Loss_Enh_Dec: -1.76961315\n",
      "| epoch  63 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  3.16 | ppl    23.63 | acc     0.64 | train_ae_norm     1.00\n",
      "[63/200][3499/4361] Loss_D: 0.01803075 (Loss_D_real: 0.01610104 Loss_D_fake: 0.00192971) Loss_G: 0.38927028 Loss_Enh_Dec: -1.96368980\n",
      "| epoch  63 |  3500/ 4361 batches | lr 0.000000 | ms/batch 402.50 | loss  3.12 | ppl    22.74 | acc     0.64 | train_ae_norm     1.00\n",
      "[63/200][3599/4361] Loss_D: 0.00635686 (Loss_D_real: 0.00301084 Loss_D_fake: 0.00334602) Loss_G: 0.41573030 Loss_Enh_Dec: -2.00470114\n",
      "| epoch  63 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  3.14 | ppl    23.17 | acc     0.64 | train_ae_norm     1.00\n",
      "[63/200][3699/4361] Loss_D: 0.00375606 (Loss_D_real: 0.00020840 Loss_D_fake: 0.00354766) Loss_G: 0.51443666 Loss_Enh_Dec: -1.54222953\n",
      "| epoch  63 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.66 | loss  3.14 | ppl    23.09 | acc     0.61 | train_ae_norm     1.00\n",
      "[63/200][3799/4361] Loss_D: 0.00572302 (Loss_D_real: 0.00051189 Loss_D_fake: 0.00521113) Loss_G: 0.45361701 Loss_Enh_Dec: -1.74418545\n",
      "| epoch  63 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  3.15 | ppl    23.33 | acc     0.69 | train_ae_norm     1.00\n",
      "[63/200][3899/4361] Loss_D: 0.01244206 (Loss_D_real: 0.01003088 Loss_D_fake: 0.00241118) Loss_G: 0.39204624 Loss_Enh_Dec: -1.90261805\n",
      "| epoch  63 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.69 | loss  3.16 | ppl    23.51 | acc     0.61 | train_ae_norm     1.00\n",
      "[63/200][3999/4361] Loss_D: 0.00454101 (Loss_D_real: 0.00315175 Loss_D_fake: 0.00138927) Loss_G: 0.37383372 Loss_Enh_Dec: -1.84927619\n",
      "| epoch  63 |  4000/ 4361 batches | lr 0.000000 | ms/batch 402.21 | loss  3.17 | ppl    23.80 | acc     0.65 | train_ae_norm     1.00\n",
      "[63/200][4099/4361] Loss_D: 0.01281649 (Loss_D_real: 0.00957878 Loss_D_fake: 0.00323770) Loss_G: 0.39061612 Loss_Enh_Dec: -1.50069559\n",
      "| epoch  63 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  3.12 | ppl    22.70 | acc     0.65 | train_ae_norm     1.00\n",
      "[63/200][4199/4361] Loss_D: 0.01088880 (Loss_D_real: 0.00703187 Loss_D_fake: 0.00385693) Loss_G: 0.38091046 Loss_Enh_Dec: -1.94936621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  63 |  4200/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  3.17 | ppl    23.78 | acc     0.69 | train_ae_norm     1.00\n",
      "[63/200][4299/4361] Loss_D: 0.00358768 (Loss_D_real: 0.00018927 Loss_D_fake: 0.00339841) Loss_G: 0.35813969 Loss_Enh_Dec: -1.38101602\n",
      "| epoch  63 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  3.13 | ppl    22.78 | acc     0.63 | train_ae_norm     1.00\n",
      "| end of epoch  63 | time: 1853.19s | test loss  3.08 | test ppl 21.77 | acc 0.684\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 64 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.705\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.490\n",
      "  Test Loss: 4.166\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  64 |     0/ 4361 batches | lr 0.000000 | ms/batch 864.57 | loss  0.03 | ppl     1.03 | acc     0.68 | train_ae_norm     1.00\n",
      "[64/200][99/4361] Loss_D: 0.01178220 (Loss_D_real: 0.00872019 Loss_D_fake: 0.00306201) Loss_G: 0.37028429 Loss_Enh_Dec: -2.04965162\n",
      "| epoch  64 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  3.16 | ppl    23.62 | acc     0.59 | train_ae_norm     1.00\n",
      "[64/200][199/4361] Loss_D: 0.01249052 (Loss_D_real: 0.00279187 Loss_D_fake: 0.00969865) Loss_G: 0.38220438 Loss_Enh_Dec: -1.30079305\n",
      "| epoch  64 |   200/ 4361 batches | lr 0.000000 | ms/batch 400.64 | loss  3.20 | ppl    24.46 | acc     0.65 | train_ae_norm     1.00\n",
      "[64/200][299/4361] Loss_D: 0.02589678 (Loss_D_real: 0.02256745 Loss_D_fake: 0.00332933) Loss_G: 0.39466935 Loss_Enh_Dec: -1.68347490\n",
      "| epoch  64 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  3.19 | ppl    24.28 | acc     0.60 | train_ae_norm     1.00\n",
      "[64/200][399/4361] Loss_D: 0.00427296 (Loss_D_real: 0.00153357 Loss_D_fake: 0.00273938) Loss_G: 0.36148241 Loss_Enh_Dec: -1.67518163\n",
      "| epoch  64 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.82 | loss  3.09 | ppl    21.94 | acc     0.64 | train_ae_norm     1.00\n",
      "[64/200][499/4361] Loss_D: 0.03562804 (Loss_D_real: 0.00235087 Loss_D_fake: 0.03327717) Loss_G: 0.66289365 Loss_Enh_Dec: -1.42475355\n",
      "| epoch  64 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.66 | loss  3.14 | ppl    23.14 | acc     0.68 | train_ae_norm     1.00\n",
      "[64/200][599/4361] Loss_D: 0.02721949 (Loss_D_real: 0.02542425 Loss_D_fake: 0.00179525) Loss_G: 0.35190019 Loss_Enh_Dec: -1.44796813\n",
      "| epoch  64 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  3.10 | ppl    22.22 | acc     0.63 | train_ae_norm     1.00\n",
      "[64/200][699/4361] Loss_D: 0.00602004 (Loss_D_real: 0.00247872 Loss_D_fake: 0.00354132) Loss_G: 0.39384565 Loss_Enh_Dec: -1.65595365\n",
      "| epoch  64 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  3.14 | ppl    23.02 | acc     0.66 | train_ae_norm     1.00\n",
      "[64/200][799/4361] Loss_D: 0.00283272 (Loss_D_real: 0.00068148 Loss_D_fake: 0.00215124) Loss_G: 0.36362275 Loss_Enh_Dec: -0.91672802\n",
      "| epoch  64 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  3.11 | ppl    22.41 | acc     0.65 | train_ae_norm     1.00\n",
      "[64/200][899/4361] Loss_D: 0.00577122 (Loss_D_real: 0.00230982 Loss_D_fake: 0.00346140) Loss_G: 0.43527356 Loss_Enh_Dec: -1.12254870\n",
      "| epoch  64 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  3.13 | ppl    22.77 | acc     0.67 | train_ae_norm     1.00\n",
      "[64/200][999/4361] Loss_D: 0.00390149 (Loss_D_real: 0.00026197 Loss_D_fake: 0.00363952) Loss_G: 0.38158765 Loss_Enh_Dec: -1.36913943\n",
      "| epoch  64 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  3.10 | ppl    22.18 | acc     0.66 | train_ae_norm     1.00\n",
      "[64/200][1099/4361] Loss_D: 0.01239930 (Loss_D_real: 0.00548130 Loss_D_fake: 0.00691800) Loss_G: 0.43069258 Loss_Enh_Dec: -1.32091892\n",
      "| epoch  64 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  3.12 | ppl    22.71 | acc     0.62 | train_ae_norm     1.00\n",
      "[64/200][1199/4361] Loss_D: 0.04996630 (Loss_D_real: 0.04893714 Loss_D_fake: 0.00102916) Loss_G: 0.40989524 Loss_Enh_Dec: -1.39045489\n",
      "| epoch  64 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  3.12 | ppl    22.74 | acc     0.65 | train_ae_norm     1.00\n",
      "[64/200][1299/4361] Loss_D: 0.00132473 (Loss_D_real: 0.00035985 Loss_D_fake: 0.00096488) Loss_G: 0.42425567 Loss_Enh_Dec: -1.40633476\n",
      "| epoch  64 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.84 | loss  3.15 | ppl    23.22 | acc     0.65 | train_ae_norm     1.00\n",
      "[64/200][1399/4361] Loss_D: 0.00390884 (Loss_D_real: 0.00164379 Loss_D_fake: 0.00226504) Loss_G: 0.37923202 Loss_Enh_Dec: -1.41821468\n",
      "| epoch  64 |  1400/ 4361 batches | lr 0.000000 | ms/batch 402.44 | loss  3.13 | ppl    22.85 | acc     0.63 | train_ae_norm     1.00\n",
      "[64/200][1499/4361] Loss_D: 0.00559228 (Loss_D_real: 0.00012986 Loss_D_fake: 0.00546242) Loss_G: 0.44794112 Loss_Enh_Dec: -1.57412672\n",
      "| epoch  64 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  3.18 | ppl    23.98 | acc     0.63 | train_ae_norm     1.00\n",
      "[64/200][1599/4361] Loss_D: 0.00386571 (Loss_D_real: 0.00291548 Loss_D_fake: 0.00095023) Loss_G: 0.41928267 Loss_Enh_Dec: -1.48543167\n",
      "| epoch  64 |  1600/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  3.15 | ppl    23.24 | acc     0.63 | train_ae_norm     1.00\n",
      "[64/200][1699/4361] Loss_D: 0.00321458 (Loss_D_real: 0.00220323 Loss_D_fake: 0.00101135) Loss_G: 0.41268539 Loss_Enh_Dec: -1.66360509\n",
      "| epoch  64 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  3.09 | ppl    22.08 | acc     0.65 | train_ae_norm     1.00\n",
      "[64/200][1799/4361] Loss_D: 0.00930285 (Loss_D_real: 0.00235151 Loss_D_fake: 0.00695134) Loss_G: 0.49955663 Loss_Enh_Dec: -1.61702025\n",
      "| epoch  64 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  3.08 | ppl    21.87 | acc     0.66 | train_ae_norm     1.00\n",
      "[64/200][1899/4361] Loss_D: 0.00674464 (Loss_D_real: 0.00463454 Loss_D_fake: 0.00211010) Loss_G: 0.40702200 Loss_Enh_Dec: -1.64974153\n",
      "| epoch  64 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  3.16 | ppl    23.66 | acc     0.64 | train_ae_norm     1.00\n",
      "[64/200][1999/4361] Loss_D: 0.00225825 (Loss_D_real: 0.00092565 Loss_D_fake: 0.00133261) Loss_G: 0.52176040 Loss_Enh_Dec: -1.80099738\n",
      "| epoch  64 |  2000/ 4361 batches | lr 0.000000 | ms/batch 402.50 | loss  3.08 | ppl    21.78 | acc     0.65 | train_ae_norm     1.00\n",
      "[64/200][2099/4361] Loss_D: 0.01407582 (Loss_D_real: 0.01258422 Loss_D_fake: 0.00149160) Loss_G: 0.43505380 Loss_Enh_Dec: -1.59291732\n",
      "| epoch  64 |  2100/ 4361 batches | lr 0.000000 | ms/batch 402.08 | loss  3.14 | ppl    23.02 | acc     0.67 | train_ae_norm     1.00\n",
      "[64/200][2199/4361] Loss_D: 0.00171816 (Loss_D_real: 0.00075331 Loss_D_fake: 0.00096485) Loss_G: 0.42286703 Loss_Enh_Dec: -1.19753575\n",
      "| epoch  64 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  3.11 | ppl    22.44 | acc     0.64 | train_ae_norm     1.00\n",
      "[64/200][2299/4361] Loss_D: 0.00332567 (Loss_D_real: 0.00067336 Loss_D_fake: 0.00265231) Loss_G: 0.50745672 Loss_Enh_Dec: -1.01765060\n",
      "| epoch  64 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  3.10 | ppl    22.28 | acc     0.67 | train_ae_norm     1.00\n",
      "[64/200][2399/4361] Loss_D: 0.05630799 (Loss_D_real: 0.05393041 Loss_D_fake: 0.00237758) Loss_G: 0.38418791 Loss_Enh_Dec: -1.32794833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  64 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  3.12 | ppl    22.66 | acc     0.62 | train_ae_norm     1.00\n",
      "[64/200][2499/4361] Loss_D: 0.00318660 (Loss_D_real: 0.00227620 Loss_D_fake: 0.00091040) Loss_G: 0.41084358 Loss_Enh_Dec: -1.33233142\n",
      "| epoch  64 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  3.15 | ppl    23.32 | acc     0.66 | train_ae_norm     1.00\n",
      "[64/200][2599/4361] Loss_D: 0.00237542 (Loss_D_real: 0.00112817 Loss_D_fake: 0.00124725) Loss_G: 0.40838391 Loss_Enh_Dec: -1.80317461\n",
      "| epoch  64 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  3.12 | ppl    22.70 | acc     0.64 | train_ae_norm     1.00\n",
      "[64/200][2699/4361] Loss_D: 0.00420977 (Loss_D_real: 0.00076981 Loss_D_fake: 0.00343997) Loss_G: 0.59580207 Loss_Enh_Dec: -1.42228401\n",
      "| epoch  64 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  3.12 | ppl    22.61 | acc     0.65 | train_ae_norm     1.00\n",
      "[64/200][2799/4361] Loss_D: 0.00434172 (Loss_D_real: 0.00275515 Loss_D_fake: 0.00158657) Loss_G: 0.43171978 Loss_Enh_Dec: -1.42435026\n",
      "| epoch  64 |  2800/ 4361 batches | lr 0.000000 | ms/batch 402.36 | loss  3.07 | ppl    21.58 | acc     0.65 | train_ae_norm     1.00\n",
      "[64/200][2899/4361] Loss_D: 0.00605070 (Loss_D_real: 0.00398540 Loss_D_fake: 0.00206530) Loss_G: 0.40758142 Loss_Enh_Dec: -1.49078977\n",
      "| epoch  64 |  2900/ 4361 batches | lr 0.000000 | ms/batch 402.03 | loss  3.10 | ppl    22.10 | acc     0.67 | train_ae_norm     1.00\n",
      "[64/200][2999/4361] Loss_D: 0.01425534 (Loss_D_real: 0.01029394 Loss_D_fake: 0.00396140) Loss_G: 0.35942447 Loss_Enh_Dec: -1.48007870\n",
      "| epoch  64 |  3000/ 4361 batches | lr 0.000000 | ms/batch 402.13 | loss  3.09 | ppl    21.93 | acc     0.66 | train_ae_norm     1.00\n",
      "[64/200][3099/4361] Loss_D: 0.00348263 (Loss_D_real: 0.00226547 Loss_D_fake: 0.00121716) Loss_G: 0.47514397 Loss_Enh_Dec: -1.39694011\n",
      "| epoch  64 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  3.11 | ppl    22.51 | acc     0.62 | train_ae_norm     1.00\n",
      "[64/200][3199/4361] Loss_D: 0.00951427 (Loss_D_real: 0.00774324 Loss_D_fake: 0.00177103) Loss_G: 0.39263883 Loss_Enh_Dec: -1.79371154\n",
      "| epoch  64 |  3200/ 4361 batches | lr 0.000000 | ms/batch 402.97 | loss  3.15 | ppl    23.22 | acc     0.65 | train_ae_norm     1.00\n",
      "[64/200][3299/4361] Loss_D: 0.00955791 (Loss_D_real: 0.00645928 Loss_D_fake: 0.00309863) Loss_G: 0.38114047 Loss_Enh_Dec: -1.93271005\n",
      "| epoch  64 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  3.15 | ppl    23.32 | acc     0.67 | train_ae_norm     1.00\n",
      "[64/200][3399/4361] Loss_D: 0.01524061 (Loss_D_real: 0.00921555 Loss_D_fake: 0.00602506) Loss_G: 0.34170055 Loss_Enh_Dec: -1.48505890\n",
      "| epoch  64 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  3.14 | ppl    23.01 | acc     0.66 | train_ae_norm     1.00\n",
      "[64/200][3499/4361] Loss_D: 0.00696222 (Loss_D_real: 0.00051213 Loss_D_fake: 0.00645010) Loss_G: 0.35729310 Loss_Enh_Dec: -1.90218663\n",
      "| epoch  64 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  3.10 | ppl    22.18 | acc     0.66 | train_ae_norm     1.00\n",
      "[64/200][3599/4361] Loss_D: 0.00519214 (Loss_D_real: 0.00170909 Loss_D_fake: 0.00348305) Loss_G: 0.38282228 Loss_Enh_Dec: -1.95053864\n",
      "| epoch  64 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  3.10 | ppl    22.28 | acc     0.63 | train_ae_norm     1.00\n",
      "[64/200][3699/4361] Loss_D: 0.02223453 (Loss_D_real: 0.01645557 Loss_D_fake: 0.00577896) Loss_G: 0.33515355 Loss_Enh_Dec: -2.27704215\n",
      "| epoch  64 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  3.12 | ppl    22.67 | acc     0.60 | train_ae_norm     1.00\n",
      "[64/200][3799/4361] Loss_D: 0.00747526 (Loss_D_real: 0.00549542 Loss_D_fake: 0.00197984) Loss_G: 0.38665989 Loss_Enh_Dec: -1.79556465\n",
      "| epoch  64 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  3.16 | ppl    23.57 | acc     0.70 | train_ae_norm     1.00\n",
      "[64/200][3899/4361] Loss_D: 0.00953913 (Loss_D_real: 0.00371227 Loss_D_fake: 0.00582687) Loss_G: 0.37036008 Loss_Enh_Dec: -2.14112544\n",
      "| epoch  64 |  3900/ 4361 batches | lr 0.000000 | ms/batch 402.04 | loss  3.16 | ppl    23.50 | acc     0.63 | train_ae_norm     1.00\n",
      "[64/200][3999/4361] Loss_D: 0.00421730 (Loss_D_real: 0.00050730 Loss_D_fake: 0.00371000) Loss_G: 0.38127810 Loss_Enh_Dec: -2.06828547\n",
      "| epoch  64 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  3.15 | ppl    23.40 | acc     0.67 | train_ae_norm     1.00\n",
      "[64/200][4099/4361] Loss_D: 0.01367658 (Loss_D_real: 0.00838331 Loss_D_fake: 0.00529327) Loss_G: 0.34123954 Loss_Enh_Dec: -2.24367142\n",
      "| epoch  64 |  4100/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  3.10 | ppl    22.29 | acc     0.64 | train_ae_norm     1.00\n",
      "[64/200][4199/4361] Loss_D: 0.04253316 (Loss_D_real: 0.03642553 Loss_D_fake: 0.00610763) Loss_G: 0.37200052 Loss_Enh_Dec: -1.95738018\n",
      "| epoch  64 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  3.16 | ppl    23.56 | acc     0.67 | train_ae_norm     1.00\n",
      "[64/200][4299/4361] Loss_D: 0.01879998 (Loss_D_real: 0.01448966 Loss_D_fake: 0.00431032) Loss_G: 0.38052148 Loss_Enh_Dec: -2.15765762\n",
      "| epoch  64 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  3.15 | ppl    23.36 | acc     0.64 | train_ae_norm     1.00\n",
      "| end of epoch  64 | time: 1853.80s | test loss  3.09 | test ppl 21.91 | acc 0.680\n",
      "bleu_self:  [3.70885185e-01 1.62241162e-01 1.13140342e-01 2.09104905e-05\n",
      " 6.00479978e-06]\n",
      "bleu_test:  [6.84659091e-01 2.97435788e-01 1.69659376e-01 3.90708080e-02\n",
      " 4.04459459e-05]\n",
      "bleu_self: [0.37088518,0.16224116,0.11314034,0.00002091,0.00000600]\n",
      "bleu_test: [0.68465909,0.29743579,0.16965938,0.03907081,0.00004045]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 65 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.708\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.482\n",
      "  Test Loss: 4.168\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  65 |     0/ 4361 batches | lr 0.000000 | ms/batch 867.75 | loss  0.03 | ppl     1.03 | acc     0.66 | train_ae_norm     1.00\n",
      "[65/200][99/4361] Loss_D: 0.00946540 (Loss_D_real: 0.00188123 Loss_D_fake: 0.00758417) Loss_G: 0.33985996 Loss_Enh_Dec: -1.75252950\n",
      "| epoch  65 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  3.17 | ppl    23.74 | acc     0.62 | train_ae_norm     1.00\n",
      "[65/200][199/4361] Loss_D: 0.00647616 (Loss_D_real: 0.00157615 Loss_D_fake: 0.00490001) Loss_G: 0.35810032 Loss_Enh_Dec: -1.73539102\n",
      "| epoch  65 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  3.19 | ppl    24.40 | acc     0.65 | train_ae_norm     1.00\n",
      "[65/200][299/4361] Loss_D: 0.01302760 (Loss_D_real: 0.00368151 Loss_D_fake: 0.00934609) Loss_G: 0.40907964 Loss_Enh_Dec: -1.81712019\n",
      "| epoch  65 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  3.20 | ppl    24.46 | acc     0.62 | train_ae_norm     1.00\n",
      "[65/200][399/4361] Loss_D: 0.04590342 (Loss_D_real: 0.00306125 Loss_D_fake: 0.04284216) Loss_G: 0.41919753 Loss_Enh_Dec: -2.03933334\n",
      "| epoch  65 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  3.12 | ppl    22.61 | acc     0.65 | train_ae_norm     1.00\n",
      "[65/200][499/4361] Loss_D: 0.00930447 (Loss_D_real: 0.00445423 Loss_D_fake: 0.00485024) Loss_G: 0.39259109 Loss_Enh_Dec: -2.20696020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  65 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  3.25 | ppl    25.68 | acc     0.65 | train_ae_norm     1.00\n",
      "[65/200][599/4361] Loss_D: 0.00345132 (Loss_D_real: 0.00110828 Loss_D_fake: 0.00234304) Loss_G: 0.38021097 Loss_Enh_Dec: -2.07487655\n",
      "| epoch  65 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  3.16 | ppl    23.52 | acc     0.61 | train_ae_norm     1.00\n",
      "[65/200][699/4361] Loss_D: 0.02242238 (Loss_D_real: 0.01286865 Loss_D_fake: 0.00955372) Loss_G: 0.36792526 Loss_Enh_Dec: -2.19119716\n",
      "| epoch  65 |   700/ 4361 batches | lr 0.000000 | ms/batch 402.06 | loss  3.17 | ppl    23.81 | acc     0.68 | train_ae_norm     1.00\n",
      "[65/200][799/4361] Loss_D: 0.01071901 (Loss_D_real: 0.00717138 Loss_D_fake: 0.00354763) Loss_G: 0.42741004 Loss_Enh_Dec: -1.87900722\n",
      "| epoch  65 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  3.14 | ppl    23.11 | acc     0.65 | train_ae_norm     1.00\n",
      "[65/200][899/4361] Loss_D: 0.01361866 (Loss_D_real: 0.00233723 Loss_D_fake: 0.01128143) Loss_G: 0.36615315 Loss_Enh_Dec: -1.66077077\n",
      "| epoch  65 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  3.14 | ppl    23.14 | acc     0.68 | train_ae_norm     1.00\n",
      "[65/200][999/4361] Loss_D: 0.00971732 (Loss_D_real: 0.00239163 Loss_D_fake: 0.00732570) Loss_G: 0.32758212 Loss_Enh_Dec: -1.38300073\n",
      "| epoch  65 |  1000/ 4361 batches | lr 0.000000 | ms/batch 402.25 | loss  3.13 | ppl    22.90 | acc     0.66 | train_ae_norm     1.00\n",
      "[65/200][1099/4361] Loss_D: 0.01274712 (Loss_D_real: 0.01009789 Loss_D_fake: 0.00264924) Loss_G: 0.38195807 Loss_Enh_Dec: -1.69154727\n",
      "| epoch  65 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  3.11 | ppl    22.34 | acc     0.64 | train_ae_norm     1.00\n",
      "[65/200][1199/4361] Loss_D: 0.01013431 (Loss_D_real: 0.00583534 Loss_D_fake: 0.00429897) Loss_G: 0.41244230 Loss_Enh_Dec: -1.01733685\n",
      "| epoch  65 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  3.12 | ppl    22.64 | acc     0.68 | train_ae_norm     1.00\n",
      "[65/200][1299/4361] Loss_D: 0.01168021 (Loss_D_real: 0.00459972 Loss_D_fake: 0.00708050) Loss_G: 0.42688861 Loss_Enh_Dec: -1.26246440\n",
      "| epoch  65 |  1300/ 4361 batches | lr 0.000000 | ms/batch 402.15 | loss  3.13 | ppl    22.92 | acc     0.66 | train_ae_norm     1.00\n",
      "[65/200][1399/4361] Loss_D: 0.00301328 (Loss_D_real: 0.00130588 Loss_D_fake: 0.00170740) Loss_G: 0.40822268 Loss_Enh_Dec: -1.16644180\n",
      "| epoch  65 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  3.14 | ppl    23.10 | acc     0.59 | train_ae_norm     1.00\n",
      "[65/200][1499/4361] Loss_D: 0.00161272 (Loss_D_real: 0.00027286 Loss_D_fake: 0.00133985) Loss_G: 0.37463021 Loss_Enh_Dec: -1.47856247\n",
      "| epoch  65 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  3.18 | ppl    24.09 | acc     0.63 | train_ae_norm     1.00\n",
      "[65/200][1599/4361] Loss_D: 0.00264412 (Loss_D_real: 0.00063416 Loss_D_fake: 0.00200995) Loss_G: 0.40811044 Loss_Enh_Dec: -1.77117920\n",
      "| epoch  65 |  1600/ 4361 batches | lr 0.000000 | ms/batch 402.16 | loss  3.16 | ppl    23.48 | acc     0.65 | train_ae_norm     1.00\n",
      "[65/200][1699/4361] Loss_D: 0.00672922 (Loss_D_real: 0.00231058 Loss_D_fake: 0.00441865) Loss_G: 0.68352842 Loss_Enh_Dec: -1.47161329\n",
      "| epoch  65 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  3.12 | ppl    22.55 | acc     0.64 | train_ae_norm     1.00\n",
      "[65/200][1799/4361] Loss_D: 0.00374854 (Loss_D_real: 0.00117668 Loss_D_fake: 0.00257186) Loss_G: 0.36188689 Loss_Enh_Dec: -1.56302106\n",
      "| epoch  65 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  3.08 | ppl    21.84 | acc     0.66 | train_ae_norm     1.00\n",
      "[65/200][1899/4361] Loss_D: 0.03432028 (Loss_D_real: 0.02696138 Loss_D_fake: 0.00735890) Loss_G: 0.44708395 Loss_Enh_Dec: -1.44966733\n",
      "| epoch  65 |  1900/ 4361 batches | lr 0.000000 | ms/batch 402.14 | loss  3.14 | ppl    23.22 | acc     0.67 | train_ae_norm     1.00\n",
      "[65/200][1999/4361] Loss_D: 0.00830700 (Loss_D_real: 0.00113938 Loss_D_fake: 0.00716761) Loss_G: 0.37522587 Loss_Enh_Dec: -1.54555821\n",
      "| epoch  65 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  3.07 | ppl    21.57 | acc     0.65 | train_ae_norm     1.00\n",
      "[65/200][2099/4361] Loss_D: 0.00474648 (Loss_D_real: 0.00056152 Loss_D_fake: 0.00418496) Loss_G: 0.38173112 Loss_Enh_Dec: -1.59790075\n",
      "| epoch  65 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  3.10 | ppl    22.17 | acc     0.65 | train_ae_norm     1.00\n",
      "[65/200][2199/4361] Loss_D: 0.00845205 (Loss_D_real: 0.00058471 Loss_D_fake: 0.00786734) Loss_G: 0.50285548 Loss_Enh_Dec: -1.61702919\n",
      "| epoch  65 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  3.08 | ppl    21.84 | acc     0.65 | train_ae_norm     1.00\n",
      "[65/200][2299/4361] Loss_D: 0.00513326 (Loss_D_real: 0.00279969 Loss_D_fake: 0.00233356) Loss_G: 0.40688744 Loss_Enh_Dec: -1.58807206\n",
      "| epoch  65 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  3.07 | ppl    21.53 | acc     0.68 | train_ae_norm     1.00\n",
      "[65/200][2399/4361] Loss_D: 0.00412101 (Loss_D_real: 0.00087785 Loss_D_fake: 0.00324316) Loss_G: 0.37682554 Loss_Enh_Dec: -1.50923085\n",
      "| epoch  65 |  2400/ 4361 batches | lr 0.000000 | ms/batch 402.27 | loss  3.09 | ppl    21.94 | acc     0.64 | train_ae_norm     1.00\n",
      "[65/200][2499/4361] Loss_D: 0.00658862 (Loss_D_real: 0.00304907 Loss_D_fake: 0.00353955) Loss_G: 0.39439651 Loss_Enh_Dec: -1.85147095\n",
      "| epoch  65 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  3.13 | ppl    22.92 | acc     0.66 | train_ae_norm     1.00\n",
      "[65/200][2599/4361] Loss_D: 0.00555309 (Loss_D_real: 0.00277378 Loss_D_fake: 0.00277931) Loss_G: 0.43183747 Loss_Enh_Dec: -1.56907117\n",
      "| epoch  65 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  3.11 | ppl    22.52 | acc     0.62 | train_ae_norm     1.00\n",
      "[65/200][2699/4361] Loss_D: 0.00198877 (Loss_D_real: 0.00091853 Loss_D_fake: 0.00107024) Loss_G: 0.41488585 Loss_Enh_Dec: -1.61343753\n",
      "| epoch  65 |  2700/ 4361 batches | lr 0.000000 | ms/batch 402.22 | loss  3.09 | ppl    22.08 | acc     0.65 | train_ae_norm     1.00\n",
      "[65/200][2799/4361] Loss_D: 0.00421640 (Loss_D_real: 0.00063008 Loss_D_fake: 0.00358632) Loss_G: 0.36463127 Loss_Enh_Dec: -1.63155365\n",
      "| epoch  65 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  3.05 | ppl    21.21 | acc     0.63 | train_ae_norm     1.00\n",
      "[65/200][2899/4361] Loss_D: 0.01557747 (Loss_D_real: 0.01390160 Loss_D_fake: 0.00167586) Loss_G: 0.39515254 Loss_Enh_Dec: -1.68106115\n",
      "| epoch  65 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  3.10 | ppl    22.15 | acc     0.67 | train_ae_norm     1.00\n",
      "[65/200][2999/4361] Loss_D: 0.00425974 (Loss_D_real: 0.00037349 Loss_D_fake: 0.00388625) Loss_G: 0.36090612 Loss_Enh_Dec: -1.54828775\n",
      "| epoch  65 |  3000/ 4361 batches | lr 0.000000 | ms/batch 402.24 | loss  3.11 | ppl    22.47 | acc     0.67 | train_ae_norm     1.00\n",
      "[65/200][3099/4361] Loss_D: 0.00153476 (Loss_D_real: 0.00012797 Loss_D_fake: 0.00140678) Loss_G: 0.44217348 Loss_Enh_Dec: -1.65083206\n",
      "| epoch  65 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  3.12 | ppl    22.62 | acc     0.63 | train_ae_norm     1.00\n",
      "[65/200][3199/4361] Loss_D: 0.00167612 (Loss_D_real: 0.00070792 Loss_D_fake: 0.00096820) Loss_G: 0.41550943 Loss_Enh_Dec: -1.63466454\n",
      "| epoch  65 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.43 | loss  3.14 | ppl    23.16 | acc     0.66 | train_ae_norm     1.00\n",
      "[65/200][3299/4361] Loss_D: 0.00214290 (Loss_D_real: 0.00051627 Loss_D_fake: 0.00162663) Loss_G: 0.40063268 Loss_Enh_Dec: -1.50304925\n",
      "| epoch  65 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.06 | loss  3.15 | ppl    23.42 | acc     0.66 | train_ae_norm     1.00\n",
      "[65/200][3399/4361] Loss_D: 0.00994267 (Loss_D_real: 0.00815960 Loss_D_fake: 0.00178307) Loss_G: 0.41548476 Loss_Enh_Dec: -1.80014157\n",
      "| epoch  65 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  3.12 | ppl    22.71 | acc     0.65 | train_ae_norm     1.00\n",
      "[65/200][3499/4361] Loss_D: 0.00144482 (Loss_D_real: 0.00079489 Loss_D_fake: 0.00064993) Loss_G: 0.44281483 Loss_Enh_Dec: -1.59562671\n",
      "| epoch  65 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  3.06 | ppl    21.24 | acc     0.66 | train_ae_norm     1.00\n",
      "[65/200][3599/4361] Loss_D: 0.00187286 (Loss_D_real: 0.00076045 Loss_D_fake: 0.00111241) Loss_G: 0.39398232 Loss_Enh_Dec: -1.97853816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  65 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  3.08 | ppl    21.81 | acc     0.66 | train_ae_norm     1.00\n",
      "[65/200][3699/4361] Loss_D: 0.00377587 (Loss_D_real: 0.00166596 Loss_D_fake: 0.00210991) Loss_G: 0.41590300 Loss_Enh_Dec: -1.63710368\n",
      "| epoch  65 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  3.10 | ppl    22.28 | acc     0.63 | train_ae_norm     1.00\n",
      "[65/200][3799/4361] Loss_D: 0.00245968 (Loss_D_real: 0.00068558 Loss_D_fake: 0.00177409) Loss_G: 0.40087295 Loss_Enh_Dec: -1.91868520\n",
      "| epoch  65 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  3.13 | ppl    22.88 | acc     0.69 | train_ae_norm     1.00\n",
      "[65/200][3899/4361] Loss_D: 0.00261964 (Loss_D_real: 0.00130598 Loss_D_fake: 0.00131366) Loss_G: 0.44765207 Loss_Enh_Dec: -1.65050316\n",
      "| epoch  65 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  3.12 | ppl    22.54 | acc     0.64 | train_ae_norm     1.00\n",
      "[65/200][3999/4361] Loss_D: 0.00342702 (Loss_D_real: 0.00212242 Loss_D_fake: 0.00130460) Loss_G: 0.40108791 Loss_Enh_Dec: -1.89085734\n",
      "| epoch  65 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.79 | loss  3.14 | ppl    23.19 | acc     0.65 | train_ae_norm     1.00\n",
      "[65/200][4099/4361] Loss_D: 0.00532450 (Loss_D_real: 0.00142013 Loss_D_fake: 0.00390438) Loss_G: 0.35852599 Loss_Enh_Dec: -1.89259815\n",
      "| epoch  65 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  3.08 | ppl    21.82 | acc     0.64 | train_ae_norm     1.00\n",
      "[65/200][4199/4361] Loss_D: 0.00330446 (Loss_D_real: 0.00195140 Loss_D_fake: 0.00135306) Loss_G: 0.39664572 Loss_Enh_Dec: -1.71526432\n",
      "| epoch  65 |  4200/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  3.12 | ppl    22.72 | acc     0.68 | train_ae_norm     1.00\n",
      "[65/200][4299/4361] Loss_D: 0.00363266 (Loss_D_real: 0.00159652 Loss_D_fake: 0.00203613) Loss_G: 0.40954572 Loss_Enh_Dec: -1.76564145\n",
      "| epoch  65 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.06 | loss  3.08 | ppl    21.81 | acc     0.68 | train_ae_norm     1.00\n",
      "| end of epoch  65 | time: 1853.75s | test loss  3.01 | test ppl 20.33 | acc 0.688\n",
      "bleu_self:  [8.85416666e-02 2.69138525e-09 5.55079867e-11 2.65887826e-10\n",
      " 7.88412649e-10]\n",
      "bleu_test:  [4.63541666e-01 2.38521965e-01 2.32247943e-06 5.58392054e-07\n",
      " 4.34445445e-07]\n",
      "bleu_self: [0.08854167,0.00000000,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.46354167,0.23852197,0.00000232,0.00000056,0.00000043]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 66 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:48.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:18.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.717\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.492\n",
      "  Test Loss: 4.033\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  66 |     0/ 4361 batches | lr 0.000000 | ms/batch 859.85 | loss  0.03 | ppl     1.03 | acc     0.69 | train_ae_norm     1.00\n",
      "[66/200][99/4361] Loss_D: 0.00387408 (Loss_D_real: 0.00059470 Loss_D_fake: 0.00327938) Loss_G: 0.47149649 Loss_Enh_Dec: -1.65693021\n",
      "| epoch  66 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  3.08 | ppl    21.74 | acc     0.64 | train_ae_norm     1.00\n",
      "[66/200][199/4361] Loss_D: 0.00174216 (Loss_D_real: 0.00043107 Loss_D_fake: 0.00131109) Loss_G: 0.48259336 Loss_Enh_Dec: -1.28498924\n",
      "| epoch  66 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  3.11 | ppl    22.33 | acc     0.65 | train_ae_norm     1.00\n",
      "[66/200][299/4361] Loss_D: 0.00357816 (Loss_D_real: 0.00238590 Loss_D_fake: 0.00119226) Loss_G: 0.41975689 Loss_Enh_Dec: -1.71659315\n",
      "| epoch  66 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  3.10 | ppl    22.26 | acc     0.64 | train_ae_norm     1.00\n",
      "[66/200][399/4361] Loss_D: 0.00256103 (Loss_D_real: 0.00029675 Loss_D_fake: 0.00226428) Loss_G: 0.42141777 Loss_Enh_Dec: -1.58456457\n",
      "| epoch  66 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  3.02 | ppl    20.45 | acc     0.66 | train_ae_norm     1.00\n",
      "[66/200][499/4361] Loss_D: 0.00208239 (Loss_D_real: 0.00045078 Loss_D_fake: 0.00163161) Loss_G: 0.43824679 Loss_Enh_Dec: -1.45716023\n",
      "| epoch  66 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  3.09 | ppl    21.95 | acc     0.68 | train_ae_norm     1.00\n",
      "[66/200][599/4361] Loss_D: 0.00157477 (Loss_D_real: 0.00099575 Loss_D_fake: 0.00057903) Loss_G: 0.40646267 Loss_Enh_Dec: -1.15342617\n",
      "| epoch  66 |   600/ 4361 batches | lr 0.000000 | ms/batch 402.06 | loss  3.04 | ppl    20.93 | acc     0.64 | train_ae_norm     1.00\n",
      "[66/200][699/4361] Loss_D: 0.00359710 (Loss_D_real: 0.00275970 Loss_D_fake: 0.00083740) Loss_G: 0.42854330 Loss_Enh_Dec: -1.54353321\n",
      "| epoch  66 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.80 | loss  3.10 | ppl    22.09 | acc     0.66 | train_ae_norm     1.00\n",
      "[66/200][799/4361] Loss_D: 0.00218249 (Loss_D_real: 0.00092654 Loss_D_fake: 0.00125595) Loss_G: 0.42353106 Loss_Enh_Dec: -1.23098969\n",
      "| epoch  66 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  3.07 | ppl    21.53 | acc     0.66 | train_ae_norm     1.00\n",
      "[66/200][899/4361] Loss_D: 0.00217662 (Loss_D_real: 0.00089938 Loss_D_fake: 0.00127724) Loss_G: 0.43447953 Loss_Enh_Dec: -1.41952956\n",
      "| epoch  66 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  3.07 | ppl    21.64 | acc     0.68 | train_ae_norm     1.00\n",
      "[66/200][999/4361] Loss_D: 0.00179118 (Loss_D_real: 0.00018959 Loss_D_fake: 0.00160158) Loss_G: 0.41471368 Loss_Enh_Dec: -1.84359646\n",
      "| epoch  66 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  3.06 | ppl    21.24 | acc     0.66 | train_ae_norm     1.00\n",
      "[66/200][1099/4361] Loss_D: 0.00065690 (Loss_D_real: 0.00061345 Loss_D_fake: 0.00004345) Loss_G: 0.96804380 Loss_Enh_Dec: -1.11399567\n",
      "| epoch  66 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  3.06 | ppl    21.37 | acc     0.62 | train_ae_norm     1.00\n",
      "[66/200][1199/4361] Loss_D: 0.00442857 (Loss_D_real: 0.00089292 Loss_D_fake: 0.00353565) Loss_G: 0.45663258 Loss_Enh_Dec: -1.35312271\n",
      "| epoch  66 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.85 | loss  3.06 | ppl    21.30 | acc     0.67 | train_ae_norm     1.00\n",
      "[66/200][1299/4361] Loss_D: 0.00119608 (Loss_D_real: 0.00012219 Loss_D_fake: 0.00107388) Loss_G: 0.39240184 Loss_Enh_Dec: -1.36219919\n",
      "| epoch  66 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  3.08 | ppl    21.85 | acc     0.66 | train_ae_norm     1.00\n",
      "[66/200][1399/4361] Loss_D: 0.00542387 (Loss_D_real: 0.00355700 Loss_D_fake: 0.00186687) Loss_G: 0.44698095 Loss_Enh_Dec: -1.57690549\n",
      "| epoch  66 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  3.09 | ppl    21.88 | acc     0.61 | train_ae_norm     1.00\n",
      "[66/200][1499/4361] Loss_D: 0.00491654 (Loss_D_real: 0.00374429 Loss_D_fake: 0.00117226) Loss_G: 0.43106708 Loss_Enh_Dec: -1.91809833\n",
      "| epoch  66 |  1500/ 4361 batches | lr 0.000000 | ms/batch 405.41 | loss  3.13 | ppl    22.78 | acc     0.64 | train_ae_norm     1.00\n",
      "[66/200][1599/4361] Loss_D: 0.00202516 (Loss_D_real: 0.00015585 Loss_D_fake: 0.00186931) Loss_G: 0.41840607 Loss_Enh_Dec: -2.07034421\n",
      "| epoch  66 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  3.08 | ppl    21.67 | acc     0.65 | train_ae_norm     1.00\n",
      "[66/200][1699/4361] Loss_D: 0.00251695 (Loss_D_real: 0.00114839 Loss_D_fake: 0.00136857) Loss_G: 0.68322504 Loss_Enh_Dec: -1.78792250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  66 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  3.05 | ppl    21.09 | acc     0.66 | train_ae_norm     1.00\n",
      "[66/200][1799/4361] Loss_D: 0.03192895 (Loss_D_real: 0.02850271 Loss_D_fake: 0.00342623) Loss_G: 0.41869593 Loss_Enh_Dec: -1.79958689\n",
      "| epoch  66 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  3.02 | ppl    20.51 | acc     0.67 | train_ae_norm     1.00\n",
      "[66/200][1899/4361] Loss_D: 0.01237552 (Loss_D_real: 0.00960159 Loss_D_fake: 0.00277393) Loss_G: 0.45598149 Loss_Enh_Dec: -1.57368803\n",
      "| epoch  66 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  3.08 | ppl    21.73 | acc     0.68 | train_ae_norm     1.00\n",
      "[66/200][1999/4361] Loss_D: 0.00635887 (Loss_D_real: 0.00090907 Loss_D_fake: 0.00544980) Loss_G: 0.39827964 Loss_Enh_Dec: -1.53291702\n",
      "| epoch  66 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  3.03 | ppl    20.60 | acc     0.68 | train_ae_norm     1.00\n",
      "[66/200][2099/4361] Loss_D: 0.00438082 (Loss_D_real: 0.00295801 Loss_D_fake: 0.00142281) Loss_G: 0.36980563 Loss_Enh_Dec: -1.38191640\n",
      "| epoch  66 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.99 | loss  3.06 | ppl    21.24 | acc     0.67 | train_ae_norm     1.00\n",
      "[66/200][2199/4361] Loss_D: 0.00181461 (Loss_D_real: 0.00030363 Loss_D_fake: 0.00151098) Loss_G: 0.38964367 Loss_Enh_Dec: -1.06249332\n",
      "| epoch  66 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  3.04 | ppl    20.94 | acc     0.66 | train_ae_norm     1.00\n",
      "[66/200][2299/4361] Loss_D: 0.10831569 (Loss_D_real: 0.00022726 Loss_D_fake: 0.10808843) Loss_G: 0.55645424 Loss_Enh_Dec: -1.59212363\n",
      "| epoch  66 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  3.02 | ppl    20.56 | acc     0.68 | train_ae_norm     1.00\n",
      "[66/200][2399/4361] Loss_D: 0.00215460 (Loss_D_real: 0.00034331 Loss_D_fake: 0.00181129) Loss_G: 0.48331499 Loss_Enh_Dec: -1.51592290\n",
      "| epoch  66 |  2400/ 4361 batches | lr 0.000000 | ms/batch 400.89 | loss  3.04 | ppl    20.92 | acc     0.64 | train_ae_norm     1.00\n",
      "[66/200][2499/4361] Loss_D: 0.00207739 (Loss_D_real: 0.00121027 Loss_D_fake: 0.00086712) Loss_G: 0.42966995 Loss_Enh_Dec: -1.26805806\n",
      "| epoch  66 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  3.07 | ppl    21.45 | acc     0.67 | train_ae_norm     1.00\n",
      "[66/200][2599/4361] Loss_D: 0.00226085 (Loss_D_real: 0.00060893 Loss_D_fake: 0.00165192) Loss_G: 0.41577607 Loss_Enh_Dec: -1.25412142\n",
      "| epoch  66 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  3.04 | ppl    20.87 | acc     0.63 | train_ae_norm     1.00\n",
      "[66/200][2699/4361] Loss_D: 0.00274191 (Loss_D_real: 0.00078518 Loss_D_fake: 0.00195673) Loss_G: 0.39502332 Loss_Enh_Dec: -1.10024893\n",
      "| epoch  66 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  3.06 | ppl    21.25 | acc     0.65 | train_ae_norm     1.00\n",
      "[66/200][2799/4361] Loss_D: 0.00725384 (Loss_D_real: 0.00565226 Loss_D_fake: 0.00160158) Loss_G: 0.38829982 Loss_Enh_Dec: -1.37956905\n",
      "| epoch  66 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  3.01 | ppl    20.31 | acc     0.66 | train_ae_norm     1.00\n",
      "[66/200][2899/4361] Loss_D: 0.00301180 (Loss_D_real: 0.00158618 Loss_D_fake: 0.00142562) Loss_G: 0.37945801 Loss_Enh_Dec: -1.02222180\n",
      "| epoch  66 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  3.04 | ppl    20.90 | acc     0.67 | train_ae_norm     1.00\n",
      "[66/200][2999/4361] Loss_D: 0.00466590 (Loss_D_real: 0.00284081 Loss_D_fake: 0.00182508) Loss_G: 0.44152728 Loss_Enh_Dec: -1.07538033\n",
      "| epoch  66 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  3.04 | ppl    20.95 | acc     0.67 | train_ae_norm     1.00\n",
      "[66/200][3099/4361] Loss_D: 0.00178069 (Loss_D_real: 0.00067770 Loss_D_fake: 0.00110298) Loss_G: 0.42915821 Loss_Enh_Dec: -1.19588220\n",
      "| epoch  66 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  3.06 | ppl    21.35 | acc     0.64 | train_ae_norm     1.00\n",
      "[66/200][3199/4361] Loss_D: 0.00260114 (Loss_D_real: 0.00068237 Loss_D_fake: 0.00191877) Loss_G: 0.39355561 Loss_Enh_Dec: -1.18589056\n",
      "| epoch  66 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  3.09 | ppl    22.03 | acc     0.66 | train_ae_norm     1.00\n",
      "[66/200][3299/4361] Loss_D: 0.00354000 (Loss_D_real: 0.00192529 Loss_D_fake: 0.00161471) Loss_G: 0.39517030 Loss_Enh_Dec: -1.22569847\n",
      "| epoch  66 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  3.09 | ppl    21.88 | acc     0.66 | train_ae_norm     1.00\n",
      "[66/200][3399/4361] Loss_D: 0.00294465 (Loss_D_real: 0.00222852 Loss_D_fake: 0.00071613) Loss_G: 0.45236072 Loss_Enh_Dec: -1.27163947\n",
      "| epoch  66 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  3.06 | ppl    21.28 | acc     0.65 | train_ae_norm     1.00\n",
      "[66/200][3499/4361] Loss_D: 0.00292051 (Loss_D_real: 0.00058532 Loss_D_fake: 0.00233518) Loss_G: 0.46522760 Loss_Enh_Dec: -0.98820537\n",
      "| epoch  66 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  3.01 | ppl    20.23 | acc     0.67 | train_ae_norm     1.00\n",
      "[66/200][3599/4361] Loss_D: 0.00342244 (Loss_D_real: 0.00064271 Loss_D_fake: 0.00277973) Loss_G: 0.40015039 Loss_Enh_Dec: -1.26594877\n",
      "| epoch  66 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  3.01 | ppl    20.33 | acc     0.65 | train_ae_norm     1.00\n",
      "[66/200][3699/4361] Loss_D: 0.00275972 (Loss_D_real: 0.00174301 Loss_D_fake: 0.00101671) Loss_G: 0.43023911 Loss_Enh_Dec: -1.35186803\n",
      "| epoch  66 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  3.05 | ppl    21.02 | acc     0.63 | train_ae_norm     1.00\n",
      "[66/200][3799/4361] Loss_D: 0.00655100 (Loss_D_real: 0.00580397 Loss_D_fake: 0.00074703) Loss_G: 0.41817141 Loss_Enh_Dec: -1.32560408\n",
      "| epoch  66 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  3.09 | ppl    22.00 | acc     0.69 | train_ae_norm     1.00\n",
      "[66/200][3899/4361] Loss_D: 0.00147236 (Loss_D_real: 0.00096786 Loss_D_fake: 0.00050450) Loss_G: 0.51567930 Loss_Enh_Dec: -1.61509347\n",
      "| epoch  66 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  3.08 | ppl    21.69 | acc     0.64 | train_ae_norm     1.00\n",
      "[66/200][3999/4361] Loss_D: 0.00178064 (Loss_D_real: 0.00076424 Loss_D_fake: 0.00101640) Loss_G: 0.42005572 Loss_Enh_Dec: -1.59783888\n",
      "| epoch  66 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  3.07 | ppl    21.48 | acc     0.67 | train_ae_norm     1.00\n",
      "[66/200][4099/4361] Loss_D: 0.00306729 (Loss_D_real: 0.00165259 Loss_D_fake: 0.00141471) Loss_G: 0.43258357 Loss_Enh_Dec: -1.30279696\n",
      "| epoch  66 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  3.03 | ppl    20.65 | acc     0.65 | train_ae_norm     1.00\n",
      "[66/200][4199/4361] Loss_D: 0.00664371 (Loss_D_real: 0.00254535 Loss_D_fake: 0.00409836) Loss_G: 0.44405016 Loss_Enh_Dec: -1.41263568\n",
      "| epoch  66 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  3.07 | ppl    21.63 | acc     0.68 | train_ae_norm     1.00\n",
      "[66/200][4299/4361] Loss_D: 0.00382872 (Loss_D_real: 0.00130582 Loss_D_fake: 0.00252290) Loss_G: 0.55914384 Loss_Enh_Dec: -1.15488613\n",
      "| epoch  66 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  3.04 | ppl    20.83 | acc     0.66 | train_ae_norm     1.00\n",
      "| end of epoch  66 | time: 1852.81s | test loss  3.00 | test ppl 20.12 | acc 0.691\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 67 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   210  of    230.    Elapsed: 0:01:18.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.710\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.492\n",
      "  Test Loss: 4.111\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  67 |     0/ 4361 batches | lr 0.000000 | ms/batch 860.38 | loss  0.03 | ppl     1.03 | acc     0.67 | train_ae_norm     1.00\n",
      "[67/200][99/4361] Loss_D: 0.00154975 (Loss_D_real: 0.00078612 Loss_D_fake: 0.00076363) Loss_G: 0.41760126 Loss_Enh_Dec: -1.46405065\n",
      "| epoch  67 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  3.06 | ppl    21.22 | acc     0.64 | train_ae_norm     1.00\n",
      "[67/200][199/4361] Loss_D: 0.00306939 (Loss_D_real: 0.00021425 Loss_D_fake: 0.00285514) Loss_G: 0.48321459 Loss_Enh_Dec: -1.08132839\n",
      "| epoch  67 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  3.08 | ppl    21.82 | acc     0.66 | train_ae_norm     1.00\n",
      "[67/200][299/4361] Loss_D: 0.00263519 (Loss_D_real: 0.00120157 Loss_D_fake: 0.00143362) Loss_G: 0.40968701 Loss_Enh_Dec: -1.09339452\n",
      "| epoch  67 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  3.09 | ppl    22.04 | acc     0.62 | train_ae_norm     1.00\n",
      "[67/200][399/4361] Loss_D: 0.01257316 (Loss_D_real: 0.01180170 Loss_D_fake: 0.00077146) Loss_G: 0.43832037 Loss_Enh_Dec: -1.63820612\n",
      "| epoch  67 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  2.99 | ppl    19.80 | acc     0.66 | train_ae_norm     1.00\n",
      "[67/200][499/4361] Loss_D: 0.00297932 (Loss_D_real: 0.00078445 Loss_D_fake: 0.00219487) Loss_G: 0.43174487 Loss_Enh_Dec: -1.25011730\n",
      "| epoch  67 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  3.07 | ppl    21.64 | acc     0.66 | train_ae_norm     1.00\n",
      "[67/200][599/4361] Loss_D: 0.00297198 (Loss_D_real: 0.00096930 Loss_D_fake: 0.00200268) Loss_G: 0.43130788 Loss_Enh_Dec: -1.23802090\n",
      "| epoch  67 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  3.05 | ppl    21.04 | acc     0.60 | train_ae_norm     1.00\n",
      "[67/200][699/4361] Loss_D: 0.00250286 (Loss_D_real: 0.00102249 Loss_D_fake: 0.00148037) Loss_G: 0.39527828 Loss_Enh_Dec: -1.29773748\n",
      "| epoch  67 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  3.09 | ppl    22.03 | acc     0.67 | train_ae_norm     1.00\n",
      "[67/200][799/4361] Loss_D: 0.00219780 (Loss_D_real: 0.00132960 Loss_D_fake: 0.00086820) Loss_G: 0.44358426 Loss_Enh_Dec: -1.68004787\n",
      "| epoch  67 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  3.04 | ppl    20.96 | acc     0.66 | train_ae_norm     1.00\n",
      "[67/200][899/4361] Loss_D: 0.00192239 (Loss_D_real: 0.00068904 Loss_D_fake: 0.00123335) Loss_G: 0.47572270 Loss_Enh_Dec: -0.98112673\n",
      "| epoch  67 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  3.07 | ppl    21.52 | acc     0.68 | train_ae_norm     1.00\n",
      "[67/200][999/4361] Loss_D: 0.00509402 (Loss_D_real: 0.00243561 Loss_D_fake: 0.00265841) Loss_G: 0.40168986 Loss_Enh_Dec: -1.29155469\n",
      "| epoch  67 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  3.04 | ppl    20.86 | acc     0.67 | train_ae_norm     1.00\n",
      "[67/200][1099/4361] Loss_D: 0.00390333 (Loss_D_real: 0.00298788 Loss_D_fake: 0.00091545) Loss_G: 0.40870151 Loss_Enh_Dec: -1.22389531\n",
      "| epoch  67 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.91 | loss  3.04 | ppl    20.91 | acc     0.64 | train_ae_norm     1.00\n",
      "[67/200][1199/4361] Loss_D: 0.01037007 (Loss_D_real: 0.00959743 Loss_D_fake: 0.00077264) Loss_G: 0.45601755 Loss_Enh_Dec: -1.39881575\n",
      "| epoch  67 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  3.04 | ppl    20.88 | acc     0.69 | train_ae_norm     1.00\n",
      "[67/200][1299/4361] Loss_D: 0.00530421 (Loss_D_real: 0.00209832 Loss_D_fake: 0.00320589) Loss_G: 0.41670415 Loss_Enh_Dec: -1.89924037\n",
      "| epoch  67 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  3.05 | ppl    21.22 | acc     0.67 | train_ae_norm     1.00\n",
      "[67/200][1399/4361] Loss_D: 0.00092354 (Loss_D_real: 0.00031578 Loss_D_fake: 0.00060776) Loss_G: 0.42904654 Loss_Enh_Dec: -1.43805921\n",
      "| epoch  67 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  3.06 | ppl    21.34 | acc     0.62 | train_ae_norm     1.00\n",
      "[67/200][1499/4361] Loss_D: 0.00508540 (Loss_D_real: 0.00164389 Loss_D_fake: 0.00344151) Loss_G: 0.39349872 Loss_Enh_Dec: -1.54872978\n",
      "| epoch  67 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  3.12 | ppl    22.55 | acc     0.65 | train_ae_norm     1.00\n",
      "[67/200][1599/4361] Loss_D: 0.00745742 (Loss_D_real: 0.00436412 Loss_D_fake: 0.00309330) Loss_G: 0.40792018 Loss_Enh_Dec: -1.55798781\n",
      "| epoch  67 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  3.09 | ppl    21.95 | acc     0.64 | train_ae_norm     1.00\n",
      "[67/200][1699/4361] Loss_D: 0.00731702 (Loss_D_real: 0.00628181 Loss_D_fake: 0.00103521) Loss_G: 0.47939897 Loss_Enh_Dec: -1.73833358\n",
      "| epoch  67 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  3.05 | ppl    21.18 | acc     0.66 | train_ae_norm     1.00\n",
      "[67/200][1799/4361] Loss_D: 0.00421066 (Loss_D_real: 0.00185473 Loss_D_fake: 0.00235593) Loss_G: 0.37052161 Loss_Enh_Dec: -1.91585338\n",
      "| epoch  67 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  3.04 | ppl    20.93 | acc     0.65 | train_ae_norm     1.00\n",
      "[67/200][1899/4361] Loss_D: 0.00129792 (Loss_D_real: 0.00018751 Loss_D_fake: 0.00111041) Loss_G: 0.42665455 Loss_Enh_Dec: -1.59794295\n",
      "| epoch  67 |  1900/ 4361 batches | lr 0.000000 | ms/batch 402.11 | loss  3.12 | ppl    22.67 | acc     0.66 | train_ae_norm     1.00\n",
      "[67/200][1999/4361] Loss_D: 0.00290332 (Loss_D_real: 0.00181759 Loss_D_fake: 0.00108574) Loss_G: 0.40237576 Loss_Enh_Dec: -1.41550827\n",
      "| epoch  67 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  3.14 | ppl    23.01 | acc     0.66 | train_ae_norm     1.00\n",
      "[67/200][2099/4361] Loss_D: 0.00458182 (Loss_D_real: 0.00150879 Loss_D_fake: 0.00307303) Loss_G: 0.39411572 Loss_Enh_Dec: -1.56023967\n",
      "| epoch  67 |  2100/ 4361 batches | lr 0.000000 | ms/batch 402.14 | loss  3.12 | ppl    22.72 | acc     0.65 | train_ae_norm     1.00\n",
      "[67/200][2199/4361] Loss_D: 0.00129303 (Loss_D_real: 0.00030715 Loss_D_fake: 0.00098588) Loss_G: 0.40054178 Loss_Enh_Dec: -1.56999052\n",
      "| epoch  67 |  2200/ 4361 batches | lr 0.000000 | ms/batch 402.28 | loss  3.10 | ppl    22.23 | acc     0.66 | train_ae_norm     1.00\n",
      "[67/200][2299/4361] Loss_D: 0.00148684 (Loss_D_real: 0.00042825 Loss_D_fake: 0.00105859) Loss_G: 0.55804843 Loss_Enh_Dec: -1.40273774\n",
      "| epoch  67 |  2300/ 4361 batches | lr 0.000000 | ms/batch 402.06 | loss  3.13 | ppl    22.79 | acc     0.66 | train_ae_norm     1.00\n",
      "[67/200][2399/4361] Loss_D: 0.00635359 (Loss_D_real: 0.00115129 Loss_D_fake: 0.00520231) Loss_G: 0.43500438 Loss_Enh_Dec: -1.20702398\n",
      "| epoch  67 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  3.11 | ppl    22.40 | acc     0.63 | train_ae_norm     1.00\n",
      "[67/200][2499/4361] Loss_D: 0.00394669 (Loss_D_real: 0.00221601 Loss_D_fake: 0.00173067) Loss_G: 0.43367955 Loss_Enh_Dec: -1.93849933\n",
      "| epoch  67 |  2500/ 4361 batches | lr 0.000000 | ms/batch 402.11 | loss  3.16 | ppl    23.60 | acc     0.64 | train_ae_norm     1.00\n",
      "[67/200][2599/4361] Loss_D: 0.00282773 (Loss_D_real: 0.00086141 Loss_D_fake: 0.00196632) Loss_G: 0.39253291 Loss_Enh_Dec: -1.45519626\n",
      "| epoch  67 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  3.12 | ppl    22.57 | acc     0.62 | train_ae_norm     1.00\n",
      "[67/200][2699/4361] Loss_D: 0.00680719 (Loss_D_real: 0.00507567 Loss_D_fake: 0.00173152) Loss_G: 0.42996460 Loss_Enh_Dec: -1.49270916\n",
      "| epoch  67 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  3.14 | ppl    23.05 | acc     0.64 | train_ae_norm     1.00\n",
      "[67/200][2799/4361] Loss_D: 0.00428971 (Loss_D_real: 0.00143623 Loss_D_fake: 0.00285347) Loss_G: 0.42529687 Loss_Enh_Dec: -1.90034997\n",
      "| epoch  67 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  3.09 | ppl    21.99 | acc     0.65 | train_ae_norm     1.00\n",
      "[67/200][2899/4361] Loss_D: 0.00679881 (Loss_D_real: 0.00241064 Loss_D_fake: 0.00438817) Loss_G: 0.39365563 Loss_Enh_Dec: -2.00879169\n",
      "| epoch  67 |  2900/ 4361 batches | lr 0.000000 | ms/batch 402.12 | loss  3.10 | ppl    22.21 | acc     0.66 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67/200][2999/4361] Loss_D: 0.00942093 (Loss_D_real: 0.00806114 Loss_D_fake: 0.00135978) Loss_G: 0.46330053 Loss_Enh_Dec: -1.66229630\n",
      "| epoch  67 |  3000/ 4361 batches | lr 0.000000 | ms/batch 402.57 | loss  3.11 | ppl    22.40 | acc     0.67 | train_ae_norm     1.00\n",
      "[67/200][3099/4361] Loss_D: 0.00816463 (Loss_D_real: 0.00674332 Loss_D_fake: 0.00142131) Loss_G: 0.61211246 Loss_Enh_Dec: -1.41926193\n",
      "| epoch  67 |  3100/ 4361 batches | lr 0.000000 | ms/batch 402.38 | loss  3.13 | ppl    22.84 | acc     0.62 | train_ae_norm     1.00\n",
      "[67/200][3199/4361] Loss_D: 0.00436477 (Loss_D_real: 0.00137439 Loss_D_fake: 0.00299038) Loss_G: 0.44474077 Loss_Enh_Dec: -1.66708171\n",
      "| epoch  67 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  3.16 | ppl    23.59 | acc     0.67 | train_ae_norm     1.00\n",
      "[67/200][3299/4361] Loss_D: 0.01999854 (Loss_D_real: 0.01907327 Loss_D_fake: 0.00092528) Loss_G: 0.42609587 Loss_Enh_Dec: -1.55964279\n",
      "| epoch  67 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  3.12 | ppl    22.67 | acc     0.63 | train_ae_norm     1.00\n",
      "[67/200][3399/4361] Loss_D: 0.00632354 (Loss_D_real: 0.00425031 Loss_D_fake: 0.00207323) Loss_G: 0.47760031 Loss_Enh_Dec: -1.30766034\n",
      "| epoch  67 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  3.11 | ppl    22.52 | acc     0.66 | train_ae_norm     1.00\n",
      "[67/200][3499/4361] Loss_D: 0.00349327 (Loss_D_real: 0.00210488 Loss_D_fake: 0.00138840) Loss_G: 0.43804595 Loss_Enh_Dec: -1.73392761\n",
      "| epoch  67 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  3.07 | ppl    21.44 | acc     0.67 | train_ae_norm     1.00\n",
      "[67/200][3599/4361] Loss_D: 0.00450266 (Loss_D_real: 0.00212401 Loss_D_fake: 0.00237865) Loss_G: 0.42905831 Loss_Enh_Dec: -1.32177198\n",
      "| epoch  67 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  3.06 | ppl    21.39 | acc     0.65 | train_ae_norm     1.00\n",
      "[67/200][3699/4361] Loss_D: 0.00353167 (Loss_D_real: 0.00019359 Loss_D_fake: 0.00333808) Loss_G: 0.48237583 Loss_Enh_Dec: -1.45163226\n",
      "| epoch  67 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.99 | loss  3.08 | ppl    21.82 | acc     0.61 | train_ae_norm     1.00\n",
      "[67/200][3799/4361] Loss_D: 0.00797408 (Loss_D_real: 0.00620752 Loss_D_fake: 0.00176655) Loss_G: 0.38079223 Loss_Enh_Dec: -1.63498652\n",
      "| epoch  67 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  3.10 | ppl    22.24 | acc     0.68 | train_ae_norm     1.00\n",
      "[67/200][3899/4361] Loss_D: 0.00168293 (Loss_D_real: 0.00018286 Loss_D_fake: 0.00150007) Loss_G: 0.39993140 Loss_Enh_Dec: -1.47360992\n",
      "| epoch  67 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  3.06 | ppl    21.38 | acc     0.60 | train_ae_norm     1.00\n",
      "[67/200][3999/4361] Loss_D: 0.00145525 (Loss_D_real: 0.00057271 Loss_D_fake: 0.00088254) Loss_G: 0.53248167 Loss_Enh_Dec: -1.58489788\n",
      "| epoch  67 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  3.08 | ppl    21.67 | acc     0.65 | train_ae_norm     1.00\n",
      "[67/200][4099/4361] Loss_D: 0.03021918 (Loss_D_real: 0.02758469 Loss_D_fake: 0.00263449) Loss_G: 0.37095150 Loss_Enh_Dec: -1.68840444\n",
      "| epoch  67 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  3.03 | ppl    20.67 | acc     0.65 | train_ae_norm     1.00\n",
      "[67/200][4199/4361] Loss_D: 0.00265923 (Loss_D_real: 0.00030999 Loss_D_fake: 0.00234924) Loss_G: 0.40306297 Loss_Enh_Dec: -1.14718878\n",
      "| epoch  67 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  3.06 | ppl    21.34 | acc     0.69 | train_ae_norm     1.00\n",
      "[67/200][4299/4361] Loss_D: 0.01943028 (Loss_D_real: 0.00457030 Loss_D_fake: 0.01485997) Loss_G: 0.70212805 Loss_Enh_Dec: -1.49432981\n",
      "| epoch  67 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  3.03 | ppl    20.69 | acc     0.68 | train_ae_norm     1.00\n",
      "| end of epoch  67 | time: 1853.70s | test loss  3.01 | test ppl 20.19 | acc 0.691\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 68 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:57.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.702\n",
      "  Training epcoh took: 0:01:27\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.487\n",
      "  Test Loss: 4.140\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  68 |     0/ 4361 batches | lr 0.000000 | ms/batch 868.93 | loss  0.03 | ppl     1.03 | acc     0.68 | train_ae_norm     1.00\n",
      "[68/200][99/4361] Loss_D: 0.00530943 (Loss_D_real: 0.00419491 Loss_D_fake: 0.00111452) Loss_G: 0.39996886 Loss_Enh_Dec: -1.57188570\n",
      "| epoch  68 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  3.05 | ppl    21.18 | acc     0.62 | train_ae_norm     1.00\n",
      "[68/200][199/4361] Loss_D: 0.00256462 (Loss_D_real: 0.00038009 Loss_D_fake: 0.00218454) Loss_G: 0.38658053 Loss_Enh_Dec: -1.55692708\n",
      "| epoch  68 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  3.07 | ppl    21.63 | acc     0.64 | train_ae_norm     1.00\n",
      "[68/200][299/4361] Loss_D: 0.00412375 (Loss_D_real: 0.00173802 Loss_D_fake: 0.00238574) Loss_G: 0.43862200 Loss_Enh_Dec: -1.56344819\n",
      "| epoch  68 |   300/ 4361 batches | lr 0.000000 | ms/batch 402.14 | loss  3.07 | ppl    21.62 | acc     0.62 | train_ae_norm     1.00\n",
      "[68/200][399/4361] Loss_D: 0.00269173 (Loss_D_real: 0.00054360 Loss_D_fake: 0.00214813) Loss_G: 0.38039538 Loss_Enh_Dec: -1.64046502\n",
      "| epoch  68 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  2.97 | ppl    19.46 | acc     0.65 | train_ae_norm     1.00\n",
      "[68/200][499/4361] Loss_D: 0.02695193 (Loss_D_real: 0.02231469 Loss_D_fake: 0.00463724) Loss_G: 0.36909389 Loss_Enh_Dec: -1.45743144\n",
      "| epoch  68 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  3.05 | ppl    21.04 | acc     0.70 | train_ae_norm     1.00\n",
      "[68/200][599/4361] Loss_D: 0.00211971 (Loss_D_real: 0.00052908 Loss_D_fake: 0.00159063) Loss_G: 0.37256849 Loss_Enh_Dec: -1.59123516\n",
      "| epoch  68 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  3.00 | ppl    20.12 | acc     0.62 | train_ae_norm     1.00\n",
      "[68/200][699/4361] Loss_D: 0.00922754 (Loss_D_real: 0.00742935 Loss_D_fake: 0.00179819) Loss_G: 0.37249422 Loss_Enh_Dec: -1.43850553\n",
      "| epoch  68 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  3.04 | ppl    21.00 | acc     0.65 | train_ae_norm     1.00\n",
      "[68/200][799/4361] Loss_D: 0.00922340 (Loss_D_real: 0.00613709 Loss_D_fake: 0.00308630) Loss_G: 0.39324102 Loss_Enh_Dec: -1.95780647\n",
      "| epoch  68 |   800/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  3.01 | ppl    20.28 | acc     0.68 | train_ae_norm     1.00\n",
      "[68/200][899/4361] Loss_D: 0.01069583 (Loss_D_real: 0.00106940 Loss_D_fake: 0.00962643) Loss_G: 0.40734822 Loss_Enh_Dec: -1.56936061\n",
      "| epoch  68 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  3.03 | ppl    20.60 | acc     0.71 | train_ae_norm     1.00\n",
      "[68/200][999/4361] Loss_D: 0.00419457 (Loss_D_real: 0.00284655 Loss_D_fake: 0.00134802) Loss_G: 0.45781288 Loss_Enh_Dec: -1.18589079\n",
      "| epoch  68 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  3.01 | ppl    20.26 | acc     0.68 | train_ae_norm     1.00\n",
      "[68/200][1099/4361] Loss_D: 0.00545938 (Loss_D_real: 0.00248833 Loss_D_fake: 0.00297105) Loss_G: 0.41416645 Loss_Enh_Dec: -1.40608513\n",
      "| epoch  68 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  3.01 | ppl    20.22 | acc     0.66 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68/200][1199/4361] Loss_D: 0.00392595 (Loss_D_real: 0.00140779 Loss_D_fake: 0.00251816) Loss_G: 0.36195096 Loss_Enh_Dec: -1.16189706\n",
      "| epoch  68 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  3.00 | ppl    20.12 | acc     0.67 | train_ae_norm     1.00\n",
      "[68/200][1299/4361] Loss_D: 0.03213280 (Loss_D_real: 0.02973115 Loss_D_fake: 0.00240164) Loss_G: 0.40557224 Loss_Enh_Dec: -1.46524811\n",
      "| epoch  68 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  3.02 | ppl    20.53 | acc     0.67 | train_ae_norm     1.00\n",
      "[68/200][1399/4361] Loss_D: 0.00335701 (Loss_D_real: 0.00210657 Loss_D_fake: 0.00125044) Loss_G: 0.39665872 Loss_Enh_Dec: -1.15419948\n",
      "| epoch  68 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  3.01 | ppl    20.34 | acc     0.63 | train_ae_norm     1.00\n",
      "[68/200][1499/4361] Loss_D: 0.02446864 (Loss_D_real: 0.02227311 Loss_D_fake: 0.00219553) Loss_G: 0.38032284 Loss_Enh_Dec: -1.36963654\n",
      "| epoch  68 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  3.06 | ppl    21.23 | acc     0.64 | train_ae_norm     1.00\n",
      "[68/200][1599/4361] Loss_D: 0.00259457 (Loss_D_real: 0.00089547 Loss_D_fake: 0.00169911) Loss_G: 0.36843726 Loss_Enh_Dec: -1.42294180\n",
      "| epoch  68 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  3.03 | ppl    20.77 | acc     0.66 | train_ae_norm     1.00\n",
      "[68/200][1699/4361] Loss_D: 0.05190554 (Loss_D_real: 0.05061409 Loss_D_fake: 0.00129145) Loss_G: 0.45270976 Loss_Enh_Dec: -1.55272698\n",
      "| epoch  68 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  3.01 | ppl    20.29 | acc     0.66 | train_ae_norm     1.00\n",
      "[68/200][1799/4361] Loss_D: 0.00208457 (Loss_D_real: 0.00018237 Loss_D_fake: 0.00190220) Loss_G: 0.41330463 Loss_Enh_Dec: -1.17088735\n",
      "| epoch  68 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  2.98 | ppl    19.71 | acc     0.66 | train_ae_norm     1.00\n",
      "[68/200][1899/4361] Loss_D: 0.00602142 (Loss_D_real: 0.00107629 Loss_D_fake: 0.00494513) Loss_G: 0.43816757 Loss_Enh_Dec: -1.38961625\n",
      "| epoch  68 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  3.03 | ppl    20.74 | acc     0.68 | train_ae_norm     1.00\n",
      "[68/200][1999/4361] Loss_D: 0.01494752 (Loss_D_real: 0.00384966 Loss_D_fake: 0.01109786) Loss_G: 0.75260127 Loss_Enh_Dec: -0.99806350\n",
      "| epoch  68 |  2000/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  2.99 | ppl    19.85 | acc     0.68 | train_ae_norm     1.00\n",
      "[68/200][2099/4361] Loss_D: 0.03383303 (Loss_D_real: 0.03065573 Loss_D_fake: 0.00317730) Loss_G: 0.49024040 Loss_Enh_Dec: -1.29776919\n",
      "| epoch  68 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  3.01 | ppl    20.38 | acc     0.68 | train_ae_norm     1.00\n",
      "[68/200][2199/4361] Loss_D: 0.01027560 (Loss_D_real: 0.00187598 Loss_D_fake: 0.00839963) Loss_G: 0.42860863 Loss_Enh_Dec: -1.03406954\n",
      "| epoch  68 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  3.01 | ppl    20.20 | acc     0.67 | train_ae_norm     1.00\n",
      "[68/200][2299/4361] Loss_D: 0.00230894 (Loss_D_real: 0.00112833 Loss_D_fake: 0.00118061) Loss_G: 0.41067430 Loss_Enh_Dec: -1.14049137\n",
      "| epoch  68 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  2.99 | ppl    19.95 | acc     0.68 | train_ae_norm     1.00\n",
      "[68/200][2399/4361] Loss_D: 0.00371329 (Loss_D_real: 0.00176460 Loss_D_fake: 0.00194869) Loss_G: 0.40849024 Loss_Enh_Dec: -1.44868660\n",
      "| epoch  68 |  2400/ 4361 batches | lr 0.000000 | ms/batch 400.90 | loss  3.00 | ppl    20.18 | acc     0.63 | train_ae_norm     1.00\n",
      "[68/200][2499/4361] Loss_D: 0.00665849 (Loss_D_real: 0.00523975 Loss_D_fake: 0.00141874) Loss_G: 0.40221539 Loss_Enh_Dec: -1.45507872\n",
      "| epoch  68 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  3.03 | ppl    20.70 | acc     0.67 | train_ae_norm     1.00\n",
      "[68/200][2599/4361] Loss_D: 0.00684333 (Loss_D_real: 0.00298312 Loss_D_fake: 0.00386021) Loss_G: 0.37408456 Loss_Enh_Dec: -0.88471490\n",
      "| epoch  68 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  3.00 | ppl    19.99 | acc     0.65 | train_ae_norm     1.00\n",
      "[68/200][2699/4361] Loss_D: 0.00357488 (Loss_D_real: 0.00066215 Loss_D_fake: 0.00291272) Loss_G: 0.36576352 Loss_Enh_Dec: -1.27895772\n",
      "| epoch  68 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  3.01 | ppl    20.32 | acc     0.66 | train_ae_norm     1.00\n",
      "[68/200][2799/4361] Loss_D: 0.00460485 (Loss_D_real: 0.00320313 Loss_D_fake: 0.00140172) Loss_G: 0.38817498 Loss_Enh_Dec: -1.27722359\n",
      "| epoch  68 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.96 | ppl    19.36 | acc     0.66 | train_ae_norm     1.00\n",
      "[68/200][2899/4361] Loss_D: 0.02069362 (Loss_D_real: 0.01861699 Loss_D_fake: 0.00207663) Loss_G: 0.41837054 Loss_Enh_Dec: -1.33516729\n",
      "| epoch  68 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  3.00 | ppl    20.00 | acc     0.66 | train_ae_norm     1.00\n",
      "[68/200][2999/4361] Loss_D: 0.00516070 (Loss_D_real: 0.00356162 Loss_D_fake: 0.00159908) Loss_G: 0.48729354 Loss_Enh_Dec: -1.35097587\n",
      "| epoch  68 |  3000/ 4361 batches | lr 0.000000 | ms/batch 402.66 | loss  2.99 | ppl    19.87 | acc     0.68 | train_ae_norm     1.00\n",
      "[68/200][3099/4361] Loss_D: 0.00738823 (Loss_D_real: 0.00614003 Loss_D_fake: 0.00124820) Loss_G: 0.40764877 Loss_Enh_Dec: -1.20720172\n",
      "| epoch  68 |  3100/ 4361 batches | lr 0.000000 | ms/batch 402.21 | loss  3.00 | ppl    20.14 | acc     0.64 | train_ae_norm     1.00\n",
      "[68/200][3199/4361] Loss_D: 0.00171080 (Loss_D_real: 0.00079666 Loss_D_fake: 0.00091413) Loss_G: 0.40056783 Loss_Enh_Dec: -1.23664582\n",
      "| epoch  68 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  3.03 | ppl    20.62 | acc     0.68 | train_ae_norm     1.00\n",
      "[68/200][3299/4361] Loss_D: 0.00217183 (Loss_D_real: 0.00072691 Loss_D_fake: 0.00144492) Loss_G: 0.38936898 Loss_Enh_Dec: -1.13156509\n",
      "| epoch  68 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  3.03 | ppl    20.60 | acc     0.67 | train_ae_norm     1.00\n",
      "[68/200][3399/4361] Loss_D: 0.00252107 (Loss_D_real: 0.00121157 Loss_D_fake: 0.00130951) Loss_G: 0.43482885 Loss_Enh_Dec: -1.16083837\n",
      "| epoch  68 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  3.01 | ppl    20.36 | acc     0.66 | train_ae_norm     1.00\n",
      "[68/200][3499/4361] Loss_D: 0.00325871 (Loss_D_real: 0.00120204 Loss_D_fake: 0.00205667) Loss_G: 0.40688798 Loss_Enh_Dec: -1.39784241\n",
      "| epoch  68 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  2.96 | ppl    19.30 | acc     0.66 | train_ae_norm     1.00\n",
      "[68/200][3599/4361] Loss_D: 0.00263409 (Loss_D_real: 0.00033198 Loss_D_fake: 0.00230211) Loss_G: 0.42855164 Loss_Enh_Dec: -1.31118715\n",
      "| epoch  68 |  3600/ 4361 batches | lr 0.000000 | ms/batch 402.29 | loss  2.97 | ppl    19.40 | acc     0.68 | train_ae_norm     1.00\n",
      "[68/200][3699/4361] Loss_D: 0.00487404 (Loss_D_real: 0.00064547 Loss_D_fake: 0.00422857) Loss_G: 0.38233453 Loss_Enh_Dec: -1.24926174\n",
      "| epoch  68 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  3.00 | ppl    20.04 | acc     0.64 | train_ae_norm     1.00\n",
      "[68/200][3799/4361] Loss_D: 0.00187881 (Loss_D_real: 0.00046002 Loss_D_fake: 0.00141880) Loss_G: 0.39271221 Loss_Enh_Dec: -1.59360433\n",
      "| epoch  68 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  3.01 | ppl    20.31 | acc     0.69 | train_ae_norm     1.00\n",
      "[68/200][3899/4361] Loss_D: 0.00247496 (Loss_D_real: 0.00102841 Loss_D_fake: 0.00144655) Loss_G: 0.42027703 Loss_Enh_Dec: -1.21510375\n",
      "| epoch  68 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.05 | loss  3.03 | ppl    20.63 | acc     0.63 | train_ae_norm     1.00\n",
      "[68/200][3999/4361] Loss_D: 0.00203655 (Loss_D_real: 0.00004508 Loss_D_fake: 0.00199148) Loss_G: 0.39643860 Loss_Enh_Dec: -1.50723922\n",
      "| epoch  68 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  3.03 | ppl    20.62 | acc     0.67 | train_ae_norm     1.00\n",
      "[68/200][4099/4361] Loss_D: 0.00712089 (Loss_D_real: 0.00070366 Loss_D_fake: 0.00641723) Loss_G: 0.47782031 Loss_Enh_Dec: -1.64186978\n",
      "| epoch  68 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  3.00 | ppl    20.03 | acc     0.66 | train_ae_norm     1.00\n",
      "[68/200][4199/4361] Loss_D: 0.00204055 (Loss_D_real: 0.00094921 Loss_D_fake: 0.00109134) Loss_G: 0.43568775 Loss_Enh_Dec: -1.63737524\n",
      "| epoch  68 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  3.04 | ppl    21.01 | acc     0.70 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68/200][4299/4361] Loss_D: 0.00280462 (Loss_D_real: 0.00159903 Loss_D_fake: 0.00120558) Loss_G: 0.40960893 Loss_Enh_Dec: -1.48770368\n",
      "| epoch  68 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.91 | loss  3.00 | ppl    20.09 | acc     0.68 | train_ae_norm     1.00\n",
      "| end of epoch  68 | time: 1853.92s | test loss  2.98 | test ppl 19.62 | acc 0.694\n",
      "bleu_self:  [2.43055556e-02 8.09898782e-10 2.71036086e-12 1.67455727e-13\n",
      " 1.58925487e-13]\n",
      "bleu_test:  [5.85813492e-01 4.16666751e-02 3.14169992e-07 9.77347997e-10\n",
      " 3.75592399e-10]\n",
      "bleu_self: [0.02430556,0.00000000,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.58581349,0.04166668,0.00000031,0.00000000,0.00000000]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 69 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.702\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.482\n",
      "  Test Loss: 4.116\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  69 |     0/ 4361 batches | lr 0.000000 | ms/batch 867.41 | loss  0.03 | ppl     1.03 | acc     0.69 | train_ae_norm     1.00\n",
      "[69/200][99/4361] Loss_D: 0.00145633 (Loss_D_real: 0.00033395 Loss_D_fake: 0.00112238) Loss_G: 0.43524930 Loss_Enh_Dec: -1.35977435\n",
      "| epoch  69 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  3.03 | ppl    20.80 | acc     0.64 | train_ae_norm     1.00\n",
      "[69/200][199/4361] Loss_D: 0.00470261 (Loss_D_real: 0.00160877 Loss_D_fake: 0.00309383) Loss_G: 0.42354080 Loss_Enh_Dec: -1.50310743\n",
      "| epoch  69 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  3.03 | ppl    20.78 | acc     0.66 | train_ae_norm     1.00\n",
      "[69/200][299/4361] Loss_D: 0.00182447 (Loss_D_real: 0.00115034 Loss_D_fake: 0.00067413) Loss_G: 0.50265414 Loss_Enh_Dec: -0.68498778\n",
      "| epoch  69 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.94 | loss  3.04 | ppl    20.99 | acc     0.64 | train_ae_norm     1.00\n",
      "[69/200][399/4361] Loss_D: 0.01130421 (Loss_D_real: 0.00491563 Loss_D_fake: 0.00638858) Loss_G: 0.42763659 Loss_Enh_Dec: -1.07878864\n",
      "| epoch  69 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  2.95 | ppl    19.01 | acc     0.67 | train_ae_norm     1.00\n",
      "[69/200][499/4361] Loss_D: 0.00390654 (Loss_D_real: 0.00218170 Loss_D_fake: 0.00172484) Loss_G: 0.37616563 Loss_Enh_Dec: -1.56235456\n",
      "| epoch  69 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  3.02 | ppl    20.39 | acc     0.66 | train_ae_norm     1.00\n",
      "[69/200][599/4361] Loss_D: 0.00535504 (Loss_D_real: 0.00195777 Loss_D_fake: 0.00339727) Loss_G: 0.70763606 Loss_Enh_Dec: -0.79546386\n",
      "| epoch  69 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  2.98 | ppl    19.76 | acc     0.64 | train_ae_norm     1.00\n",
      "[69/200][699/4361] Loss_D: 0.00571043 (Loss_D_real: 0.00308349 Loss_D_fake: 0.00262694) Loss_G: 0.38798997 Loss_Enh_Dec: -0.75351793\n",
      "| epoch  69 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  3.02 | ppl    20.55 | acc     0.66 | train_ae_norm     1.00\n",
      "[69/200][799/4361] Loss_D: 0.00407110 (Loss_D_real: 0.00082926 Loss_D_fake: 0.00324184) Loss_G: 0.38844129 Loss_Enh_Dec: -0.95531887\n",
      "| epoch  69 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  2.99 | ppl    19.89 | acc     0.67 | train_ae_norm     1.00\n",
      "[69/200][899/4361] Loss_D: 0.00346450 (Loss_D_real: 0.00182517 Loss_D_fake: 0.00163933) Loss_G: 0.39105135 Loss_Enh_Dec: -1.56514108\n",
      "| epoch  69 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  3.02 | ppl    20.40 | acc     0.69 | train_ae_norm     1.00\n",
      "[69/200][1199/4361] Loss_D: 0.00197708 (Loss_D_real: 0.00109476 Loss_D_fake: 0.00088231) Loss_G: 0.39979082 Loss_Enh_Dec: -1.76776946\n",
      "| epoch  69 |  1200/ 4361 batches | lr 0.000000 | ms/batch 402.09 | loss  2.99 | ppl    19.93 | acc     0.68 | train_ae_norm     1.00\n",
      "[69/200][1299/4361] Loss_D: 0.00428708 (Loss_D_real: 0.00122790 Loss_D_fake: 0.00305918) Loss_G: 0.40809774 Loss_Enh_Dec: -1.73337519\n",
      "| epoch  69 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  3.02 | ppl    20.53 | acc     0.67 | train_ae_norm     1.00\n",
      "[69/200][1399/4361] Loss_D: 0.00246861 (Loss_D_real: 0.00051981 Loss_D_fake: 0.00194881) Loss_G: 0.46244135 Loss_Enh_Dec: -1.42032456\n",
      "| epoch  69 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.06 | loss  3.02 | ppl    20.52 | acc     0.63 | train_ae_norm     1.00\n",
      "[69/200][1499/4361] Loss_D: 0.00255160 (Loss_D_real: 0.00113619 Loss_D_fake: 0.00141540) Loss_G: 0.39602336 Loss_Enh_Dec: -1.80698967\n",
      "| epoch  69 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  3.07 | ppl    21.60 | acc     0.63 | train_ae_norm     1.00\n",
      "[69/200][1599/4361] Loss_D: 0.08127254 (Loss_D_real: 0.07877772 Loss_D_fake: 0.00249482) Loss_G: 0.40307426 Loss_Enh_Dec: -1.62925708\n",
      "| epoch  69 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.91 | loss  3.04 | ppl    20.92 | acc     0.65 | train_ae_norm     1.00\n",
      "[69/200][1699/4361] Loss_D: 0.00421437 (Loss_D_real: 0.00175325 Loss_D_fake: 0.00246112) Loss_G: 0.35171005 Loss_Enh_Dec: -1.49321139\n",
      "| epoch  69 |  1700/ 4361 batches | lr 0.000000 | ms/batch 402.53 | loss  3.04 | ppl    20.84 | acc     0.65 | train_ae_norm     1.00\n",
      "[69/200][1799/4361] Loss_D: 0.00346733 (Loss_D_real: 0.00030687 Loss_D_fake: 0.00316046) Loss_G: 0.36558244 Loss_Enh_Dec: -1.73949301\n",
      "| epoch  69 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  3.02 | ppl    20.51 | acc     0.68 | train_ae_norm     1.00\n",
      "[69/200][1899/4361] Loss_D: 0.01062718 (Loss_D_real: 0.00304302 Loss_D_fake: 0.00758416) Loss_G: 0.43751618 Loss_Enh_Dec: -1.44239199\n",
      "| epoch  69 |  1900/ 4361 batches | lr 0.000000 | ms/batch 402.04 | loss  3.09 | ppl    21.96 | acc     0.68 | train_ae_norm     1.00\n",
      "[69/200][1999/4361] Loss_D: 0.00370681 (Loss_D_real: 0.00054826 Loss_D_fake: 0.00315855) Loss_G: 0.35759476 Loss_Enh_Dec: -1.41537917\n",
      "| epoch  69 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  3.01 | ppl    20.37 | acc     0.68 | train_ae_norm     1.00\n",
      "[69/200][2099/4361] Loss_D: 0.00292076 (Loss_D_real: 0.00153198 Loss_D_fake: 0.00138878) Loss_G: 0.42421427 Loss_Enh_Dec: -0.96722603\n",
      "| epoch  69 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  3.05 | ppl    21.04 | acc     0.68 | train_ae_norm     1.00\n",
      "[69/200][2199/4361] Loss_D: 0.00161874 (Loss_D_real: 0.00091304 Loss_D_fake: 0.00070570) Loss_G: 0.43473849 Loss_Enh_Dec: -1.23662150\n",
      "| epoch  69 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  3.02 | ppl    20.48 | acc     0.67 | train_ae_norm     1.00\n",
      "[69/200][2299/4361] Loss_D: 0.00474495 (Loss_D_real: 0.00153805 Loss_D_fake: 0.00320690) Loss_G: 0.42619205 Loss_Enh_Dec: -1.31040907\n",
      "| epoch  69 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  3.00 | ppl    20.12 | acc     0.69 | train_ae_norm     1.00\n",
      "[69/200][2399/4361] Loss_D: 0.00220278 (Loss_D_real: 0.00033041 Loss_D_fake: 0.00187236) Loss_G: 0.39755124 Loss_Enh_Dec: -1.09204257\n",
      "| epoch  69 |  2400/ 4361 batches | lr 0.000000 | ms/batch 402.07 | loss  3.00 | ppl    20.11 | acc     0.61 | train_ae_norm     1.00\n",
      "[69/200][2499/4361] Loss_D: 0.00088440 (Loss_D_real: 0.00049696 Loss_D_fake: 0.00038745) Loss_G: 0.46885321 Loss_Enh_Dec: -1.00809765\n",
      "| epoch  69 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  3.04 | ppl    20.89 | acc     0.66 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69/200][2599/4361] Loss_D: 0.00400827 (Loss_D_real: 0.00207679 Loss_D_fake: 0.00193148) Loss_G: 0.46092430 Loss_Enh_Dec: -0.72687000\n",
      "| epoch  69 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  3.03 | ppl    20.64 | acc     0.63 | train_ae_norm     1.00\n",
      "[69/200][2699/4361] Loss_D: 0.00371645 (Loss_D_real: 0.00199772 Loss_D_fake: 0.00171873) Loss_G: 0.40244213 Loss_Enh_Dec: -1.41057658\n",
      "| epoch  69 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  3.02 | ppl    20.39 | acc     0.65 | train_ae_norm     1.00\n",
      "[69/200][2799/4361] Loss_D: 0.00737751 (Loss_D_real: 0.00435354 Loss_D_fake: 0.00302397) Loss_G: 0.44778839 Loss_Enh_Dec: -1.69946599\n",
      "| epoch  69 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.91 | loss  2.97 | ppl    19.49 | acc     0.64 | train_ae_norm     1.00\n",
      "[69/200][2899/4361] Loss_D: 0.00118132 (Loss_D_real: 0.00025257 Loss_D_fake: 0.00092875) Loss_G: 0.46875104 Loss_Enh_Dec: -1.45107484\n",
      "| epoch  69 |  2900/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  3.00 | ppl    19.99 | acc     0.67 | train_ae_norm     1.00\n",
      "[69/200][2999/4361] Loss_D: 0.00663697 (Loss_D_real: 0.00345564 Loss_D_fake: 0.00318133) Loss_G: 0.82963240 Loss_Enh_Dec: -1.63513780\n",
      "| epoch  69 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.94 | loss  2.99 | ppl    19.98 | acc     0.67 | train_ae_norm     1.00\n",
      "[69/200][3099/4361] Loss_D: 0.00304371 (Loss_D_real: 0.00077277 Loss_D_fake: 0.00227094) Loss_G: 0.46915036 Loss_Enh_Dec: -1.64813292\n",
      "| epoch  69 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  3.01 | ppl    20.38 | acc     0.65 | train_ae_norm     1.00\n",
      "[69/200][3199/4361] Loss_D: 0.00251534 (Loss_D_real: 0.00037005 Loss_D_fake: 0.00214528) Loss_G: 0.40756464 Loss_Enh_Dec: -1.41068864\n",
      "| epoch  69 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  3.04 | ppl    20.93 | acc     0.66 | train_ae_norm     1.00\n",
      "[69/200][3299/4361] Loss_D: 0.00306314 (Loss_D_real: 0.00094737 Loss_D_fake: 0.00211577) Loss_G: 0.37265378 Loss_Enh_Dec: -1.89298785\n",
      "| epoch  69 |  3300/ 4361 batches | lr 0.000000 | ms/batch 402.13 | loss  3.05 | ppl    21.21 | acc     0.66 | train_ae_norm     1.00\n",
      "[69/200][3399/4361] Loss_D: 0.00440775 (Loss_D_real: 0.00056428 Loss_D_fake: 0.00384348) Loss_G: 0.41318974 Loss_Enh_Dec: -1.51553833\n",
      "| epoch  69 |  3400/ 4361 batches | lr 0.000000 | ms/batch 402.17 | loss  3.04 | ppl    20.82 | acc     0.64 | train_ae_norm     1.00\n",
      "[69/200][3499/4361] Loss_D: 0.01063188 (Loss_D_real: 0.00486816 Loss_D_fake: 0.00576372) Loss_G: 0.38986671 Loss_Enh_Dec: -1.70925796\n",
      "| epoch  69 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  2.97 | ppl    19.56 | acc     0.66 | train_ae_norm     1.00\n",
      "[69/200][3599/4361] Loss_D: 0.01288253 (Loss_D_real: 0.00587044 Loss_D_fake: 0.00701209) Loss_G: 0.40518305 Loss_Enh_Dec: -1.09701240\n",
      "| epoch  69 |  3600/ 4361 batches | lr 0.000000 | ms/batch 402.32 | loss  2.99 | ppl    19.92 | acc     0.68 | train_ae_norm     1.00\n",
      "[69/200][3699/4361] Loss_D: 0.00569433 (Loss_D_real: 0.00044981 Loss_D_fake: 0.00524453) Loss_G: 0.38680926 Loss_Enh_Dec: -1.39479434\n",
      "| epoch  69 |  3700/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  3.01 | ppl    20.31 | acc     0.62 | train_ae_norm     1.00\n",
      "[69/200][3799/4361] Loss_D: 0.00360786 (Loss_D_real: 0.00247378 Loss_D_fake: 0.00113407) Loss_G: 0.39041016 Loss_Enh_Dec: -1.62203979\n",
      "| epoch  69 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  3.03 | ppl    20.71 | acc     0.68 | train_ae_norm     1.00\n",
      "[69/200][3899/4361] Loss_D: 0.00283199 (Loss_D_real: 0.00011228 Loss_D_fake: 0.00271971) Loss_G: 0.39106828 Loss_Enh_Dec: -1.44540024\n",
      "| epoch  69 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  3.03 | ppl    20.73 | acc     0.63 | train_ae_norm     1.00\n",
      "[69/200][3999/4361] Loss_D: 0.01431599 (Loss_D_real: 0.00195306 Loss_D_fake: 0.01236294) Loss_G: 0.37949914 Loss_Enh_Dec: -1.11266744\n",
      "| epoch  69 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  3.05 | ppl    21.10 | acc     0.68 | train_ae_norm     1.00\n",
      "[69/200][4099/4361] Loss_D: 0.00400819 (Loss_D_real: 0.00220402 Loss_D_fake: 0.00180417) Loss_G: 0.45857963 Loss_Enh_Dec: -1.14628637\n",
      "| epoch  69 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  3.03 | ppl    20.64 | acc     0.65 | train_ae_norm     1.00\n",
      "[69/200][4199/4361] Loss_D: 0.00799281 (Loss_D_real: 0.00111704 Loss_D_fake: 0.00687577) Loss_G: 0.38309392 Loss_Enh_Dec: -0.86642677\n",
      "| epoch  69 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  3.04 | ppl    20.97 | acc     0.69 | train_ae_norm     1.00\n",
      "[69/200][4299/4361] Loss_D: 0.00212061 (Loss_D_real: 0.00046901 Loss_D_fake: 0.00165160) Loss_G: 0.36014611 Loss_Enh_Dec: -0.69634312\n",
      "| epoch  69 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  3.01 | ppl    20.21 | acc     0.68 | train_ae_norm     1.00\n",
      "| end of epoch  69 | time: 1854.62s | test loss  2.98 | test ppl 19.61 | acc 0.693\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 70 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:57.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.704\n",
      "  Training epcoh took: 0:01:27\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.492\n",
      "  Test Loss: 4.154\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  70 |     0/ 4361 batches | lr 0.000000 | ms/batch 868.72 | loss  0.03 | ppl     1.03 | acc     0.69 | train_ae_norm     1.00\n",
      "[70/200][99/4361] Loss_D: 0.00325164 (Loss_D_real: 0.00200103 Loss_D_fake: 0.00125061) Loss_G: 0.42033759 Loss_Enh_Dec: -0.60204637\n",
      "| epoch  70 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  3.03 | ppl    20.68 | acc     0.60 | train_ae_norm     1.00\n",
      "[70/200][199/4361] Loss_D: 0.00203334 (Loss_D_real: 0.00092927 Loss_D_fake: 0.00110408) Loss_G: 0.43115112 Loss_Enh_Dec: -0.83062232\n",
      "| epoch  70 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  3.06 | ppl    21.38 | acc     0.63 | train_ae_norm     1.00\n",
      "[70/200][299/4361] Loss_D: 0.01259466 (Loss_D_real: 0.00981652 Loss_D_fake: 0.00277814) Loss_G: 0.38683531 Loss_Enh_Dec: -0.65013772\n",
      "| epoch  70 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  3.05 | ppl    21.13 | acc     0.63 | train_ae_norm     1.00\n",
      "[70/200][399/4361] Loss_D: 0.00121514 (Loss_D_real: 0.00030565 Loss_D_fake: 0.00090949) Loss_G: 0.35763779 Loss_Enh_Dec: -0.54791832\n",
      "| epoch  70 |   400/ 4361 batches | lr 0.000000 | ms/batch 402.26 | loss  2.98 | ppl    19.61 | acc     0.66 | train_ae_norm     1.00\n",
      "[70/200][499/4361] Loss_D: 0.03288726 (Loss_D_real: 0.03019957 Loss_D_fake: 0.00268769) Loss_G: 0.40255004 Loss_Enh_Dec: -0.84149802\n",
      "| epoch  70 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  3.03 | ppl    20.78 | acc     0.65 | train_ae_norm     1.00\n",
      "[70/200][599/4361] Loss_D: 0.00604553 (Loss_D_real: 0.00351117 Loss_D_fake: 0.00253436) Loss_G: 0.36580580 Loss_Enh_Dec: -1.17647099\n",
      "| epoch  70 |   600/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  2.98 | ppl    19.77 | acc     0.63 | train_ae_norm     1.00\n",
      "[70/200][699/4361] Loss_D: 0.10960174 (Loss_D_real: 0.10647612 Loss_D_fake: 0.00312562) Loss_G: 0.35707846 Loss_Enh_Dec: -1.98396671\n",
      "| epoch  70 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  3.03 | ppl    20.70 | acc     0.65 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70/200][799/4361] Loss_D: 0.00605763 (Loss_D_real: 0.00192653 Loss_D_fake: 0.00413110) Loss_G: 0.44769469 Loss_Enh_Dec: -1.92228210\n",
      "| epoch  70 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  3.03 | ppl    20.75 | acc     0.66 | train_ae_norm     1.00\n",
      "[70/200][899/4361] Loss_D: 0.03891055 (Loss_D_real: 0.03730682 Loss_D_fake: 0.00160373) Loss_G: 0.79217368 Loss_Enh_Dec: -1.53396440\n",
      "| epoch  70 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  3.03 | ppl    20.68 | acc     0.68 | train_ae_norm     1.00\n",
      "[70/200][1199/4361] Loss_D: 0.01059160 (Loss_D_real: 0.00937404 Loss_D_fake: 0.00121756) Loss_G: 0.44760299 Loss_Enh_Dec: -1.61026502\n",
      "| epoch  70 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  3.00 | ppl    20.00 | acc     0.68 | train_ae_norm     1.00\n",
      "[70/200][1299/4361] Loss_D: 0.01373950 (Loss_D_real: 0.00873803 Loss_D_fake: 0.00500148) Loss_G: 0.43894741 Loss_Enh_Dec: -1.28898084\n",
      "| epoch  70 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  3.01 | ppl    20.22 | acc     0.68 | train_ae_norm     1.00\n",
      "[70/200][1399/4361] Loss_D: 0.01197038 (Loss_D_real: 0.00769661 Loss_D_fake: 0.00427377) Loss_G: 0.41588759 Loss_Enh_Dec: -1.52460206\n",
      "| epoch  70 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  3.01 | ppl    20.34 | acc     0.62 | train_ae_norm     1.00\n",
      "[70/200][1499/4361] Loss_D: 0.00320731 (Loss_D_real: 0.00038027 Loss_D_fake: 0.00282704) Loss_G: 0.39748037 Loss_Enh_Dec: -1.32499313\n",
      "| epoch  70 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  3.06 | ppl    21.30 | acc     0.63 | train_ae_norm     1.00\n",
      "[70/200][1599/4361] Loss_D: 0.00435599 (Loss_D_real: 0.00066877 Loss_D_fake: 0.00368723) Loss_G: 0.39315853 Loss_Enh_Dec: -1.77075851\n",
      "| epoch  70 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  3.01 | ppl    20.34 | acc     0.66 | train_ae_norm     1.00\n",
      "[70/200][1699/4361] Loss_D: 0.00369599 (Loss_D_real: 0.00090458 Loss_D_fake: 0.00279141) Loss_G: 0.50375116 Loss_Enh_Dec: -1.48919713\n",
      "| epoch  70 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  3.00 | ppl    20.14 | acc     0.65 | train_ae_norm     1.00\n",
      "[70/200][1799/4361] Loss_D: 0.02233324 (Loss_D_real: 0.00670880 Loss_D_fake: 0.01562444) Loss_G: 0.43881971 Loss_Enh_Dec: -1.39788139\n",
      "| epoch  70 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  2.96 | ppl    19.27 | acc     0.69 | train_ae_norm     1.00\n",
      "[70/200][1899/4361] Loss_D: 0.00253405 (Loss_D_real: 0.00016629 Loss_D_fake: 0.00236776) Loss_G: 0.35603178 Loss_Enh_Dec: -1.79737782\n",
      "| epoch  70 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  3.03 | ppl    20.70 | acc     0.67 | train_ae_norm     1.00\n",
      "[70/200][1999/4361] Loss_D: 0.00586872 (Loss_D_real: 0.00329770 Loss_D_fake: 0.00257102) Loss_G: 0.41219088 Loss_Enh_Dec: -1.72148037\n",
      "| epoch  70 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.96 | ppl    19.21 | acc     0.68 | train_ae_norm     1.00\n",
      "[70/200][2099/4361] Loss_D: 0.00261083 (Loss_D_real: 0.00097332 Loss_D_fake: 0.00163751) Loss_G: 0.44939050 Loss_Enh_Dec: -1.90537727\n",
      "| epoch  70 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  2.99 | ppl    19.92 | acc     0.68 | train_ae_norm     1.00\n",
      "[70/200][2199/4361] Loss_D: 0.01496236 (Loss_D_real: 0.00114903 Loss_D_fake: 0.01381333) Loss_G: 0.38751906 Loss_Enh_Dec: -1.65546644\n",
      "| epoch  70 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  2.99 | ppl    19.82 | acc     0.68 | train_ae_norm     1.00\n",
      "[70/200][2299/4361] Loss_D: 0.00247945 (Loss_D_real: 0.00101236 Loss_D_fake: 0.00146710) Loss_G: 0.49301511 Loss_Enh_Dec: -1.65688074\n",
      "| epoch  70 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  2.95 | ppl    19.10 | acc     0.68 | train_ae_norm     1.00\n",
      "[70/200][2399/4361] Loss_D: 0.00497472 (Loss_D_real: 0.00102919 Loss_D_fake: 0.00394553) Loss_G: 0.38286167 Loss_Enh_Dec: -2.13350654\n",
      "| epoch  70 |  2400/ 4361 batches | lr 0.000000 | ms/batch 402.68 | loss  2.98 | ppl    19.65 | acc     0.63 | train_ae_norm     1.00\n",
      "[70/200][2499/4361] Loss_D: 0.01274338 (Loss_D_real: 0.01138304 Loss_D_fake: 0.00136033) Loss_G: 0.37022942 Loss_Enh_Dec: -1.92414844\n",
      "| epoch  70 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.91 | loss  3.03 | ppl    20.64 | acc     0.67 | train_ae_norm     1.00\n",
      "[70/200][2599/4361] Loss_D: 0.00369900 (Loss_D_real: 0.00032966 Loss_D_fake: 0.00336935) Loss_G: 0.35019091 Loss_Enh_Dec: -1.96102750\n",
      "| epoch  70 |  2600/ 4361 batches | lr 0.000000 | ms/batch 402.00 | loss  3.06 | ppl    21.29 | acc     0.64 | train_ae_norm     1.00\n",
      "[70/200][2699/4361] Loss_D: 0.00410636 (Loss_D_real: 0.00117166 Loss_D_fake: 0.00293471) Loss_G: 0.40577456 Loss_Enh_Dec: -1.85933876\n",
      "| epoch  70 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  3.02 | ppl    20.48 | acc     0.66 | train_ae_norm     1.00\n",
      "[70/200][2799/4361] Loss_D: 0.00654993 (Loss_D_real: 0.00528790 Loss_D_fake: 0.00126204) Loss_G: 0.39556536 Loss_Enh_Dec: -1.78713453\n",
      "| epoch  70 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  2.95 | ppl    19.06 | acc     0.66 | train_ae_norm     1.00\n",
      "[70/200][2899/4361] Loss_D: 0.00259338 (Loss_D_real: 0.00037267 Loss_D_fake: 0.00222071) Loss_G: 0.34239271 Loss_Enh_Dec: -1.92772901\n",
      "| epoch  70 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  2.97 | ppl    19.55 | acc     0.67 | train_ae_norm     1.00\n",
      "[70/200][2999/4361] Loss_D: 0.03046285 (Loss_D_real: 0.02355655 Loss_D_fake: 0.00690630) Loss_G: 0.39473519 Loss_Enh_Dec: -2.06169105\n",
      "| epoch  70 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  3.00 | ppl    20.12 | acc     0.68 | train_ae_norm     1.00\n",
      "[70/200][3099/4361] Loss_D: 0.00592730 (Loss_D_real: 0.00335222 Loss_D_fake: 0.00257508) Loss_G: 0.35025537 Loss_Enh_Dec: -1.89447594\n",
      "| epoch  70 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  2.99 | ppl    19.85 | acc     0.65 | train_ae_norm     1.00\n",
      "[70/200][3199/4361] Loss_D: 0.01262118 (Loss_D_real: 0.00963706 Loss_D_fake: 0.00298412) Loss_G: 0.39772996 Loss_Enh_Dec: -2.18243527\n",
      "| epoch  70 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  3.03 | ppl    20.67 | acc     0.66 | train_ae_norm     1.00\n",
      "[70/200][3299/4361] Loss_D: 0.02464191 (Loss_D_real: 0.01808018 Loss_D_fake: 0.00656173) Loss_G: 0.44455501 Loss_Enh_Dec: -1.36040425\n",
      "| epoch  70 |  3300/ 4361 batches | lr 0.000000 | ms/batch 402.00 | loss  3.01 | ppl    20.33 | acc     0.67 | train_ae_norm     1.00\n",
      "[70/200][3399/4361] Loss_D: 0.00471023 (Loss_D_real: 0.00012100 Loss_D_fake: 0.00458923) Loss_G: 0.35313243 Loss_Enh_Dec: -1.53388345\n",
      "| epoch  70 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.99 | ppl    19.90 | acc     0.66 | train_ae_norm     1.00\n",
      "[70/200][3499/4361] Loss_D: 0.02806395 (Loss_D_real: 0.02683022 Loss_D_fake: 0.00123374) Loss_G: 0.46564779 Loss_Enh_Dec: -1.48542249\n",
      "| epoch  70 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  2.94 | ppl    18.99 | acc     0.67 | train_ae_norm     1.00\n",
      "[70/200][3599/4361] Loss_D: 0.00135200 (Loss_D_real: 0.00044221 Loss_D_fake: 0.00090978) Loss_G: 0.49334392 Loss_Enh_Dec: -1.79775894\n",
      "| epoch  70 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.97 | ppl    19.54 | acc     0.65 | train_ae_norm     1.00\n",
      "[70/200][3699/4361] Loss_D: 0.01282822 (Loss_D_real: 0.01190170 Loss_D_fake: 0.00092651) Loss_G: 0.42731592 Loss_Enh_Dec: -1.60061991\n",
      "| epoch  70 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  2.98 | ppl    19.59 | acc     0.63 | train_ae_norm     1.00\n",
      "[70/200][3799/4361] Loss_D: 0.01227603 (Loss_D_real: 0.00880357 Loss_D_fake: 0.00347247) Loss_G: 0.38037464 Loss_Enh_Dec: -1.73443413\n",
      "| epoch  70 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  3.00 | ppl    20.10 | acc     0.69 | train_ae_norm     1.00\n",
      "[70/200][3899/4361] Loss_D: 0.00372662 (Loss_D_real: 0.00030204 Loss_D_fake: 0.00342458) Loss_G: 0.35889021 Loss_Enh_Dec: -1.45423007\n",
      "| epoch  70 |  3900/ 4361 batches | lr 0.000000 | ms/batch 402.45 | loss  2.99 | ppl    19.93 | acc     0.64 | train_ae_norm     1.00\n",
      "[70/200][3999/4361] Loss_D: 0.00650760 (Loss_D_real: 0.00365301 Loss_D_fake: 0.00285459) Loss_G: 0.39367375 Loss_Enh_Dec: -1.52012205\n",
      "| epoch  70 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.99 | ppl    19.87 | acc     0.68 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70/200][4099/4361] Loss_D: 0.05997207 (Loss_D_real: 0.05698431 Loss_D_fake: 0.00298776) Loss_G: 0.36554739 Loss_Enh_Dec: -1.22628665\n",
      "| epoch  70 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.95 | ppl    19.05 | acc     0.68 | train_ae_norm     1.00\n",
      "[70/200][4199/4361] Loss_D: 0.00742769 (Loss_D_real: 0.00270155 Loss_D_fake: 0.00472614) Loss_G: 0.36275330 Loss_Enh_Dec: -1.54879630\n",
      "| epoch  70 |  4200/ 4361 batches | lr 0.000000 | ms/batch 402.21 | loss  2.97 | ppl    19.59 | acc     0.70 | train_ae_norm     1.00\n",
      "[70/200][4299/4361] Loss_D: 0.01462752 (Loss_D_real: 0.00301025 Loss_D_fake: 0.01161727) Loss_G: 0.62353057 Loss_Enh_Dec: -1.30982745\n",
      "| epoch  70 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  2.94 | ppl    18.96 | acc     0.69 | train_ae_norm     1.00\n",
      "| end of epoch  70 | time: 1854.21s | test loss  2.94 | test ppl 18.93 | acc 0.698\n",
      "bleu_self:  [3.12500000e-02 1.05644315e-09 3.79468392e-12 6.89594141e-12\n",
      " 5.54107232e-11]\n",
      "bleu_test:  [5.62500000e-01 7.65465678e-02 5.15932276e-07 3.88809781e-08\n",
      " 5.54596261e-08]\n",
      "bleu_self: [0.03125000,0.00000000,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.56250000,0.07654657,0.00000052,0.00000004,0.00000006]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 71 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.703\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.492\n",
      "  Test Loss: 4.123\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  71 |     0/ 4361 batches | lr 0.000000 | ms/batch 866.73 | loss  0.03 | ppl     1.03 | acc     0.70 | train_ae_norm     1.00\n",
      "[71/200][99/4361] Loss_D: 0.00633457 (Loss_D_real: 0.00086438 Loss_D_fake: 0.00547020) Loss_G: 0.40124986 Loss_Enh_Dec: -0.82958525\n",
      "| epoch  71 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  2.97 | ppl    19.52 | acc     0.64 | train_ae_norm     1.00\n",
      "[71/200][199/4361] Loss_D: 0.00882205 (Loss_D_real: 0.00564557 Loss_D_fake: 0.00317649) Loss_G: 0.36064535 Loss_Enh_Dec: -1.75924838\n",
      "| epoch  71 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  3.01 | ppl    20.20 | acc     0.66 | train_ae_norm     1.00\n",
      "[71/200][299/4361] Loss_D: 0.00419741 (Loss_D_real: 0.00027012 Loss_D_fake: 0.00392728) Loss_G: 0.38586834 Loss_Enh_Dec: -1.60686839\n",
      "| epoch  71 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  3.01 | ppl    20.33 | acc     0.66 | train_ae_norm     1.00\n",
      "[71/200][399/4361] Loss_D: 0.00272640 (Loss_D_real: 0.00093444 Loss_D_fake: 0.00179196) Loss_G: 0.37347290 Loss_Enh_Dec: -1.72699153\n",
      "| epoch  71 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.91 | ppl    18.31 | acc     0.67 | train_ae_norm     1.00\n",
      "[71/200][499/4361] Loss_D: 0.00281886 (Loss_D_real: 0.00125023 Loss_D_fake: 0.00156863) Loss_G: 0.39812604 Loss_Enh_Dec: -1.96990895\n",
      "| epoch  71 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  2.99 | ppl    19.95 | acc     0.68 | train_ae_norm     1.00\n",
      "[71/200][599/4361] Loss_D: 0.00897969 (Loss_D_real: 0.00764254 Loss_D_fake: 0.00133714) Loss_G: 0.40562722 Loss_Enh_Dec: -1.57579410\n",
      "| epoch  71 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.99 | loss  2.94 | ppl    19.00 | acc     0.63 | train_ae_norm     1.00\n",
      "[71/200][699/4361] Loss_D: 0.00353052 (Loss_D_real: 0.00104733 Loss_D_fake: 0.00248319) Loss_G: 0.37351605 Loss_Enh_Dec: -2.07447886\n",
      "| epoch  71 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.98 | ppl    19.67 | acc     0.68 | train_ae_norm     1.00\n",
      "[71/200][799/4361] Loss_D: 0.07922608 (Loss_D_real: 0.03681829 Loss_D_fake: 0.04240780) Loss_G: 0.62323093 Loss_Enh_Dec: -1.59987092\n",
      "| epoch  71 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  2.95 | ppl    19.14 | acc     0.67 | train_ae_norm     1.00\n",
      "[71/200][899/4361] Loss_D: 0.00488066 (Loss_D_real: 0.00339721 Loss_D_fake: 0.00148346) Loss_G: 0.42930946 Loss_Enh_Dec: -1.29311478\n",
      "| epoch  71 |   900/ 4361 batches | lr 0.000000 | ms/batch 402.13 | loss  2.98 | ppl    19.62 | acc     0.69 | train_ae_norm     1.00\n",
      "[71/200][999/4361] Loss_D: 0.00594231 (Loss_D_real: 0.00136703 Loss_D_fake: 0.00457529) Loss_G: 0.49896207 Loss_Enh_Dec: -1.64661503\n",
      "| epoch  71 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  2.97 | ppl    19.50 | acc     0.67 | train_ae_norm     1.00\n",
      "[71/200][1099/4361] Loss_D: 0.00624797 (Loss_D_real: 0.00344320 Loss_D_fake: 0.00280477) Loss_G: 0.48204300 Loss_Enh_Dec: -1.48253977\n",
      "| epoch  71 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  2.95 | ppl    19.18 | acc     0.64 | train_ae_norm     1.00\n",
      "[71/200][1199/4361] Loss_D: 0.01958681 (Loss_D_real: 0.01678735 Loss_D_fake: 0.00279946) Loss_G: 0.38629299 Loss_Enh_Dec: -1.19204450\n",
      "| epoch  71 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  2.96 | ppl    19.26 | acc     0.68 | train_ae_norm     1.00\n",
      "[71/200][1299/4361] Loss_D: 0.00603363 (Loss_D_real: 0.00367712 Loss_D_fake: 0.00235651) Loss_G: 0.40888453 Loss_Enh_Dec: -1.96979272\n",
      "| epoch  71 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  2.96 | ppl    19.32 | acc     0.66 | train_ae_norm     1.00\n",
      "[71/200][1399/4361] Loss_D: 0.00719511 (Loss_D_real: 0.00114464 Loss_D_fake: 0.00605048) Loss_G: 0.36975631 Loss_Enh_Dec: -1.73913693\n",
      "| epoch  71 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  2.96 | ppl    19.27 | acc     0.64 | train_ae_norm     1.00\n",
      "[71/200][1499/4361] Loss_D: 0.00438228 (Loss_D_real: 0.00170933 Loss_D_fake: 0.00267294) Loss_G: 0.38101706 Loss_Enh_Dec: -1.40840948\n",
      "| epoch  71 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  3.02 | ppl    20.51 | acc     0.63 | train_ae_norm     1.00\n",
      "[71/200][1599/4361] Loss_D: 0.01224746 (Loss_D_real: 0.01020106 Loss_D_fake: 0.00204639) Loss_G: 0.40083215 Loss_Enh_Dec: -1.45127904\n",
      "| epoch  71 |  1600/ 4361 batches | lr 0.000000 | ms/batch 402.06 | loss  2.99 | ppl    19.98 | acc     0.66 | train_ae_norm     1.00\n",
      "[71/200][1699/4361] Loss_D: 0.02259164 (Loss_D_real: 0.01694780 Loss_D_fake: 0.00564384) Loss_G: 0.36986580 Loss_Enh_Dec: -2.00593996\n",
      "| epoch  71 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  2.96 | ppl    19.35 | acc     0.66 | train_ae_norm     1.00\n",
      "[71/200][1799/4361] Loss_D: 0.00334210 (Loss_D_real: 0.00122270 Loss_D_fake: 0.00211940) Loss_G: 0.41504732 Loss_Enh_Dec: -1.97644794\n",
      "| epoch  71 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  2.95 | ppl    19.11 | acc     0.68 | train_ae_norm     1.00\n",
      "[71/200][1899/4361] Loss_D: 0.00347206 (Loss_D_real: 0.00127260 Loss_D_fake: 0.00219946) Loss_G: 0.40097865 Loss_Enh_Dec: -2.01296878\n",
      "| epoch  71 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  3.00 | ppl    20.02 | acc     0.70 | train_ae_norm     1.00\n",
      "[71/200][1999/4361] Loss_D: 0.00468433 (Loss_D_real: 0.00043231 Loss_D_fake: 0.00425203) Loss_G: 0.40796682 Loss_Enh_Dec: -1.96298718\n",
      "| epoch  71 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  2.94 | ppl    18.90 | acc     0.67 | train_ae_norm     1.00\n",
      "[71/200][2099/4361] Loss_D: 0.00269597 (Loss_D_real: 0.00051319 Loss_D_fake: 0.00218278) Loss_G: 0.41345730 Loss_Enh_Dec: -1.82876360\n",
      "| epoch  71 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  2.98 | ppl    19.70 | acc     0.68 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71/200][2199/4361] Loss_D: 0.00306458 (Loss_D_real: 0.00092140 Loss_D_fake: 0.00214317) Loss_G: 0.37225652 Loss_Enh_Dec: -2.09676147\n",
      "| epoch  71 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  2.96 | ppl    19.21 | acc     0.68 | train_ae_norm     1.00\n",
      "[71/200][2299/4361] Loss_D: 0.00585611 (Loss_D_real: 0.00182009 Loss_D_fake: 0.00403601) Loss_G: 0.41841349 Loss_Enh_Dec: -2.01599860\n",
      "| epoch  71 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  2.95 | ppl    19.15 | acc     0.69 | train_ae_norm     1.00\n",
      "[71/200][2399/4361] Loss_D: 0.00319700 (Loss_D_real: 0.00071657 Loss_D_fake: 0.00248043) Loss_G: 0.40999356 Loss_Enh_Dec: -1.79946768\n",
      "| epoch  71 |  2400/ 4361 batches | lr 0.000000 | ms/batch 402.26 | loss  2.94 | ppl    18.99 | acc     0.66 | train_ae_norm     1.00\n",
      "[71/200][2499/4361] Loss_D: 0.00645628 (Loss_D_real: 0.00222028 Loss_D_fake: 0.00423600) Loss_G: 0.55121750 Loss_Enh_Dec: -1.77900970\n",
      "| epoch  71 |  2500/ 4361 batches | lr 0.000000 | ms/batch 402.13 | loss  3.01 | ppl    20.20 | acc     0.67 | train_ae_norm     1.00\n",
      "[71/200][2599/4361] Loss_D: 0.00340961 (Loss_D_real: 0.00218748 Loss_D_fake: 0.00122213) Loss_G: 0.51928580 Loss_Enh_Dec: -1.90069640\n",
      "| epoch  71 |  2600/ 4361 batches | lr 0.000000 | ms/batch 402.06 | loss  2.96 | ppl    19.38 | acc     0.66 | train_ae_norm     1.00\n",
      "[71/200][2699/4361] Loss_D: 0.00167554 (Loss_D_real: 0.00035091 Loss_D_fake: 0.00132462) Loss_G: 0.39807948 Loss_Enh_Dec: -1.85849380\n",
      "| epoch  71 |  2700/ 4361 batches | lr 0.000000 | ms/batch 402.40 | loss  2.97 | ppl    19.59 | acc     0.66 | train_ae_norm     1.00\n",
      "[71/200][2799/4361] Loss_D: 0.00192943 (Loss_D_real: 0.00050841 Loss_D_fake: 0.00142102) Loss_G: 0.41013160 Loss_Enh_Dec: -2.06504202\n",
      "| epoch  71 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  2.92 | ppl    18.55 | acc     0.65 | train_ae_norm     1.00\n",
      "[71/200][2899/4361] Loss_D: 0.00623291 (Loss_D_real: 0.00172756 Loss_D_fake: 0.00450535) Loss_G: 0.41719618 Loss_Enh_Dec: -2.17716670\n",
      "| epoch  71 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  2.95 | ppl    19.03 | acc     0.66 | train_ae_norm     1.00\n",
      "[71/200][2999/4361] Loss_D: 0.00491298 (Loss_D_real: 0.00248067 Loss_D_fake: 0.00243230) Loss_G: 0.43279997 Loss_Enh_Dec: -1.92233086\n",
      "| epoch  71 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  2.97 | ppl    19.43 | acc     0.66 | train_ae_norm     1.00\n",
      "[71/200][3099/4361] Loss_D: 0.00166266 (Loss_D_real: 0.00068449 Loss_D_fake: 0.00097817) Loss_G: 0.39088598 Loss_Enh_Dec: -1.97575498\n",
      "| epoch  71 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  2.97 | ppl    19.50 | acc     0.65 | train_ae_norm     1.00\n",
      "[71/200][3199/4361] Loss_D: 0.00408297 (Loss_D_real: 0.00069412 Loss_D_fake: 0.00338885) Loss_G: 0.40151101 Loss_Enh_Dec: -1.96475494\n",
      "| epoch  71 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  3.03 | ppl    20.75 | acc     0.67 | train_ae_norm     1.00\n",
      "[71/200][3299/4361] Loss_D: 0.00513407 (Loss_D_real: 0.00090246 Loss_D_fake: 0.00423161) Loss_G: 0.40476844 Loss_Enh_Dec: -2.08338451\n",
      "| epoch  71 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  3.01 | ppl    20.28 | acc     0.68 | train_ae_norm     1.00\n",
      "[71/200][3399/4361] Loss_D: 0.00525071 (Loss_D_real: 0.00369480 Loss_D_fake: 0.00155592) Loss_G: 0.46311209 Loss_Enh_Dec: -1.96726823\n",
      "| epoch  71 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.98 | ppl    19.78 | acc     0.65 | train_ae_norm     1.00\n",
      "[71/200][3499/4361] Loss_D: 0.00381480 (Loss_D_real: 0.00130339 Loss_D_fake: 0.00251141) Loss_G: 0.61299461 Loss_Enh_Dec: -1.45906532\n",
      "| epoch  71 |  3500/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  2.93 | ppl    18.79 | acc     0.67 | train_ae_norm     1.00\n",
      "[71/200][3599/4361] Loss_D: 0.00347434 (Loss_D_real: 0.00275303 Loss_D_fake: 0.00072131) Loss_G: 0.44505763 Loss_Enh_Dec: -1.48320699\n",
      "| epoch  71 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.97 | ppl    19.41 | acc     0.66 | train_ae_norm     1.00\n",
      "[71/200][3699/4361] Loss_D: 0.00553651 (Loss_D_real: 0.00143836 Loss_D_fake: 0.00409815) Loss_G: 0.40698701 Loss_Enh_Dec: -1.46427917\n",
      "| epoch  71 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  3.00 | ppl    20.01 | acc     0.64 | train_ae_norm     1.00\n",
      "[71/200][3799/4361] Loss_D: 0.00432528 (Loss_D_real: 0.00271755 Loss_D_fake: 0.00160774) Loss_G: 0.42479941 Loss_Enh_Dec: -1.48177445\n",
      "| epoch  71 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  2.97 | ppl    19.48 | acc     0.69 | train_ae_norm     1.00\n",
      "[71/200][3899/4361] Loss_D: 0.00589299 (Loss_D_real: 0.00283799 Loss_D_fake: 0.00305500) Loss_G: 0.49417162 Loss_Enh_Dec: -1.25676942\n",
      "| epoch  71 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.94 | loss  2.96 | ppl    19.36 | acc     0.64 | train_ae_norm     1.00\n",
      "[71/200][3999/4361] Loss_D: 0.00213106 (Loss_D_real: 0.00031306 Loss_D_fake: 0.00181801) Loss_G: 0.50313526 Loss_Enh_Dec: -1.36482525\n",
      "| epoch  71 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  2.97 | ppl    19.54 | acc     0.69 | train_ae_norm     1.00\n",
      "[71/200][4099/4361] Loss_D: 0.03976773 (Loss_D_real: 0.03573280 Loss_D_fake: 0.00403492) Loss_G: 0.41335398 Loss_Enh_Dec: -1.40389431\n",
      "| epoch  71 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.99 | loss  2.93 | ppl    18.68 | acc     0.67 | train_ae_norm     1.00\n",
      "[71/200][4199/4361] Loss_D: 0.00471004 (Loss_D_real: 0.00125337 Loss_D_fake: 0.00345667) Loss_G: 0.50318855 Loss_Enh_Dec: -1.51608622\n",
      "| epoch  71 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.05 | loss  2.96 | ppl    19.23 | acc     0.70 | train_ae_norm     1.00\n",
      "[71/200][4299/4361] Loss_D: 0.00264771 (Loss_D_real: 0.00066717 Loss_D_fake: 0.00198054) Loss_G: 0.42427596 Loss_Enh_Dec: -1.77333438\n",
      "| epoch  71 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  2.92 | ppl    18.62 | acc     0.69 | train_ae_norm     1.00\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.704\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.487\n",
      "  Test Loss: 4.178\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  72 |     0/ 4361 batches | lr 0.000000 | ms/batch 866.46 | loss  0.03 | ppl     1.03 | acc     0.71 | train_ae_norm     1.00\n",
      "[72/200][99/4361] Loss_D: 0.00134469 (Loss_D_real: 0.00039057 Loss_D_fake: 0.00095412) Loss_G: 0.43151236 Loss_Enh_Dec: -1.79846632\n",
      "| epoch  72 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  2.94 | ppl    18.99 | acc     0.63 | train_ae_norm     1.00\n",
      "[72/200][199/4361] Loss_D: 0.00925510 (Loss_D_real: 0.00617504 Loss_D_fake: 0.00308006) Loss_G: 0.47589588 Loss_Enh_Dec: -1.72253644\n",
      "| epoch  72 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.97 | ppl    19.58 | acc     0.66 | train_ae_norm     1.00\n",
      "[72/200][299/4361] Loss_D: 0.00568971 (Loss_D_real: 0.00253746 Loss_D_fake: 0.00315225) Loss_G: 0.36849043 Loss_Enh_Dec: -1.52407444\n",
      "| epoch  72 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  2.98 | ppl    19.62 | acc     0.62 | train_ae_norm     1.00\n",
      "[72/200][399/4361] Loss_D: 0.00206701 (Loss_D_real: 0.00120391 Loss_D_fake: 0.00086310) Loss_G: 0.38724753 Loss_Enh_Dec: -1.31328392\n",
      "| epoch  72 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  2.89 | ppl    17.91 | acc     0.68 | train_ae_norm     1.00\n",
      "[72/200][499/4361] Loss_D: 0.00217915 (Loss_D_real: 0.00021510 Loss_D_fake: 0.00196405) Loss_G: 0.38416100 Loss_Enh_Dec: -1.45606613\n",
      "| epoch  72 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  2.96 | ppl    19.22 | acc     0.67 | train_ae_norm     1.00\n",
      "[72/200][599/4361] Loss_D: 0.00233456 (Loss_D_real: 0.00034169 Loss_D_fake: 0.00199287) Loss_G: 0.41107726 Loss_Enh_Dec: -1.48928702\n",
      "| epoch  72 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  2.91 | ppl    18.45 | acc     0.63 | train_ae_norm     1.00\n",
      "[72/200][699/4361] Loss_D: 0.00178097 (Loss_D_real: 0.00061356 Loss_D_fake: 0.00116741) Loss_G: 0.55176574 Loss_Enh_Dec: -1.53749931\n",
      "| epoch  72 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  2.96 | ppl    19.35 | acc     0.67 | train_ae_norm     1.00\n",
      "[72/200][799/4361] Loss_D: 0.00049044 (Loss_D_real: 0.00006646 Loss_D_fake: 0.00042398) Loss_G: 0.45776159 Loss_Enh_Dec: -0.90390253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  72 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  2.95 | ppl    19.07 | acc     0.68 | train_ae_norm     1.00\n",
      "[72/200][899/4361] Loss_D: 0.00586119 (Loss_D_real: 0.00455052 Loss_D_fake: 0.00131067) Loss_G: 0.51478916 Loss_Enh_Dec: -1.50457036\n",
      "| epoch  72 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  2.96 | ppl    19.35 | acc     0.71 | train_ae_norm     1.00\n",
      "[72/200][999/4361] Loss_D: 0.00330692 (Loss_D_real: 0.00218462 Loss_D_fake: 0.00112230) Loss_G: 0.37688860 Loss_Enh_Dec: -1.20780015\n",
      "| epoch  72 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.95 | ppl    19.11 | acc     0.68 | train_ae_norm     1.00\n",
      "[72/200][1099/4361] Loss_D: 0.00573304 (Loss_D_real: 0.00022653 Loss_D_fake: 0.00550652) Loss_G: 0.43891969 Loss_Enh_Dec: -1.43785632\n",
      "| epoch  72 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  2.95 | ppl    19.17 | acc     0.66 | train_ae_norm     1.00\n",
      "[72/200][1199/4361] Loss_D: 0.01223872 (Loss_D_real: 0.00110519 Loss_D_fake: 0.01113353) Loss_G: 0.37156126 Loss_Enh_Dec: -1.65463638\n",
      "| epoch  72 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  2.96 | ppl    19.29 | acc     0.68 | train_ae_norm     1.00\n",
      "[72/200][1299/4361] Loss_D: 0.00158747 (Loss_D_real: 0.00044640 Loss_D_fake: 0.00114108) Loss_G: 0.41574869 Loss_Enh_Dec: -1.56840932\n",
      "| epoch  72 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  2.96 | ppl    19.36 | acc     0.67 | train_ae_norm     1.00\n",
      "[72/200][1399/4361] Loss_D: 0.01593134 (Loss_D_real: 0.00432278 Loss_D_fake: 0.01160856) Loss_G: 0.38099137 Loss_Enh_Dec: -1.69492662\n",
      "| epoch  72 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  2.96 | ppl    19.34 | acc     0.63 | train_ae_norm     1.00\n",
      "[72/200][1499/4361] Loss_D: 0.00352444 (Loss_D_real: 0.00155333 Loss_D_fake: 0.00197111) Loss_G: 0.46908113 Loss_Enh_Dec: -1.49323356\n",
      "| epoch  72 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  3.01 | ppl    20.24 | acc     0.64 | train_ae_norm     1.00\n",
      "[72/200][1599/4361] Loss_D: 0.00373151 (Loss_D_real: 0.00102307 Loss_D_fake: 0.00270844) Loss_G: 0.39645135 Loss_Enh_Dec: -1.58228683\n",
      "| epoch  72 |  1600/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  2.99 | ppl    19.87 | acc     0.66 | train_ae_norm     1.00\n",
      "[72/200][1699/4361] Loss_D: 0.00537165 (Loss_D_real: 0.00087890 Loss_D_fake: 0.00449276) Loss_G: 0.39360398 Loss_Enh_Dec: -1.69423485\n",
      "| epoch  72 |  1700/ 4361 batches | lr 0.000000 | ms/batch 402.06 | loss  2.96 | ppl    19.39 | acc     0.65 | train_ae_norm     1.00\n",
      "[72/200][1799/4361] Loss_D: 0.00172971 (Loss_D_real: 0.00016327 Loss_D_fake: 0.00156645) Loss_G: 0.41522598 Loss_Enh_Dec: -1.54260099\n",
      "| epoch  72 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  2.91 | ppl    18.34 | acc     0.67 | train_ae_norm     1.00\n",
      "[72/200][1899/4361] Loss_D: 0.00253862 (Loss_D_real: 0.00089014 Loss_D_fake: 0.00164848) Loss_G: 0.38969010 Loss_Enh_Dec: -1.56227529\n",
      "| epoch  72 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  2.97 | ppl    19.58 | acc     0.70 | train_ae_norm     1.00\n",
      "[72/200][1999/4361] Loss_D: 0.00322751 (Loss_D_real: 0.00096645 Loss_D_fake: 0.00226106) Loss_G: 0.42440200 Loss_Enh_Dec: -1.59643018\n",
      "| epoch  72 |  2000/ 4361 batches | lr 0.000000 | ms/batch 402.41 | loss  2.93 | ppl    18.65 | acc     0.68 | train_ae_norm     1.00\n",
      "[72/200][2099/4361] Loss_D: 0.00229968 (Loss_D_real: 0.00012782 Loss_D_fake: 0.00217185) Loss_G: 0.43905669 Loss_Enh_Dec: -1.41614258\n",
      "| epoch  72 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.96 | ppl    19.35 | acc     0.68 | train_ae_norm     1.00\n",
      "[72/200][2199/4361] Loss_D: 0.00104645 (Loss_D_real: 0.00056561 Loss_D_fake: 0.00048084) Loss_G: 0.40771267 Loss_Enh_Dec: -1.34705269\n",
      "| epoch  72 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  2.93 | ppl    18.73 | acc     0.66 | train_ae_norm     1.00\n",
      "[72/200][2299/4361] Loss_D: 0.00669250 (Loss_D_real: 0.00551546 Loss_D_fake: 0.00117704) Loss_G: 0.40784341 Loss_Enh_Dec: -1.51305866\n",
      "| epoch  72 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.94 | ppl    18.88 | acc     0.69 | train_ae_norm     1.00\n",
      "[72/200][2399/4361] Loss_D: 0.00844297 (Loss_D_real: 0.00610526 Loss_D_fake: 0.00233771) Loss_G: 0.79277128 Loss_Enh_Dec: -1.08924472\n",
      "| epoch  72 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  2.94 | ppl    18.89 | acc     0.63 | train_ae_norm     1.00\n",
      "[72/200][2499/4361] Loss_D: 0.00389502 (Loss_D_real: 0.00225161 Loss_D_fake: 0.00164341) Loss_G: 0.44300231 Loss_Enh_Dec: -1.38993287\n",
      "| epoch  72 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  2.98 | ppl    19.63 | acc     0.68 | train_ae_norm     1.00\n",
      "[72/200][2599/4361] Loss_D: 0.00249633 (Loss_D_real: 0.00087020 Loss_D_fake: 0.00162614) Loss_G: 0.42729029 Loss_Enh_Dec: -0.63439059\n",
      "| epoch  72 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.93 | ppl    18.79 | acc     0.63 | train_ae_norm     1.00\n",
      "[72/200][2699/4361] Loss_D: 0.01341209 (Loss_D_real: 0.01084433 Loss_D_fake: 0.00256775) Loss_G: 0.40270063 Loss_Enh_Dec: -1.44452000\n",
      "| epoch  72 |  2700/ 4361 batches | lr 0.000000 | ms/batch 402.20 | loss  2.95 | ppl    19.10 | acc     0.67 | train_ae_norm     1.00\n",
      "[72/200][2799/4361] Loss_D: 0.00129780 (Loss_D_real: 0.00014796 Loss_D_fake: 0.00114984) Loss_G: 0.42671117 Loss_Enh_Dec: -0.91528362\n",
      "| epoch  72 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  2.90 | ppl    18.17 | acc     0.67 | train_ae_norm     1.00\n",
      "[72/200][2899/4361] Loss_D: 0.02132939 (Loss_D_real: 0.02026503 Loss_D_fake: 0.00106436) Loss_G: 0.42055622 Loss_Enh_Dec: -1.18760586\n",
      "| epoch  72 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.92 | ppl    18.52 | acc     0.68 | train_ae_norm     1.00\n",
      "[72/200][2999/4361] Loss_D: 0.00669637 (Loss_D_real: 0.00053804 Loss_D_fake: 0.00615833) Loss_G: 0.47571087 Loss_Enh_Dec: -1.18060005\n",
      "| epoch  72 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  2.94 | ppl    18.92 | acc     0.67 | train_ae_norm     1.00\n",
      "[72/200][3099/4361] Loss_D: 0.00233745 (Loss_D_real: 0.00084361 Loss_D_fake: 0.00149384) Loss_G: 0.42192698 Loss_Enh_Dec: -1.39263618\n",
      "| epoch  72 |  3100/ 4361 batches | lr 0.000000 | ms/batch 402.29 | loss  2.93 | ppl    18.71 | acc     0.66 | train_ae_norm     1.00\n",
      "[72/200][3199/4361] Loss_D: 0.00251540 (Loss_D_real: 0.00149032 Loss_D_fake: 0.00102509) Loss_G: 0.39778069 Loss_Enh_Dec: -1.77091110\n",
      "| epoch  72 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  2.97 | ppl    19.52 | acc     0.69 | train_ae_norm     1.00\n",
      "[72/200][3299/4361] Loss_D: 0.00192093 (Loss_D_real: 0.00014272 Loss_D_fake: 0.00177821) Loss_G: 0.38735700 Loss_Enh_Dec: -1.67927635\n",
      "| epoch  72 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  2.96 | ppl    19.29 | acc     0.69 | train_ae_norm     1.00\n",
      "[72/200][3399/4361] Loss_D: 0.00984730 (Loss_D_real: 0.00931699 Loss_D_fake: 0.00053031) Loss_G: 0.49373087 Loss_Enh_Dec: -1.34734380\n",
      "| epoch  72 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  2.96 | ppl    19.23 | acc     0.68 | train_ae_norm     1.00\n",
      "[72/200][3499/4361] Loss_D: 0.01595685 (Loss_D_real: 0.01149538 Loss_D_fake: 0.00446147) Loss_G: 0.43354684 Loss_Enh_Dec: -1.79056060\n",
      "| epoch  72 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.05 | loss  2.87 | ppl    17.70 | acc     0.67 | train_ae_norm     1.00\n",
      "[72/200][3599/4361] Loss_D: 0.01508356 (Loss_D_real: 0.01352268 Loss_D_fake: 0.00156088) Loss_G: 0.41560212 Loss_Enh_Dec: -1.59230077\n",
      "| epoch  72 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  2.91 | ppl    18.31 | acc     0.67 | train_ae_norm     1.00\n",
      "[72/200][3699/4361] Loss_D: 0.00344737 (Loss_D_real: 0.00196929 Loss_D_fake: 0.00147809) Loss_G: 0.48132753 Loss_Enh_Dec: -1.39476693\n",
      "| epoch  72 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.95 | ppl    19.06 | acc     0.66 | train_ae_norm     1.00\n",
      "[72/200][3799/4361] Loss_D: 0.00445915 (Loss_D_real: 0.00320000 Loss_D_fake: 0.00125915) Loss_G: 0.42014286 Loss_Enh_Dec: -1.88638616\n",
      "| epoch  72 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  2.96 | ppl    19.30 | acc     0.71 | train_ae_norm     1.00\n",
      "[72/200][4099/4361] Loss_D: 0.00275594 (Loss_D_real: 0.00040821 Loss_D_fake: 0.00234773) Loss_G: 0.40686733 Loss_Enh_Dec: -1.08507717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  72 |  4100/ 4361 batches | lr 0.000000 | ms/batch 402.04 | loss  2.90 | ppl    18.08 | acc     0.67 | train_ae_norm     1.00\n",
      "[72/200][4199/4361] Loss_D: 0.00722845 (Loss_D_real: 0.00538846 Loss_D_fake: 0.00183999) Loss_G: 0.51696527 Loss_Enh_Dec: -1.96409726\n",
      "| epoch  72 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  2.95 | ppl    19.15 | acc     0.70 | train_ae_norm     1.00\n",
      "[72/200][4299/4361] Loss_D: 0.00292463 (Loss_D_real: 0.00043755 Loss_D_fake: 0.00248708) Loss_G: 0.41609287 Loss_Enh_Dec: -1.85144842\n",
      "| epoch  72 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  2.92 | ppl    18.53 | acc     0.69 | train_ae_norm     1.00\n",
      "| end of epoch  72 | time: 1854.05s | test loss  2.92 | test ppl 18.62 | acc 0.701\n",
      "bleu_self:  [8.93763758e-02 2.55115291e-09 8.13927989e-12 5.03119840e-13\n",
      " 7.84325627e-13]\n",
      "bleu_test:  [6.89285714e-01 1.35305266e-01 8.99462693e-07 2.57435860e-09\n",
      " 8.50919362e-10]\n",
      "bleu_self: [0.08937638,0.00000000,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.68928571,0.13530527,0.00000090,0.00000000,0.00000000]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 73 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:57.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.702\n",
      "  Training epcoh took: 0:01:27\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.487\n",
      "  Test Loss: 4.194\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  73 |     0/ 4361 batches | lr 0.000000 | ms/batch 869.73 | loss  0.03 | ppl     1.03 | acc     0.72 | train_ae_norm     1.00\n",
      "[73/200][99/4361] Loss_D: 0.00511145 (Loss_D_real: 0.00171212 Loss_D_fake: 0.00339933) Loss_G: 0.40938789 Loss_Enh_Dec: -1.75289118\n",
      "| epoch  73 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.61 | loss  2.92 | ppl    18.54 | acc     0.65 | train_ae_norm     1.00\n",
      "[73/200][199/4361] Loss_D: 0.00192935 (Loss_D_real: 0.00082669 Loss_D_fake: 0.00110265) Loss_G: 0.45823994 Loss_Enh_Dec: -1.68548870\n",
      "| epoch  73 |   200/ 4361 batches | lr 0.000000 | ms/batch 402.21 | loss  2.95 | ppl    19.13 | acc     0.66 | train_ae_norm     1.00\n",
      "[73/200][299/4361] Loss_D: 0.00190319 (Loss_D_real: 0.00017431 Loss_D_fake: 0.00172888) Loss_G: 0.40863743 Loss_Enh_Dec: -1.59457672\n",
      "| epoch  73 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  2.96 | ppl    19.25 | acc     0.64 | train_ae_norm     1.00\n",
      "[73/200][399/4361] Loss_D: 0.00241123 (Loss_D_real: 0.00159681 Loss_D_fake: 0.00081442) Loss_G: 0.42405868 Loss_Enh_Dec: -1.28340459\n",
      "| epoch  73 |   400/ 4361 batches | lr 0.000000 | ms/batch 402.42 | loss  2.89 | ppl    17.98 | acc     0.69 | train_ae_norm     1.00\n",
      "[73/200][499/4361] Loss_D: 0.00162834 (Loss_D_real: 0.00038529 Loss_D_fake: 0.00124305) Loss_G: 0.42443362 Loss_Enh_Dec: -1.47830379\n",
      "| epoch  73 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  2.95 | ppl    19.19 | acc     0.68 | train_ae_norm     1.00\n",
      "[73/200][599/4361] Loss_D: 0.00123784 (Loss_D_real: 0.00023162 Loss_D_fake: 0.00100622) Loss_G: 0.42389402 Loss_Enh_Dec: -1.88859749\n",
      "| epoch  73 |   600/ 4361 batches | lr 0.000000 | ms/batch 402.79 | loss  2.91 | ppl    18.30 | acc     0.65 | train_ae_norm     1.00\n",
      "[73/200][699/4361] Loss_D: 0.00466449 (Loss_D_real: 0.00245181 Loss_D_fake: 0.00221267) Loss_G: 0.41945264 Loss_Enh_Dec: -1.94737041\n",
      "| epoch  73 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.67 | loss  2.95 | ppl    19.03 | acc     0.67 | train_ae_norm     1.00\n",
      "[73/200][799/4361] Loss_D: 0.00258157 (Loss_D_real: 0.00119050 Loss_D_fake: 0.00139107) Loss_G: 0.41162077 Loss_Enh_Dec: -1.68377626\n",
      "| epoch  73 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.90 | ppl    18.16 | acc     0.69 | train_ae_norm     1.00\n",
      "[73/200][899/4361] Loss_D: 0.00299302 (Loss_D_real: 0.00039683 Loss_D_fake: 0.00259619) Loss_G: 0.42460117 Loss_Enh_Dec: -1.71270263\n",
      "| epoch  73 |   900/ 4361 batches | lr 0.000000 | ms/batch 402.12 | loss  2.92 | ppl    18.53 | acc     0.71 | train_ae_norm     1.00\n",
      "[73/200][999/4361] Loss_D: 0.00143132 (Loss_D_real: 0.00017426 Loss_D_fake: 0.00125706) Loss_G: 0.43265587 Loss_Enh_Dec: -1.49348676\n",
      "| epoch  73 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  2.91 | ppl    18.42 | acc     0.70 | train_ae_norm     1.00\n",
      "[73/200][1099/4361] Loss_D: 0.00289693 (Loss_D_real: 0.00179921 Loss_D_fake: 0.00109772) Loss_G: 0.40719810 Loss_Enh_Dec: -1.15091991\n",
      "| epoch  73 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  2.90 | ppl    18.18 | acc     0.66 | train_ae_norm     1.00\n",
      "[73/200][1199/4361] Loss_D: 0.00176424 (Loss_D_real: 0.00041598 Loss_D_fake: 0.00134826) Loss_G: 0.41671211 Loss_Enh_Dec: -1.46596324\n",
      "| epoch  73 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.91 | ppl    18.43 | acc     0.69 | train_ae_norm     1.00\n",
      "[73/200][1299/4361] Loss_D: 0.00351999 (Loss_D_real: 0.00279738 Loss_D_fake: 0.00072261) Loss_G: 0.40384293 Loss_Enh_Dec: -1.87423670\n",
      "| epoch  73 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.92 | ppl    18.51 | acc     0.68 | train_ae_norm     1.00\n",
      "[73/200][1399/4361] Loss_D: 0.00261876 (Loss_D_real: 0.00139136 Loss_D_fake: 0.00122739) Loss_G: 0.68111461 Loss_Enh_Dec: -1.32038403\n",
      "| epoch  73 |  1400/ 4361 batches | lr 0.000000 | ms/batch 402.23 | loss  2.94 | ppl    18.84 | acc     0.63 | train_ae_norm     1.00\n",
      "[73/200][1499/4361] Loss_D: 0.00152195 (Loss_D_real: 0.00072989 Loss_D_fake: 0.00079206) Loss_G: 0.47068450 Loss_Enh_Dec: -1.39018369\n",
      "| epoch  73 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  2.96 | ppl    19.35 | acc     0.66 | train_ae_norm     1.00\n",
      "[73/200][1599/4361] Loss_D: 0.00962258 (Loss_D_real: 0.00870708 Loss_D_fake: 0.00091550) Loss_G: 0.48283491 Loss_Enh_Dec: -1.59734654\n",
      "| epoch  73 |  1600/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  2.94 | ppl    18.84 | acc     0.67 | train_ae_norm     1.00\n",
      "[73/200][1699/4361] Loss_D: 0.00082388 (Loss_D_real: 0.00032583 Loss_D_fake: 0.00049805) Loss_G: 0.41859904 Loss_Enh_Dec: -1.46268189\n",
      "| epoch  73 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  2.90 | ppl    18.19 | acc     0.66 | train_ae_norm     1.00\n",
      "[73/200][1999/4361] Loss_D: 0.00319983 (Loss_D_real: 0.00134014 Loss_D_fake: 0.00185969) Loss_G: 0.40095100 Loss_Enh_Dec: -1.44554508\n",
      "| epoch  73 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  2.89 | ppl    17.94 | acc     0.66 | train_ae_norm     1.00\n",
      "[73/200][2099/4361] Loss_D: 0.00158020 (Loss_D_real: 0.00023853 Loss_D_fake: 0.00134167) Loss_G: 0.41848859 Loss_Enh_Dec: -1.34569347\n",
      "| epoch  73 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  2.92 | ppl    18.63 | acc     0.68 | train_ae_norm     1.00\n",
      "[73/200][2199/4361] Loss_D: 0.00136547 (Loss_D_real: 0.00048491 Loss_D_fake: 0.00088056) Loss_G: 0.42178082 Loss_Enh_Dec: -1.78178871\n",
      "| epoch  73 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.99 | loss  2.90 | ppl    18.24 | acc     0.66 | train_ae_norm     1.00\n",
      "[73/200][2299/4361] Loss_D: 0.00162769 (Loss_D_real: 0.00013564 Loss_D_fake: 0.00149206) Loss_G: 0.39011523 Loss_Enh_Dec: -1.29851639\n",
      "| epoch  73 |  2300/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  2.90 | ppl    18.23 | acc     0.69 | train_ae_norm     1.00\n",
      "[73/200][2399/4361] Loss_D: 0.00385357 (Loss_D_real: 0.00253976 Loss_D_fake: 0.00131381) Loss_G: 0.40311289 Loss_Enh_Dec: -1.51777804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  73 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.90 | ppl    18.20 | acc     0.65 | train_ae_norm     1.00\n",
      "[73/200][2499/4361] Loss_D: 0.00195337 (Loss_D_real: 0.00023371 Loss_D_fake: 0.00171965) Loss_G: 0.44146916 Loss_Enh_Dec: -1.77708995\n",
      "| epoch  73 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  2.95 | ppl    19.04 | acc     0.68 | train_ae_norm     1.00\n",
      "[73/200][2599/4361] Loss_D: 0.00210754 (Loss_D_real: 0.00149494 Loss_D_fake: 0.00061260) Loss_G: 0.54270428 Loss_Enh_Dec: -1.47203624\n",
      "| epoch  73 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  2.90 | ppl    18.10 | acc     0.65 | train_ae_norm     1.00\n",
      "[73/200][2699/4361] Loss_D: 0.03120418 (Loss_D_real: 0.03028314 Loss_D_fake: 0.00092104) Loss_G: 0.48563910 Loss_Enh_Dec: -1.50792813\n",
      "| epoch  73 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  2.92 | ppl    18.45 | acc     0.67 | train_ae_norm     1.00\n",
      "[73/200][2799/4361] Loss_D: 0.00996684 (Loss_D_real: 0.00732696 Loss_D_fake: 0.00263988) Loss_G: 0.41276684 Loss_Enh_Dec: -1.37197483\n",
      "| epoch  73 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  2.88 | ppl    17.84 | acc     0.67 | train_ae_norm     1.00\n",
      "[73/200][2899/4361] Loss_D: 0.00131474 (Loss_D_real: 0.00057516 Loss_D_fake: 0.00073958) Loss_G: 0.47871089 Loss_Enh_Dec: -1.52783084\n",
      "| epoch  73 |  2900/ 4361 batches | lr 0.000000 | ms/batch 402.04 | loss  2.89 | ppl    18.05 | acc     0.69 | train_ae_norm     1.00\n",
      "[73/200][2999/4361] Loss_D: 0.00133083 (Loss_D_real: 0.00026600 Loss_D_fake: 0.00106483) Loss_G: 0.37412557 Loss_Enh_Dec: -1.38126302\n",
      "| epoch  73 |  3000/ 4361 batches | lr 0.000000 | ms/batch 402.09 | loss  2.90 | ppl    18.17 | acc     0.68 | train_ae_norm     1.00\n",
      "[73/200][3099/4361] Loss_D: 0.00153417 (Loss_D_real: 0.00035484 Loss_D_fake: 0.00117934) Loss_G: 0.42367050 Loss_Enh_Dec: -1.61681330\n",
      "| epoch  73 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  2.93 | ppl    18.67 | acc     0.64 | train_ae_norm     1.00\n",
      "[73/200][3199/4361] Loss_D: 0.00177348 (Loss_D_real: 0.00018064 Loss_D_fake: 0.00159284) Loss_G: 0.36955461 Loss_Enh_Dec: -1.25198352\n",
      "| epoch  73 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  2.98 | ppl    19.68 | acc     0.67 | train_ae_norm     1.00\n",
      "[73/200][3299/4361] Loss_D: 0.00142587 (Loss_D_real: 0.00020048 Loss_D_fake: 0.00122538) Loss_G: 0.41966960 Loss_Enh_Dec: -1.55573559\n",
      "| epoch  73 |  3300/ 4361 batches | lr 0.000000 | ms/batch 402.00 | loss  2.97 | ppl    19.54 | acc     0.68 | train_ae_norm     1.00\n",
      "[73/200][3399/4361] Loss_D: 0.01291290 (Loss_D_real: 0.01043141 Loss_D_fake: 0.00248149) Loss_G: 0.54847425 Loss_Enh_Dec: -1.16308367\n",
      "| epoch  73 |  3400/ 4361 batches | lr 0.000000 | ms/batch 402.50 | loss  2.95 | ppl    19.13 | acc     0.66 | train_ae_norm     1.00\n",
      "[73/200][3499/4361] Loss_D: 0.00453367 (Loss_D_real: 0.00302594 Loss_D_fake: 0.00150773) Loss_G: 0.45073238 Loss_Enh_Dec: -0.87575883\n",
      "| epoch  73 |  3500/ 4361 batches | lr 0.000000 | ms/batch 402.30 | loss  2.88 | ppl    17.87 | acc     0.67 | train_ae_norm     1.00\n",
      "[73/200][3599/4361] Loss_D: 0.01342074 (Loss_D_real: 0.01301028 Loss_D_fake: 0.00041046) Loss_G: 0.45025000 Loss_Enh_Dec: -0.99940360\n",
      "| epoch  73 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  2.90 | ppl    18.13 | acc     0.68 | train_ae_norm     1.00\n",
      "[73/200][3699/4361] Loss_D: 0.00161038 (Loss_D_real: 0.00026517 Loss_D_fake: 0.00134522) Loss_G: 0.40109167 Loss_Enh_Dec: -0.57304662\n",
      "| epoch  73 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  2.91 | ppl    18.38 | acc     0.66 | train_ae_norm     1.00\n",
      "[73/200][3799/4361] Loss_D: 0.00566805 (Loss_D_real: 0.00461614 Loss_D_fake: 0.00105192) Loss_G: 0.40536720 Loss_Enh_Dec: -0.88975906\n",
      "| epoch  73 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  2.93 | ppl    18.69 | acc     0.71 | train_ae_norm     1.00\n",
      "[73/200][3899/4361] Loss_D: 0.00148115 (Loss_D_real: 0.00052211 Loss_D_fake: 0.00095903) Loss_G: 0.44734064 Loss_Enh_Dec: -0.72224194\n",
      "| epoch  73 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.86 | loss  2.92 | ppl    18.63 | acc     0.63 | train_ae_norm     1.00\n",
      "[73/200][3999/4361] Loss_D: 0.00342685 (Loss_D_real: 0.00213520 Loss_D_fake: 0.00129165) Loss_G: 0.41356793 Loss_Enh_Dec: -1.15877414\n",
      "| epoch  73 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  2.92 | ppl    18.48 | acc     0.68 | train_ae_norm     1.00\n",
      "[73/200][4099/4361] Loss_D: 0.00415118 (Loss_D_real: 0.00218517 Loss_D_fake: 0.00196600) Loss_G: 0.42816001 Loss_Enh_Dec: -1.19618976\n",
      "| epoch  73 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  2.88 | ppl    17.88 | acc     0.66 | train_ae_norm     1.00\n",
      "[73/200][4199/4361] Loss_D: 0.00315501 (Loss_D_real: 0.00090914 Loss_D_fake: 0.00224586) Loss_G: 0.51430237 Loss_Enh_Dec: -1.60904920\n",
      "| epoch  73 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  2.92 | ppl    18.59 | acc     0.72 | train_ae_norm     1.00\n",
      "[73/200][4299/4361] Loss_D: 0.00550651 (Loss_D_real: 0.00385531 Loss_D_fake: 0.00165120) Loss_G: 0.54316419 Loss_Enh_Dec: -1.62196887\n",
      "| epoch  73 |  4300/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  2.89 | ppl    18.07 | acc     0.67 | train_ae_norm     1.00\n",
      "| end of epoch  73 | time: 1854.64s | test loss  2.90 | test ppl 18.19 | acc 0.700\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 74 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:57.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:16.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.702\n",
      "  Training epcoh took: 0:01:27\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.495\n",
      "  Test Loss: 4.195\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  74 |     0/ 4361 batches | lr 0.000000 | ms/batch 871.07 | loss  0.03 | ppl     1.03 | acc     0.70 | train_ae_norm     1.00\n",
      "[74/200][99/4361] Loss_D: 0.00178638 (Loss_D_real: 0.00051729 Loss_D_fake: 0.00126909) Loss_G: 0.48913532 Loss_Enh_Dec: -1.91913879\n",
      "| epoch  74 |   100/ 4361 batches | lr 0.000000 | ms/batch 403.17 | loss  2.91 | ppl    18.36 | acc     0.64 | train_ae_norm     1.00\n",
      "[74/200][199/4361] Loss_D: 0.00337533 (Loss_D_real: 0.00102252 Loss_D_fake: 0.00235281) Loss_G: 0.40275684 Loss_Enh_Dec: -1.42255247\n",
      "| epoch  74 |   200/ 4361 batches | lr 0.000000 | ms/batch 402.22 | loss  2.93 | ppl    18.76 | acc     0.68 | train_ae_norm     1.00\n",
      "[74/200][299/4361] Loss_D: 0.00393159 (Loss_D_real: 0.00257186 Loss_D_fake: 0.00135972) Loss_G: 0.43264404 Loss_Enh_Dec: -1.92911780\n",
      "| epoch  74 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  2.94 | ppl    18.96 | acc     0.65 | train_ae_norm     1.00\n",
      "[74/200][399/4361] Loss_D: 0.00247714 (Loss_D_real: 0.00126104 Loss_D_fake: 0.00121610) Loss_G: 0.46474648 Loss_Enh_Dec: -1.86992764\n",
      "| epoch  74 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  2.85 | ppl    17.26 | acc     0.69 | train_ae_norm     1.00\n",
      "[74/200][499/4361] Loss_D: 0.00207547 (Loss_D_real: 0.00013603 Loss_D_fake: 0.00193945) Loss_G: 0.41883689 Loss_Enh_Dec: -1.66961539\n",
      "| epoch  74 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  2.92 | ppl    18.57 | acc     0.68 | train_ae_norm     1.00\n",
      "[74/200][599/4361] Loss_D: 0.00943953 (Loss_D_real: 0.00591197 Loss_D_fake: 0.00352755) Loss_G: 0.63327438 Loss_Enh_Dec: -1.53222334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  74 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  2.88 | ppl    17.86 | acc     0.63 | train_ae_norm     1.00\n",
      "[74/200][699/4361] Loss_D: 0.00922135 (Loss_D_real: 0.00113704 Loss_D_fake: 0.00808431) Loss_G: 0.39568517 Loss_Enh_Dec: -1.79501081\n",
      "| epoch  74 |   700/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  2.92 | ppl    18.55 | acc     0.67 | train_ae_norm     1.00\n",
      "[74/200][799/4361] Loss_D: 0.00295650 (Loss_D_real: 0.00221729 Loss_D_fake: 0.00073921) Loss_G: 0.55489558 Loss_Enh_Dec: -1.54412079\n",
      "| epoch  74 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.89 | ppl    18.07 | acc     0.67 | train_ae_norm     1.00\n",
      "[74/200][899/4361] Loss_D: 0.00256650 (Loss_D_real: 0.00136675 Loss_D_fake: 0.00119975) Loss_G: 0.53718579 Loss_Enh_Dec: -1.43318307\n",
      "| epoch  74 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.88 | loss  2.92 | ppl    18.47 | acc     0.70 | train_ae_norm     1.00\n",
      "[74/200][999/4361] Loss_D: 0.00721384 (Loss_D_real: 0.00427816 Loss_D_fake: 0.00293568) Loss_G: 0.43922883 Loss_Enh_Dec: -1.68680501\n",
      "| epoch  74 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  2.88 | ppl    17.87 | acc     0.70 | train_ae_norm     1.00\n",
      "[74/200][1099/4361] Loss_D: 0.00452728 (Loss_D_real: 0.00332941 Loss_D_fake: 0.00119788) Loss_G: 0.39387280 Loss_Enh_Dec: -1.71167719\n",
      "| epoch  74 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  2.88 | ppl    17.88 | acc     0.68 | train_ae_norm     1.00\n",
      "[74/200][1199/4361] Loss_D: 0.00174562 (Loss_D_real: 0.00045388 Loss_D_fake: 0.00129174) Loss_G: 0.42149621 Loss_Enh_Dec: -1.55284178\n",
      "| epoch  74 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  2.89 | ppl    18.03 | acc     0.69 | train_ae_norm     1.00\n",
      "[74/200][1299/4361] Loss_D: 0.00152142 (Loss_D_real: 0.00030347 Loss_D_fake: 0.00121794) Loss_G: 0.39394099 Loss_Enh_Dec: -1.63428044\n",
      "| epoch  74 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  2.92 | ppl    18.60 | acc     0.67 | train_ae_norm     1.00\n",
      "[74/200][1399/4361] Loss_D: 0.00162314 (Loss_D_real: 0.00012870 Loss_D_fake: 0.00149444) Loss_G: 0.39681241 Loss_Enh_Dec: -1.55880928\n",
      "| epoch  74 |  1400/ 4361 batches | lr 0.000000 | ms/batch 402.41 | loss  2.90 | ppl    18.26 | acc     0.64 | train_ae_norm     1.00\n",
      "[74/200][1499/4361] Loss_D: 0.00163089 (Loss_D_real: 0.00064200 Loss_D_fake: 0.00098889) Loss_G: 0.41685113 Loss_Enh_Dec: -1.80820274\n",
      "| epoch  74 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.96 | ppl    19.22 | acc     0.66 | train_ae_norm     1.00\n",
      "[74/200][1599/4361] Loss_D: 0.00243405 (Loss_D_real: 0.00102275 Loss_D_fake: 0.00141130) Loss_G: 0.37690791 Loss_Enh_Dec: -1.84566236\n",
      "| epoch  74 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  2.91 | ppl    18.39 | acc     0.66 | train_ae_norm     1.00\n",
      "[74/200][1699/4361] Loss_D: 0.00173098 (Loss_D_real: 0.00058572 Loss_D_fake: 0.00114526) Loss_G: 0.42230153 Loss_Enh_Dec: -1.75864089\n",
      "| epoch  74 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  2.89 | ppl    17.96 | acc     0.66 | train_ae_norm     1.00\n",
      "[74/200][1799/4361] Loss_D: 0.00281914 (Loss_D_real: 0.00040418 Loss_D_fake: 0.00241496) Loss_G: 0.43474069 Loss_Enh_Dec: -1.61193073\n",
      "| epoch  74 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.91 | loss  2.88 | ppl    17.86 | acc     0.66 | train_ae_norm     1.00\n",
      "[74/200][1899/4361] Loss_D: 0.00192597 (Loss_D_real: 0.00134788 Loss_D_fake: 0.00057809) Loss_G: 0.48546562 Loss_Enh_Dec: -1.47143734\n",
      "| epoch  74 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  2.94 | ppl    18.86 | acc     0.70 | train_ae_norm     1.00\n",
      "[74/200][1999/4361] Loss_D: 0.02269378 (Loss_D_real: 0.02185396 Loss_D_fake: 0.00083983) Loss_G: 0.83932400 Loss_Enh_Dec: -1.31035352\n",
      "| epoch  74 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  2.88 | ppl    17.80 | acc     0.68 | train_ae_norm     1.00\n",
      "[74/200][2099/4361] Loss_D: 0.00436509 (Loss_D_real: 0.00149117 Loss_D_fake: 0.00287392) Loss_G: 0.45746508 Loss_Enh_Dec: -1.32001364\n",
      "| epoch  74 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.90 | ppl    18.19 | acc     0.69 | train_ae_norm     1.00\n",
      "[74/200][2199/4361] Loss_D: 0.00294894 (Loss_D_real: 0.00094400 Loss_D_fake: 0.00200494) Loss_G: 0.41732073 Loss_Enh_Dec: -1.11747694\n",
      "| epoch  74 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  2.91 | ppl    18.27 | acc     0.67 | train_ae_norm     1.00\n",
      "[74/200][2299/4361] Loss_D: 0.00327416 (Loss_D_real: 0.00239682 Loss_D_fake: 0.00087734) Loss_G: 0.54476988 Loss_Enh_Dec: -1.56638181\n",
      "| epoch  74 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  2.89 | ppl    18.06 | acc     0.70 | train_ae_norm     1.00\n",
      "[74/200][2399/4361] Loss_D: 0.00200751 (Loss_D_real: 0.00086257 Loss_D_fake: 0.00114494) Loss_G: 0.42016372 Loss_Enh_Dec: -1.55376422\n",
      "| epoch  74 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  2.90 | ppl    18.13 | acc     0.65 | train_ae_norm     1.00\n",
      "[74/200][2499/4361] Loss_D: 0.00357972 (Loss_D_real: 0.00274263 Loss_D_fake: 0.00083709) Loss_G: 0.43854174 Loss_Enh_Dec: -0.77387178\n",
      "| epoch  74 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  2.95 | ppl    19.20 | acc     0.67 | train_ae_norm     1.00\n",
      "[74/200][2599/4361] Loss_D: 0.00126940 (Loss_D_real: 0.00022499 Loss_D_fake: 0.00104441) Loss_G: 0.48518610 Loss_Enh_Dec: -0.94093496\n",
      "| epoch  74 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.94 | ppl    18.85 | acc     0.65 | train_ae_norm     1.00\n",
      "[74/200][2699/4361] Loss_D: 0.01337667 (Loss_D_real: 0.00691416 Loss_D_fake: 0.00646251) Loss_G: 0.41850168 Loss_Enh_Dec: -1.33251655\n",
      "| epoch  74 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  2.95 | ppl    19.17 | acc     0.66 | train_ae_norm     1.00\n",
      "[74/200][2799/4361] Loss_D: 0.00179466 (Loss_D_real: 0.00068648 Loss_D_fake: 0.00110817) Loss_G: 0.49359772 Loss_Enh_Dec: -1.46941841\n",
      "| epoch  74 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  2.91 | ppl    18.40 | acc     0.67 | train_ae_norm     1.00\n",
      "[74/200][2899/4361] Loss_D: 0.00998497 (Loss_D_real: 0.00270129 Loss_D_fake: 0.00728368) Loss_G: 0.43159562 Loss_Enh_Dec: -1.07465422\n",
      "| epoch  74 |  2900/ 4361 batches | lr 0.000000 | ms/batch 402.03 | loss  2.94 | ppl    18.93 | acc     0.68 | train_ae_norm     1.00\n",
      "[74/200][2999/4361] Loss_D: 0.00216014 (Loss_D_real: 0.00028843 Loss_D_fake: 0.00187171) Loss_G: 0.52957946 Loss_Enh_Dec: -1.49848783\n",
      "| epoch  74 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  2.94 | ppl    18.92 | acc     0.69 | train_ae_norm     1.00\n",
      "[74/200][3099/4361] Loss_D: 0.00306475 (Loss_D_real: 0.00078501 Loss_D_fake: 0.00227974) Loss_G: 0.38468045 Loss_Enh_Dec: -1.39431560\n",
      "| epoch  74 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  2.94 | ppl    18.91 | acc     0.64 | train_ae_norm     1.00\n",
      "[74/200][3199/4361] Loss_D: 0.00214947 (Loss_D_real: 0.00060173 Loss_D_fake: 0.00154774) Loss_G: 0.42331043 Loss_Enh_Dec: -1.25102806\n",
      "| epoch  74 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  2.96 | ppl    19.35 | acc     0.67 | train_ae_norm     1.00\n",
      "[74/200][3299/4361] Loss_D: 0.00206393 (Loss_D_real: 0.00017819 Loss_D_fake: 0.00188574) Loss_G: 0.52871031 Loss_Enh_Dec: -1.41867912\n",
      "| epoch  74 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  2.98 | ppl    19.62 | acc     0.68 | train_ae_norm     1.00\n",
      "[74/200][3399/4361] Loss_D: 0.00248746 (Loss_D_real: 0.00108786 Loss_D_fake: 0.00139960) Loss_G: 0.40215236 Loss_Enh_Dec: -1.69164407\n",
      "| epoch  74 |  3400/ 4361 batches | lr 0.000000 | ms/batch 402.14 | loss  2.95 | ppl    19.14 | acc     0.65 | train_ae_norm     1.00\n",
      "[74/200][3499/4361] Loss_D: 0.00227967 (Loss_D_real: 0.00121993 Loss_D_fake: 0.00105975) Loss_G: 0.44005567 Loss_Enh_Dec: -1.53633714\n",
      "| epoch  74 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  2.88 | ppl    17.83 | acc     0.68 | train_ae_norm     1.00\n",
      "[74/200][3599/4361] Loss_D: 0.00510196 (Loss_D_real: 0.00408071 Loss_D_fake: 0.00102124) Loss_G: 0.40433836 Loss_Enh_Dec: -1.51685441\n",
      "| epoch  74 |  3600/ 4361 batches | lr 0.000000 | ms/batch 402.11 | loss  2.90 | ppl    18.15 | acc     0.68 | train_ae_norm     1.00\n",
      "[74/200][3699/4361] Loss_D: 0.00323963 (Loss_D_real: 0.00141594 Loss_D_fake: 0.00182369) Loss_G: 0.53078169 Loss_Enh_Dec: -1.77735829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  74 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  2.92 | ppl    18.53 | acc     0.63 | train_ae_norm     1.00\n",
      "[74/200][3799/4361] Loss_D: 0.00364391 (Loss_D_real: 0.00117436 Loss_D_fake: 0.00246955) Loss_G: 0.38703004 Loss_Enh_Dec: -1.69600189\n",
      "| epoch  74 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  2.92 | ppl    18.57 | acc     0.69 | train_ae_norm     1.00\n",
      "[74/200][3899/4361] Loss_D: 0.00788140 (Loss_D_real: 0.00738064 Loss_D_fake: 0.00050075) Loss_G: 0.53740501 Loss_Enh_Dec: -1.32387447\n",
      "| epoch  74 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  2.91 | ppl    18.43 | acc     0.65 | train_ae_norm     1.00\n",
      "[74/200][3999/4361] Loss_D: 0.00176830 (Loss_D_real: 0.00085970 Loss_D_fake: 0.00090860) Loss_G: 0.50053930 Loss_Enh_Dec: -1.78400958\n",
      "| epoch  74 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  2.92 | ppl    18.59 | acc     0.67 | train_ae_norm     1.00\n",
      "[74/200][4099/4361] Loss_D: 0.00150713 (Loss_D_real: 0.00072646 Loss_D_fake: 0.00078067) Loss_G: 0.42931515 Loss_Enh_Dec: -1.88438511\n",
      "| epoch  74 |  4100/ 4361 batches | lr 0.000000 | ms/batch 402.07 | loss  2.88 | ppl    17.83 | acc     0.67 | train_ae_norm     1.00\n",
      "[74/200][4199/4361] Loss_D: 0.00114174 (Loss_D_real: 0.00053503 Loss_D_fake: 0.00060671) Loss_G: 0.75592083 Loss_Enh_Dec: -1.86892307\n",
      "| epoch  74 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.92 | ppl    18.59 | acc     0.69 | train_ae_norm     1.00\n",
      "[74/200][4299/4361] Loss_D: 0.00089064 (Loss_D_real: 0.00020266 Loss_D_fake: 0.00068797) Loss_G: 0.45433521 Loss_Enh_Dec: -1.61686289\n",
      "| epoch  74 |  4300/ 4361 batches | lr 0.000000 | ms/batch 402.04 | loss  2.88 | ppl    17.80 | acc     0.70 | train_ae_norm     1.00\n",
      "| end of epoch  74 | time: 1854.54s | test loss  2.87 | test ppl 17.62 | acc 0.705\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 75 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.705\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.477\n",
      "  Test Loss: 4.241\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  75 |     0/ 4361 batches | lr 0.000000 | ms/batch 866.17 | loss  0.03 | ppl     1.03 | acc     0.71 | train_ae_norm     1.00\n",
      "[75/200][99/4361] Loss_D: 0.00747903 (Loss_D_real: 0.00525520 Loss_D_fake: 0.00222383) Loss_G: 0.42218086 Loss_Enh_Dec: -1.78691256\n",
      "| epoch  75 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  2.89 | ppl    18.01 | acc     0.67 | train_ae_norm     1.00\n",
      "[75/200][199/4361] Loss_D: 0.00291268 (Loss_D_real: 0.00117173 Loss_D_fake: 0.00174095) Loss_G: 0.41795346 Loss_Enh_Dec: -1.84518051\n",
      "| epoch  75 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  2.92 | ppl    18.48 | acc     0.66 | train_ae_norm     1.00\n",
      "[75/200][299/4361] Loss_D: 0.00131570 (Loss_D_real: 0.00016825 Loss_D_fake: 0.00114745) Loss_G: 0.42159352 Loss_Enh_Dec: -1.84801090\n",
      "| epoch  75 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  2.92 | ppl    18.56 | acc     0.64 | train_ae_norm     1.00\n",
      "[75/200][399/4361] Loss_D: 0.00259809 (Loss_D_real: 0.00173313 Loss_D_fake: 0.00086497) Loss_G: 0.45762321 Loss_Enh_Dec: -1.70207822\n",
      "| epoch  75 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  2.84 | ppl    17.11 | acc     0.68 | train_ae_norm     1.00\n",
      "[75/200][499/4361] Loss_D: 0.01155722 (Loss_D_real: 0.00996432 Loss_D_fake: 0.00159290) Loss_G: 0.51727551 Loss_Enh_Dec: -1.97550738\n",
      "| epoch  75 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  2.91 | ppl    18.37 | acc     0.69 | train_ae_norm     1.00\n",
      "[75/200][599/4361] Loss_D: 1.08835292 (Loss_D_real: 0.00021596 Loss_D_fake: 1.08813691) Loss_G: 0.58260375 Loss_Enh_Dec: -1.84548855\n",
      "| epoch  75 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  2.87 | ppl    17.56 | acc     0.63 | train_ae_norm     1.00\n",
      "[75/200][699/4361] Loss_D: 0.00291913 (Loss_D_real: 0.00160486 Loss_D_fake: 0.00131427) Loss_G: 0.43537435 Loss_Enh_Dec: -1.69250679\n",
      "| epoch  75 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.91 | ppl    18.31 | acc     0.69 | train_ae_norm     1.00\n",
      "[75/200][799/4361] Loss_D: 0.00282808 (Loss_D_real: 0.00102342 Loss_D_fake: 0.00180466) Loss_G: 0.40904960 Loss_Enh_Dec: -1.75403023\n",
      "| epoch  75 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  2.88 | ppl    17.83 | acc     0.68 | train_ae_norm     1.00\n",
      "[75/200][899/4361] Loss_D: 0.00812002 (Loss_D_real: 0.00731040 Loss_D_fake: 0.00080962) Loss_G: 0.40473437 Loss_Enh_Dec: -1.36375654\n",
      "| epoch  75 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  2.91 | ppl    18.32 | acc     0.69 | train_ae_norm     1.00\n",
      "[75/200][999/4361] Loss_D: 0.00334673 (Loss_D_real: 0.00066367 Loss_D_fake: 0.00268306) Loss_G: 0.38231429 Loss_Enh_Dec: -1.34326291\n",
      "| epoch  75 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.89 | ppl    18.05 | acc     0.70 | train_ae_norm     1.00\n",
      "[75/200][1099/4361] Loss_D: 0.00624156 (Loss_D_real: 0.00156519 Loss_D_fake: 0.00467636) Loss_G: 0.40443173 Loss_Enh_Dec: -1.23200178\n",
      "| epoch  75 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  2.89 | ppl    17.93 | acc     0.66 | train_ae_norm     1.00\n",
      "[75/200][1199/4361] Loss_D: 0.00247645 (Loss_D_real: 0.00117617 Loss_D_fake: 0.00130028) Loss_G: 0.43934470 Loss_Enh_Dec: -1.61210942\n",
      "| epoch  75 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.08 | loss  2.89 | ppl    17.96 | acc     0.69 | train_ae_norm     1.00\n",
      "[75/200][1299/4361] Loss_D: 0.00198410 (Loss_D_real: 0.00046886 Loss_D_fake: 0.00151524) Loss_G: 0.43360782 Loss_Enh_Dec: -1.44084644\n",
      "| epoch  75 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  2.91 | ppl    18.37 | acc     0.69 | train_ae_norm     1.00\n",
      "[75/200][1399/4361] Loss_D: 0.00481431 (Loss_D_real: 0.00039128 Loss_D_fake: 0.00442303) Loss_G: 0.40831217 Loss_Enh_Dec: -1.81374514\n",
      "| epoch  75 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.91 | ppl    18.39 | acc     0.63 | train_ae_norm     1.00\n",
      "[75/200][1499/4361] Loss_D: 0.05607579 (Loss_D_real: 0.00107400 Loss_D_fake: 0.05500180) Loss_G: 0.60386372 Loss_Enh_Dec: -1.85909832\n",
      "| epoch  75 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  2.97 | ppl    19.46 | acc     0.65 | train_ae_norm     1.00\n",
      "[75/200][1599/4361] Loss_D: 0.00227885 (Loss_D_real: 0.00154338 Loss_D_fake: 0.00073548) Loss_G: 0.45542216 Loss_Enh_Dec: -1.75582719\n",
      "| epoch  75 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  2.94 | ppl    18.91 | acc     0.65 | train_ae_norm     1.00\n",
      "[75/200][1699/4361] Loss_D: 0.00406018 (Loss_D_real: 0.00290506 Loss_D_fake: 0.00115512) Loss_G: 0.44738874 Loss_Enh_Dec: -1.58384621\n",
      "| epoch  75 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.94 | ppl    18.84 | acc     0.67 | train_ae_norm     1.00\n",
      "[75/200][1799/4361] Loss_D: 0.00125913 (Loss_D_real: 0.00090959 Loss_D_fake: 0.00034955) Loss_G: 0.47755042 Loss_Enh_Dec: -1.66587150\n",
      "| epoch  75 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.88 | ppl    17.86 | acc     0.67 | train_ae_norm     1.00\n",
      "[75/200][1899/4361] Loss_D: 0.01688988 (Loss_D_real: 0.01474750 Loss_D_fake: 0.00214238) Loss_G: 0.38824376 Loss_Enh_Dec: -1.86002088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  75 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  2.96 | ppl    19.21 | acc     0.70 | train_ae_norm     1.00\n",
      "[75/200][1999/4361] Loss_D: 0.01471943 (Loss_D_real: 0.01175992 Loss_D_fake: 0.00295951) Loss_G: 0.41226292 Loss_Enh_Dec: -1.50211525\n",
      "| epoch  75 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  2.88 | ppl    17.78 | acc     0.67 | train_ae_norm     1.00\n",
      "[75/200][2099/4361] Loss_D: 0.00638597 (Loss_D_real: 0.00399488 Loss_D_fake: 0.00239109) Loss_G: 0.46559715 Loss_Enh_Dec: -1.98990083\n",
      "| epoch  75 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.93 | ppl    18.68 | acc     0.67 | train_ae_norm     1.00\n",
      "[75/200][2199/4361] Loss_D: 0.01094142 (Loss_D_real: 0.00872089 Loss_D_fake: 0.00222053) Loss_G: 0.44073692 Loss_Enh_Dec: -1.41697311\n",
      "| epoch  75 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.91 | ppl    18.39 | acc     0.68 | train_ae_norm     1.00\n",
      "[75/200][2299/4361] Loss_D: 0.00457703 (Loss_D_real: 0.00100380 Loss_D_fake: 0.00357323) Loss_G: 0.61526221 Loss_Enh_Dec: -1.41437519\n",
      "| epoch  75 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  2.90 | ppl    18.16 | acc     0.70 | train_ae_norm     1.00\n",
      "[75/200][2399/4361] Loss_D: 0.00222759 (Loss_D_real: 0.00184380 Loss_D_fake: 0.00038379) Loss_G: 0.46443415 Loss_Enh_Dec: -1.73232174\n",
      "| epoch  75 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  2.91 | ppl    18.30 | acc     0.63 | train_ae_norm     1.00\n",
      "[75/200][2499/4361] Loss_D: 0.00431107 (Loss_D_real: 0.00263705 Loss_D_fake: 0.00167403) Loss_G: 0.57894266 Loss_Enh_Dec: -1.56747425\n",
      "| epoch  75 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  2.95 | ppl    19.06 | acc     0.69 | train_ae_norm     1.00\n",
      "[75/200][2599/4361] Loss_D: 0.00096410 (Loss_D_real: 0.00040537 Loss_D_fake: 0.00055874) Loss_G: 0.39589915 Loss_Enh_Dec: -1.61967695\n",
      "| epoch  75 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  2.91 | ppl    18.44 | acc     0.66 | train_ae_norm     1.00\n",
      "[75/200][2699/4361] Loss_D: 0.00765059 (Loss_D_real: 0.00107650 Loss_D_fake: 0.00657409) Loss_G: 0.48371720 Loss_Enh_Dec: -1.71230912\n",
      "| epoch  75 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  2.91 | ppl    18.29 | acc     0.65 | train_ae_norm     1.00\n",
      "[75/200][2799/4361] Loss_D: 0.00329431 (Loss_D_real: 0.00241564 Loss_D_fake: 0.00087867) Loss_G: 0.45829603 Loss_Enh_Dec: -1.65920949\n",
      "| epoch  75 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  2.87 | ppl    17.68 | acc     0.65 | train_ae_norm     1.00\n",
      "[75/200][2899/4361] Loss_D: 0.00262609 (Loss_D_real: 0.00054032 Loss_D_fake: 0.00208576) Loss_G: 0.43571669 Loss_Enh_Dec: -1.19677484\n",
      "| epoch  75 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.89 | ppl    18.04 | acc     0.67 | train_ae_norm     1.00\n",
      "[75/200][2999/4361] Loss_D: 0.00541015 (Loss_D_real: 0.00123574 Loss_D_fake: 0.00417441) Loss_G: 0.41889325 Loss_Enh_Dec: -1.16508138\n",
      "| epoch  75 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.43 | loss  2.92 | ppl    18.56 | acc     0.66 | train_ae_norm     1.00\n",
      "[75/200][3099/4361] Loss_D: 0.00179978 (Loss_D_real: 0.00118652 Loss_D_fake: 0.00061326) Loss_G: 0.50618809 Loss_Enh_Dec: -1.03703964\n",
      "| epoch  75 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.89 | loss  3.00 | ppl    20.01 | acc     0.65 | train_ae_norm     1.00\n",
      "[75/200][3199/4361] Loss_D: 0.00488992 (Loss_D_real: 0.00175576 Loss_D_fake: 0.00313416) Loss_G: 0.42878208 Loss_Enh_Dec: -1.68530548\n",
      "| epoch  75 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  2.97 | ppl    19.41 | acc     0.66 | train_ae_norm     1.00\n",
      "[75/200][3299/4361] Loss_D: 0.00300531 (Loss_D_real: 0.00148254 Loss_D_fake: 0.00152277) Loss_G: 0.65219253 Loss_Enh_Dec: -1.62024617\n",
      "| epoch  75 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  2.94 | ppl    18.89 | acc     0.66 | train_ae_norm     1.00\n",
      "[75/200][3399/4361] Loss_D: 0.01342654 (Loss_D_real: 0.01256984 Loss_D_fake: 0.00085670) Loss_G: 0.45576420 Loss_Enh_Dec: -1.56663883\n",
      "| epoch  75 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  2.93 | ppl    18.81 | acc     0.66 | train_ae_norm     1.00\n",
      "[75/200][3499/4361] Loss_D: 0.00399069 (Loss_D_real: 0.00317644 Loss_D_fake: 0.00081426) Loss_G: 0.40289241 Loss_Enh_Dec: -1.31979215\n",
      "| epoch  75 |  3500/ 4361 batches | lr 0.000000 | ms/batch 402.34 | loss  2.87 | ppl    17.57 | acc     0.65 | train_ae_norm     1.00\n",
      "[75/200][3599/4361] Loss_D: 0.00197707 (Loss_D_real: 0.00057977 Loss_D_fake: 0.00139729) Loss_G: 0.43480587 Loss_Enh_Dec: -1.88616979\n",
      "| epoch  75 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  2.90 | ppl    18.14 | acc     0.69 | train_ae_norm     1.00\n",
      "[75/200][3699/4361] Loss_D: 0.00135586 (Loss_D_real: 0.00029186 Loss_D_fake: 0.00106400) Loss_G: 0.39932057 Loss_Enh_Dec: -1.93029368\n",
      "| epoch  75 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  2.92 | ppl    18.52 | acc     0.65 | train_ae_norm     1.00\n",
      "[75/200][3799/4361] Loss_D: 0.00114535 (Loss_D_real: 0.00049790 Loss_D_fake: 0.00064745) Loss_G: 0.50921518 Loss_Enh_Dec: -1.74874437\n",
      "| epoch  75 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  2.92 | ppl    18.51 | acc     0.71 | train_ae_norm     1.00\n",
      "[75/200][3899/4361] Loss_D: 0.00144145 (Loss_D_real: 0.00046830 Loss_D_fake: 0.00097315) Loss_G: 0.39501759 Loss_Enh_Dec: -1.46328282\n",
      "| epoch  75 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  2.92 | ppl    18.47 | acc     0.65 | train_ae_norm     1.00\n",
      "[75/200][3999/4361] Loss_D: 0.00216896 (Loss_D_real: 0.00047722 Loss_D_fake: 0.00169174) Loss_G: 0.41953650 Loss_Enh_Dec: -1.88236129\n",
      "| epoch  75 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  2.92 | ppl    18.46 | acc     0.67 | train_ae_norm     1.00\n",
      "[75/200][4099/4361] Loss_D: 0.02955151 (Loss_D_real: 0.02930275 Loss_D_fake: 0.00024875) Loss_G: 0.66780329 Loss_Enh_Dec: -1.55286849\n",
      "| epoch  75 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  2.88 | ppl    17.80 | acc     0.67 | train_ae_norm     1.00\n",
      "[75/200][4199/4361] Loss_D: 0.00209720 (Loss_D_real: 0.00114761 Loss_D_fake: 0.00094960) Loss_G: 0.49298206 Loss_Enh_Dec: -1.44945204\n",
      "| epoch  75 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  2.94 | ppl    18.90 | acc     0.71 | train_ae_norm     1.00\n",
      "[75/200][4299/4361] Loss_D: 0.03461455 (Loss_D_real: 0.03322332 Loss_D_fake: 0.00139123) Loss_G: 0.46659428 Loss_Enh_Dec: -1.62081683\n",
      "| epoch  75 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  2.93 | ppl    18.79 | acc     0.68 | train_ae_norm     1.00\n",
      "| end of epoch  75 | time: 1853.11s | test loss  2.87 | test ppl 17.61 | acc 0.702\n",
      "bleu_self:  [2.99865109e-01 1.08690399e-01 1.03734118e-06 3.38873159e-09\n",
      " 1.15980398e-10]\n",
      "bleu_test:  [7.84077381e-01 2.91733524e-01 2.23274247e-06 6.51400148e-09\n",
      " 2.07167847e-10]\n",
      "bleu_self: [0.29986511,0.10869040,0.00000104,0.00000000,0.00000000]\n",
      "bleu_test: [0.78407738,0.29173352,0.00000223,0.00000001,0.00000000]\n",
      "New saving model: epoch 075.\n",
      "Saving models to ./results/yahoo_merge_assigned_results\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 76 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.701\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.482\n",
      "  Test Loss: 4.281\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  76 |     0/ 4361 batches | lr 0.000000 | ms/batch 866.21 | loss  0.03 | ppl     1.03 | acc     0.69 | train_ae_norm     1.00\n",
      "[76/200][99/4361] Loss_D: 0.00349368 (Loss_D_real: 0.00261977 Loss_D_fake: 0.00087391) Loss_G: 0.37847432 Loss_Enh_Dec: -1.85769582\n",
      "| epoch  76 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.39 | loss  2.91 | ppl    18.32 | acc     0.65 | train_ae_norm     1.00\n",
      "[76/200][199/4361] Loss_D: 0.00375500 (Loss_D_real: 0.00294697 Loss_D_fake: 0.00080802) Loss_G: 0.41857585 Loss_Enh_Dec: -1.61602676\n",
      "| epoch  76 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  2.93 | ppl    18.77 | acc     0.67 | train_ae_norm     1.00\n",
      "[76/200][299/4361] Loss_D: 0.00395804 (Loss_D_real: 0.00246876 Loss_D_fake: 0.00148928) Loss_G: 0.41322193 Loss_Enh_Dec: -1.65908849\n",
      "| epoch  76 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  2.93 | ppl    18.67 | acc     0.62 | train_ae_norm     1.00\n",
      "[76/200][399/4361] Loss_D: 0.00365074 (Loss_D_real: 0.00164030 Loss_D_fake: 0.00201043) Loss_G: 0.42847005 Loss_Enh_Dec: -1.81978762\n",
      "| epoch  76 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.84 | ppl    17.03 | acc     0.68 | train_ae_norm     1.00\n",
      "[76/200][499/4361] Loss_D: 0.00758256 (Loss_D_real: 0.00634637 Loss_D_fake: 0.00123619) Loss_G: 0.42486054 Loss_Enh_Dec: -1.92168105\n",
      "| epoch  76 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  2.89 | ppl    18.04 | acc     0.69 | train_ae_norm     1.00\n",
      "[76/200][599/4361] Loss_D: 0.01526681 (Loss_D_real: 0.01486375 Loss_D_fake: 0.00040306) Loss_G: 0.48536035 Loss_Enh_Dec: -1.99092603\n",
      "| epoch  76 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  2.85 | ppl    17.36 | acc     0.63 | train_ae_norm     1.00\n",
      "[76/200][699/4361] Loss_D: 0.00261728 (Loss_D_real: 0.00094787 Loss_D_fake: 0.00166941) Loss_G: 0.43107864 Loss_Enh_Dec: -1.81881547\n",
      "| epoch  76 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  2.90 | ppl    18.13 | acc     0.68 | train_ae_norm     1.00\n",
      "[76/200][799/4361] Loss_D: 0.00488172 (Loss_D_real: 0.00301187 Loss_D_fake: 0.00186985) Loss_G: 0.41025066 Loss_Enh_Dec: -2.00983906\n",
      "| epoch  76 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  2.85 | ppl    17.32 | acc     0.68 | train_ae_norm     1.00\n",
      "[76/200][899/4361] Loss_D: 0.00770513 (Loss_D_real: 0.00289657 Loss_D_fake: 0.00480856) Loss_G: 0.82577974 Loss_Enh_Dec: -1.93566453\n",
      "| epoch  76 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  2.87 | ppl    17.59 | acc     0.70 | train_ae_norm     1.00\n",
      "[76/200][999/4361] Loss_D: 0.00160806 (Loss_D_real: 0.00005139 Loss_D_fake: 0.00155667) Loss_G: 0.39163303 Loss_Enh_Dec: -1.74005640\n",
      "| epoch  76 |  1000/ 4361 batches | lr 0.000000 | ms/batch 402.03 | loss  2.86 | ppl    17.47 | acc     0.68 | train_ae_norm     1.00\n",
      "[76/200][1099/4361] Loss_D: 0.00132709 (Loss_D_real: 0.00039714 Loss_D_fake: 0.00092995) Loss_G: 0.39719141 Loss_Enh_Dec: -1.68011987\n",
      "| epoch  76 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  2.86 | ppl    17.41 | acc     0.68 | train_ae_norm     1.00\n",
      "[76/200][1199/4361] Loss_D: 0.00099992 (Loss_D_real: 0.00030698 Loss_D_fake: 0.00069294) Loss_G: 0.59416062 Loss_Enh_Dec: -1.60552216\n",
      "| epoch  76 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  2.85 | ppl    17.27 | acc     0.70 | train_ae_norm     1.00\n",
      "[76/200][1299/4361] Loss_D: 0.00637382 (Loss_D_real: 0.00085050 Loss_D_fake: 0.00552333) Loss_G: 0.43504366 Loss_Enh_Dec: -1.64446342\n",
      "| epoch  76 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.88 | ppl    17.89 | acc     0.70 | train_ae_norm     1.00\n",
      "[76/200][1399/4361] Loss_D: 0.00488095 (Loss_D_real: 0.00340977 Loss_D_fake: 0.00147118) Loss_G: 0.43915629 Loss_Enh_Dec: -1.77500951\n",
      "| epoch  76 |  1400/ 4361 batches | lr 0.000000 | ms/batch 402.16 | loss  2.88 | ppl    17.75 | acc     0.64 | train_ae_norm     1.00\n",
      "[76/200][1499/4361] Loss_D: 0.00088817 (Loss_D_real: 0.00045302 Loss_D_fake: 0.00043515) Loss_G: 0.43341121 Loss_Enh_Dec: -1.51621163\n",
      "| epoch  76 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  2.93 | ppl    18.68 | acc     0.67 | train_ae_norm     1.00\n",
      "[76/200][1599/4361] Loss_D: 0.00251111 (Loss_D_real: 0.00056882 Loss_D_fake: 0.00194229) Loss_G: 0.46915260 Loss_Enh_Dec: -1.37176192\n",
      "| epoch  76 |  1600/ 4361 batches | lr 0.000000 | ms/batch 402.08 | loss  2.89 | ppl    17.94 | acc     0.68 | train_ae_norm     1.00\n",
      "[76/200][1699/4361] Loss_D: 0.00473771 (Loss_D_real: 0.00457279 Loss_D_fake: 0.00016492) Loss_G: 0.61425889 Loss_Enh_Dec: -1.82151496\n",
      "| epoch  76 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  2.88 | ppl    17.81 | acc     0.67 | train_ae_norm     1.00\n",
      "[76/200][1799/4361] Loss_D: 0.00216208 (Loss_D_real: 0.00062143 Loss_D_fake: 0.00154066) Loss_G: 0.47259498 Loss_Enh_Dec: -1.53614175\n",
      "| epoch  76 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  2.85 | ppl    17.27 | acc     0.67 | train_ae_norm     1.00\n",
      "[76/200][1899/4361] Loss_D: 0.00696315 (Loss_D_real: 0.00509822 Loss_D_fake: 0.00186492) Loss_G: 0.39966497 Loss_Enh_Dec: -1.83003223\n",
      "| epoch  76 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  2.90 | ppl    18.18 | acc     0.69 | train_ae_norm     1.00\n",
      "[76/200][1999/4361] Loss_D: 0.00122120 (Loss_D_real: 0.00022087 Loss_D_fake: 0.00100033) Loss_G: 0.40272352 Loss_Enh_Dec: -1.71813285\n",
      "| epoch  76 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  2.85 | ppl    17.35 | acc     0.68 | train_ae_norm     1.00\n",
      "[76/200][2099/4361] Loss_D: 0.00322095 (Loss_D_real: 0.00072731 Loss_D_fake: 0.00249364) Loss_G: 0.39062744 Loss_Enh_Dec: -1.65672815\n",
      "| epoch  76 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  2.86 | ppl    17.52 | acc     0.70 | train_ae_norm     1.00\n",
      "[76/200][2199/4361] Loss_D: 0.00375987 (Loss_D_real: 0.00217044 Loss_D_fake: 0.00158943) Loss_G: 0.56498784 Loss_Enh_Dec: -1.34180820\n",
      "| epoch  76 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  2.87 | ppl    17.61 | acc     0.69 | train_ae_norm     1.00\n",
      "[76/200][2299/4361] Loss_D: 0.00294276 (Loss_D_real: 0.00055439 Loss_D_fake: 0.00238837) Loss_G: 0.51400065 Loss_Enh_Dec: -1.07018876\n",
      "| epoch  76 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.64 | loss  2.86 | ppl    17.45 | acc     0.70 | train_ae_norm     1.00\n",
      "[76/200][2399/4361] Loss_D: 0.00782064 (Loss_D_real: 0.00527436 Loss_D_fake: 0.00254628) Loss_G: 0.41465455 Loss_Enh_Dec: -1.14315641\n",
      "| epoch  76 |  2400/ 4361 batches | lr 0.000000 | ms/batch 400.26 | loss  2.86 | ppl    17.51 | acc     0.66 | train_ae_norm     1.00\n",
      "[76/200][2499/4361] Loss_D: 0.00218485 (Loss_D_real: 0.00036333 Loss_D_fake: 0.00182152) Loss_G: 0.39384800 Loss_Enh_Dec: -1.37205374\n",
      "| epoch  76 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  2.91 | ppl    18.38 | acc     0.69 | train_ae_norm     1.00\n",
      "[76/200][2599/4361] Loss_D: 0.00171405 (Loss_D_real: 0.00106779 Loss_D_fake: 0.00064626) Loss_G: 0.47894692 Loss_Enh_Dec: -1.70194936\n",
      "| epoch  76 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  2.87 | ppl    17.55 | acc     0.64 | train_ae_norm     1.00\n",
      "[76/200][2699/4361] Loss_D: 0.00389139 (Loss_D_real: 0.00046845 Loss_D_fake: 0.00342294) Loss_G: 0.39702091 Loss_Enh_Dec: -1.64220107\n",
      "| epoch  76 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.90 | ppl    18.17 | acc     0.67 | train_ae_norm     1.00\n",
      "[76/200][2799/4361] Loss_D: 0.00218426 (Loss_D_real: 0.00072044 Loss_D_fake: 0.00146382) Loss_G: 0.44347024 Loss_Enh_Dec: -1.73967779\n",
      "| epoch  76 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  2.83 | ppl    16.93 | acc     0.65 | train_ae_norm     1.00\n",
      "[76/200][2899/4361] Loss_D: 0.00152065 (Loss_D_real: 0.00089990 Loss_D_fake: 0.00062076) Loss_G: 0.51614517 Loss_Enh_Dec: -1.79407787\n",
      "| epoch  76 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  2.86 | ppl    17.39 | acc     0.66 | train_ae_norm     1.00\n",
      "[76/200][2999/4361] Loss_D: 0.00175144 (Loss_D_real: 0.00091905 Loss_D_fake: 0.00083239) Loss_G: 0.45580515 Loss_Enh_Dec: -2.12461543\n",
      "| epoch  76 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  2.87 | ppl    17.70 | acc     0.67 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[76/200][3099/4361] Loss_D: 0.00385935 (Loss_D_real: 0.00023867 Loss_D_fake: 0.00362069) Loss_G: 0.42552757 Loss_Enh_Dec: -1.88443089\n",
      "| epoch  76 |  3100/ 4361 batches | lr 0.000000 | ms/batch 402.24 | loss  2.89 | ppl    17.93 | acc     0.66 | train_ae_norm     1.00\n",
      "[76/200][3199/4361] Loss_D: 0.00150411 (Loss_D_real: 0.00019264 Loss_D_fake: 0.00131147) Loss_G: 0.39784524 Loss_Enh_Dec: -1.64748538\n",
      "| epoch  76 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  2.92 | ppl    18.46 | acc     0.67 | train_ae_norm     1.00\n",
      "[76/200][3299/4361] Loss_D: 0.02395430 (Loss_D_real: 0.02312623 Loss_D_fake: 0.00082807) Loss_G: 0.54903406 Loss_Enh_Dec: -1.46361649\n",
      "| epoch  76 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  2.91 | ppl    18.44 | acc     0.68 | train_ae_norm     1.00\n",
      "[76/200][3399/4361] Loss_D: 0.05668312 (Loss_D_real: 0.05588569 Loss_D_fake: 0.00079743) Loss_G: 0.75600642 Loss_Enh_Dec: -1.50674236\n",
      "| epoch  76 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  2.91 | ppl    18.27 | acc     0.68 | train_ae_norm     1.00\n",
      "[76/200][3499/4361] Loss_D: 0.00133895 (Loss_D_real: 0.00045023 Loss_D_fake: 0.00088873) Loss_G: 0.52590168 Loss_Enh_Dec: -1.31535566\n",
      "| epoch  76 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  2.85 | ppl    17.25 | acc     0.65 | train_ae_norm     1.00\n",
      "[76/200][3599/4361] Loss_D: 0.00131403 (Loss_D_real: 0.00064787 Loss_D_fake: 0.00066616) Loss_G: 0.40485331 Loss_Enh_Dec: -1.75457692\n",
      "| epoch  76 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  2.88 | ppl    17.88 | acc     0.67 | train_ae_norm     1.00\n",
      "[76/200][3699/4361] Loss_D: 0.00213829 (Loss_D_real: 0.00030522 Loss_D_fake: 0.00183306) Loss_G: 0.44181117 Loss_Enh_Dec: -1.69604194\n",
      "| epoch  76 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  2.89 | ppl    18.06 | acc     0.65 | train_ae_norm     1.00\n",
      "[76/200][3799/4361] Loss_D: 0.00418401 (Loss_D_real: 0.00148031 Loss_D_fake: 0.00270370) Loss_G: 0.37562633 Loss_Enh_Dec: -1.38068449\n",
      "| epoch  76 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  2.90 | ppl    18.10 | acc     0.71 | train_ae_norm     1.00\n",
      "[76/200][3899/4361] Loss_D: 0.00099946 (Loss_D_real: 0.00019560 Loss_D_fake: 0.00080386) Loss_G: 0.44769999 Loss_Enh_Dec: -1.56986880\n",
      "| epoch  76 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.90 | ppl    18.23 | acc     0.65 | train_ae_norm     1.00\n",
      "[76/200][3999/4361] Loss_D: 0.00106833 (Loss_D_real: 0.00042118 Loss_D_fake: 0.00064715) Loss_G: 0.48637983 Loss_Enh_Dec: -1.58930969\n",
      "| epoch  76 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.82 | loss  2.90 | ppl    18.10 | acc     0.67 | train_ae_norm     1.00\n",
      "[76/200][4099/4361] Loss_D: 0.00129024 (Loss_D_real: 0.00056496 Loss_D_fake: 0.00072528) Loss_G: 0.50546038 Loss_Enh_Dec: -1.35889804\n",
      "| epoch  76 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.38 | loss  2.85 | ppl    17.37 | acc     0.68 | train_ae_norm     1.00\n",
      "[76/200][4199/4361] Loss_D: 0.00085082 (Loss_D_real: 0.00021506 Loss_D_fake: 0.00063576) Loss_G: 0.47665760 Loss_Enh_Dec: -1.77073562\n",
      "| epoch  76 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  2.90 | ppl    18.17 | acc     0.69 | train_ae_norm     1.00\n",
      "[76/200][4299/4361] Loss_D: 0.00122434 (Loss_D_real: 0.00087660 Loss_D_fake: 0.00034774) Loss_G: 0.50983334 Loss_Enh_Dec: -1.78472364\n",
      "| epoch  76 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.63 | loss  2.86 | ppl    17.43 | acc     0.69 | train_ae_norm     1.00\n",
      "| end of epoch  76 | time: 1852.97s | test loss  2.83 | test ppl 16.86 | acc 0.708\n",
      "bleu_self:  [4.16666667e-02 1.44337586e-09 5.12883946e-12 2.35312586e-12\n",
      " 1.69938871e-11]\n",
      "bleu_test:  [7.75595238e-01 1.50334838e-08 1.25323655e-08 2.23097844e-08\n",
      " 3.17466159e-08]\n",
      "bleu_self: [0.04166667,0.00000000,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.77559524,0.00000002,0.00000001,0.00000002,0.00000003]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 77 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.703\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.492\n",
      "  Test Loss: 4.251\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  77 |     0/ 4361 batches | lr 0.000000 | ms/batch 862.25 | loss  0.03 | ppl     1.03 | acc     0.71 | train_ae_norm     1.00\n",
      "[77/200][99/4361] Loss_D: 0.00320143 (Loss_D_real: 0.00140780 Loss_D_fake: 0.00179364) Loss_G: 0.40569511 Loss_Enh_Dec: -1.47641551\n",
      "| epoch  77 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  2.87 | ppl    17.61 | acc     0.64 | train_ae_norm     1.00\n",
      "[77/200][199/4361] Loss_D: 0.00821374 (Loss_D_real: 0.00749596 Loss_D_fake: 0.00071778) Loss_G: 0.43920231 Loss_Enh_Dec: -1.47845161\n",
      "| epoch  77 |   200/ 4361 batches | lr 0.000000 | ms/batch 400.89 | loss  2.89 | ppl    18.08 | acc     0.68 | train_ae_norm     1.00\n",
      "[77/200][299/4361] Loss_D: 0.00132332 (Loss_D_real: 0.00020870 Loss_D_fake: 0.00111462) Loss_G: 0.44737440 Loss_Enh_Dec: -1.85079539\n",
      "| epoch  77 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  2.90 | ppl    18.20 | acc     0.64 | train_ae_norm     1.00\n",
      "[77/200][399/4361] Loss_D: 0.00460945 (Loss_D_real: 0.00357172 Loss_D_fake: 0.00103773) Loss_G: 0.43549687 Loss_Enh_Dec: -1.49677789\n",
      "| epoch  77 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.53 | loss  2.83 | ppl    16.87 | acc     0.65 | train_ae_norm     1.00\n",
      "[77/200][499/4361] Loss_D: 0.00542947 (Loss_D_real: 0.00306577 Loss_D_fake: 0.00236371) Loss_G: 0.42372116 Loss_Enh_Dec: -0.95678520\n",
      "| epoch  77 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.38 | loss  2.89 | ppl    18.04 | acc     0.69 | train_ae_norm     1.00\n",
      "[77/200][599/4361] Loss_D: 0.00251212 (Loss_D_real: 0.00087032 Loss_D_fake: 0.00164180) Loss_G: 0.59891105 Loss_Enh_Dec: -0.45163271\n",
      "| epoch  77 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.61 | loss  2.86 | ppl    17.47 | acc     0.64 | train_ae_norm     1.00\n",
      "[77/200][699/4361] Loss_D: 0.00219595 (Loss_D_real: 0.00109232 Loss_D_fake: 0.00110362) Loss_G: 0.43641558 Loss_Enh_Dec: -1.41722238\n",
      "| epoch  77 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.89 | loss  2.90 | ppl    18.17 | acc     0.67 | train_ae_norm     1.00\n",
      "[77/200][799/4361] Loss_D: 0.00283074 (Loss_D_real: 0.00156627 Loss_D_fake: 0.00126447) Loss_G: 0.38323268 Loss_Enh_Dec: -0.97495335\n",
      "| epoch  77 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.86 | ppl    17.48 | acc     0.67 | train_ae_norm     1.00\n",
      "[77/200][899/4361] Loss_D: 0.00137659 (Loss_D_real: 0.00064158 Loss_D_fake: 0.00073502) Loss_G: 0.48703766 Loss_Enh_Dec: -0.90029871\n",
      "| epoch  77 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.88 | ppl    17.78 | acc     0.70 | train_ae_norm     1.00\n",
      "[77/200][999/4361] Loss_D: 0.00161087 (Loss_D_real: 0.00033352 Loss_D_fake: 0.00127735) Loss_G: 0.39137623 Loss_Enh_Dec: -0.93342000\n",
      "| epoch  77 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  2.86 | ppl    17.45 | acc     0.69 | train_ae_norm     1.00\n",
      "[77/200][1099/4361] Loss_D: 0.03344933 (Loss_D_real: 0.03258863 Loss_D_fake: 0.00086070) Loss_G: 0.46547222 Loss_Enh_Dec: -1.07573831\n",
      "| epoch  77 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  2.87 | ppl    17.60 | acc     0.67 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77/200][1199/4361] Loss_D: 0.08776291 (Loss_D_real: 0.00199201 Loss_D_fake: 0.08577091) Loss_G: 0.72572708 Loss_Enh_Dec: -1.62572765\n",
      "| epoch  77 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.86 | ppl    17.49 | acc     0.69 | train_ae_norm     1.00\n",
      "[77/200][1499/4361] Loss_D: 0.00666342 (Loss_D_real: 0.00518122 Loss_D_fake: 0.00148220) Loss_G: 0.42202851 Loss_Enh_Dec: -1.38136375\n",
      "| epoch  77 |  1500/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  2.94 | ppl    18.93 | acc     0.66 | train_ae_norm     1.00\n",
      "[77/200][1599/4361] Loss_D: 0.00236333 (Loss_D_real: 0.00147088 Loss_D_fake: 0.00089245) Loss_G: 0.52209049 Loss_Enh_Dec: -0.82880020\n",
      "| epoch  77 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  2.91 | ppl    18.39 | acc     0.65 | train_ae_norm     1.00\n",
      "[77/200][1699/4361] Loss_D: 0.00477615 (Loss_D_real: 0.00128986 Loss_D_fake: 0.00348629) Loss_G: 0.39033675 Loss_Enh_Dec: -1.14484048\n",
      "| epoch  77 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  2.87 | ppl    17.56 | acc     0.65 | train_ae_norm     1.00\n",
      "[77/200][1799/4361] Loss_D: 0.00359140 (Loss_D_real: 0.00216783 Loss_D_fake: 0.00142356) Loss_G: 0.45501515 Loss_Enh_Dec: -1.43000829\n",
      "| epoch  77 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.86 | ppl    17.49 | acc     0.68 | train_ae_norm     1.00\n",
      "[77/200][1899/4361] Loss_D: 0.00334189 (Loss_D_real: 0.00128274 Loss_D_fake: 0.00205915) Loss_G: 0.51056856 Loss_Enh_Dec: -1.47403967\n",
      "| epoch  77 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.92 | ppl    18.61 | acc     0.70 | train_ae_norm     1.00\n",
      "[77/200][1999/4361] Loss_D: 0.00336943 (Loss_D_real: 0.00049466 Loss_D_fake: 0.00287477) Loss_G: 0.43526554 Loss_Enh_Dec: -1.65159070\n",
      "| epoch  77 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.84 | ppl    17.03 | acc     0.69 | train_ae_norm     1.00\n",
      "[77/200][2099/4361] Loss_D: 0.00110452 (Loss_D_real: 0.00064157 Loss_D_fake: 0.00046295) Loss_G: 0.53356344 Loss_Enh_Dec: -1.02831185\n",
      "| epoch  77 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.89 | ppl    17.91 | acc     0.68 | train_ae_norm     1.00\n",
      "[77/200][2199/4361] Loss_D: 0.00424019 (Loss_D_real: 0.00336782 Loss_D_fake: 0.00087237) Loss_G: 0.39769474 Loss_Enh_Dec: -0.93991804\n",
      "| epoch  77 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  2.88 | ppl    17.81 | acc     0.65 | train_ae_norm     1.00\n",
      "[77/200][2299/4361] Loss_D: 0.00239780 (Loss_D_real: 0.00130037 Loss_D_fake: 0.00109744) Loss_G: 0.43317762 Loss_Enh_Dec: -1.50034130\n",
      "| epoch  77 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.89 | loss  2.86 | ppl    17.47 | acc     0.69 | train_ae_norm     1.00\n",
      "[77/200][2399/4361] Loss_D: 0.00184548 (Loss_D_real: 0.00034484 Loss_D_fake: 0.00150064) Loss_G: 0.39563066 Loss_Enh_Dec: -1.78589630\n",
      "| epoch  77 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.87 | ppl    17.70 | acc     0.65 | train_ae_norm     1.00\n",
      "[77/200][2499/4361] Loss_D: 0.00775639 (Loss_D_real: 0.00219263 Loss_D_fake: 0.00556377) Loss_G: 0.52222699 Loss_Enh_Dec: -1.49238133\n",
      "| epoch  77 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  2.91 | ppl    18.36 | acc     0.68 | train_ae_norm     1.00\n",
      "[77/200][2599/4361] Loss_D: 0.00088561 (Loss_D_real: 0.00012792 Loss_D_fake: 0.00075768) Loss_G: 0.41800863 Loss_Enh_Dec: -1.59749603\n",
      "| epoch  77 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  2.88 | ppl    17.83 | acc     0.65 | train_ae_norm     1.00\n",
      "[77/200][2699/4361] Loss_D: 0.00755397 (Loss_D_real: 0.00727462 Loss_D_fake: 0.00027936) Loss_G: 0.46864921 Loss_Enh_Dec: -1.50945747\n",
      "| epoch  77 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  2.90 | ppl    18.14 | acc     0.68 | train_ae_norm     1.00\n",
      "[77/200][2799/4361] Loss_D: 0.00275244 (Loss_D_real: 0.00150491 Loss_D_fake: 0.00124752) Loss_G: 0.40510893 Loss_Enh_Dec: -1.42424095\n",
      "| epoch  77 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  2.85 | ppl    17.30 | acc     0.67 | train_ae_norm     1.00\n",
      "[77/200][2899/4361] Loss_D: 0.03494335 (Loss_D_real: 0.03299645 Loss_D_fake: 0.00194690) Loss_G: 0.42995960 Loss_Enh_Dec: -1.39369655\n",
      "| epoch  77 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  2.87 | ppl    17.67 | acc     0.67 | train_ae_norm     1.00\n",
      "[77/200][2999/4361] Loss_D: 0.00121396 (Loss_D_real: 0.00011402 Loss_D_fake: 0.00109994) Loss_G: 0.40932789 Loss_Enh_Dec: -1.69651759\n",
      "| epoch  77 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  2.86 | ppl    17.52 | acc     0.68 | train_ae_norm     1.00\n",
      "[77/200][3099/4361] Loss_D: 0.00309182 (Loss_D_real: 0.00169066 Loss_D_fake: 0.00140115) Loss_G: 0.39898241 Loss_Enh_Dec: -1.89085543\n",
      "| epoch  77 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  2.87 | ppl    17.63 | acc     0.64 | train_ae_norm     1.00\n",
      "[77/200][3199/4361] Loss_D: 0.00196749 (Loss_D_real: 0.00045517 Loss_D_fake: 0.00151232) Loss_G: 0.43924639 Loss_Enh_Dec: -1.85550344\n",
      "| epoch  77 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  2.90 | ppl    18.15 | acc     0.69 | train_ae_norm     1.00\n",
      "[77/200][3299/4361] Loss_D: 0.00160420 (Loss_D_real: 0.00007554 Loss_D_fake: 0.00152866) Loss_G: 0.40788594 Loss_Enh_Dec: -1.48811996\n",
      "| epoch  77 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  2.90 | ppl    18.10 | acc     0.69 | train_ae_norm     1.00\n",
      "[77/200][3399/4361] Loss_D: 0.00208476 (Loss_D_real: 0.00019997 Loss_D_fake: 0.00188479) Loss_G: 0.41481286 Loss_Enh_Dec: -2.02322650\n",
      "| epoch  77 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  2.91 | ppl    18.30 | acc     0.68 | train_ae_norm     1.00\n",
      "[77/200][3499/4361] Loss_D: 0.00505198 (Loss_D_real: 0.00332398 Loss_D_fake: 0.00172800) Loss_G: 0.41581392 Loss_Enh_Dec: -1.88659561\n",
      "| epoch  77 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  2.83 | ppl    16.97 | acc     0.69 | train_ae_norm     1.00\n",
      "[77/200][3599/4361] Loss_D: 0.00395067 (Loss_D_real: 0.00092211 Loss_D_fake: 0.00302856) Loss_G: 0.40664634 Loss_Enh_Dec: -1.61915433\n",
      "| epoch  77 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.06 | loss  2.85 | ppl    17.24 | acc     0.68 | train_ae_norm     1.00\n",
      "[77/200][3699/4361] Loss_D: 0.00220165 (Loss_D_real: 0.00050231 Loss_D_fake: 0.00169934) Loss_G: 0.47249198 Loss_Enh_Dec: -1.17513466\n",
      "| epoch  77 |  3700/ 4361 batches | lr 0.000000 | ms/batch 402.59 | loss  2.87 | ppl    17.67 | acc     0.63 | train_ae_norm     1.00\n",
      "[77/200][3799/4361] Loss_D: 0.00156889 (Loss_D_real: 0.00081227 Loss_D_fake: 0.00075662) Loss_G: 0.43047127 Loss_Enh_Dec: -1.63562298\n",
      "| epoch  77 |  3800/ 4361 batches | lr 0.000000 | ms/batch 402.00 | loss  2.88 | ppl    17.87 | acc     0.70 | train_ae_norm     1.00\n",
      "[77/200][3899/4361] Loss_D: 0.00180551 (Loss_D_real: 0.00023249 Loss_D_fake: 0.00157302) Loss_G: 0.45145965 Loss_Enh_Dec: -1.48146367\n",
      "| epoch  77 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.88 | ppl    17.81 | acc     0.64 | train_ae_norm     1.00\n",
      "[77/200][3999/4361] Loss_D: 0.00206540 (Loss_D_real: 0.00071618 Loss_D_fake: 0.00134922) Loss_G: 0.47692084 Loss_Enh_Dec: -1.74933612\n",
      "| epoch  77 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  2.87 | ppl    17.57 | acc     0.67 | train_ae_norm     1.00\n",
      "[77/200][4099/4361] Loss_D: 0.01157478 (Loss_D_real: 0.00272841 Loss_D_fake: 0.00884637) Loss_G: 0.41887972 Loss_Enh_Dec: -1.61045015\n",
      "| epoch  77 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.85 | ppl    17.33 | acc     0.66 | train_ae_norm     1.00\n",
      "[77/200][4199/4361] Loss_D: 0.00164848 (Loss_D_real: 0.00030408 Loss_D_fake: 0.00134440) Loss_G: 0.50566626 Loss_Enh_Dec: -1.57618260\n",
      "| epoch  77 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  2.89 | ppl    17.97 | acc     0.72 | train_ae_norm     1.00\n",
      "[77/200][4299/4361] Loss_D: 0.00281547 (Loss_D_real: 0.00116003 Loss_D_fake: 0.00165544) Loss_G: 0.42961255 Loss_Enh_Dec: -1.82569778\n",
      "| epoch  77 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  2.85 | ppl    17.21 | acc     0.70 | train_ae_norm     1.00\n",
      "| end of epoch  77 | time: 1852.24s | test loss  2.85 | test ppl 17.34 | acc 0.707\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 78 / 200 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.706\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.480\n",
      "  Test Loss: 4.265\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  78 |     0/ 4361 batches | lr 0.000000 | ms/batch 863.75 | loss  0.03 | ppl     1.03 | acc     0.72 | train_ae_norm     1.00\n",
      "[78/200][99/4361] Loss_D: 0.00366827 (Loss_D_real: 0.00039684 Loss_D_fake: 0.00327144) Loss_G: 0.43568632 Loss_Enh_Dec: -1.49136460\n",
      "| epoch  78 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  2.87 | ppl    17.61 | acc     0.66 | train_ae_norm     1.00\n",
      "[78/200][199/4361] Loss_D: 0.00412829 (Loss_D_real: 0.00214759 Loss_D_fake: 0.00198070) Loss_G: 0.45560342 Loss_Enh_Dec: -1.34460700\n",
      "| epoch  78 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  2.90 | ppl    18.19 | acc     0.68 | train_ae_norm     1.00\n",
      "[78/200][299/4361] Loss_D: 0.00534935 (Loss_D_real: 0.00395298 Loss_D_fake: 0.00139636) Loss_G: 0.43896762 Loss_Enh_Dec: -1.60841978\n",
      "| epoch  78 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  2.89 | ppl    17.91 | acc     0.62 | train_ae_norm     1.00\n",
      "[78/200][399/4361] Loss_D: 0.00958113 (Loss_D_real: 0.00843773 Loss_D_fake: 0.00114340) Loss_G: 0.40436968 Loss_Enh_Dec: -1.91515660\n",
      "| epoch  78 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  2.82 | ppl    16.85 | acc     0.69 | train_ae_norm     1.00\n",
      "[78/200][499/4361] Loss_D: 0.00211050 (Loss_D_real: 0.00090205 Loss_D_fake: 0.00120845) Loss_G: 0.42227823 Loss_Enh_Dec: -1.43934286\n",
      "| epoch  78 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.74 | loss  2.87 | ppl    17.65 | acc     0.71 | train_ae_norm     1.00\n",
      "[78/200][599/4361] Loss_D: 0.00469305 (Loss_D_real: 0.00326867 Loss_D_fake: 0.00142439) Loss_G: 0.59254342 Loss_Enh_Dec: -1.32021427\n",
      "| epoch  78 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  2.84 | ppl    17.09 | acc     0.65 | train_ae_norm     1.00\n",
      "[78/200][699/4361] Loss_D: 0.00430670 (Loss_D_real: 0.00103258 Loss_D_fake: 0.00327412) Loss_G: 0.45447770 Loss_Enh_Dec: -1.44051552\n",
      "| epoch  78 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.87 | ppl    17.63 | acc     0.67 | train_ae_norm     1.00\n",
      "[78/200][799/4361] Loss_D: 0.00326762 (Loss_D_real: 0.00243653 Loss_D_fake: 0.00083110) Loss_G: 0.60847706 Loss_Enh_Dec: -1.18563390\n",
      "| epoch  78 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  2.84 | ppl    17.10 | acc     0.67 | train_ae_norm     1.00\n",
      "[78/200][899/4361] Loss_D: 0.00327533 (Loss_D_real: 0.00118714 Loss_D_fake: 0.00208819) Loss_G: 0.45702124 Loss_Enh_Dec: -1.24996519\n",
      "| epoch  78 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.77 | loss  2.87 | ppl    17.55 | acc     0.68 | train_ae_norm     1.00\n",
      "[78/200][999/4361] Loss_D: 0.00047646 (Loss_D_real: 0.00011216 Loss_D_fake: 0.00036430) Loss_G: 0.49153501 Loss_Enh_Dec: -1.10924053\n",
      "| epoch  78 |  1000/ 4361 batches | lr 0.000000 | ms/batch 400.68 | loss  2.85 | ppl    17.23 | acc     0.70 | train_ae_norm     1.00\n",
      "[78/200][1099/4361] Loss_D: 0.00127707 (Loss_D_real: 0.00028479 Loss_D_fake: 0.00099228) Loss_G: 0.43777734 Loss_Enh_Dec: -1.16085231\n",
      "| epoch  78 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.41 | loss  2.83 | ppl    16.94 | acc     0.67 | train_ae_norm     1.00\n",
      "[78/200][1199/4361] Loss_D: 0.00242667 (Loss_D_real: 0.00089984 Loss_D_fake: 0.00152683) Loss_G: 0.40848327 Loss_Enh_Dec: -1.34300184\n",
      "| epoch  78 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  2.85 | ppl    17.25 | acc     0.70 | train_ae_norm     1.00\n",
      "[78/200][1299/4361] Loss_D: 0.00322015 (Loss_D_real: 0.00101660 Loss_D_fake: 0.00220355) Loss_G: 0.40898782 Loss_Enh_Dec: -1.52266073\n",
      "| epoch  78 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.24 | loss  2.87 | ppl    17.64 | acc     0.68 | train_ae_norm     1.00\n",
      "[78/200][1399/4361] Loss_D: 0.00200815 (Loss_D_real: 0.00107694 Loss_D_fake: 0.00093122) Loss_G: 0.45405036 Loss_Enh_Dec: -1.26288354\n",
      "| epoch  78 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.92 | loss  2.87 | ppl    17.65 | acc     0.64 | train_ae_norm     1.00\n",
      "[78/200][1499/4361] Loss_D: 0.00194768 (Loss_D_real: 0.00047319 Loss_D_fake: 0.00147449) Loss_G: 0.45988041 Loss_Enh_Dec: -1.38487351\n",
      "| epoch  78 |  1500/ 4361 batches | lr 0.000000 | ms/batch 400.28 | loss  2.93 | ppl    18.76 | acc     0.66 | train_ae_norm     1.00\n",
      "[78/200][1599/4361] Loss_D: 0.00190414 (Loss_D_real: 0.00032831 Loss_D_fake: 0.00157583) Loss_G: 0.42501432 Loss_Enh_Dec: -1.62778020\n",
      "| epoch  78 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.88 | ppl    17.73 | acc     0.67 | train_ae_norm     1.00\n",
      "[78/200][1699/4361] Loss_D: 0.00621096 (Loss_D_real: 0.00537692 Loss_D_fake: 0.00083404) Loss_G: 0.42609406 Loss_Enh_Dec: -1.28485930\n",
      "| epoch  78 |  1700/ 4361 batches | lr 0.000000 | ms/batch 402.29 | loss  2.85 | ppl    17.21 | acc     0.66 | train_ae_norm     1.00\n",
      "[78/200][1799/4361] Loss_D: 0.00223912 (Loss_D_real: 0.00066834 Loss_D_fake: 0.00157078) Loss_G: 0.46553302 Loss_Enh_Dec: -1.73877835\n",
      "| epoch  78 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  2.81 | ppl    16.62 | acc     0.69 | train_ae_norm     1.00\n",
      "[78/200][1899/4361] Loss_D: 0.00217607 (Loss_D_real: 0.00081174 Loss_D_fake: 0.00136433) Loss_G: 0.48339015 Loss_Enh_Dec: -1.70082152\n",
      "| epoch  78 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  2.87 | ppl    17.71 | acc     0.68 | train_ae_norm     1.00\n",
      "[78/200][1999/4361] Loss_D: 0.00218321 (Loss_D_real: 0.00013702 Loss_D_fake: 0.00204619) Loss_G: 0.51242614 Loss_Enh_Dec: -1.61114883\n",
      "| epoch  78 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  2.83 | ppl    16.91 | acc     0.68 | train_ae_norm     1.00\n",
      "[78/200][2099/4361] Loss_D: 0.00978425 (Loss_D_real: 0.00843658 Loss_D_fake: 0.00134767) Loss_G: 0.44465485 Loss_Enh_Dec: -2.18895793\n",
      "| epoch  78 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.86 | ppl    17.53 | acc     0.68 | train_ae_norm     1.00\n",
      "[78/200][2199/4361] Loss_D: 0.00051840 (Loss_D_real: 0.00018178 Loss_D_fake: 0.00033662) Loss_G: 0.45397383 Loss_Enh_Dec: -1.52900374\n",
      "| epoch  78 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.85 | ppl    17.31 | acc     0.70 | train_ae_norm     1.00\n",
      "[78/200][2299/4361] Loss_D: 0.00150360 (Loss_D_real: 0.00073174 Loss_D_fake: 0.00077186) Loss_G: 0.50720853 Loss_Enh_Dec: -1.98681927\n",
      "| epoch  78 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.84 | ppl    17.14 | acc     0.71 | train_ae_norm     1.00\n",
      "[78/200][2399/4361] Loss_D: 0.00111538 (Loss_D_real: 0.00062532 Loss_D_fake: 0.00049006) Loss_G: 0.41824684 Loss_Enh_Dec: -1.70408762\n",
      "| epoch  78 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  2.85 | ppl    17.33 | acc     0.65 | train_ae_norm     1.00\n",
      "[78/200][2499/4361] Loss_D: 0.00080036 (Loss_D_real: 0.00029367 Loss_D_fake: 0.00050669) Loss_G: 0.42693025 Loss_Enh_Dec: -1.90263426\n",
      "| epoch  78 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  2.91 | ppl    18.28 | acc     0.69 | train_ae_norm     1.00\n",
      "[78/200][2599/4361] Loss_D: 0.00147891 (Loss_D_real: 0.00095700 Loss_D_fake: 0.00052191) Loss_G: 0.46334514 Loss_Enh_Dec: -2.02285480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  78 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.87 | ppl    17.59 | acc     0.64 | train_ae_norm     1.00\n",
      "[78/200][2699/4361] Loss_D: 0.00149030 (Loss_D_real: 0.00013583 Loss_D_fake: 0.00135447) Loss_G: 0.42890984 Loss_Enh_Dec: -2.05236793\n",
      "| epoch  78 |  2700/ 4361 batches | lr 0.000000 | ms/batch 400.85 | loss  2.86 | ppl    17.52 | acc     0.69 | train_ae_norm     1.00\n",
      "[78/200][2799/4361] Loss_D: 0.00105115 (Loss_D_real: 0.00040742 Loss_D_fake: 0.00064373) Loss_G: 0.42310148 Loss_Enh_Dec: -1.34745395\n",
      "| epoch  78 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  2.82 | ppl    16.82 | acc     0.66 | train_ae_norm     1.00\n",
      "[78/200][2899/4361] Loss_D: 0.00234774 (Loss_D_real: 0.00123844 Loss_D_fake: 0.00110931) Loss_G: 0.51279038 Loss_Enh_Dec: -1.52169216\n",
      "| epoch  78 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  2.85 | ppl    17.24 | acc     0.68 | train_ae_norm     1.00\n",
      "[78/200][2999/4361] Loss_D: 0.00063151 (Loss_D_real: 0.00010083 Loss_D_fake: 0.00053068) Loss_G: 0.44856605 Loss_Enh_Dec: -1.66932392\n",
      "| epoch  78 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  2.84 | ppl    17.19 | acc     0.68 | train_ae_norm     1.00\n",
      "[78/200][3099/4361] Loss_D: 0.00081566 (Loss_D_real: 0.00040344 Loss_D_fake: 0.00041222) Loss_G: 0.45007855 Loss_Enh_Dec: -1.96352029\n",
      "| epoch  78 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  2.84 | ppl    17.15 | acc     0.64 | train_ae_norm     1.00\n",
      "[78/200][3199/4361] Loss_D: 0.01383955 (Loss_D_real: 0.01266568 Loss_D_fake: 0.00117387) Loss_G: 0.78070349 Loss_Enh_Dec: -1.94468141\n",
      "| epoch  78 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  2.89 | ppl    17.91 | acc     0.68 | train_ae_norm     1.00\n",
      "[78/200][3299/4361] Loss_D: 0.00073399 (Loss_D_real: 0.00047498 Loss_D_fake: 0.00025901) Loss_G: 0.57980287 Loss_Enh_Dec: -1.67230797\n",
      "| epoch  78 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.89 | ppl    18.05 | acc     0.66 | train_ae_norm     1.00\n",
      "[78/200][3399/4361] Loss_D: 0.00100672 (Loss_D_real: 0.00040217 Loss_D_fake: 0.00060455) Loss_G: 0.50152344 Loss_Enh_Dec: -1.55411208\n",
      "| epoch  78 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  2.86 | ppl    17.49 | acc     0.68 | train_ae_norm     1.00\n",
      "[78/200][3499/4361] Loss_D: 0.00146511 (Loss_D_real: 0.00013437 Loss_D_fake: 0.00133075) Loss_G: 0.48883152 Loss_Enh_Dec: -1.97525978\n",
      "| epoch  78 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  2.82 | ppl    16.70 | acc     0.68 | train_ae_norm     1.00\n",
      "[78/200][3599/4361] Loss_D: 0.00188616 (Loss_D_real: 0.00128157 Loss_D_fake: 0.00060459) Loss_G: 0.49453259 Loss_Enh_Dec: -1.65993428\n",
      "| epoch  78 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.81 | ppl    16.66 | acc     0.68 | train_ae_norm     1.00\n",
      "[78/200][3699/4361] Loss_D: 0.00077697 (Loss_D_real: 0.00009944 Loss_D_fake: 0.00067753) Loss_G: 0.48722777 Loss_Enh_Dec: -1.36171854\n",
      "| epoch  78 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  2.83 | ppl    16.94 | acc     0.65 | train_ae_norm     1.00\n",
      "[78/200][3799/4361] Loss_D: 0.00297010 (Loss_D_real: 0.00199453 Loss_D_fake: 0.00097557) Loss_G: 0.50441247 Loss_Enh_Dec: -1.86790931\n",
      "| epoch  78 |  3800/ 4361 batches | lr 0.000000 | ms/batch 400.90 | loss  2.87 | ppl    17.62 | acc     0.71 | train_ae_norm     1.00\n",
      "[78/200][3899/4361] Loss_D: 0.00348176 (Loss_D_real: 0.00117146 Loss_D_fake: 0.00231030) Loss_G: 0.44963977 Loss_Enh_Dec: -1.71874905\n",
      "| epoch  78 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  2.89 | ppl    18.08 | acc     0.66 | train_ae_norm     1.00\n",
      "[78/200][3999/4361] Loss_D: 0.00137878 (Loss_D_real: 0.00060691 Loss_D_fake: 0.00077187) Loss_G: 0.44171134 Loss_Enh_Dec: -2.10271144\n",
      "| epoch  78 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  2.84 | ppl    17.19 | acc     0.67 | train_ae_norm     1.00\n",
      "[78/200][4099/4361] Loss_D: 0.00158657 (Loss_D_real: 0.00014545 Loss_D_fake: 0.00144112) Loss_G: 0.41439018 Loss_Enh_Dec: -2.01468968\n",
      "| epoch  78 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  2.81 | ppl    16.61 | acc     0.69 | train_ae_norm     1.00\n",
      "[78/200][4199/4361] Loss_D: 0.00069095 (Loss_D_real: 0.00026665 Loss_D_fake: 0.00042430) Loss_G: 0.47534919 Loss_Enh_Dec: -1.96078897\n",
      "| epoch  78 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.85 | ppl    17.29 | acc     0.71 | train_ae_norm     1.00\n",
      "[78/200][4299/4361] Loss_D: 0.00070858 (Loss_D_real: 0.00015252 Loss_D_fake: 0.00055606) Loss_G: 0.44503918 Loss_Enh_Dec: -1.79157472\n",
      "| epoch  78 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  2.81 | ppl    16.64 | acc     0.69 | train_ae_norm     1.00\n",
      "| end of epoch  78 | time: 1852.33s | test loss  2.81 | test ppl 16.59 | acc 0.710\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 79 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.709\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.480\n",
      "  Test Loss: 4.165\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  79 |     0/ 4361 batches | lr 0.000000 | ms/batch 866.00 | loss  0.03 | ppl     1.03 | acc     0.70 | train_ae_norm     1.00\n",
      "[79/200][99/4361] Loss_D: 0.00123662 (Loss_D_real: 0.00051678 Loss_D_fake: 0.00071984) Loss_G: 0.48593932 Loss_Enh_Dec: -1.97301698\n",
      "| epoch  79 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.04 | loss  2.83 | ppl    17.01 | acc     0.65 | train_ae_norm     1.00\n",
      "[79/200][199/4361] Loss_D: 0.00265354 (Loss_D_real: 0.00016982 Loss_D_fake: 0.00248372) Loss_G: 0.51044977 Loss_Enh_Dec: -1.63644063\n",
      "| epoch  79 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  2.86 | ppl    17.47 | acc     0.68 | train_ae_norm     1.00\n",
      "[79/200][299/4361] Loss_D: 0.00392874 (Loss_D_real: 0.00207161 Loss_D_fake: 0.00185713) Loss_G: 0.50946915 Loss_Enh_Dec: -1.88269961\n",
      "| epoch  79 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  2.86 | ppl    17.52 | acc     0.65 | train_ae_norm     1.00\n",
      "[79/200][399/4361] Loss_D: 0.00376314 (Loss_D_real: 0.00323061 Loss_D_fake: 0.00053253) Loss_G: 0.46185693 Loss_Enh_Dec: -1.55273283\n",
      "| epoch  79 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.77 | ppl    15.89 | acc     0.69 | train_ae_norm     1.00\n",
      "[79/200][499/4361] Loss_D: 0.00180269 (Loss_D_real: 0.00044987 Loss_D_fake: 0.00135281) Loss_G: 0.43441573 Loss_Enh_Dec: -1.87012327\n",
      "| epoch  79 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  2.84 | ppl    17.05 | acc     0.69 | train_ae_norm     1.00\n",
      "[79/200][599/4361] Loss_D: 0.00474523 (Loss_D_real: 0.00453527 Loss_D_fake: 0.00020996) Loss_G: 0.62717295 Loss_Enh_Dec: -1.57340086\n",
      "| epoch  79 |   600/ 4361 batches | lr 0.000000 | ms/batch 402.14 | loss  2.79 | ppl    16.27 | acc     0.67 | train_ae_norm     1.00\n",
      "[79/200][699/4361] Loss_D: 0.00793920 (Loss_D_real: 0.00193723 Loss_D_fake: 0.00600197) Loss_G: 0.50249416 Loss_Enh_Dec: -1.64782298\n",
      "| epoch  79 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.05 | loss  2.82 | ppl    16.80 | acc     0.68 | train_ae_norm     1.00\n",
      "[79/200][799/4361] Loss_D: 0.00261231 (Loss_D_real: 0.00128380 Loss_D_fake: 0.00132851) Loss_G: 0.45239982 Loss_Enh_Dec: -1.57164896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  79 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  2.80 | ppl    16.45 | acc     0.68 | train_ae_norm     1.00\n",
      "[79/200][899/4361] Loss_D: 0.00327274 (Loss_D_real: 0.00073582 Loss_D_fake: 0.00253691) Loss_G: 0.48251700 Loss_Enh_Dec: -1.37457228\n",
      "| epoch  79 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.83 | ppl    16.97 | acc     0.71 | train_ae_norm     1.00\n",
      "[79/200][999/4361] Loss_D: 0.00137679 (Loss_D_real: 0.00087348 Loss_D_fake: 0.00050332) Loss_G: 0.51288593 Loss_Enh_Dec: -1.41704643\n",
      "| epoch  79 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  2.83 | ppl    16.87 | acc     0.70 | train_ae_norm     1.00\n",
      "[79/200][1099/4361] Loss_D: 0.03380706 (Loss_D_real: 0.03312711 Loss_D_fake: 0.00067995) Loss_G: 0.41894242 Loss_Enh_Dec: -1.58395326\n",
      "| epoch  79 |  1100/ 4361 batches | lr 0.000000 | ms/batch 402.08 | loss  2.82 | ppl    16.70 | acc     0.68 | train_ae_norm     1.00\n",
      "[79/200][1199/4361] Loss_D: 0.00356578 (Loss_D_real: 0.00283102 Loss_D_fake: 0.00073476) Loss_G: 0.49569130 Loss_Enh_Dec: -1.28115141\n",
      "| epoch  79 |  1200/ 4361 batches | lr 0.000000 | ms/batch 402.13 | loss  2.82 | ppl    16.79 | acc     0.71 | train_ae_norm     1.00\n",
      "[79/200][1299/4361] Loss_D: 0.00581222 (Loss_D_real: 0.00551909 Loss_D_fake: 0.00029313) Loss_G: 0.46737194 Loss_Enh_Dec: -1.59475732\n",
      "| epoch  79 |  1300/ 4361 batches | lr 0.000000 | ms/batch 402.53 | loss  2.84 | ppl    17.14 | acc     0.69 | train_ae_norm     1.00\n",
      "[79/200][1399/4361] Loss_D: 0.00146733 (Loss_D_real: 0.00060712 Loss_D_fake: 0.00086021) Loss_G: 0.45388317 Loss_Enh_Dec: -1.55896628\n",
      "| epoch  79 |  1400/ 4361 batches | lr 0.000000 | ms/batch 402.31 | loss  2.84 | ppl    17.05 | acc     0.65 | train_ae_norm     1.00\n",
      "[79/200][1499/4361] Loss_D: 0.00091798 (Loss_D_real: 0.00035804 Loss_D_fake: 0.00055994) Loss_G: 0.43022129 Loss_Enh_Dec: -1.62509120\n",
      "| epoch  79 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  2.89 | ppl    17.95 | acc     0.68 | train_ae_norm     1.00\n",
      "[79/200][1599/4361] Loss_D: 0.00087660 (Loss_D_real: 0.00032374 Loss_D_fake: 0.00055286) Loss_G: 0.42996082 Loss_Enh_Dec: -1.97707939\n",
      "| epoch  79 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  2.88 | ppl    17.79 | acc     0.65 | train_ae_norm     1.00\n",
      "[79/200][1699/4361] Loss_D: 0.00141955 (Loss_D_real: 0.00077421 Loss_D_fake: 0.00064534) Loss_G: 0.62487251 Loss_Enh_Dec: -1.26570308\n",
      "| epoch  79 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  2.82 | ppl    16.81 | acc     0.68 | train_ae_norm     1.00\n",
      "[79/200][1799/4361] Loss_D: 0.00210399 (Loss_D_real: 0.00096525 Loss_D_fake: 0.00113875) Loss_G: 0.47026554 Loss_Enh_Dec: -0.91564369\n",
      "| epoch  79 |  1800/ 4361 batches | lr 0.000000 | ms/batch 402.05 | loss  2.81 | ppl    16.64 | acc     0.67 | train_ae_norm     1.00\n",
      "[79/200][1899/4361] Loss_D: 0.00256547 (Loss_D_real: 0.00072836 Loss_D_fake: 0.00183711) Loss_G: 0.42785454 Loss_Enh_Dec: -1.49652505\n",
      "| epoch  79 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  2.87 | ppl    17.58 | acc     0.69 | train_ae_norm     1.00\n",
      "[79/200][1999/4361] Loss_D: 0.00345813 (Loss_D_real: 0.00122595 Loss_D_fake: 0.00223218) Loss_G: 0.44132909 Loss_Enh_Dec: -1.54876590\n",
      "| epoch  79 |  2000/ 4361 batches | lr 0.000000 | ms/batch 402.17 | loss  2.80 | ppl    16.37 | acc     0.69 | train_ae_norm     1.00\n",
      "[79/200][2099/4361] Loss_D: 0.00268985 (Loss_D_real: 0.00189002 Loss_D_fake: 0.00079982) Loss_G: 0.43883428 Loss_Enh_Dec: -1.05584133\n",
      "| epoch  79 |  2100/ 4361 batches | lr 0.000000 | ms/batch 402.26 | loss  2.82 | ppl    16.80 | acc     0.70 | train_ae_norm     1.00\n",
      "[79/200][2199/4361] Loss_D: 0.00134553 (Loss_D_real: 0.00104542 Loss_D_fake: 0.00030011) Loss_G: 0.48893815 Loss_Enh_Dec: -1.53668475\n",
      "| epoch  79 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  2.80 | ppl    16.48 | acc     0.66 | train_ae_norm     1.00\n",
      "[79/200][2299/4361] Loss_D: 0.00209647 (Loss_D_real: 0.00028706 Loss_D_fake: 0.00180941) Loss_G: 0.50223917 Loss_Enh_Dec: -1.21859467\n",
      "| epoch  79 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  2.79 | ppl    16.31 | acc     0.70 | train_ae_norm     1.00\n",
      "[79/200][2399/4361] Loss_D: 0.00092561 (Loss_D_real: 0.00038818 Loss_D_fake: 0.00053743) Loss_G: 0.45508280 Loss_Enh_Dec: -1.61411440\n",
      "| epoch  79 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  2.81 | ppl    16.60 | acc     0.65 | train_ae_norm     1.00\n",
      "[79/200][2499/4361] Loss_D: 0.00365981 (Loss_D_real: 0.00047390 Loss_D_fake: 0.00318591) Loss_G: 0.42440686 Loss_Enh_Dec: -1.03431666\n",
      "| epoch  79 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  2.86 | ppl    17.48 | acc     0.68 | train_ae_norm     1.00\n",
      "[79/200][2599/4361] Loss_D: 0.00179212 (Loss_D_real: 0.00072785 Loss_D_fake: 0.00106427) Loss_G: 0.47673789 Loss_Enh_Dec: -1.80870080\n",
      "| epoch  79 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  2.83 | ppl    16.91 | acc     0.65 | train_ae_norm     1.00\n",
      "[79/200][2699/4361] Loss_D: 0.00212149 (Loss_D_real: 0.00089570 Loss_D_fake: 0.00122579) Loss_G: 0.46896037 Loss_Enh_Dec: -1.57300901\n",
      "| epoch  79 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.82 | ppl    16.72 | acc     0.68 | train_ae_norm     1.00\n",
      "[79/200][2799/4361] Loss_D: 0.00717953 (Loss_D_real: 0.00522403 Loss_D_fake: 0.00195550) Loss_G: 0.89225465 Loss_Enh_Dec: -1.13224089\n",
      "| epoch  79 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  2.77 | ppl    16.02 | acc     0.68 | train_ae_norm     1.00\n",
      "[79/200][2899/4361] Loss_D: 0.00194243 (Loss_D_real: 0.00087478 Loss_D_fake: 0.00106766) Loss_G: 0.50081962 Loss_Enh_Dec: -1.49223650\n",
      "| epoch  79 |  2900/ 4361 batches | lr 0.000000 | ms/batch 402.09 | loss  2.80 | ppl    16.44 | acc     0.67 | train_ae_norm     1.00\n",
      "[79/200][2999/4361] Loss_D: 0.00200424 (Loss_D_real: 0.00052377 Loss_D_fake: 0.00148047) Loss_G: 0.48860589 Loss_Enh_Dec: -1.10483396\n",
      "| epoch  79 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  2.80 | ppl    16.42 | acc     0.70 | train_ae_norm     1.00\n",
      "[79/200][3099/4361] Loss_D: 0.00116319 (Loss_D_real: 0.00054018 Loss_D_fake: 0.00062301) Loss_G: 0.41763726 Loss_Enh_Dec: -1.04902554\n",
      "| epoch  79 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  2.81 | ppl    16.64 | acc     0.64 | train_ae_norm     1.00\n",
      "[79/200][3199/4361] Loss_D: 0.00073071 (Loss_D_real: 0.00038876 Loss_D_fake: 0.00034195) Loss_G: 0.48157588 Loss_Enh_Dec: -1.40245557\n",
      "| epoch  79 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  2.84 | ppl    17.11 | acc     0.68 | train_ae_norm     1.00\n",
      "[79/200][3299/4361] Loss_D: 0.00161771 (Loss_D_real: 0.00025379 Loss_D_fake: 0.00136392) Loss_G: 0.46036229 Loss_Enh_Dec: -0.92817557\n",
      "| epoch  79 |  3300/ 4361 batches | lr 0.000000 | ms/batch 402.58 | loss  2.84 | ppl    17.18 | acc     0.69 | train_ae_norm     1.00\n",
      "[79/200][3399/4361] Loss_D: 0.00141608 (Loss_D_real: 0.00032513 Loss_D_fake: 0.00109095) Loss_G: 0.41434985 Loss_Enh_Dec: -1.45733440\n",
      "| epoch  79 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  2.83 | ppl    16.88 | acc     0.67 | train_ae_norm     1.00\n",
      "[79/200][3499/4361] Loss_D: 0.00241882 (Loss_D_real: 0.00058146 Loss_D_fake: 0.00183737) Loss_G: 0.39839980 Loss_Enh_Dec: -1.29411924\n",
      "| epoch  79 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  2.76 | ppl    15.82 | acc     0.67 | train_ae_norm     1.00\n",
      "[79/200][3599/4361] Loss_D: 0.00195521 (Loss_D_real: 0.00057537 Loss_D_fake: 0.00137984) Loss_G: 0.41586810 Loss_Enh_Dec: -1.56342494\n",
      "| epoch  79 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  2.77 | ppl    15.99 | acc     0.69 | train_ae_norm     1.00\n",
      "[79/200][3699/4361] Loss_D: 0.00068466 (Loss_D_real: 0.00026803 Loss_D_fake: 0.00041663) Loss_G: 0.46576306 Loss_Enh_Dec: -1.30740237\n",
      "| epoch  79 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  2.80 | ppl    16.36 | acc     0.67 | train_ae_norm     1.00\n",
      "[79/200][3799/4361] Loss_D: 0.00135685 (Loss_D_real: 0.00034685 Loss_D_fake: 0.00100999) Loss_G: 0.42204782 Loss_Enh_Dec: -1.42943180\n",
      "| epoch  79 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  2.80 | ppl    16.49 | acc     0.72 | train_ae_norm     1.00\n",
      "[79/200][3899/4361] Loss_D: 1.21162963 (Loss_D_real: 0.00144561 Loss_D_fake: 1.21018398) Loss_G: 0.91409796 Loss_Enh_Dec: -0.76532215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  79 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.81 | ppl    16.64 | acc     0.65 | train_ae_norm     1.00\n",
      "[79/200][3999/4361] Loss_D: 0.00357600 (Loss_D_real: 0.00236772 Loss_D_fake: 0.00120829) Loss_G: 0.56404918 Loss_Enh_Dec: -0.76773721\n",
      "| epoch  79 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.65 | loss  2.83 | ppl    16.90 | acc     0.67 | train_ae_norm     1.00\n",
      "[79/200][4099/4361] Loss_D: 0.00148150 (Loss_D_real: 0.00051925 Loss_D_fake: 0.00096225) Loss_G: 0.45135275 Loss_Enh_Dec: -1.03425276\n",
      "| epoch  79 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.39 | loss  2.77 | ppl    15.94 | acc     0.68 | train_ae_norm     1.00\n",
      "[79/200][4199/4361] Loss_D: 0.00115652 (Loss_D_real: 0.00033711 Loss_D_fake: 0.00081941) Loss_G: 0.59767687 Loss_Enh_Dec: -0.54081804\n",
      "| epoch  79 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.83 | ppl    16.88 | acc     0.73 | train_ae_norm     1.00\n",
      "[79/200][4299/4361] Loss_D: 0.00221041 (Loss_D_real: 0.00082974 Loss_D_fake: 0.00138067) Loss_G: 0.48480746 Loss_Enh_Dec: -0.92799550\n",
      "| epoch  79 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.77 | ppl    16.03 | acc     0.69 | train_ae_norm     1.00\n",
      "| end of epoch  79 | time: 1853.86s | test loss  2.79 | test ppl 16.20 | acc 0.712\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 80 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.702\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.495\n",
      "  Test Loss: 4.244\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  80 |     0/ 4361 batches | lr 0.000000 | ms/batch 867.27 | loss  0.02 | ppl     1.02 | acc     0.72 | train_ae_norm     1.00\n",
      "[80/200][99/4361] Loss_D: 0.00161721 (Loss_D_real: 0.00019858 Loss_D_fake: 0.00141863) Loss_G: 0.40024987 Loss_Enh_Dec: -0.79533023\n",
      "| epoch  80 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  2.79 | ppl    16.24 | acc     0.65 | train_ae_norm     1.00\n",
      "[80/200][199/4361] Loss_D: 0.00957903 (Loss_D_real: 0.00927334 Loss_D_fake: 0.00030569) Loss_G: 0.45737168 Loss_Enh_Dec: -1.17062950\n",
      "| epoch  80 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  2.81 | ppl    16.55 | acc     0.69 | train_ae_norm     1.00\n",
      "[80/200][299/4361] Loss_D: 0.00119166 (Loss_D_real: 0.00099482 Loss_D_fake: 0.00019684) Loss_G: 0.43827325 Loss_Enh_Dec: -1.21274340\n",
      "| epoch  80 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  2.81 | ppl    16.69 | acc     0.65 | train_ae_norm     1.00\n",
      "[80/200][399/4361] Loss_D: 0.00452438 (Loss_D_real: 0.00361644 Loss_D_fake: 0.00090794) Loss_G: 0.41171399 Loss_Enh_Dec: -0.79635406\n",
      "| epoch  80 |   400/ 4361 batches | lr 0.000000 | ms/batch 402.25 | loss  2.74 | ppl    15.44 | acc     0.67 | train_ae_norm     1.00\n",
      "[80/200][499/4361] Loss_D: 0.00220030 (Loss_D_real: 0.00090157 Loss_D_fake: 0.00129873) Loss_G: 0.41023406 Loss_Enh_Dec: -0.64484859\n",
      "| epoch  80 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  2.80 | ppl    16.46 | acc     0.71 | train_ae_norm     1.00\n",
      "[80/200][599/4361] Loss_D: 0.00292007 (Loss_D_real: 0.00059604 Loss_D_fake: 0.00232403) Loss_G: 0.56760943 Loss_Enh_Dec: -1.27245331\n",
      "| epoch  80 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.77 | loss  2.75 | ppl    15.68 | acc     0.66 | train_ae_norm     1.00\n",
      "[80/200][699/4361] Loss_D: 0.00058565 (Loss_D_real: 0.00028202 Loss_D_fake: 0.00030363) Loss_G: 0.46912980 Loss_Enh_Dec: -1.02844799\n",
      "| epoch  80 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.79 | ppl    16.35 | acc     0.69 | train_ae_norm     1.00\n",
      "[80/200][799/4361] Loss_D: 0.00322490 (Loss_D_real: 0.00053807 Loss_D_fake: 0.00268683) Loss_G: 0.47553411 Loss_Enh_Dec: -0.75721872\n",
      "| epoch  80 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  2.77 | ppl    15.89 | acc     0.69 | train_ae_norm     1.00\n",
      "[80/200][899/4361] Loss_D: 0.00132474 (Loss_D_real: 0.00044535 Loss_D_fake: 0.00087939) Loss_G: 0.43660975 Loss_Enh_Dec: -1.56401527\n",
      "| epoch  80 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  2.79 | ppl    16.28 | acc     0.70 | train_ae_norm     1.00\n",
      "[80/200][999/4361] Loss_D: 0.00400967 (Loss_D_real: 0.00042168 Loss_D_fake: 0.00358799) Loss_G: 0.53033990 Loss_Enh_Dec: -1.06272340\n",
      "| epoch  80 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.08 | loss  2.80 | ppl    16.41 | acc     0.70 | train_ae_norm     1.00\n",
      "[80/200][1099/4361] Loss_D: 0.00873345 (Loss_D_real: 0.00038512 Loss_D_fake: 0.00834833) Loss_G: 0.52377999 Loss_Enh_Dec: -1.17351902\n",
      "| epoch  80 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.79 | ppl    16.22 | acc     0.65 | train_ae_norm     1.00\n",
      "[80/200][1199/4361] Loss_D: 0.00107458 (Loss_D_real: 0.00046543 Loss_D_fake: 0.00060915) Loss_G: 0.58514541 Loss_Enh_Dec: -1.35205841\n",
      "| epoch  80 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.94 | loss  2.80 | ppl    16.37 | acc     0.71 | train_ae_norm     1.00\n",
      "[80/200][1299/4361] Loss_D: 0.00200660 (Loss_D_real: 0.00017543 Loss_D_fake: 0.00183116) Loss_G: 0.47830603 Loss_Enh_Dec: -1.35487926\n",
      "| epoch  80 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  2.79 | ppl    16.34 | acc     0.71 | train_ae_norm     1.00\n",
      "[80/200][1399/4361] Loss_D: 0.00065877 (Loss_D_real: 0.00016686 Loss_D_fake: 0.00049191) Loss_G: 0.50218260 Loss_Enh_Dec: -1.52779603\n",
      "| epoch  80 |  1400/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  2.83 | ppl    16.86 | acc     0.64 | train_ae_norm     1.00\n",
      "[80/200][1499/4361] Loss_D: 0.00235986 (Loss_D_real: 0.00072105 Loss_D_fake: 0.00163881) Loss_G: 0.48953161 Loss_Enh_Dec: -1.51903415\n",
      "| epoch  80 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  2.85 | ppl    17.28 | acc     0.66 | train_ae_norm     1.00\n",
      "[80/200][1599/4361] Loss_D: 0.00067174 (Loss_D_real: 0.00014975 Loss_D_fake: 0.00052199) Loss_G: 0.44894662 Loss_Enh_Dec: -1.43523443\n",
      "| epoch  80 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  2.82 | ppl    16.77 | acc     0.68 | train_ae_norm     1.00\n",
      "[80/200][1699/4361] Loss_D: 0.00409377 (Loss_D_real: 0.00309661 Loss_D_fake: 0.00099715) Loss_G: 0.42496729 Loss_Enh_Dec: -1.60433698\n",
      "| epoch  80 |  1700/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  2.78 | ppl    16.16 | acc     0.67 | train_ae_norm     1.00\n",
      "[80/200][1799/4361] Loss_D: 0.00101834 (Loss_D_real: 0.00036663 Loss_D_fake: 0.00065171) Loss_G: 0.63794976 Loss_Enh_Dec: -0.66734356\n",
      "| epoch  80 |  1800/ 4361 batches | lr 0.000000 | ms/batch 402.52 | loss  2.77 | ppl    15.97 | acc     0.69 | train_ae_norm     1.00\n",
      "[80/200][1899/4361] Loss_D: 0.00257360 (Loss_D_real: 0.00135496 Loss_D_fake: 0.00121865) Loss_G: 0.48639807 Loss_Enh_Dec: -1.15086496\n",
      "| epoch  80 |  1900/ 4361 batches | lr 0.000000 | ms/batch 402.39 | loss  2.84 | ppl    17.06 | acc     0.69 | train_ae_norm     1.00\n",
      "[80/200][1999/4361] Loss_D: 0.01060765 (Loss_D_real: 0.00112834 Loss_D_fake: 0.00947931) Loss_G: 0.45746014 Loss_Enh_Dec: -1.32689238\n",
      "| epoch  80 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  2.78 | ppl    16.15 | acc     0.70 | train_ae_norm     1.00\n",
      "[80/200][2099/4361] Loss_D: 0.00229713 (Loss_D_real: 0.00103279 Loss_D_fake: 0.00126434) Loss_G: 0.43797943 Loss_Enh_Dec: -1.07592523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  80 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  2.81 | ppl    16.61 | acc     0.68 | train_ae_norm     1.00\n",
      "[80/200][2199/4361] Loss_D: 0.00808076 (Loss_D_real: 0.00572428 Loss_D_fake: 0.00235648) Loss_G: 0.44217211 Loss_Enh_Dec: -0.97251123\n",
      "| epoch  80 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.82 | ppl    16.86 | acc     0.68 | train_ae_norm     1.00\n",
      "[80/200][2299/4361] Loss_D: 0.00116016 (Loss_D_real: 0.00050027 Loss_D_fake: 0.00065989) Loss_G: 0.51846522 Loss_Enh_Dec: -1.73203182\n",
      "| epoch  80 |  2300/ 4361 batches | lr 0.000000 | ms/batch 402.09 | loss  2.81 | ppl    16.56 | acc     0.70 | train_ae_norm     1.00\n",
      "[80/200][2399/4361] Loss_D: 0.00243917 (Loss_D_real: 0.00156781 Loss_D_fake: 0.00087136) Loss_G: 0.57350707 Loss_Enh_Dec: -0.95251673\n",
      "| epoch  80 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  2.81 | ppl    16.66 | acc     0.65 | train_ae_norm     1.00\n",
      "[80/200][2499/4361] Loss_D: 0.00321439 (Loss_D_real: 0.00185185 Loss_D_fake: 0.00136254) Loss_G: 0.60317916 Loss_Enh_Dec: -0.98638582\n",
      "| epoch  80 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  2.86 | ppl    17.45 | acc     0.70 | train_ae_norm     1.00\n",
      "[80/200][2599/4361] Loss_D: 0.00195122 (Loss_D_real: 0.00122299 Loss_D_fake: 0.00072822) Loss_G: 0.57821077 Loss_Enh_Dec: -0.86799258\n",
      "| epoch  80 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  2.81 | ppl    16.57 | acc     0.66 | train_ae_norm     1.00\n",
      "[80/200][2699/4361] Loss_D: 0.01112207 (Loss_D_real: 0.00997430 Loss_D_fake: 0.00114776) Loss_G: 0.52582639 Loss_Enh_Dec: -0.86820680\n",
      "| epoch  80 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  2.82 | ppl    16.70 | acc     0.68 | train_ae_norm     1.00\n",
      "[80/200][2799/4361] Loss_D: 0.00189671 (Loss_D_real: 0.00122778 Loss_D_fake: 0.00066892) Loss_G: 0.42520171 Loss_Enh_Dec: -1.19279659\n",
      "| epoch  80 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.77 | ppl    15.89 | acc     0.67 | train_ae_norm     1.00\n",
      "[80/200][2899/4361] Loss_D: 0.00278961 (Loss_D_real: 0.00039873 Loss_D_fake: 0.00239088) Loss_G: 0.46462569 Loss_Enh_Dec: -1.39228272\n",
      "| epoch  80 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.80 | ppl    16.41 | acc     0.68 | train_ae_norm     1.00\n",
      "[80/200][2999/4361] Loss_D: 0.00394576 (Loss_D_real: 0.00157596 Loss_D_fake: 0.00236981) Loss_G: 0.38358432 Loss_Enh_Dec: -0.99320167\n",
      "| epoch  80 |  3000/ 4361 batches | lr 0.000000 | ms/batch 402.08 | loss  2.81 | ppl    16.65 | acc     0.69 | train_ae_norm     1.00\n",
      "[80/200][3099/4361] Loss_D: 0.00201264 (Loss_D_real: 0.00119205 Loss_D_fake: 0.00082058) Loss_G: 0.42548972 Loss_Enh_Dec: -1.36765289\n",
      "| epoch  80 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  2.81 | ppl    16.61 | acc     0.67 | train_ae_norm     1.00\n",
      "[80/200][3199/4361] Loss_D: 0.00057345 (Loss_D_real: 0.00024094 Loss_D_fake: 0.00033251) Loss_G: 0.46666542 Loss_Enh_Dec: -0.81546634\n",
      "| epoch  80 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.85 | ppl    17.28 | acc     0.68 | train_ae_norm     1.00\n",
      "[80/200][3299/4361] Loss_D: 0.00473727 (Loss_D_real: 0.00203363 Loss_D_fake: 0.00270364) Loss_G: 0.50708866 Loss_Enh_Dec: -0.79576486\n",
      "| epoch  80 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  2.84 | ppl    17.19 | acc     0.68 | train_ae_norm     1.00\n",
      "[80/200][3399/4361] Loss_D: 0.00180550 (Loss_D_real: 0.00034569 Loss_D_fake: 0.00145981) Loss_G: 0.40907690 Loss_Enh_Dec: -0.77592027\n",
      "| epoch  80 |  3400/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  2.84 | ppl    17.12 | acc     0.68 | train_ae_norm     1.00\n",
      "[80/200][3499/4361] Loss_D: 0.00132076 (Loss_D_real: 0.00057419 Loss_D_fake: 0.00074657) Loss_G: 0.42128307 Loss_Enh_Dec: -1.55786264\n",
      "| epoch  80 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  2.79 | ppl    16.27 | acc     0.67 | train_ae_norm     1.00\n",
      "[80/200][3599/4361] Loss_D: 0.00424568 (Loss_D_real: 0.00372005 Loss_D_fake: 0.00052562) Loss_G: 0.52531159 Loss_Enh_Dec: -0.74719292\n",
      "| epoch  80 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  2.80 | ppl    16.37 | acc     0.68 | train_ae_norm     1.00\n",
      "[80/200][3699/4361] Loss_D: 0.00190268 (Loss_D_real: 0.00033458 Loss_D_fake: 0.00156811) Loss_G: 0.41667557 Loss_Enh_Dec: -1.24839711\n",
      "| epoch  80 |  3700/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  2.82 | ppl    16.69 | acc     0.66 | train_ae_norm     1.00\n",
      "[80/200][3799/4361] Loss_D: 0.00240978 (Loss_D_real: 0.00016212 Loss_D_fake: 0.00224766) Loss_G: 0.42005047 Loss_Enh_Dec: -1.71020055\n",
      "| epoch  80 |  3800/ 4361 batches | lr 0.000000 | ms/batch 402.16 | loss  2.82 | ppl    16.73 | acc     0.72 | train_ae_norm     1.00\n",
      "[80/200][3899/4361] Loss_D: 0.00061645 (Loss_D_real: 0.00025229 Loss_D_fake: 0.00036416) Loss_G: 0.46162206 Loss_Enh_Dec: -1.48270857\n",
      "| epoch  80 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  2.83 | ppl    16.87 | acc     0.66 | train_ae_norm     1.00\n",
      "[80/200][3999/4361] Loss_D: 0.00090488 (Loss_D_real: 0.00024884 Loss_D_fake: 0.00065604) Loss_G: 0.47547847 Loss_Enh_Dec: -1.26503432\n",
      "| epoch  80 |  4000/ 4361 batches | lr 0.000000 | ms/batch 402.12 | loss  2.81 | ppl    16.69 | acc     0.68 | train_ae_norm     1.00\n",
      "[80/200][4099/4361] Loss_D: 0.15253073 (Loss_D_real: 0.15216132 Loss_D_fake: 0.00036941) Loss_G: 0.44948435 Loss_Enh_Dec: -0.64095360\n",
      "| epoch  80 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  2.77 | ppl    15.88 | acc     0.66 | train_ae_norm     1.00\n",
      "[80/200][4199/4361] Loss_D: 0.00058057 (Loss_D_real: 0.00018790 Loss_D_fake: 0.00039267) Loss_G: 0.45561814 Loss_Enh_Dec: -0.92430085\n",
      "| epoch  80 |  4200/ 4361 batches | lr 0.000000 | ms/batch 402.12 | loss  2.80 | ppl    16.43 | acc     0.70 | train_ae_norm     1.00\n",
      "[80/200][4299/4361] Loss_D: 0.00264197 (Loss_D_real: 0.00236937 Loss_D_fake: 0.00027260) Loss_G: 0.59244603 Loss_Enh_Dec: -1.41622221\n",
      "| epoch  80 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  2.78 | ppl    16.08 | acc     0.69 | train_ae_norm     1.00\n",
      "| end of epoch  80 | time: 1854.35s | test loss  2.80 | test ppl 16.38 | acc 0.712\n",
      "bleu_self:  [7.35042735e-02 2.20491757e-09 7.16130906e-12 4.24225552e-13\n",
      " 1.26793718e-13]\n",
      "bleu_test:  [6.64209402e-01 1.32411477e-01 3.19301632e-02 4.91192392e-06\n",
      " 2.65480118e-08]\n",
      "bleu_self: [0.07350427,0.00000000,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.66420940,0.13241148,0.03193016,0.00000491,0.00000003]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 81 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:57.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.700\n",
      "  Training epcoh took: 0:01:27\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.487\n",
      "  Test Loss: 4.306\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  81 |     0/ 4361 batches | lr 0.000000 | ms/batch 869.58 | loss  0.03 | ppl     1.03 | acc     0.71 | train_ae_norm     1.00\n",
      "[81/200][99/4361] Loss_D: 0.00146128 (Loss_D_real: 0.00062095 Loss_D_fake: 0.00084033) Loss_G: 0.56061310 Loss_Enh_Dec: -1.54053724\n",
      "| epoch  81 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  2.80 | ppl    16.36 | acc     0.64 | train_ae_norm     1.00\n",
      "[81/200][199/4361] Loss_D: 0.00267041 (Loss_D_real: 0.00097873 Loss_D_fake: 0.00169167) Loss_G: 0.40568683 Loss_Enh_Dec: -0.88119107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  81 |   200/ 4361 batches | lr 0.000000 | ms/batch 402.34 | loss  2.83 | ppl    16.87 | acc     0.68 | train_ae_norm     1.00\n",
      "[81/200][299/4361] Loss_D: 0.00040188 (Loss_D_real: 0.00017657 Loss_D_fake: 0.00022532) Loss_G: 0.45503256 Loss_Enh_Dec: -1.09148717\n",
      "| epoch  81 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  2.82 | ppl    16.85 | acc     0.64 | train_ae_norm     1.00\n",
      "[81/200][399/4361] Loss_D: 0.00332044 (Loss_D_real: 0.00284322 Loss_D_fake: 0.00047722) Loss_G: 0.54402232 Loss_Enh_Dec: -1.07462847\n",
      "| epoch  81 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.76 | ppl    15.86 | acc     0.68 | train_ae_norm     1.00\n",
      "[81/200][499/4361] Loss_D: 0.00232546 (Loss_D_real: 0.00082054 Loss_D_fake: 0.00150492) Loss_G: 0.42109686 Loss_Enh_Dec: -1.22973776\n",
      "| epoch  81 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  2.83 | ppl    16.99 | acc     0.71 | train_ae_norm     1.00\n",
      "[81/200][599/4361] Loss_D: 0.00501066 (Loss_D_real: 0.00307949 Loss_D_fake: 0.00193117) Loss_G: 0.47397456 Loss_Enh_Dec: -1.48617363\n",
      "| epoch  81 |   600/ 4361 batches | lr 0.000000 | ms/batch 402.19 | loss  2.77 | ppl    16.00 | acc     0.66 | train_ae_norm     1.00\n",
      "[81/200][699/4361] Loss_D: 0.00165549 (Loss_D_real: 0.00050435 Loss_D_fake: 0.00115114) Loss_G: 0.47372332 Loss_Enh_Dec: -1.51269782\n",
      "| epoch  81 |   700/ 4361 batches | lr 0.000000 | ms/batch 402.23 | loss  2.81 | ppl    16.63 | acc     0.68 | train_ae_norm     1.00\n",
      "[81/200][799/4361] Loss_D: 0.00235710 (Loss_D_real: 0.00095176 Loss_D_fake: 0.00140535) Loss_G: 0.46560165 Loss_Enh_Dec: -1.74139941\n",
      "| epoch  81 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  2.78 | ppl    16.13 | acc     0.68 | train_ae_norm     1.00\n",
      "[81/200][899/4361] Loss_D: 0.00110204 (Loss_D_real: 0.00004650 Loss_D_fake: 0.00105553) Loss_G: 0.44709507 Loss_Enh_Dec: -1.55079830\n",
      "| epoch  81 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  2.81 | ppl    16.55 | acc     0.69 | train_ae_norm     1.00\n",
      "[81/200][999/4361] Loss_D: 0.00209261 (Loss_D_real: 0.00019682 Loss_D_fake: 0.00189579) Loss_G: 0.50931579 Loss_Enh_Dec: -0.99697989\n",
      "| epoch  81 |  1000/ 4361 batches | lr 0.000000 | ms/batch 402.25 | loss  2.80 | ppl    16.51 | acc     0.71 | train_ae_norm     1.00\n",
      "[81/200][1099/4361] Loss_D: 0.00267530 (Loss_D_real: 0.00118098 Loss_D_fake: 0.00149432) Loss_G: 0.48346931 Loss_Enh_Dec: -1.36887085\n",
      "| epoch  81 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  2.79 | ppl    16.24 | acc     0.67 | train_ae_norm     1.00\n",
      "[81/200][1199/4361] Loss_D: 0.00406546 (Loss_D_real: 0.00165897 Loss_D_fake: 0.00240649) Loss_G: 0.39267966 Loss_Enh_Dec: -1.37957704\n",
      "| epoch  81 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  2.79 | ppl    16.35 | acc     0.70 | train_ae_norm     1.00\n",
      "[81/200][1299/4361] Loss_D: 0.00281711 (Loss_D_real: 0.00223666 Loss_D_fake: 0.00058045) Loss_G: 0.39916581 Loss_Enh_Dec: -1.31197321\n",
      "| epoch  81 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  2.82 | ppl    16.82 | acc     0.67 | train_ae_norm     1.00\n",
      "[81/200][1399/4361] Loss_D: 0.00252686 (Loss_D_real: 0.00150881 Loss_D_fake: 0.00101805) Loss_G: 0.49414030 Loss_Enh_Dec: -1.46153796\n",
      "| epoch  81 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  2.81 | ppl    16.61 | acc     0.65 | train_ae_norm     1.00\n",
      "[81/200][1499/4361] Loss_D: 0.00115411 (Loss_D_real: 0.00050423 Loss_D_fake: 0.00064988) Loss_G: 0.45358109 Loss_Enh_Dec: -1.63815331\n",
      "| epoch  81 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  2.87 | ppl    17.58 | acc     0.68 | train_ae_norm     1.00\n",
      "[81/200][1599/4361] Loss_D: 0.00254027 (Loss_D_real: 0.00022600 Loss_D_fake: 0.00231427) Loss_G: 0.44340250 Loss_Enh_Dec: -1.26696515\n",
      "| epoch  81 |  1600/ 4361 batches | lr 0.000000 | ms/batch 402.36 | loss  2.84 | ppl    17.18 | acc     0.67 | train_ae_norm     1.00\n",
      "[81/200][1699/4361] Loss_D: 0.00218379 (Loss_D_real: 0.00155286 Loss_D_fake: 0.00063093) Loss_G: 0.49075374 Loss_Enh_Dec: -1.62194252\n",
      "| epoch  81 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  2.81 | ppl    16.54 | acc     0.67 | train_ae_norm     1.00\n",
      "[81/200][1799/4361] Loss_D: 0.00132808 (Loss_D_real: 0.00007176 Loss_D_fake: 0.00125632) Loss_G: 0.49330407 Loss_Enh_Dec: -1.54258001\n",
      "| epoch  81 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  2.78 | ppl    16.17 | acc     0.69 | train_ae_norm     1.00\n",
      "[81/200][1899/4361] Loss_D: 0.00164536 (Loss_D_real: 0.00061164 Loss_D_fake: 0.00103373) Loss_G: 0.77033204 Loss_Enh_Dec: -1.86280787\n",
      "| epoch  81 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.85 | ppl    17.30 | acc     0.70 | train_ae_norm     1.00\n",
      "[81/200][1999/4361] Loss_D: 0.00096241 (Loss_D_real: 0.00032086 Loss_D_fake: 0.00064155) Loss_G: 0.49280438 Loss_Enh_Dec: -1.80323756\n",
      "| epoch  81 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.83 | loss  2.79 | ppl    16.35 | acc     0.69 | train_ae_norm     1.00\n",
      "[81/200][2099/4361] Loss_D: 0.00421843 (Loss_D_real: 0.00075800 Loss_D_fake: 0.00346043) Loss_G: 0.45019665 Loss_Enh_Dec: -1.87270010\n",
      "| epoch  81 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.55 | loss  2.82 | ppl    16.82 | acc     0.70 | train_ae_norm     1.00\n",
      "[81/200][2199/4361] Loss_D: 0.00801765 (Loss_D_real: 0.00441534 Loss_D_fake: 0.00360230) Loss_G: 0.79981834 Loss_Enh_Dec: -1.48268640\n",
      "| epoch  81 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.20 | loss  2.82 | ppl    16.76 | acc     0.69 | train_ae_norm     1.00\n",
      "[81/200][2299/4361] Loss_D: 0.00261978 (Loss_D_real: 0.00136037 Loss_D_fake: 0.00125941) Loss_G: 0.50916439 Loss_Enh_Dec: -1.77604711\n",
      "| epoch  81 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.57 | loss  2.78 | ppl    16.07 | acc     0.70 | train_ae_norm     1.00\n",
      "[81/200][2399/4361] Loss_D: 0.00483522 (Loss_D_real: 0.00291033 Loss_D_fake: 0.00192489) Loss_G: 0.39225841 Loss_Enh_Dec: -1.89057565\n",
      "| epoch  81 |  2400/ 4361 batches | lr 0.000000 | ms/batch 400.92 | loss  2.79 | ppl    16.30 | acc     0.66 | train_ae_norm     1.00\n",
      "[81/200][2499/4361] Loss_D: 0.00215698 (Loss_D_real: 0.00068464 Loss_D_fake: 0.00147234) Loss_G: 0.45608774 Loss_Enh_Dec: -1.49968600\n",
      "| epoch  81 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  2.83 | ppl    17.02 | acc     0.70 | train_ae_norm     1.00\n",
      "[81/200][2599/4361] Loss_D: 0.02307718 (Loss_D_real: 0.00050738 Loss_D_fake: 0.02256980) Loss_G: 0.56324655 Loss_Enh_Dec: -1.24430645\n",
      "| epoch  81 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  2.79 | ppl    16.30 | acc     0.64 | train_ae_norm     1.00\n",
      "[81/200][2699/4361] Loss_D: 0.00495962 (Loss_D_real: 0.00425592 Loss_D_fake: 0.00070370) Loss_G: 0.44452760 Loss_Enh_Dec: -1.43086708\n",
      "| epoch  81 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  2.80 | ppl    16.43 | acc     0.68 | train_ae_norm     1.00\n",
      "[81/200][2799/4361] Loss_D: 0.00353973 (Loss_D_real: 0.00183644 Loss_D_fake: 0.00170329) Loss_G: 0.43302685 Loss_Enh_Dec: -1.41293478\n",
      "| epoch  81 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  2.75 | ppl    15.65 | acc     0.68 | train_ae_norm     1.00\n",
      "[81/200][2899/4361] Loss_D: 0.00481820 (Loss_D_real: 0.00346127 Loss_D_fake: 0.00135694) Loss_G: 0.44139272 Loss_Enh_Dec: -1.29146135\n",
      "| epoch  81 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.81 | loss  2.78 | ppl    16.09 | acc     0.68 | train_ae_norm     1.00\n",
      "[81/200][2999/4361] Loss_D: 0.00113701 (Loss_D_real: 0.00019964 Loss_D_fake: 0.00093737) Loss_G: 0.51556844 Loss_Enh_Dec: -1.70021558\n",
      "| epoch  81 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  2.78 | ppl    16.18 | acc     0.70 | train_ae_norm     1.00\n",
      "[81/200][3099/4361] Loss_D: 0.00315729 (Loss_D_real: 0.00257882 Loss_D_fake: 0.00057847) Loss_G: 0.42529884 Loss_Enh_Dec: -1.92859960\n",
      "| epoch  81 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  2.79 | ppl    16.24 | acc     0.66 | train_ae_norm     1.00\n",
      "[81/200][3199/4361] Loss_D: 0.01295071 (Loss_D_real: 0.01018456 Loss_D_fake: 0.00276615) Loss_G: 0.47700247 Loss_Enh_Dec: -1.63139379\n",
      "| epoch  81 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  2.82 | ppl    16.76 | acc     0.69 | train_ae_norm     1.00\n",
      "[81/200][3299/4361] Loss_D: 0.00257234 (Loss_D_real: 0.00244255 Loss_D_fake: 0.00012980) Loss_G: 0.60908204 Loss_Enh_Dec: -1.13147914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  81 |  3300/ 4361 batches | lr 0.000000 | ms/batch 402.05 | loss  2.82 | ppl    16.78 | acc     0.69 | train_ae_norm     1.00\n",
      "[81/200][3399/4361] Loss_D: 0.00449497 (Loss_D_real: 0.00384613 Loss_D_fake: 0.00064884) Loss_G: 0.54467386 Loss_Enh_Dec: -0.71299505\n",
      "| epoch  81 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  2.82 | ppl    16.80 | acc     0.67 | train_ae_norm     1.00\n",
      "[81/200][3499/4361] Loss_D: 0.00033057 (Loss_D_real: 0.00024746 Loss_D_fake: 0.00008310) Loss_G: 0.60045761 Loss_Enh_Dec: -0.66086078\n",
      "| epoch  81 |  3500/ 4361 batches | lr 0.000000 | ms/batch 402.32 | loss  2.75 | ppl    15.66 | acc     0.68 | train_ae_norm     1.00\n",
      "[81/200][3599/4361] Loss_D: 0.00302905 (Loss_D_real: 0.00103060 Loss_D_fake: 0.00199845) Loss_G: 0.46420947 Loss_Enh_Dec: -1.35552311\n",
      "| epoch  81 |  3600/ 4361 batches | lr 0.000000 | ms/batch 402.14 | loss  2.77 | ppl    15.90 | acc     0.71 | train_ae_norm     1.00\n",
      "[81/200][3699/4361] Loss_D: 0.00081945 (Loss_D_real: 0.00018425 Loss_D_fake: 0.00063520) Loss_G: 0.48905307 Loss_Enh_Dec: -1.36888587\n",
      "| epoch  81 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  2.78 | ppl    16.09 | acc     0.66 | train_ae_norm     1.00\n",
      "[81/200][3799/4361] Loss_D: 0.00125900 (Loss_D_real: 0.00059054 Loss_D_fake: 0.00066846) Loss_G: 0.44087249 Loss_Enh_Dec: -1.33654344\n",
      "| epoch  81 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  2.79 | ppl    16.31 | acc     0.73 | train_ae_norm     1.00\n",
      "[81/200][3899/4361] Loss_D: 0.00066959 (Loss_D_real: 0.00031447 Loss_D_fake: 0.00035512) Loss_G: 0.55256146 Loss_Enh_Dec: -0.54953665\n",
      "| epoch  81 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  2.80 | ppl    16.39 | acc     0.65 | train_ae_norm     1.00\n",
      "[81/200][3999/4361] Loss_D: 0.00916734 (Loss_D_real: 0.00493245 Loss_D_fake: 0.00423489) Loss_G: 0.60546112 Loss_Enh_Dec: -1.27923584\n",
      "| epoch  81 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  2.80 | ppl    16.40 | acc     0.70 | train_ae_norm     1.00\n",
      "[81/200][4099/4361] Loss_D: 0.00426047 (Loss_D_real: 0.00051707 Loss_D_fake: 0.00374340) Loss_G: 0.45653448 Loss_Enh_Dec: -1.51291180\n",
      "| epoch  81 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  2.77 | ppl    15.99 | acc     0.68 | train_ae_norm     1.00\n",
      "[81/200][4199/4361] Loss_D: 0.00339341 (Loss_D_real: 0.00295495 Loss_D_fake: 0.00043846) Loss_G: 0.53808892 Loss_Enh_Dec: -1.50667071\n",
      "| epoch  81 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  2.81 | ppl    16.67 | acc     0.72 | train_ae_norm     1.00\n",
      "[81/200][4299/4361] Loss_D: 0.01135188 (Loss_D_real: 0.01071541 Loss_D_fake: 0.00063647) Loss_G: 0.44370508 Loss_Enh_Dec: -1.34222519\n",
      "| epoch  81 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  2.77 | ppl    16.03 | acc     0.71 | train_ae_norm     1.00\n",
      "| end of epoch  81 | time: 1853.86s | test loss  2.77 | test ppl 16.02 | acc 0.715\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 82 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.701\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.485\n",
      "  Test Loss: 4.315\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  82 |     0/ 4361 batches | lr 0.000000 | ms/batch 867.97 | loss  0.02 | ppl     1.02 | acc     0.74 | train_ae_norm     1.00\n",
      "[82/200][99/4361] Loss_D: 0.00122226 (Loss_D_real: 0.00090008 Loss_D_fake: 0.00032218) Loss_G: 0.48641166 Loss_Enh_Dec: -1.06625044\n",
      "| epoch  82 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  2.78 | ppl    16.11 | acc     0.67 | train_ae_norm     1.00\n",
      "[82/200][199/4361] Loss_D: 0.00127955 (Loss_D_real: 0.00043701 Loss_D_fake: 0.00084254) Loss_G: 0.44018766 Loss_Enh_Dec: -1.65162432\n",
      "| epoch  82 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.79 | ppl    16.36 | acc     0.67 | train_ae_norm     1.00\n",
      "[82/200][299/4361] Loss_D: 0.00268557 (Loss_D_real: 0.00202582 Loss_D_fake: 0.00065975) Loss_G: 0.42355052 Loss_Enh_Dec: -1.67365003\n",
      "| epoch  82 |   300/ 4361 batches | lr 0.000000 | ms/batch 402.00 | loss  2.80 | ppl    16.39 | acc     0.65 | train_ae_norm     1.00\n",
      "[82/200][399/4361] Loss_D: 0.00146259 (Loss_D_real: 0.00083412 Loss_D_fake: 0.00062846) Loss_G: 0.42866775 Loss_Enh_Dec: -1.38328207\n",
      "| epoch  82 |   400/ 4361 batches | lr 0.000000 | ms/batch 402.11 | loss  2.72 | ppl    15.15 | acc     0.68 | train_ae_norm     1.00\n",
      "[82/200][499/4361] Loss_D: 0.00167566 (Loss_D_real: 0.00050070 Loss_D_fake: 0.00117496) Loss_G: 0.52283436 Loss_Enh_Dec: -1.29397392\n",
      "| epoch  82 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  2.78 | ppl    16.05 | acc     0.71 | train_ae_norm     1.00\n",
      "[82/200][599/4361] Loss_D: 0.00124002 (Loss_D_real: 0.00073268 Loss_D_fake: 0.00050735) Loss_G: 0.51010364 Loss_Enh_Dec: -1.75966775\n",
      "| epoch  82 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  2.74 | ppl    15.51 | acc     0.67 | train_ae_norm     1.00\n",
      "[82/200][699/4361] Loss_D: 0.00697123 (Loss_D_real: 0.00672675 Loss_D_fake: 0.00024448) Loss_G: 0.64184469 Loss_Enh_Dec: -1.32289791\n",
      "| epoch  82 |   700/ 4361 batches | lr 0.000000 | ms/batch 402.06 | loss  2.80 | ppl    16.52 | acc     0.68 | train_ae_norm     1.00\n",
      "[82/200][799/4361] Loss_D: 0.00220688 (Loss_D_real: 0.00114293 Loss_D_fake: 0.00106395) Loss_G: 0.49072456 Loss_Enh_Dec: -1.44180191\n",
      "| epoch  82 |   800/ 4361 batches | lr 0.000000 | ms/batch 402.26 | loss  2.76 | ppl    15.80 | acc     0.69 | train_ae_norm     1.00\n",
      "[82/200][899/4361] Loss_D: 0.00421301 (Loss_D_real: 0.00238799 Loss_D_fake: 0.00182502) Loss_G: 0.43797675 Loss_Enh_Dec: -1.16745317\n",
      "| epoch  82 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.78 | ppl    16.14 | acc     0.71 | train_ae_norm     1.00\n",
      "[82/200][999/4361] Loss_D: 0.00163004 (Loss_D_real: 0.00032646 Loss_D_fake: 0.00130357) Loss_G: 0.41185337 Loss_Enh_Dec: -1.33532894\n",
      "| epoch  82 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.77 | ppl    15.99 | acc     0.71 | train_ae_norm     1.00\n",
      "[82/200][1099/4361] Loss_D: 0.00142551 (Loss_D_real: 0.00058168 Loss_D_fake: 0.00084383) Loss_G: 0.45191836 Loss_Enh_Dec: -1.51098764\n",
      "| epoch  82 |  1100/ 4361 batches | lr 0.000000 | ms/batch 402.14 | loss  2.76 | ppl    15.87 | acc     0.67 | train_ae_norm     1.00\n",
      "[82/200][1199/4361] Loss_D: 0.00108391 (Loss_D_real: 0.00044011 Loss_D_fake: 0.00064380) Loss_G: 0.44363719 Loss_Enh_Dec: -1.07397830\n",
      "| epoch  82 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  2.77 | ppl    15.91 | acc     0.71 | train_ae_norm     1.00\n",
      "[82/200][1299/4361] Loss_D: 0.00073271 (Loss_D_real: 0.00062792 Loss_D_fake: 0.00010479) Loss_G: 0.52311772 Loss_Enh_Dec: -1.35310829\n",
      "| epoch  82 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  2.79 | ppl    16.30 | acc     0.70 | train_ae_norm     1.00\n",
      "[82/200][1399/4361] Loss_D: 0.00171365 (Loss_D_real: 0.00113155 Loss_D_fake: 0.00058210) Loss_G: 0.47704020 Loss_Enh_Dec: -1.59461701\n",
      "| epoch  82 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  2.79 | ppl    16.36 | acc     0.62 | train_ae_norm     1.00\n",
      "[82/200][1499/4361] Loss_D: 0.00547563 (Loss_D_real: 0.00423068 Loss_D_fake: 0.00124495) Loss_G: 0.42874795 Loss_Enh_Dec: -1.08757341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  82 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.84 | ppl    17.07 | acc     0.67 | train_ae_norm     1.00\n",
      "[82/200][1599/4361] Loss_D: 0.00104570 (Loss_D_real: 0.00038947 Loss_D_fake: 0.00065623) Loss_G: 0.41726303 Loss_Enh_Dec: -1.42236233\n",
      "| epoch  82 |  1600/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.80 | ppl    16.52 | acc     0.67 | train_ae_norm     1.00\n",
      "[82/200][1699/4361] Loss_D: 0.00051029 (Loss_D_real: 0.00018614 Loss_D_fake: 0.00032415) Loss_G: 0.42595974 Loss_Enh_Dec: -1.72512472\n",
      "| epoch  82 |  1700/ 4361 batches | lr 0.000000 | ms/batch 402.03 | loss  2.78 | ppl    16.05 | acc     0.66 | train_ae_norm     1.00\n",
      "[82/200][1799/4361] Loss_D: 0.00298264 (Loss_D_real: 0.00182970 Loss_D_fake: 0.00115294) Loss_G: 0.89865649 Loss_Enh_Dec: -1.14954507\n",
      "| epoch  82 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.77 | ppl    15.94 | acc     0.68 | train_ae_norm     1.00\n",
      "[82/200][1899/4361] Loss_D: 0.00118801 (Loss_D_real: 0.00042508 Loss_D_fake: 0.00076293) Loss_G: 0.55057144 Loss_Enh_Dec: -1.66524398\n",
      "| epoch  82 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  2.84 | ppl    17.12 | acc     0.69 | train_ae_norm     1.00\n",
      "[82/200][1999/4361] Loss_D: 0.00248521 (Loss_D_real: 0.00184849 Loss_D_fake: 0.00063672) Loss_G: 0.44530407 Loss_Enh_Dec: -1.21065700\n",
      "| epoch  82 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  2.77 | ppl    15.98 | acc     0.68 | train_ae_norm     1.00\n",
      "[82/200][2099/4361] Loss_D: 0.00122403 (Loss_D_real: 0.00034318 Loss_D_fake: 0.00088085) Loss_G: 0.44577900 Loss_Enh_Dec: -1.34290946\n",
      "| epoch  82 |  2100/ 4361 batches | lr 0.000000 | ms/batch 402.03 | loss  2.81 | ppl    16.69 | acc     0.70 | train_ae_norm     1.00\n",
      "[82/200][2199/4361] Loss_D: 0.00132203 (Loss_D_real: 0.00019441 Loss_D_fake: 0.00112763) Loss_G: 0.52061558 Loss_Enh_Dec: -0.99461979\n",
      "| epoch  82 |  2200/ 4361 batches | lr 0.000000 | ms/batch 402.38 | loss  2.78 | ppl    16.17 | acc     0.69 | train_ae_norm     1.00\n",
      "[82/200][2299/4361] Loss_D: 0.00306465 (Loss_D_real: 0.00090903 Loss_D_fake: 0.00215562) Loss_G: 0.42476401 Loss_Enh_Dec: -1.70138824\n",
      "| epoch  82 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  2.74 | ppl    15.52 | acc     0.71 | train_ae_norm     1.00\n",
      "[82/200][2399/4361] Loss_D: 0.00057134 (Loss_D_real: 0.00018107 Loss_D_fake: 0.00039026) Loss_G: 0.59443676 Loss_Enh_Dec: -1.49648798\n",
      "| epoch  82 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.76 | ppl    15.84 | acc     0.66 | train_ae_norm     1.00\n",
      "[82/200][2499/4361] Loss_D: 0.00105696 (Loss_D_real: 0.00034470 Loss_D_fake: 0.00071226) Loss_G: 0.41809016 Loss_Enh_Dec: -1.24880528\n",
      "| epoch  82 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  2.82 | ppl    16.80 | acc     0.71 | train_ae_norm     1.00\n",
      "[82/200][2599/4361] Loss_D: 0.00138490 (Loss_D_real: 0.00058729 Loss_D_fake: 0.00079761) Loss_G: 0.45900059 Loss_Enh_Dec: -1.19370103\n",
      "| epoch  82 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.99 | loss  2.79 | ppl    16.27 | acc     0.66 | train_ae_norm     1.00\n",
      "[82/200][2699/4361] Loss_D: 0.00153729 (Loss_D_real: 0.00047979 Loss_D_fake: 0.00105750) Loss_G: 0.45133790 Loss_Enh_Dec: -1.15451419\n",
      "| epoch  82 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  2.79 | ppl    16.32 | acc     0.68 | train_ae_norm     1.00\n",
      "[82/200][2799/4361] Loss_D: 0.01715837 (Loss_D_real: 0.01669193 Loss_D_fake: 0.00046644) Loss_G: 0.48625913 Loss_Enh_Dec: -1.56968141\n",
      "| epoch  82 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  2.74 | ppl    15.50 | acc     0.68 | train_ae_norm     1.00\n",
      "[82/200][2899/4361] Loss_D: 0.00129614 (Loss_D_real: 0.00090335 Loss_D_fake: 0.00039278) Loss_G: 0.47515175 Loss_Enh_Dec: -1.42461169\n",
      "| epoch  82 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.94 | loss  2.76 | ppl    15.77 | acc     0.68 | train_ae_norm     1.00\n",
      "[82/200][2999/4361] Loss_D: 0.00118376 (Loss_D_real: 0.00031734 Loss_D_fake: 0.00086642) Loss_G: 0.42542535 Loss_Enh_Dec: -1.15327358\n",
      "| epoch  82 |  3000/ 4361 batches | lr 0.000000 | ms/batch 402.08 | loss  2.77 | ppl    15.93 | acc     0.69 | train_ae_norm     1.00\n",
      "[82/200][3099/4361] Loss_D: 0.01481868 (Loss_D_real: 0.01373070 Loss_D_fake: 0.00108798) Loss_G: 0.44700199 Loss_Enh_Dec: -1.64883840\n",
      "| epoch  82 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  2.77 | ppl    15.92 | acc     0.65 | train_ae_norm     1.00\n",
      "[82/200][3199/4361] Loss_D: 0.00096552 (Loss_D_real: 0.00065205 Loss_D_fake: 0.00031347) Loss_G: 0.64993495 Loss_Enh_Dec: -1.47629917\n",
      "| epoch  82 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  2.81 | ppl    16.67 | acc     0.68 | train_ae_norm     1.00\n",
      "[82/200][3299/4361] Loss_D: 0.00351698 (Loss_D_real: 0.00081686 Loss_D_fake: 0.00270012) Loss_G: 0.42165157 Loss_Enh_Dec: -1.37642288\n",
      "| epoch  82 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  2.78 | ppl    16.18 | acc     0.68 | train_ae_norm     1.00\n",
      "[82/200][3399/4361] Loss_D: 0.00156639 (Loss_D_real: 0.00053745 Loss_D_fake: 0.00102894) Loss_G: 0.39231357 Loss_Enh_Dec: -1.28924894\n",
      "| epoch  82 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  2.77 | ppl    16.03 | acc     0.68 | train_ae_norm     1.00\n",
      "[82/200][3499/4361] Loss_D: 0.00138582 (Loss_D_real: 0.00011776 Loss_D_fake: 0.00126807) Loss_G: 0.39647198 Loss_Enh_Dec: -1.20888126\n",
      "| epoch  82 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  2.71 | ppl    15.00 | acc     0.68 | train_ae_norm     1.00\n",
      "[82/200][3599/4361] Loss_D: 0.00274531 (Loss_D_real: 0.00040448 Loss_D_fake: 0.00234083) Loss_G: 0.42078868 Loss_Enh_Dec: -1.17488921\n",
      "| epoch  82 |  3600/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  2.72 | ppl    15.21 | acc     0.70 | train_ae_norm     1.00\n",
      "[82/200][3699/4361] Loss_D: 0.00222712 (Loss_D_real: 0.00150014 Loss_D_fake: 0.00072698) Loss_G: 0.43494797 Loss_Enh_Dec: -1.38165891\n",
      "| epoch  82 |  3700/ 4361 batches | lr 0.000000 | ms/batch 402.09 | loss  2.74 | ppl    15.51 | acc     0.66 | train_ae_norm     1.00\n",
      "[82/200][3799/4361] Loss_D: 0.00122113 (Loss_D_real: 0.00058066 Loss_D_fake: 0.00064047) Loss_G: 0.44733095 Loss_Enh_Dec: -1.24565566\n",
      "| epoch  82 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.76 | ppl    15.85 | acc     0.71 | train_ae_norm     1.00\n",
      "[82/200][3899/4361] Loss_D: 0.00422326 (Loss_D_real: 0.00400396 Loss_D_fake: 0.00021930) Loss_G: 0.64860994 Loss_Enh_Dec: -1.50013006\n",
      "| epoch  82 |  3900/ 4361 batches | lr 0.000000 | ms/batch 402.28 | loss  2.76 | ppl    15.75 | acc     0.67 | train_ae_norm     1.00\n",
      "[82/200][3999/4361] Loss_D: 0.00186192 (Loss_D_real: 0.00110648 Loss_D_fake: 0.00075544) Loss_G: 0.43988487 Loss_Enh_Dec: -1.18843555\n",
      "| epoch  82 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  2.76 | ppl    15.83 | acc     0.71 | train_ae_norm     1.00\n",
      "[82/200][4099/4361] Loss_D: 0.00096116 (Loss_D_real: 0.00021759 Loss_D_fake: 0.00074357) Loss_G: 0.43814498 Loss_Enh_Dec: -0.79071277\n",
      "| epoch  82 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  2.71 | ppl    14.99 | acc     0.69 | train_ae_norm     1.00\n",
      "[82/200][4199/4361] Loss_D: 0.00151971 (Loss_D_real: 0.00065898 Loss_D_fake: 0.00086073) Loss_G: 0.45810720 Loss_Enh_Dec: -0.92683357\n",
      "| epoch  82 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.76 | ppl    15.80 | acc     0.71 | train_ae_norm     1.00\n",
      "[82/200][4299/4361] Loss_D: 0.00790955 (Loss_D_real: 0.00667168 Loss_D_fake: 0.00123787) Loss_G: 0.42459837 Loss_Enh_Dec: -0.96543008\n",
      "| epoch  82 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  2.74 | ppl    15.42 | acc     0.68 | train_ae_norm     1.00\n",
      "| end of epoch  82 | time: 1854.34s | test loss  2.73 | test ppl 15.36 | acc 0.719\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 83 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.703\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.485\n",
      "  Test Loss: 4.263\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  83 |     0/ 4361 batches | lr 0.000000 | ms/batch 863.13 | loss  0.02 | ppl     1.02 | acc     0.73 | train_ae_norm     1.00\n",
      "[83/200][99/4361] Loss_D: 0.00597751 (Loss_D_real: 0.00257569 Loss_D_fake: 0.00340182) Loss_G: 0.47996140 Loss_Enh_Dec: -0.61649221\n",
      "| epoch  83 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  2.75 | ppl    15.59 | acc     0.65 | train_ae_norm     1.00\n",
      "[83/200][199/4361] Loss_D: 0.00214104 (Loss_D_real: 0.00172378 Loss_D_fake: 0.00041726) Loss_G: 0.87618619 Loss_Enh_Dec: -0.94831890\n",
      "| epoch  83 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  2.75 | ppl    15.59 | acc     0.70 | train_ae_norm     1.00\n",
      "[83/200][299/4361] Loss_D: 0.00185103 (Loss_D_real: 0.00140084 Loss_D_fake: 0.00045020) Loss_G: 0.46819791 Loss_Enh_Dec: -0.97935069\n",
      "| epoch  83 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.43 | loss  2.75 | ppl    15.60 | acc     0.65 | train_ae_norm     1.00\n",
      "[83/200][399/4361] Loss_D: 0.00157010 (Loss_D_real: 0.00064164 Loss_D_fake: 0.00092846) Loss_G: 0.50234532 Loss_Enh_Dec: -1.20522296\n",
      "| epoch  83 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.68 | ppl    14.54 | acc     0.70 | train_ae_norm     1.00\n",
      "[83/200][499/4361] Loss_D: 0.00658551 (Loss_D_real: 0.00075809 Loss_D_fake: 0.00582742) Loss_G: 0.58026540 Loss_Enh_Dec: -0.84761459\n",
      "| epoch  83 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  2.75 | ppl    15.62 | acc     0.70 | train_ae_norm     1.00\n",
      "[83/200][599/4361] Loss_D: 0.02151499 (Loss_D_real: 0.01991194 Loss_D_fake: 0.00160305) Loss_G: 0.47056505 Loss_Enh_Dec: -0.87940866\n",
      "| epoch  83 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.79 | loss  2.68 | ppl    14.59 | acc     0.65 | train_ae_norm     1.00\n",
      "[83/200][699/4361] Loss_D: 0.01060916 (Loss_D_real: 0.00784745 Loss_D_fake: 0.00276172) Loss_G: 0.45706072 Loss_Enh_Dec: -1.31771779\n",
      "| epoch  83 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.83 | loss  2.73 | ppl    15.34 | acc     0.69 | train_ae_norm     1.00\n",
      "[83/200][799/4361] Loss_D: 0.00367882 (Loss_D_real: 0.00016987 Loss_D_fake: 0.00350895) Loss_G: 0.42200103 Loss_Enh_Dec: -0.88286728\n",
      "| epoch  83 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  2.73 | ppl    15.39 | acc     0.67 | train_ae_norm     1.00\n",
      "[83/200][899/4361] Loss_D: 0.00222112 (Loss_D_real: 0.00061508 Loss_D_fake: 0.00160603) Loss_G: 0.43499050 Loss_Enh_Dec: -1.00834167\n",
      "| epoch  83 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.75 | ppl    15.57 | acc     0.71 | train_ae_norm     1.00\n",
      "[83/200][999/4361] Loss_D: 0.00375238 (Loss_D_real: 0.00185233 Loss_D_fake: 0.00190004) Loss_G: 0.41968116 Loss_Enh_Dec: -1.37676203\n",
      "| epoch  83 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  2.73 | ppl    15.37 | acc     0.69 | train_ae_norm     1.00\n",
      "[83/200][1099/4361] Loss_D: 0.02501378 (Loss_D_real: 0.02397467 Loss_D_fake: 0.00103911) Loss_G: 0.47479296 Loss_Enh_Dec: -1.61124253\n",
      "| epoch  83 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  2.73 | ppl    15.35 | acc     0.68 | train_ae_norm     1.00\n",
      "[83/200][1199/4361] Loss_D: 0.00311522 (Loss_D_real: 0.00284985 Loss_D_fake: 0.00026537) Loss_G: 0.50644201 Loss_Enh_Dec: -1.19900417\n",
      "| epoch  83 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  2.72 | ppl    15.17 | acc     0.71 | train_ae_norm     1.00\n",
      "[83/200][1299/4361] Loss_D: 0.00207543 (Loss_D_real: 0.00057246 Loss_D_fake: 0.00150297) Loss_G: 0.40425977 Loss_Enh_Dec: -1.69062650\n",
      "| epoch  83 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.73 | ppl    15.39 | acc     0.70 | train_ae_norm     1.00\n",
      "[83/200][1399/4361] Loss_D: 0.00042204 (Loss_D_real: 0.00013456 Loss_D_fake: 0.00028749) Loss_G: 0.43156719 Loss_Enh_Dec: -1.08857095\n",
      "| epoch  83 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.84 | loss  2.75 | ppl    15.68 | acc     0.68 | train_ae_norm     1.00\n",
      "[83/200][1499/4361] Loss_D: 0.00113566 (Loss_D_real: 0.00049501 Loss_D_fake: 0.00064065) Loss_G: 0.45953846 Loss_Enh_Dec: -1.36836910\n",
      "| epoch  83 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.78 | ppl    16.17 | acc     0.67 | train_ae_norm     1.00\n",
      "[83/200][1599/4361] Loss_D: 0.00144571 (Loss_D_real: 0.00057934 Loss_D_fake: 0.00086637) Loss_G: 0.43808624 Loss_Enh_Dec: -1.62417662\n",
      "| epoch  83 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  2.78 | ppl    16.07 | acc     0.67 | train_ae_norm     1.00\n",
      "[83/200][1699/4361] Loss_D: 0.00225268 (Loss_D_real: 0.00068068 Loss_D_fake: 0.00157200) Loss_G: 0.50558537 Loss_Enh_Dec: -1.72123992\n",
      "| epoch  83 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.86 | loss  2.74 | ppl    15.52 | acc     0.67 | train_ae_norm     1.00\n",
      "[83/200][1799/4361] Loss_D: 0.00442301 (Loss_D_real: 0.00140005 Loss_D_fake: 0.00302295) Loss_G: 0.44188920 Loss_Enh_Dec: -1.88522232\n",
      "| epoch  83 |  1800/ 4361 batches | lr 0.000000 | ms/batch 402.32 | loss  2.73 | ppl    15.30 | acc     0.68 | train_ae_norm     1.00\n",
      "[83/200][1899/4361] Loss_D: 0.00090865 (Loss_D_real: 0.00019462 Loss_D_fake: 0.00071403) Loss_G: 0.52324432 Loss_Enh_Dec: -0.95766813\n",
      "| epoch  83 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.78 | loss  2.77 | ppl    16.01 | acc     0.73 | train_ae_norm     1.00\n",
      "[83/200][1999/4361] Loss_D: 0.00100708 (Loss_D_real: 0.00029203 Loss_D_fake: 0.00071505) Loss_G: 0.41991386 Loss_Enh_Dec: -0.90291446\n",
      "| epoch  83 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.78 | loss  2.71 | ppl    15.00 | acc     0.70 | train_ae_norm     1.00\n",
      "[83/200][2099/4361] Loss_D: 0.00240033 (Loss_D_real: 0.00172436 Loss_D_fake: 0.00067597) Loss_G: 0.44441667 Loss_Enh_Dec: -1.55293155\n",
      "| epoch  83 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.56 | loss  2.75 | ppl    15.61 | acc     0.70 | train_ae_norm     1.00\n",
      "[83/200][2199/4361] Loss_D: 0.00486045 (Loss_D_real: 0.00342555 Loss_D_fake: 0.00143490) Loss_G: 0.69243753 Loss_Enh_Dec: -0.79303044\n",
      "| epoch  83 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.06 | loss  2.73 | ppl    15.34 | acc     0.71 | train_ae_norm     1.00\n",
      "[83/200][2299/4361] Loss_D: 0.00959145 (Loss_D_real: 0.00890175 Loss_D_fake: 0.00068970) Loss_G: 0.53515023 Loss_Enh_Dec: -0.57398790\n",
      "| epoch  83 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.71 | loss  2.70 | ppl    14.94 | acc     0.72 | train_ae_norm     1.00\n",
      "[83/200][2399/4361] Loss_D: 0.00142465 (Loss_D_real: 0.00026321 Loss_D_fake: 0.00116144) Loss_G: 0.39517543 Loss_Enh_Dec: -1.36797667\n",
      "| epoch  83 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  2.72 | ppl    15.12 | acc     0.65 | train_ae_norm     1.00\n",
      "[83/200][2499/4361] Loss_D: 0.00116309 (Loss_D_real: 0.00047023 Loss_D_fake: 0.00069287) Loss_G: 0.49064323 Loss_Enh_Dec: -1.28037131\n",
      "| epoch  83 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  2.75 | ppl    15.65 | acc     0.70 | train_ae_norm     1.00\n",
      "[83/200][2599/4361] Loss_D: 0.00730084 (Loss_D_real: 0.00596048 Loss_D_fake: 0.00134037) Loss_G: 0.60154182 Loss_Enh_Dec: -1.07618058\n",
      "| epoch  83 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.74 | ppl    15.42 | acc     0.67 | train_ae_norm     1.00\n",
      "[83/200][2699/4361] Loss_D: 0.00209114 (Loss_D_real: 0.00105313 Loss_D_fake: 0.00103801) Loss_G: 0.57831496 Loss_Enh_Dec: -1.20127714\n",
      "| epoch  83 |  2700/ 4361 batches | lr 0.000000 | ms/batch 400.91 | loss  2.73 | ppl    15.38 | acc     0.69 | train_ae_norm     1.00\n",
      "[83/200][2799/4361] Loss_D: 0.00102395 (Loss_D_real: 0.00062345 Loss_D_fake: 0.00040050) Loss_G: 0.48404551 Loss_Enh_Dec: -1.26614976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  83 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  2.69 | ppl    14.68 | acc     0.65 | train_ae_norm     1.00\n",
      "[83/200][2899/4361] Loss_D: 0.00168858 (Loss_D_real: 0.00102310 Loss_D_fake: 0.00066548) Loss_G: 0.45433989 Loss_Enh_Dec: -1.23811400\n",
      "| epoch  83 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  2.71 | ppl    14.97 | acc     0.69 | train_ae_norm     1.00\n",
      "[83/200][2999/4361] Loss_D: 0.00088936 (Loss_D_real: 0.00063823 Loss_D_fake: 0.00025113) Loss_G: 0.45877352 Loss_Enh_Dec: -1.08275187\n",
      "| epoch  83 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  2.73 | ppl    15.37 | acc     0.69 | train_ae_norm     1.00\n",
      "[83/200][3099/4361] Loss_D: 0.00121409 (Loss_D_real: 0.00042761 Loss_D_fake: 0.00078648) Loss_G: 0.47036019 Loss_Enh_Dec: -1.30821109\n",
      "| epoch  83 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  2.72 | ppl    15.23 | acc     0.67 | train_ae_norm     1.00\n",
      "[83/200][3199/4361] Loss_D: 0.00132782 (Loss_D_real: 0.00005608 Loss_D_fake: 0.00127174) Loss_G: 0.44912919 Loss_Enh_Dec: -1.56682956\n",
      "| epoch  83 |  3200/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  2.76 | ppl    15.87 | acc     0.71 | train_ae_norm     1.00\n",
      "[83/200][3299/4361] Loss_D: 0.00092694 (Loss_D_real: 0.00054913 Loss_D_fake: 0.00037780) Loss_G: 0.43820819 Loss_Enh_Dec: -1.40467513\n",
      "| epoch  83 |  3300/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  2.76 | ppl    15.81 | acc     0.70 | train_ae_norm     1.00\n",
      "[83/200][3399/4361] Loss_D: 0.00265026 (Loss_D_real: 0.00173508 Loss_D_fake: 0.00091517) Loss_G: 0.68476909 Loss_Enh_Dec: -0.78948742\n",
      "| epoch  83 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  2.72 | ppl    15.22 | acc     0.69 | train_ae_norm     1.00\n",
      "[83/200][3499/4361] Loss_D: 0.00060910 (Loss_D_real: 0.00015022 Loss_D_fake: 0.00045888) Loss_G: 0.48690706 Loss_Enh_Dec: -1.20038784\n",
      "| epoch  83 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  2.68 | ppl    14.61 | acc     0.68 | train_ae_norm     1.00\n",
      "[83/200][3599/4361] Loss_D: 0.00220484 (Loss_D_real: 0.00158965 Loss_D_fake: 0.00061519) Loss_G: 0.42995235 Loss_Enh_Dec: -1.26852405\n",
      "| epoch  83 |  3600/ 4361 batches | lr 0.000000 | ms/batch 400.67 | loss  2.70 | ppl    14.85 | acc     0.69 | train_ae_norm     1.00\n",
      "[83/200][3699/4361] Loss_D: 0.00131341 (Loss_D_real: 0.00060877 Loss_D_fake: 0.00070464) Loss_G: 0.49152929 Loss_Enh_Dec: -1.65130961\n",
      "| epoch  83 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  2.71 | ppl    15.10 | acc     0.68 | train_ae_norm     1.00\n",
      "[83/200][3799/4361] Loss_D: 0.00313151 (Loss_D_real: 0.00273829 Loss_D_fake: 0.00039322) Loss_G: 0.51045877 Loss_Enh_Dec: -1.50034976\n",
      "| epoch  83 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.74 | ppl    15.55 | acc     0.72 | train_ae_norm     1.00\n",
      "[83/200][3899/4361] Loss_D: 0.00117177 (Loss_D_real: 0.00021812 Loss_D_fake: 0.00095365) Loss_G: 0.43759671 Loss_Enh_Dec: -1.54013658\n",
      "| epoch  83 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  2.76 | ppl    15.81 | acc     0.65 | train_ae_norm     1.00\n",
      "[83/200][3999/4361] Loss_D: 0.00206795 (Loss_D_real: 0.00038507 Loss_D_fake: 0.00168288) Loss_G: 0.44130117 Loss_Enh_Dec: -1.38720357\n",
      "| epoch  83 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  2.74 | ppl    15.52 | acc     0.68 | train_ae_norm     1.00\n",
      "[83/200][4099/4361] Loss_D: 0.00231143 (Loss_D_real: 0.00181805 Loss_D_fake: 0.00049338) Loss_G: 0.46558723 Loss_Enh_Dec: -1.37458026\n",
      "| epoch  83 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.61 | loss  2.70 | ppl    14.85 | acc     0.69 | train_ae_norm     1.00\n",
      "[83/200][4199/4361] Loss_D: 0.00488587 (Loss_D_real: 0.00063500 Loss_D_fake: 0.00425086) Loss_G: 0.47794944 Loss_Enh_Dec: -1.54930043\n",
      "| epoch  83 |  4200/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  2.75 | ppl    15.71 | acc     0.72 | train_ae_norm     1.00\n",
      "[83/200][4299/4361] Loss_D: 0.00163129 (Loss_D_real: 0.00064948 Loss_D_fake: 0.00098181) Loss_G: 0.51691693 Loss_Enh_Dec: -1.38424289\n",
      "| epoch  83 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.72 | ppl    15.18 | acc     0.71 | train_ae_norm     1.00\n",
      "| end of epoch  83 | time: 1851.49s | test loss  2.72 | test ppl 15.23 | acc 0.721\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 84 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.702\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.480\n",
      "  Test Loss: 4.348\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  84 |     0/ 4361 batches | lr 0.000000 | ms/batch 863.84 | loss  0.02 | ppl     1.02 | acc     0.73 | train_ae_norm     1.00\n",
      "[84/200][99/4361] Loss_D: 0.00204821 (Loss_D_real: 0.00082986 Loss_D_fake: 0.00121834) Loss_G: 0.45568705 Loss_Enh_Dec: -1.14975989\n",
      "| epoch  84 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.05 | loss  2.73 | ppl    15.38 | acc     0.65 | train_ae_norm     1.00\n",
      "[84/200][199/4361] Loss_D: 0.00420770 (Loss_D_real: 0.00271719 Loss_D_fake: 0.00149052) Loss_G: 0.46172401 Loss_Enh_Dec: -1.47243106\n",
      "| epoch  84 |   200/ 4361 batches | lr 0.000000 | ms/batch 400.78 | loss  2.77 | ppl    16.04 | acc     0.68 | train_ae_norm     1.00\n",
      "[84/200][299/4361] Loss_D: 0.00128389 (Loss_D_real: 0.00061884 Loss_D_fake: 0.00066505) Loss_G: 0.43264285 Loss_Enh_Dec: -1.79609585\n",
      "| epoch  84 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.72 | loss  2.77 | ppl    16.00 | acc     0.64 | train_ae_norm     1.00\n",
      "[84/200][399/4361] Loss_D: 0.00968472 (Loss_D_real: 0.00254576 Loss_D_fake: 0.00713896) Loss_G: 0.89064580 Loss_Enh_Dec: -1.61097109\n",
      "| epoch  84 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  2.70 | ppl    14.91 | acc     0.68 | train_ae_norm     1.00\n",
      "[84/200][499/4361] Loss_D: 0.00080462 (Loss_D_real: 0.00034705 Loss_D_fake: 0.00045757) Loss_G: 0.56607383 Loss_Enh_Dec: -1.64698851\n",
      "| epoch  84 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.63 | loss  2.77 | ppl    15.92 | acc     0.72 | train_ae_norm     1.00\n",
      "[84/200][599/4361] Loss_D: 0.00367955 (Loss_D_real: 0.00300624 Loss_D_fake: 0.00067331) Loss_G: 0.50207698 Loss_Enh_Dec: -1.72201085\n",
      "| epoch  84 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.06 | loss  2.72 | ppl    15.12 | acc     0.67 | train_ae_norm     1.00\n",
      "[84/200][699/4361] Loss_D: 0.01051607 (Loss_D_real: 0.00325701 Loss_D_fake: 0.00725907) Loss_G: 0.50219721 Loss_Enh_Dec: -1.66806686\n",
      "| epoch  84 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.18 | loss  2.77 | ppl    15.89 | acc     0.69 | train_ae_norm     1.00\n",
      "[84/200][799/4361] Loss_D: 0.00292582 (Loss_D_real: 0.00193434 Loss_D_fake: 0.00099148) Loss_G: 0.45723781 Loss_Enh_Dec: -1.68874133\n",
      "| epoch  84 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.11 | loss  2.74 | ppl    15.46 | acc     0.71 | train_ae_norm     1.00\n",
      "[84/200][899/4361] Loss_D: 0.01268004 (Loss_D_real: 0.01038289 Loss_D_fake: 0.00229715) Loss_G: 0.47546458 Loss_Enh_Dec: -1.82685077\n",
      "| epoch  84 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.35 | loss  2.76 | ppl    15.74 | acc     0.70 | train_ae_norm     1.00\n",
      "[84/200][999/4361] Loss_D: 0.00067321 (Loss_D_real: 0.00005086 Loss_D_fake: 0.00062235) Loss_G: 0.46464214 Loss_Enh_Dec: -1.63600671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  84 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  2.72 | ppl    15.20 | acc     0.70 | train_ae_norm     1.00\n",
      "[84/200][1099/4361] Loss_D: 0.00088619 (Loss_D_real: 0.00019896 Loss_D_fake: 0.00068723) Loss_G: 0.46425802 Loss_Enh_Dec: -1.97531021\n",
      "| epoch  84 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  2.73 | ppl    15.28 | acc     0.68 | train_ae_norm     1.00\n",
      "[84/200][1199/4361] Loss_D: 0.00521399 (Loss_D_real: 0.00247030 Loss_D_fake: 0.00274369) Loss_G: 0.97994119 Loss_Enh_Dec: -1.28040612\n",
      "| epoch  84 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.31 | loss  2.75 | ppl    15.62 | acc     0.70 | train_ae_norm     1.00\n",
      "[84/200][1299/4361] Loss_D: 0.00173025 (Loss_D_real: 0.00024229 Loss_D_fake: 0.00148797) Loss_G: 0.56929755 Loss_Enh_Dec: -1.20617104\n",
      "| epoch  84 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.81 | ppl    16.63 | acc     0.68 | train_ae_norm     1.00\n",
      "[84/200][1399/4361] Loss_D: 0.00157380 (Loss_D_real: 0.00025173 Loss_D_fake: 0.00132208) Loss_G: 0.46794224 Loss_Enh_Dec: -1.62941861\n",
      "| epoch  84 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  2.81 | ppl    16.58 | acc     0.64 | train_ae_norm     1.00\n",
      "[84/200][1499/4361] Loss_D: 0.00451401 (Loss_D_real: 0.00089486 Loss_D_fake: 0.00361915) Loss_G: 0.54793203 Loss_Enh_Dec: -1.71145654\n",
      "| epoch  84 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.84 | ppl    17.12 | acc     0.67 | train_ae_norm     1.00\n",
      "[84/200][1599/4361] Loss_D: 0.00199074 (Loss_D_real: 0.00015507 Loss_D_fake: 0.00183567) Loss_G: 0.49619538 Loss_Enh_Dec: -1.43178689\n",
      "| epoch  84 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  2.77 | ppl    15.93 | acc     0.68 | train_ae_norm     1.00\n",
      "[84/200][1699/4361] Loss_D: 0.02784560 (Loss_D_real: 0.00246940 Loss_D_fake: 0.02537620) Loss_G: 0.70009857 Loss_Enh_Dec: -1.28735435\n",
      "| epoch  84 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  2.73 | ppl    15.34 | acc     0.68 | train_ae_norm     1.00\n",
      "[84/200][1799/4361] Loss_D: 0.00333449 (Loss_D_real: 0.00185232 Loss_D_fake: 0.00148217) Loss_G: 0.56799865 Loss_Enh_Dec: -1.70432973\n",
      "| epoch  84 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.71 | ppl    15.00 | acc     0.70 | train_ae_norm     1.00\n",
      "[84/200][1899/4361] Loss_D: 0.00259595 (Loss_D_real: 0.00190132 Loss_D_fake: 0.00069463) Loss_G: 0.50873804 Loss_Enh_Dec: -1.83478320\n",
      "| epoch  84 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  2.78 | ppl    16.06 | acc     0.71 | train_ae_norm     1.00\n",
      "[84/200][1999/4361] Loss_D: 0.00351117 (Loss_D_real: 0.00281225 Loss_D_fake: 0.00069892) Loss_G: 0.57488328 Loss_Enh_Dec: -1.67736244\n",
      "| epoch  84 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  2.73 | ppl    15.26 | acc     0.71 | train_ae_norm     1.00\n",
      "[84/200][2099/4361] Loss_D: 0.00250805 (Loss_D_real: 0.00019058 Loss_D_fake: 0.00231748) Loss_G: 0.45319900 Loss_Enh_Dec: -1.50434601\n",
      "| epoch  84 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  2.79 | ppl    16.31 | acc     0.70 | train_ae_norm     1.00\n",
      "[84/200][2199/4361] Loss_D: 0.00522625 (Loss_D_real: 0.00480877 Loss_D_fake: 0.00041747) Loss_G: 0.49067053 Loss_Enh_Dec: -1.32193482\n",
      "| epoch  84 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.77 | ppl    16.03 | acc     0.69 | train_ae_norm     1.00\n",
      "[84/200][2299/4361] Loss_D: 0.00063629 (Loss_D_real: 0.00016758 Loss_D_fake: 0.00046871) Loss_G: 0.71587861 Loss_Enh_Dec: -1.47281492\n",
      "| epoch  84 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.23 | loss  2.73 | ppl    15.38 | acc     0.70 | train_ae_norm     1.00\n",
      "[84/200][2399/4361] Loss_D: 0.00254600 (Loss_D_real: 0.00160673 Loss_D_fake: 0.00093927) Loss_G: 0.44892940 Loss_Enh_Dec: -1.24070489\n",
      "| epoch  84 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  2.74 | ppl    15.43 | acc     0.66 | train_ae_norm     1.00\n",
      "[84/200][2499/4361] Loss_D: 0.00286918 (Loss_D_real: 0.00044503 Loss_D_fake: 0.00242414) Loss_G: 0.41787058 Loss_Enh_Dec: -1.20529890\n",
      "| epoch  84 |  2500/ 4361 batches | lr 0.000000 | ms/batch 400.79 | loss  2.76 | ppl    15.82 | acc     0.70 | train_ae_norm     1.00\n",
      "[84/200][2599/4361] Loss_D: 0.00261215 (Loss_D_real: 0.00087107 Loss_D_fake: 0.00174109) Loss_G: 0.45793197 Loss_Enh_Dec: -1.13502371\n",
      "| epoch  84 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  2.73 | ppl    15.39 | acc     0.66 | train_ae_norm     1.00\n",
      "[84/200][2699/4361] Loss_D: 0.00417471 (Loss_D_real: 0.00254201 Loss_D_fake: 0.00163270) Loss_G: 0.54451686 Loss_Enh_Dec: -1.35693622\n",
      "| epoch  84 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  2.73 | ppl    15.28 | acc     0.69 | train_ae_norm     1.00\n",
      "[84/200][2799/4361] Loss_D: 0.00276119 (Loss_D_real: 0.00227653 Loss_D_fake: 0.00048466) Loss_G: 0.46049261 Loss_Enh_Dec: -1.59076428\n",
      "| epoch  84 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.08 | loss  2.70 | ppl    14.88 | acc     0.66 | train_ae_norm     1.00\n",
      "[84/200][2899/4361] Loss_D: 0.01717321 (Loss_D_real: 0.01412234 Loss_D_fake: 0.00305088) Loss_G: 0.45673305 Loss_Enh_Dec: -1.53080499\n",
      "| epoch  84 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.66 | loss  2.73 | ppl    15.27 | acc     0.69 | train_ae_norm     1.00\n",
      "[84/200][2999/4361] Loss_D: 0.00166898 (Loss_D_real: 0.00007589 Loss_D_fake: 0.00159309) Loss_G: 0.45797750 Loss_Enh_Dec: -1.08414972\n",
      "| epoch  84 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.88 | loss  2.73 | ppl    15.29 | acc     0.70 | train_ae_norm     1.00\n",
      "[84/200][3099/4361] Loss_D: 0.00390292 (Loss_D_real: 0.00057829 Loss_D_fake: 0.00332463) Loss_G: 0.53678602 Loss_Enh_Dec: -1.42308116\n",
      "| epoch  84 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  2.75 | ppl    15.70 | acc     0.69 | train_ae_norm     1.00\n",
      "[84/200][3199/4361] Loss_D: 0.00053989 (Loss_D_real: 0.00052553 Loss_D_fake: 0.00001436) Loss_G: 0.91718513 Loss_Enh_Dec: -1.35678971\n",
      "| epoch  84 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.44 | loss  2.77 | ppl    16.01 | acc     0.69 | train_ae_norm     1.00\n",
      "[84/200][3299/4361] Loss_D: 0.00174457 (Loss_D_real: 0.00131926 Loss_D_fake: 0.00042531) Loss_G: 0.51338208 Loss_Enh_Dec: -0.83808243\n",
      "| epoch  84 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  2.76 | ppl    15.84 | acc     0.71 | train_ae_norm     1.00\n",
      "[84/200][3399/4361] Loss_D: 0.00786063 (Loss_D_real: 0.00702239 Loss_D_fake: 0.00083824) Loss_G: 0.48271808 Loss_Enh_Dec: -1.55566394\n",
      "| epoch  84 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  2.75 | ppl    15.60 | acc     0.67 | train_ae_norm     1.00\n",
      "[84/200][3499/4361] Loss_D: 0.02623362 (Loss_D_real: 0.00092172 Loss_D_fake: 0.02531190) Loss_G: 0.54744184 Loss_Enh_Dec: -1.12864876\n",
      "| epoch  84 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  2.68 | ppl    14.58 | acc     0.68 | train_ae_norm     1.00\n",
      "[84/200][3599/4361] Loss_D: 0.00184281 (Loss_D_real: 0.00091497 Loss_D_fake: 0.00092784) Loss_G: 0.49184209 Loss_Enh_Dec: -1.06206572\n",
      "| epoch  84 |  3600/ 4361 batches | lr 0.000000 | ms/batch 400.66 | loss  2.70 | ppl    14.88 | acc     0.70 | train_ae_norm     1.00\n",
      "[84/200][3699/4361] Loss_D: 0.00148797 (Loss_D_real: 0.00005972 Loss_D_fake: 0.00142825) Loss_G: 0.46962124 Loss_Enh_Dec: -1.21965718\n",
      "| epoch  84 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.90 | loss  2.71 | ppl    15.05 | acc     0.66 | train_ae_norm     1.00\n",
      "[84/200][3799/4361] Loss_D: 0.00441682 (Loss_D_real: 0.00054779 Loss_D_fake: 0.00386903) Loss_G: 0.45553389 Loss_Enh_Dec: -1.17111266\n",
      "| epoch  84 |  3800/ 4361 batches | lr 0.000000 | ms/batch 400.90 | loss  2.73 | ppl    15.34 | acc     0.74 | train_ae_norm     1.00\n",
      "[84/200][3899/4361] Loss_D: 0.00430166 (Loss_D_real: 0.00109554 Loss_D_fake: 0.00320612) Loss_G: 0.53772753 Loss_Enh_Dec: -1.02619171\n",
      "| epoch  84 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  2.74 | ppl    15.45 | acc     0.67 | train_ae_norm     1.00\n",
      "[84/200][3999/4361] Loss_D: 0.00091602 (Loss_D_real: 0.00034864 Loss_D_fake: 0.00056738) Loss_G: 0.46788859 Loss_Enh_Dec: -1.31755888\n",
      "| epoch  84 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  2.72 | ppl    15.23 | acc     0.70 | train_ae_norm     1.00\n",
      "[84/200][4099/4361] Loss_D: 0.00118925 (Loss_D_real: 0.00045384 Loss_D_fake: 0.00073541) Loss_G: 0.42599541 Loss_Enh_Dec: -1.28993917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  84 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.60 | loss  2.70 | ppl    14.83 | acc     0.69 | train_ae_norm     1.00\n",
      "[84/200][4199/4361] Loss_D: 0.00204874 (Loss_D_real: 0.00016231 Loss_D_fake: 0.00188643) Loss_G: 0.41643792 Loss_Enh_Dec: -1.83284950\n",
      "| epoch  84 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  2.75 | ppl    15.67 | acc     0.73 | train_ae_norm     1.00\n",
      "[84/200][4299/4361] Loss_D: 0.00090392 (Loss_D_real: 0.00042528 Loss_D_fake: 0.00047864) Loss_G: 0.49696198 Loss_Enh_Dec: -1.77017653\n",
      "| epoch  84 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.72 | loss  2.72 | ppl    15.25 | acc     0.70 | train_ae_norm     1.00\n",
      "| end of epoch  84 | time: 1850.91s | test loss  2.73 | test ppl 15.29 | acc 0.721\n",
      "bleu_self:  [3.88888889e-02 1.29918098e-09 4.36022037e-12 2.90108687e-13\n",
      " 7.45582650e-13]\n",
      "bleu_test:  [6.30555555e-01 1.11886667e-08 3.43280454e-11 1.90860284e-10\n",
      " 7.99105085e-10]\n",
      "bleu_self: [0.03888889,0.00000000,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.63055556,0.00000001,0.00000000,0.00000000,0.00000000]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 85 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.704\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.495\n",
      "  Test Loss: 4.356\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  85 |     0/ 4361 batches | lr 0.000000 | ms/batch 867.31 | loss  0.02 | ppl     1.02 | acc     0.70 | train_ae_norm     1.00\n",
      "[85/200][99/4361] Loss_D: 0.02427588 (Loss_D_real: 0.00473717 Loss_D_fake: 0.01953871) Loss_G: 0.54509640 Loss_Enh_Dec: -1.16957200\n",
      "| epoch  85 |   100/ 4361 batches | lr 0.000000 | ms/batch 400.76 | loss  2.72 | ppl    15.13 | acc     0.67 | train_ae_norm     1.00\n",
      "[85/200][199/4361] Loss_D: 0.01503831 (Loss_D_real: 0.01468581 Loss_D_fake: 0.00035250) Loss_G: 0.60500276 Loss_Enh_Dec: -1.23086810\n",
      "| epoch  85 |   200/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  2.75 | ppl    15.57 | acc     0.69 | train_ae_norm     1.00\n",
      "[85/200][299/4361] Loss_D: 0.00163846 (Loss_D_real: 0.00126504 Loss_D_fake: 0.00037342) Loss_G: 0.51292396 Loss_Enh_Dec: -1.58053625\n",
      "| epoch  85 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.52 | loss  2.74 | ppl    15.43 | acc     0.65 | train_ae_norm     1.00\n",
      "[85/200][399/4361] Loss_D: 0.01427036 (Loss_D_real: 0.01398393 Loss_D_fake: 0.00028643) Loss_G: 0.41874057 Loss_Enh_Dec: -1.50354421\n",
      "| epoch  85 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  2.67 | ppl    14.39 | acc     0.70 | train_ae_norm     1.00\n",
      "[85/200][499/4361] Loss_D: 0.00260393 (Loss_D_real: 0.00205356 Loss_D_fake: 0.00055038) Loss_G: 0.42595243 Loss_Enh_Dec: -1.66861498\n",
      "| epoch  85 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.06 | loss  2.73 | ppl    15.41 | acc     0.71 | train_ae_norm     1.00\n",
      "[85/200][599/4361] Loss_D: 0.00277210 (Loss_D_real: 0.00040073 Loss_D_fake: 0.00237137) Loss_G: 0.42673451 Loss_Enh_Dec: -1.79803360\n",
      "| epoch  85 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.76 | loss  2.68 | ppl    14.57 | acc     0.67 | train_ae_norm     1.00\n",
      "[85/200][699/4361] Loss_D: 0.00088197 (Loss_D_real: 0.00016559 Loss_D_fake: 0.00071637) Loss_G: 0.49792552 Loss_Enh_Dec: -1.70331037\n",
      "| epoch  85 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.31 | loss  2.72 | ppl    15.13 | acc     0.70 | train_ae_norm     1.00\n",
      "[85/200][799/4361] Loss_D: 0.00430708 (Loss_D_real: 0.00132534 Loss_D_fake: 0.00298174) Loss_G: 0.52786392 Loss_Enh_Dec: -1.43640888\n",
      "| epoch  85 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.86 | loss  2.71 | ppl    15.02 | acc     0.70 | train_ae_norm     1.00\n",
      "[85/200][899/4361] Loss_D: 0.00195239 (Loss_D_real: 0.00042362 Loss_D_fake: 0.00152877) Loss_G: 0.41082206 Loss_Enh_Dec: -1.60586131\n",
      "| epoch  85 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.41 | loss  2.72 | ppl    15.24 | acc     0.71 | train_ae_norm     1.00\n",
      "[85/200][999/4361] Loss_D: 0.00132802 (Loss_D_real: 0.00058455 Loss_D_fake: 0.00074348) Loss_G: 0.48776451 Loss_Enh_Dec: -1.22032011\n",
      "| epoch  85 |  1000/ 4361 batches | lr 0.000000 | ms/batch 400.22 | loss  2.69 | ppl    14.78 | acc     0.70 | train_ae_norm     1.00\n",
      "[85/200][1099/4361] Loss_D: 0.00160757 (Loss_D_real: 0.00090956 Loss_D_fake: 0.00069801) Loss_G: 0.44545570 Loss_Enh_Dec: -1.53817403\n",
      "| epoch  85 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  2.71 | ppl    14.99 | acc     0.65 | train_ae_norm     1.00\n",
      "[85/200][1199/4361] Loss_D: 0.00120391 (Loss_D_real: 0.00028194 Loss_D_fake: 0.00092197) Loss_G: 0.45972571 Loss_Enh_Dec: -1.51343060\n",
      "| epoch  85 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.68 | loss  2.71 | ppl    15.01 | acc     0.70 | train_ae_norm     1.00\n",
      "[85/200][1299/4361] Loss_D: 0.00227116 (Loss_D_real: 0.00101639 Loss_D_fake: 0.00125477) Loss_G: 0.48412251 Loss_Enh_Dec: -1.40681744\n",
      "| epoch  85 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.46 | loss  2.72 | ppl    15.15 | acc     0.68 | train_ae_norm     1.00\n",
      "[85/200][1399/4361] Loss_D: 0.00196209 (Loss_D_real: 0.00086272 Loss_D_fake: 0.00109937) Loss_G: 0.57340109 Loss_Enh_Dec: -1.59100223\n",
      "| epoch  85 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.39 | loss  2.73 | ppl    15.30 | acc     0.65 | train_ae_norm     1.00\n",
      "[85/200][1499/4361] Loss_D: 0.00207078 (Loss_D_real: 0.00081272 Loss_D_fake: 0.00125806) Loss_G: 0.47229949 Loss_Enh_Dec: -1.14764249\n",
      "| epoch  85 |  1500/ 4361 batches | lr 0.000000 | ms/batch 400.78 | loss  2.75 | ppl    15.64 | acc     0.65 | train_ae_norm     1.00\n",
      "[85/200][1599/4361] Loss_D: 0.02885044 (Loss_D_real: 0.02815491 Loss_D_fake: 0.00069553) Loss_G: 0.46125069 Loss_Enh_Dec: -1.33330464\n",
      "| epoch  85 |  1600/ 4361 batches | lr 0.000000 | ms/batch 400.48 | loss  2.73 | ppl    15.34 | acc     0.68 | train_ae_norm     1.00\n",
      "[85/200][1699/4361] Loss_D: 0.00209661 (Loss_D_real: 0.00168937 Loss_D_fake: 0.00040725) Loss_G: 0.69723964 Loss_Enh_Dec: -1.42065704\n",
      "| epoch  85 |  1700/ 4361 batches | lr 0.000000 | ms/batch 399.70 | loss  2.70 | ppl    14.92 | acc     0.69 | train_ae_norm     1.00\n",
      "[85/200][1799/4361] Loss_D: 0.00241214 (Loss_D_real: 0.00091820 Loss_D_fake: 0.00149394) Loss_G: 0.47429058 Loss_Enh_Dec: -1.41420066\n",
      "| epoch  85 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.37 | loss  2.67 | ppl    14.46 | acc     0.70 | train_ae_norm     1.00\n",
      "[85/200][1899/4361] Loss_D: 0.00081373 (Loss_D_real: 0.00013691 Loss_D_fake: 0.00067682) Loss_G: 0.42349625 Loss_Enh_Dec: -1.06606781\n",
      "| epoch  85 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.56 | loss  2.74 | ppl    15.44 | acc     0.73 | train_ae_norm     1.00\n",
      "[85/200][1999/4361] Loss_D: 0.00229528 (Loss_D_real: 0.00045579 Loss_D_fake: 0.00183949) Loss_G: 0.51022953 Loss_Enh_Dec: -1.68591082\n",
      "| epoch  85 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.68 | loss  2.66 | ppl    14.30 | acc     0.71 | train_ae_norm     1.00\n",
      "[85/200][2099/4361] Loss_D: 0.00063764 (Loss_D_real: 0.00032385 Loss_D_fake: 0.00031379) Loss_G: 0.51421326 Loss_Enh_Dec: -1.73603082\n",
      "| epoch  85 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.89 | loss  2.72 | ppl    15.14 | acc     0.70 | train_ae_norm     1.00\n",
      "[85/200][2199/4361] Loss_D: 0.00162100 (Loss_D_real: 0.00099916 Loss_D_fake: 0.00062184) Loss_G: 0.42348987 Loss_Enh_Dec: -1.28881359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  85 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.66 | loss  2.72 | ppl    15.25 | acc     0.69 | train_ae_norm     1.00\n",
      "[85/200][2299/4361] Loss_D: 0.00217584 (Loss_D_real: 0.00154866 Loss_D_fake: 0.00062718) Loss_G: 0.50608057 Loss_Enh_Dec: -1.42412603\n",
      "| epoch  85 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.93 | loss  2.71 | ppl    15.05 | acc     0.71 | train_ae_norm     1.00\n",
      "[85/200][2399/4361] Loss_D: 0.00189619 (Loss_D_real: 0.00111775 Loss_D_fake: 0.00077845) Loss_G: 0.43599817 Loss_Enh_Dec: -1.78932703\n",
      "| epoch  85 |  2400/ 4361 batches | lr 0.000000 | ms/batch 400.31 | loss  2.73 | ppl    15.27 | acc     0.65 | train_ae_norm     1.00\n",
      "[85/200][2499/4361] Loss_D: 0.00096879 (Loss_D_real: 0.00060581 Loss_D_fake: 0.00036297) Loss_G: 0.76979882 Loss_Enh_Dec: -1.66387784\n",
      "| epoch  85 |  2500/ 4361 batches | lr 0.000000 | ms/batch 400.93 | loss  2.73 | ppl    15.39 | acc     0.70 | train_ae_norm     1.00\n",
      "[85/200][2599/4361] Loss_D: 0.00997998 (Loss_D_real: 0.00053076 Loss_D_fake: 0.00944922) Loss_G: 0.76801032 Loss_Enh_Dec: -1.54042590\n",
      "| epoch  85 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  2.71 | ppl    15.04 | acc     0.65 | train_ae_norm     1.00\n",
      "[85/200][2699/4361] Loss_D: 0.00262655 (Loss_D_real: 0.00030812 Loss_D_fake: 0.00231843) Loss_G: 0.61938971 Loss_Enh_Dec: -1.47298110\n",
      "| epoch  85 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  2.71 | ppl    15.00 | acc     0.68 | train_ae_norm     1.00\n",
      "[85/200][2799/4361] Loss_D: 0.00097262 (Loss_D_real: 0.00018366 Loss_D_fake: 0.00078896) Loss_G: 0.48547727 Loss_Enh_Dec: -1.50612855\n",
      "| epoch  85 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  2.68 | ppl    14.53 | acc     0.68 | train_ae_norm     1.00\n",
      "[85/200][2899/4361] Loss_D: 0.00455466 (Loss_D_real: 0.00034528 Loss_D_fake: 0.00420938) Loss_G: 0.55255938 Loss_Enh_Dec: -1.60162914\n",
      "| epoch  85 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  2.70 | ppl    14.95 | acc     0.69 | train_ae_norm     1.00\n",
      "[85/200][2999/4361] Loss_D: 0.00083096 (Loss_D_real: 0.00031716 Loss_D_fake: 0.00051380) Loss_G: 0.45933315 Loss_Enh_Dec: -1.38682544\n",
      "| epoch  85 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  2.70 | ppl    14.89 | acc     0.72 | train_ae_norm     1.00\n",
      "[85/200][3099/4361] Loss_D: 0.00149756 (Loss_D_real: 0.00080628 Loss_D_fake: 0.00069128) Loss_G: 0.47522077 Loss_Enh_Dec: -1.50508261\n",
      "| epoch  85 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  2.70 | ppl    14.82 | acc     0.66 | train_ae_norm     1.00\n",
      "[85/200][3199/4361] Loss_D: 0.00266843 (Loss_D_real: 0.00030962 Loss_D_fake: 0.00235881) Loss_G: 0.47634736 Loss_Enh_Dec: -1.74352098\n",
      "| epoch  85 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  2.74 | ppl    15.43 | acc     0.69 | train_ae_norm     1.00\n",
      "[85/200][3299/4361] Loss_D: 0.00136839 (Loss_D_real: 0.00032637 Loss_D_fake: 0.00104202) Loss_G: 0.47938138 Loss_Enh_Dec: -1.87974381\n",
      "| epoch  85 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  2.73 | ppl    15.37 | acc     0.70 | train_ae_norm     1.00\n",
      "[85/200][3399/4361] Loss_D: 0.00330489 (Loss_D_real: 0.00235066 Loss_D_fake: 0.00095422) Loss_G: 0.39877892 Loss_Enh_Dec: -1.60282254\n",
      "| epoch  85 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  2.70 | ppl    14.94 | acc     0.67 | train_ae_norm     1.00\n",
      "[85/200][3499/4361] Loss_D: 0.00067729 (Loss_D_real: 0.00010524 Loss_D_fake: 0.00057205) Loss_G: 0.48459989 Loss_Enh_Dec: -1.70606768\n",
      "| epoch  85 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.85 | loss  2.65 | ppl    14.19 | acc     0.69 | train_ae_norm     1.00\n",
      "[85/200][3599/4361] Loss_D: 0.00058814 (Loss_D_real: 0.00014246 Loss_D_fake: 0.00044568) Loss_G: 0.46424127 Loss_Enh_Dec: -1.57217467\n",
      "| epoch  85 |  3600/ 4361 batches | lr 0.000000 | ms/batch 400.90 | loss  2.67 | ppl    14.51 | acc     0.71 | train_ae_norm     1.00\n",
      "[85/200][3699/4361] Loss_D: 0.03469357 (Loss_D_real: 0.03436482 Loss_D_fake: 0.00032874) Loss_G: 0.46246225 Loss_Enh_Dec: -1.77151227\n",
      "| epoch  85 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.70 | ppl    14.86 | acc     0.67 | train_ae_norm     1.00\n",
      "[85/200][3799/4361] Loss_D: 0.00126837 (Loss_D_real: 0.00029908 Loss_D_fake: 0.00096930) Loss_G: 0.47046939 Loss_Enh_Dec: -1.65468717\n",
      "| epoch  85 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  2.69 | ppl    14.77 | acc     0.73 | train_ae_norm     1.00\n",
      "[85/200][3899/4361] Loss_D: 0.00541038 (Loss_D_real: 0.00037943 Loss_D_fake: 0.00503095) Loss_G: 0.52748877 Loss_Enh_Dec: -1.89334548\n",
      "| epoch  85 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  2.73 | ppl    15.40 | acc     0.64 | train_ae_norm     1.00\n",
      "[85/200][3999/4361] Loss_D: 0.00133512 (Loss_D_real: 0.00097827 Loss_D_fake: 0.00035685) Loss_G: 0.50643891 Loss_Enh_Dec: -2.02122474\n",
      "| epoch  85 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  2.74 | ppl    15.43 | acc     0.70 | train_ae_norm     1.00\n",
      "[85/200][4099/4361] Loss_D: 0.00107720 (Loss_D_real: 0.00095227 Loss_D_fake: 0.00012493) Loss_G: 0.99482691 Loss_Enh_Dec: -1.53128684\n",
      "| epoch  85 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  2.69 | ppl    14.68 | acc     0.68 | train_ae_norm     1.00\n",
      "[85/200][4199/4361] Loss_D: 0.00336745 (Loss_D_real: 0.00227246 Loss_D_fake: 0.00109498) Loss_G: 0.55411410 Loss_Enh_Dec: -1.59881473\n",
      "| epoch  85 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  2.72 | ppl    15.24 | acc     0.72 | train_ae_norm     1.00\n",
      "[85/200][4299/4361] Loss_D: 0.00196268 (Loss_D_real: 0.00067538 Loss_D_fake: 0.00128730) Loss_G: 0.42282078 Loss_Enh_Dec: -1.63512695\n",
      "| epoch  85 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  2.70 | ppl    14.81 | acc     0.68 | train_ae_norm     1.00\n",
      "| end of epoch  85 | time: 1850.87s | test loss  2.73 | test ppl 15.33 | acc 0.720\n",
      "bleu_self:  [1.04893080e-01 3.68198422e-09 1.94305340e-11 3.58739426e-11\n",
      " 1.02020374e-10]\n",
      "bleu_test:  [6.84722222e-01 7.34771984e-02 1.30643703e-05 3.95447157e-06\n",
      " 1.98117162e-06]\n",
      "bleu_self: [0.10489308,0.00000000,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.68472222,0.07347720,0.00001306,0.00000395,0.00000198]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 86 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.703\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.490\n",
      "  Test Loss: 4.341\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  86 |     0/ 4361 batches | lr 0.000000 | ms/batch 866.40 | loss  0.02 | ppl     1.02 | acc     0.73 | train_ae_norm     1.00\n",
      "[86/200][99/4361] Loss_D: 0.00064500 (Loss_D_real: 0.00006616 Loss_D_fake: 0.00057884) Loss_G: 0.53941327 Loss_Enh_Dec: -1.53207231\n",
      "| epoch  86 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.94 | loss  2.70 | ppl    14.85 | acc     0.65 | train_ae_norm     1.00\n",
      "[86/200][199/4361] Loss_D: 0.00705285 (Loss_D_real: 0.00052126 Loss_D_fake: 0.00653160) Loss_G: 0.47170019 Loss_Enh_Dec: -1.75594997\n",
      "| epoch  86 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.73 | ppl    15.30 | acc     0.70 | train_ae_norm     1.00\n",
      "[86/200][299/4361] Loss_D: 0.00149229 (Loss_D_real: 0.00070635 Loss_D_fake: 0.00078594) Loss_G: 0.51863545 Loss_Enh_Dec: -1.91155815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  86 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.71 | loss  2.73 | ppl    15.33 | acc     0.64 | train_ae_norm     1.00\n",
      "[86/200][399/4361] Loss_D: 0.00114571 (Loss_D_real: 0.00092580 Loss_D_fake: 0.00021991) Loss_G: 0.62474984 Loss_Enh_Dec: -1.38110065\n",
      "| epoch  86 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.84 | loss  2.64 | ppl    14.02 | acc     0.67 | train_ae_norm     1.00\n",
      "[86/200][499/4361] Loss_D: 0.00621464 (Loss_D_real: 0.00059977 Loss_D_fake: 0.00561488) Loss_G: 0.45659038 Loss_Enh_Dec: -1.55303335\n",
      "| epoch  86 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  2.71 | ppl    14.98 | acc     0.71 | train_ae_norm     1.00\n",
      "[86/200][599/4361] Loss_D: 0.00264433 (Loss_D_real: 0.00054060 Loss_D_fake: 0.00210374) Loss_G: 0.48390028 Loss_Enh_Dec: -1.75576866\n",
      "| epoch  86 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  2.66 | ppl    14.28 | acc     0.65 | train_ae_norm     1.00\n",
      "[86/200][899/4361] Loss_D: 0.00152507 (Loss_D_real: 0.00049725 Loss_D_fake: 0.00102783) Loss_G: 0.61768496 Loss_Enh_Dec: -1.38371646\n",
      "| epoch  86 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  2.67 | ppl    14.50 | acc     0.71 | train_ae_norm     1.00\n",
      "[86/200][999/4361] Loss_D: 0.00321672 (Loss_D_real: 0.00258476 Loss_D_fake: 0.00063196) Loss_G: 0.50661862 Loss_Enh_Dec: -1.45893860\n",
      "| epoch  86 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.67 | ppl    14.43 | acc     0.71 | train_ae_norm     1.00\n",
      "[86/200][1099/4361] Loss_D: 0.00315548 (Loss_D_real: 0.00113920 Loss_D_fake: 0.00201628) Loss_G: 0.52976376 Loss_Enh_Dec: -1.44399142\n",
      "| epoch  86 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  2.67 | ppl    14.44 | acc     0.67 | train_ae_norm     1.00\n",
      "[86/200][1199/4361] Loss_D: 0.01144362 (Loss_D_real: 0.00094557 Loss_D_fake: 0.01049805) Loss_G: 0.45713302 Loss_Enh_Dec: -1.64869559\n",
      "| epoch  86 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.66 | ppl    14.34 | acc     0.71 | train_ae_norm     1.00\n",
      "[86/200][1299/4361] Loss_D: 0.00530522 (Loss_D_real: 0.00150500 Loss_D_fake: 0.00380021) Loss_G: 0.51103705 Loss_Enh_Dec: -1.16415489\n",
      "| epoch  86 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  2.69 | ppl    14.72 | acc     0.71 | train_ae_norm     1.00\n",
      "[86/200][1399/4361] Loss_D: 0.05937669 (Loss_D_real: 0.02068630 Loss_D_fake: 0.03869038) Loss_G: 0.60726780 Loss_Enh_Dec: -1.17613888\n",
      "| epoch  86 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  2.70 | ppl    14.92 | acc     0.66 | train_ae_norm     1.00\n",
      "[86/200][1499/4361] Loss_D: 0.00145074 (Loss_D_real: 0.00062425 Loss_D_fake: 0.00082650) Loss_G: 0.49322939 Loss_Enh_Dec: -1.57652009\n",
      "| epoch  86 |  1500/ 4361 batches | lr 0.000000 | ms/batch 402.21 | loss  2.73 | ppl    15.36 | acc     0.67 | train_ae_norm     1.00\n",
      "[86/200][1599/4361] Loss_D: 0.00310563 (Loss_D_real: 0.00167704 Loss_D_fake: 0.00142859) Loss_G: 0.55021363 Loss_Enh_Dec: -1.16095650\n",
      "| epoch  86 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  2.70 | ppl    14.89 | acc     0.69 | train_ae_norm     1.00\n",
      "[86/200][1699/4361] Loss_D: 0.00203338 (Loss_D_real: 0.00058846 Loss_D_fake: 0.00144491) Loss_G: 0.46067134 Loss_Enh_Dec: -1.29122531\n",
      "| epoch  86 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.68 | ppl    14.65 | acc     0.69 | train_ae_norm     1.00\n",
      "[86/200][1799/4361] Loss_D: 0.01016957 (Loss_D_real: 0.00301479 Loss_D_fake: 0.00715478) Loss_G: 0.32430845 Loss_Enh_Dec: -0.98582858\n",
      "| epoch  86 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.66 | ppl    14.35 | acc     0.69 | train_ae_norm     1.00\n",
      "[86/200][1899/4361] Loss_D: 0.01635300 (Loss_D_real: 0.01032331 Loss_D_fake: 0.00602969) Loss_G: 0.25684631 Loss_Enh_Dec: -1.08725488\n",
      "| epoch  86 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  2.71 | ppl    15.02 | acc     0.73 | train_ae_norm     1.00\n",
      "[86/200][1999/4361] Loss_D: 0.01024170 (Loss_D_real: 0.00454107 Loss_D_fake: 0.00570063) Loss_G: 0.26572528 Loss_Enh_Dec: -1.05641460\n",
      "| epoch  86 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  2.67 | ppl    14.42 | acc     0.72 | train_ae_norm     1.00\n",
      "[86/200][2099/4361] Loss_D: 0.00468147 (Loss_D_real: 0.00072656 Loss_D_fake: 0.00395491) Loss_G: 0.30019251 Loss_Enh_Dec: -1.30753744\n",
      "| epoch  86 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  2.68 | ppl    14.57 | acc     0.71 | train_ae_norm     1.00\n",
      "[86/200][2199/4361] Loss_D: 0.00629842 (Loss_D_real: 0.00154761 Loss_D_fake: 0.00475081) Loss_G: 0.27744666 Loss_Enh_Dec: -1.41259110\n",
      "| epoch  86 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  2.68 | ppl    14.59 | acc     0.71 | train_ae_norm     1.00\n",
      "[86/200][2299/4361] Loss_D: 0.00468423 (Loss_D_real: 0.00028109 Loss_D_fake: 0.00440314) Loss_G: 0.27673718 Loss_Enh_Dec: -1.17473221\n",
      "| epoch  86 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  2.67 | ppl    14.46 | acc     0.73 | train_ae_norm     1.00\n",
      "[86/200][2399/4361] Loss_D: 0.00897521 (Loss_D_real: 0.00522063 Loss_D_fake: 0.00375458) Loss_G: 0.29270190 Loss_Enh_Dec: -1.19494081\n",
      "| epoch  86 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  2.66 | ppl    14.31 | acc     0.68 | train_ae_norm     1.00\n",
      "[86/200][2499/4361] Loss_D: 0.01146825 (Loss_D_real: 0.00746720 Loss_D_fake: 0.00400105) Loss_G: 0.28579414 Loss_Enh_Dec: -1.19598353\n",
      "| epoch  86 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  2.72 | ppl    15.16 | acc     0.71 | train_ae_norm     1.00\n",
      "[86/200][2599/4361] Loss_D: 0.00653001 (Loss_D_real: 0.00073578 Loss_D_fake: 0.00579423) Loss_G: 0.26973274 Loss_Enh_Dec: -1.19334173\n",
      "| epoch  86 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  2.68 | ppl    14.60 | acc     0.67 | train_ae_norm     1.00\n",
      "[86/200][2699/4361] Loss_D: 0.00634836 (Loss_D_real: 0.00229940 Loss_D_fake: 0.00404896) Loss_G: 0.28588110 Loss_Enh_Dec: -0.96033573\n",
      "| epoch  86 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  2.67 | ppl    14.50 | acc     0.68 | train_ae_norm     1.00\n",
      "[86/200][2799/4361] Loss_D: 0.00845495 (Loss_D_real: 0.00523454 Loss_D_fake: 0.00322041) Loss_G: 0.29219541 Loss_Enh_Dec: -1.33885956\n",
      "| epoch  86 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.66 | ppl    14.23 | acc     0.68 | train_ae_norm     1.00\n",
      "[86/200][2899/4361] Loss_D: 0.00413317 (Loss_D_real: 0.00068561 Loss_D_fake: 0.00344756) Loss_G: 0.28668252 Loss_Enh_Dec: -1.20096266\n",
      "| epoch  86 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  2.66 | ppl    14.28 | acc     0.69 | train_ae_norm     1.00\n",
      "[86/200][2999/4361] Loss_D: 0.03642562 (Loss_D_real: 0.03316010 Loss_D_fake: 0.00326552) Loss_G: 0.28962657 Loss_Enh_Dec: -1.25541401\n",
      "| epoch  86 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.66 | ppl    14.29 | acc     0.69 | train_ae_norm     1.00\n",
      "[86/200][3099/4361] Loss_D: 0.00347650 (Loss_D_real: 0.00044310 Loss_D_fake: 0.00303340) Loss_G: 0.29141957 Loss_Enh_Dec: -1.17745924\n",
      "| epoch  86 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  2.68 | ppl    14.65 | acc     0.67 | train_ae_norm     1.00\n",
      "[86/200][3199/4361] Loss_D: 0.00377017 (Loss_D_real: 0.00116238 Loss_D_fake: 0.00260780) Loss_G: 0.29576856 Loss_Enh_Dec: -1.71048963\n",
      "| epoch  86 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.71 | ppl    15.10 | acc     0.68 | train_ae_norm     1.00\n",
      "[86/200][3299/4361] Loss_D: 0.00410327 (Loss_D_real: 0.00114499 Loss_D_fake: 0.00295828) Loss_G: 0.29588783 Loss_Enh_Dec: -1.50702751\n",
      "| epoch  86 |  3300/ 4361 batches | lr 0.000000 | ms/batch 402.54 | loss  2.71 | ppl    15.05 | acc     0.69 | train_ae_norm     1.00\n",
      "[86/200][3399/4361] Loss_D: 0.00326497 (Loss_D_real: 0.00104013 Loss_D_fake: 0.00222484) Loss_G: 0.31547371 Loss_Enh_Dec: -1.48477972\n",
      "| epoch  86 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  2.70 | ppl    14.85 | acc     0.68 | train_ae_norm     1.00\n",
      "[86/200][3499/4361] Loss_D: 0.00474828 (Loss_D_real: 0.00243984 Loss_D_fake: 0.00230844) Loss_G: 0.30424437 Loss_Enh_Dec: -0.98220998\n",
      "| epoch  86 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  2.63 | ppl    13.89 | acc     0.68 | train_ae_norm     1.00\n",
      "[86/200][3599/4361] Loss_D: 0.00311504 (Loss_D_real: 0.00056810 Loss_D_fake: 0.00254693) Loss_G: 0.29885322 Loss_Enh_Dec: -1.48591495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  86 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  2.67 | ppl    14.43 | acc     0.71 | train_ae_norm     1.00\n",
      "[86/200][3699/4361] Loss_D: 0.00272644 (Loss_D_real: 0.00047449 Loss_D_fake: 0.00225195) Loss_G: 0.30868530 Loss_Enh_Dec: -1.50929344\n",
      "| epoch  86 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  2.68 | ppl    14.54 | acc     0.68 | train_ae_norm     1.00\n",
      "[86/200][3799/4361] Loss_D: 0.00311611 (Loss_D_real: 0.00019223 Loss_D_fake: 0.00292388) Loss_G: 0.29987928 Loss_Enh_Dec: -1.69385076\n",
      "| epoch  86 |  3800/ 4361 batches | lr 0.000000 | ms/batch 402.09 | loss  2.70 | ppl    14.89 | acc     0.74 | train_ae_norm     1.00\n",
      "[86/200][3899/4361] Loss_D: 0.00400157 (Loss_D_real: 0.00026194 Loss_D_fake: 0.00373963) Loss_G: 0.28819415 Loss_Enh_Dec: -1.84820461\n",
      "| epoch  86 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  2.69 | ppl    14.79 | acc     0.68 | train_ae_norm     1.00\n",
      "[86/200][3999/4361] Loss_D: 0.00238471 (Loss_D_real: 0.00018638 Loss_D_fake: 0.00219833) Loss_G: 0.30998886 Loss_Enh_Dec: -1.76325977\n",
      "| epoch  86 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.99 | loss  2.70 | ppl    14.86 | acc     0.70 | train_ae_norm     1.00\n",
      "[86/200][4099/4361] Loss_D: 0.00370973 (Loss_D_real: 0.00014618 Loss_D_fake: 0.00356356) Loss_G: 0.29881707 Loss_Enh_Dec: -1.77537429\n",
      "| epoch  86 |  4100/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  2.68 | ppl    14.53 | acc     0.69 | train_ae_norm     1.00\n",
      "[86/200][4199/4361] Loss_D: 0.00402533 (Loss_D_real: 0.00164606 Loss_D_fake: 0.00237927) Loss_G: 0.30754206 Loss_Enh_Dec: -1.96822858\n",
      "| epoch  86 |  4200/ 4361 batches | lr 0.000000 | ms/batch 402.24 | loss  2.71 | ppl    15.02 | acc     0.71 | train_ae_norm     1.00\n",
      "[86/200][4299/4361] Loss_D: 0.00436075 (Loss_D_real: 0.00235716 Loss_D_fake: 0.00200358) Loss_G: 0.31039950 Loss_Enh_Dec: -1.73910511\n",
      "| epoch  86 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  2.68 | ppl    14.53 | acc     0.68 | train_ae_norm     1.00\n",
      "| end of epoch  86 | time: 1853.61s | test loss  2.71 | test ppl 15.03 | acc 0.724\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 87 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.702\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.490\n",
      "  Test Loss: 4.325\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  87 |     0/ 4361 batches | lr 0.000000 | ms/batch 864.36 | loss  0.02 | ppl     1.02 | acc     0.74 | train_ae_norm     1.00\n",
      "[87/200][99/4361] Loss_D: 0.00320659 (Loss_D_real: 0.00117891 Loss_D_fake: 0.00202768) Loss_G: 0.32565337 Loss_Enh_Dec: -1.93263364\n",
      "| epoch  87 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  2.67 | ppl    14.45 | acc     0.65 | train_ae_norm     1.00\n",
      "[87/200][199/4361] Loss_D: 0.00241247 (Loss_D_real: 0.00066599 Loss_D_fake: 0.00174647) Loss_G: 0.32989225 Loss_Enh_Dec: -1.67067683\n",
      "| epoch  87 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  2.71 | ppl    15.01 | acc     0.68 | train_ae_norm     1.00\n",
      "[87/200][299/4361] Loss_D: 0.00381353 (Loss_D_real: 0.00232667 Loss_D_fake: 0.00148685) Loss_G: 0.33756158 Loss_Enh_Dec: -1.46298778\n",
      "| epoch  87 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  2.71 | ppl    15.02 | acc     0.65 | train_ae_norm     1.00\n",
      "[87/200][399/4361] Loss_D: 0.00354760 (Loss_D_real: 0.00165845 Loss_D_fake: 0.00188915) Loss_G: 0.32875690 Loss_Enh_Dec: -1.72032726\n",
      "| epoch  87 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.76 | loss  2.62 | ppl    13.72 | acc     0.71 | train_ae_norm     1.00\n",
      "[87/200][499/4361] Loss_D: 0.01024141 (Loss_D_real: 0.00901202 Loss_D_fake: 0.00122939) Loss_G: 0.34787014 Loss_Enh_Dec: -1.42190516\n",
      "| epoch  87 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.70 | ppl    14.92 | acc     0.73 | train_ae_norm     1.00\n",
      "[87/200][599/4361] Loss_D: 0.00245460 (Loss_D_real: 0.00077760 Loss_D_fake: 0.00167700) Loss_G: 0.34717408 Loss_Enh_Dec: -1.99556005\n",
      "| epoch  87 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.66 | ppl    14.25 | acc     0.66 | train_ae_norm     1.00\n",
      "[87/200][699/4361] Loss_D: 0.00227015 (Loss_D_real: 0.00022879 Loss_D_fake: 0.00204136) Loss_G: 0.31498742 Loss_Enh_Dec: -2.08621216\n",
      "| epoch  87 |   700/ 4361 batches | lr 0.000000 | ms/batch 402.52 | loss  2.70 | ppl    14.94 | acc     0.71 | train_ae_norm     1.00\n",
      "[87/200][799/4361] Loss_D: 0.00867530 (Loss_D_real: 0.00661155 Loss_D_fake: 0.00206375) Loss_G: 0.32504645 Loss_Enh_Dec: -2.20184398\n",
      "| epoch  87 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  2.68 | ppl    14.61 | acc     0.70 | train_ae_norm     1.00\n",
      "[87/200][899/4361] Loss_D: 0.00177484 (Loss_D_real: 0.00026666 Loss_D_fake: 0.00150818) Loss_G: 0.33441448 Loss_Enh_Dec: -2.22024918\n",
      "| epoch  87 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  2.70 | ppl    14.91 | acc     0.70 | train_ae_norm     1.00\n",
      "[87/200][999/4361] Loss_D: 0.00142235 (Loss_D_real: 0.00015805 Loss_D_fake: 0.00126430) Loss_G: 0.35820127 Loss_Enh_Dec: -2.05391574\n",
      "| epoch  87 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  2.67 | ppl    14.46 | acc     0.71 | train_ae_norm     1.00\n",
      "[87/200][1099/4361] Loss_D: 0.00257163 (Loss_D_real: 0.00130946 Loss_D_fake: 0.00126217) Loss_G: 0.33329961 Loss_Enh_Dec: -2.16814041\n",
      "| epoch  87 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.68 | ppl    14.62 | acc     0.68 | train_ae_norm     1.00\n",
      "[87/200][1199/4361] Loss_D: 0.00434113 (Loss_D_real: 0.00235515 Loss_D_fake: 0.00198598) Loss_G: 0.32240826 Loss_Enh_Dec: -1.76556671\n",
      "| epoch  87 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  2.69 | ppl    14.69 | acc     0.70 | train_ae_norm     1.00\n",
      "[87/200][1299/4361] Loss_D: 0.00173464 (Loss_D_real: 0.00017099 Loss_D_fake: 0.00156365) Loss_G: 0.33318204 Loss_Enh_Dec: -2.13815284\n",
      "| epoch  87 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  2.72 | ppl    15.19 | acc     0.70 | train_ae_norm     1.00\n",
      "[87/200][1399/4361] Loss_D: 0.00136347 (Loss_D_real: 0.00017315 Loss_D_fake: 0.00119032) Loss_G: 0.34511310 Loss_Enh_Dec: -2.02586746\n",
      "| epoch  87 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  2.72 | ppl    15.11 | acc     0.66 | train_ae_norm     1.00\n",
      "[87/200][1499/4361] Loss_D: 0.00170331 (Loss_D_real: 0.00025233 Loss_D_fake: 0.00145098) Loss_G: 0.32768494 Loss_Enh_Dec: -2.24071264\n",
      "| epoch  87 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  2.79 | ppl    16.28 | acc     0.66 | train_ae_norm     1.00\n",
      "[87/200][1599/4361] Loss_D: 0.00132984 (Loss_D_real: 0.00013722 Loss_D_fake: 0.00119262) Loss_G: 0.33960930 Loss_Enh_Dec: -1.89832914\n",
      "| epoch  87 |  1600/ 4361 batches | lr 0.000000 | ms/batch 402.22 | loss  2.76 | ppl    15.75 | acc     0.67 | train_ae_norm     1.00\n",
      "[87/200][1699/4361] Loss_D: 0.00123044 (Loss_D_real: 0.00004702 Loss_D_fake: 0.00118342) Loss_G: 0.34931728 Loss_Enh_Dec: -1.77969193\n",
      "| epoch  87 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.91 | loss  2.71 | ppl    15.02 | acc     0.68 | train_ae_norm     1.00\n",
      "[87/200][1899/4361] Loss_D: 0.00144766 (Loss_D_real: 0.00014136 Loss_D_fake: 0.00130630) Loss_G: 0.33988062 Loss_Enh_Dec: -2.22124100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  87 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  2.77 | ppl    16.04 | acc     0.69 | train_ae_norm     1.00\n",
      "[87/200][1999/4361] Loss_D: 0.00228405 (Loss_D_real: 0.00090382 Loss_D_fake: 0.00138024) Loss_G: 0.33761624 Loss_Enh_Dec: -2.28333187\n",
      "| epoch  87 |  2000/ 4361 batches | lr 0.000000 | ms/batch 402.31 | loss  2.72 | ppl    15.15 | acc     0.69 | train_ae_norm     1.00\n",
      "[87/200][2099/4361] Loss_D: 0.00401868 (Loss_D_real: 0.00007443 Loss_D_fake: 0.00394425) Loss_G: 0.32924125 Loss_Enh_Dec: -2.37027812\n",
      "| epoch  87 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  2.76 | ppl    15.80 | acc     0.70 | train_ae_norm     1.00\n",
      "[87/200][2199/4361] Loss_D: 0.00224448 (Loss_D_real: 0.00080908 Loss_D_fake: 0.00143541) Loss_G: 0.33096480 Loss_Enh_Dec: -1.34551942\n",
      "| epoch  87 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  2.73 | ppl    15.40 | acc     0.70 | train_ae_norm     1.00\n",
      "[87/200][2299/4361] Loss_D: 0.00172675 (Loss_D_real: 0.00007235 Loss_D_fake: 0.00165440) Loss_G: 0.32761642 Loss_Enh_Dec: -2.05727959\n",
      "| epoch  87 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  2.72 | ppl    15.22 | acc     0.70 | train_ae_norm     1.00\n",
      "[87/200][2399/4361] Loss_D: 0.00202863 (Loss_D_real: 0.00041479 Loss_D_fake: 0.00161384) Loss_G: 0.32171628 Loss_Enh_Dec: -1.76166570\n",
      "| epoch  87 |  2400/ 4361 batches | lr 0.000000 | ms/batch 402.36 | loss  2.73 | ppl    15.26 | acc     0.65 | train_ae_norm     1.00\n",
      "[87/200][2499/4361] Loss_D: 0.00116733 (Loss_D_real: 0.00006601 Loss_D_fake: 0.00110132) Loss_G: 0.34746704 Loss_Enh_Dec: -2.16961026\n",
      "| epoch  87 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  2.76 | ppl    15.74 | acc     0.69 | train_ae_norm     1.00\n",
      "[87/200][2599/4361] Loss_D: 0.00167960 (Loss_D_real: 0.00007548 Loss_D_fake: 0.00160412) Loss_G: 0.35062957 Loss_Enh_Dec: -2.36838317\n",
      "| epoch  87 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  2.73 | ppl    15.30 | acc     0.66 | train_ae_norm     1.00\n",
      "[87/200][2699/4361] Loss_D: 0.00160674 (Loss_D_real: 0.00056411 Loss_D_fake: 0.00104263) Loss_G: 0.35336435 Loss_Enh_Dec: -2.42391825\n",
      "| epoch  87 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  2.72 | ppl    15.23 | acc     0.67 | train_ae_norm     1.00\n",
      "[87/200][2799/4361] Loss_D: 0.00120263 (Loss_D_real: 0.00013210 Loss_D_fake: 0.00107053) Loss_G: 0.35155198 Loss_Enh_Dec: -2.41653347\n",
      "| epoch  87 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  2.67 | ppl    14.42 | acc     0.68 | train_ae_norm     1.00\n",
      "[87/200][2899/4361] Loss_D: 0.00157614 (Loss_D_real: 0.00026727 Loss_D_fake: 0.00130886) Loss_G: 0.32913589 Loss_Enh_Dec: -2.75936198\n",
      "| epoch  87 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  2.70 | ppl    14.86 | acc     0.69 | train_ae_norm     1.00\n",
      "[87/200][2999/4361] Loss_D: 0.00204826 (Loss_D_real: 0.00065694 Loss_D_fake: 0.00139133) Loss_G: 0.33642420 Loss_Enh_Dec: -2.51060653\n",
      "| epoch  87 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  2.70 | ppl    14.83 | acc     0.69 | train_ae_norm     1.00\n",
      "[87/200][3099/4361] Loss_D: 0.00128320 (Loss_D_real: 0.00031775 Loss_D_fake: 0.00096545) Loss_G: 0.36448112 Loss_Enh_Dec: -2.34135103\n",
      "| epoch  87 |  3100/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  2.71 | ppl    14.97 | acc     0.68 | train_ae_norm     1.00\n",
      "[87/200][3199/4361] Loss_D: 0.00125587 (Loss_D_real: 0.00004447 Loss_D_fake: 0.00121140) Loss_G: 0.34738776 Loss_Enh_Dec: -2.29217982\n",
      "| epoch  87 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.50 | loss  2.74 | ppl    15.41 | acc     0.70 | train_ae_norm     1.00\n",
      "[87/200][3299/4361] Loss_D: 0.00159613 (Loss_D_real: 0.00013710 Loss_D_fake: 0.00145903) Loss_G: 0.34622782 Loss_Enh_Dec: -2.42360806\n",
      "| epoch  87 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  2.74 | ppl    15.44 | acc     0.68 | train_ae_norm     1.00\n",
      "[87/200][3399/4361] Loss_D: 0.00105330 (Loss_D_real: 0.00043090 Loss_D_fake: 0.00062240) Loss_G: 0.39670390 Loss_Enh_Dec: -2.36393952\n",
      "| epoch  87 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.59 | loss  2.72 | ppl    15.13 | acc     0.67 | train_ae_norm     1.00\n",
      "[87/200][3499/4361] Loss_D: 0.00123130 (Loss_D_real: 0.00052550 Loss_D_fake: 0.00070579) Loss_G: 0.37489042 Loss_Enh_Dec: -2.16612220\n",
      "| epoch  87 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  2.66 | ppl    14.31 | acc     0.69 | train_ae_norm     1.00\n",
      "[87/200][3599/4361] Loss_D: 0.00209332 (Loss_D_real: 0.00074826 Loss_D_fake: 0.00134506) Loss_G: 0.35923019 Loss_Enh_Dec: -2.22684145\n",
      "| epoch  87 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  2.69 | ppl    14.77 | acc     0.72 | train_ae_norm     1.00\n",
      "[87/200][3699/4361] Loss_D: 0.00332198 (Loss_D_real: 0.00258855 Loss_D_fake: 0.00073343) Loss_G: 0.37843591 Loss_Enh_Dec: -2.30938721\n",
      "| epoch  87 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  2.71 | ppl    15.07 | acc     0.65 | train_ae_norm     1.00\n",
      "[87/200][3799/4361] Loss_D: 0.00219640 (Loss_D_real: 0.00091298 Loss_D_fake: 0.00128342) Loss_G: 0.34807944 Loss_Enh_Dec: -2.31462550\n",
      "| epoch  87 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  2.72 | ppl    15.22 | acc     0.74 | train_ae_norm     1.00\n",
      "[87/200][3899/4361] Loss_D: 0.00275241 (Loss_D_real: 0.00176418 Loss_D_fake: 0.00098822) Loss_G: 0.35376754 Loss_Enh_Dec: -2.20426702\n",
      "| epoch  87 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  2.76 | ppl    15.73 | acc     0.65 | train_ae_norm     1.00\n",
      "[87/200][3999/4361] Loss_D: 0.00252044 (Loss_D_real: 0.00174093 Loss_D_fake: 0.00077951) Loss_G: 0.38515911 Loss_Enh_Dec: -2.37111068\n",
      "| epoch  87 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  2.74 | ppl    15.55 | acc     0.71 | train_ae_norm     1.00\n",
      "[87/200][4099/4361] Loss_D: 0.00199674 (Loss_D_real: 0.00098251 Loss_D_fake: 0.00101422) Loss_G: 0.34735203 Loss_Enh_Dec: -2.14158487\n",
      "| epoch  87 |  4100/ 4361 batches | lr 0.000000 | ms/batch 402.11 | loss  2.71 | ppl    15.01 | acc     0.66 | train_ae_norm     1.00\n",
      "[87/200][4199/4361] Loss_D: 0.00238423 (Loss_D_real: 0.00125264 Loss_D_fake: 0.00113159) Loss_G: 0.35474545 Loss_Enh_Dec: -2.06442952\n",
      "| epoch  87 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.77 | ppl    15.92 | acc     0.73 | train_ae_norm     1.00\n",
      "[87/200][4299/4361] Loss_D: 0.00155742 (Loss_D_real: 0.00002270 Loss_D_fake: 0.00153472) Loss_G: 0.33827797 Loss_Enh_Dec: -2.07095408\n",
      "| epoch  87 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.73 | loss  2.75 | ppl    15.56 | acc     0.70 | train_ae_norm     1.00\n",
      "| end of epoch  87 | time: 1853.09s | test loss  2.75 | test ppl 15.59 | acc 0.720\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 88 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.702\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.490\n",
      "  Test Loss: 4.228\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  88 |     0/ 4361 batches | lr 0.000000 | ms/batch 865.31 | loss  0.02 | ppl     1.02 | acc     0.71 | train_ae_norm     1.00\n",
      "[88/200][99/4361] Loss_D: 0.00180094 (Loss_D_real: 0.00060444 Loss_D_fake: 0.00119650) Loss_G: 0.34335297 Loss_Enh_Dec: -2.38098741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  88 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  2.75 | ppl    15.58 | acc     0.65 | train_ae_norm     1.00\n",
      "[88/200][199/4361] Loss_D: 0.00140763 (Loss_D_real: 0.00014114 Loss_D_fake: 0.00126648) Loss_G: 0.34010673 Loss_Enh_Dec: -2.10969853\n",
      "| epoch  88 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  2.77 | ppl    16.01 | acc     0.70 | train_ae_norm     1.00\n",
      "[88/200][299/4361] Loss_D: 0.00251467 (Loss_D_real: 0.00223173 Loss_D_fake: 0.00028293) Loss_G: 0.49105516 Loss_Enh_Dec: -1.63195789\n",
      "| epoch  88 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.84 | loss  2.77 | ppl    15.99 | acc     0.64 | train_ae_norm     1.00\n",
      "[88/200][399/4361] Loss_D: 0.00210737 (Loss_D_real: 0.00039619 Loss_D_fake: 0.00171118) Loss_G: 0.32756168 Loss_Enh_Dec: -2.15370154\n",
      "| epoch  88 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  2.71 | ppl    15.05 | acc     0.69 | train_ae_norm     1.00\n",
      "[88/200][499/4361] Loss_D: 0.00140027 (Loss_D_real: 0.00034349 Loss_D_fake: 0.00105678) Loss_G: 0.37222716 Loss_Enh_Dec: -2.25714850\n",
      "| epoch  88 |   500/ 4361 batches | lr 0.000000 | ms/batch 402.17 | loss  2.79 | ppl    16.26 | acc     0.71 | train_ae_norm     1.00\n",
      "[88/200][599/4361] Loss_D: 0.00654889 (Loss_D_real: 0.00149254 Loss_D_fake: 0.00505634) Loss_G: 0.36070690 Loss_Enh_Dec: -1.57318389\n",
      "| epoch  88 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.83 | loss  2.77 | ppl    15.97 | acc     0.64 | train_ae_norm     1.00\n",
      "[88/200][699/4361] Loss_D: 0.00150825 (Loss_D_real: 0.00054131 Loss_D_fake: 0.00096693) Loss_G: 0.36970463 Loss_Enh_Dec: -1.77227402\n",
      "| epoch  88 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.77 | ppl    15.92 | acc     0.68 | train_ae_norm     1.00\n",
      "[88/200][799/4361] Loss_D: 0.00252157 (Loss_D_real: 0.00117282 Loss_D_fake: 0.00134875) Loss_G: 0.35495022 Loss_Enh_Dec: -1.91403043\n",
      "| epoch  88 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.90 | loss  2.75 | ppl    15.71 | acc     0.67 | train_ae_norm     1.00\n",
      "[88/200][899/4361] Loss_D: 0.00127145 (Loss_D_real: 0.00027597 Loss_D_fake: 0.00099548) Loss_G: 0.36955523 Loss_Enh_Dec: -2.00039101\n",
      "| epoch  88 |   900/ 4361 batches | lr 0.000000 | ms/batch 402.13 | loss  2.75 | ppl    15.61 | acc     0.71 | train_ae_norm     1.00\n",
      "[88/200][999/4361] Loss_D: 0.00090395 (Loss_D_real: 0.00012774 Loss_D_fake: 0.00077621) Loss_G: 0.38859177 Loss_Enh_Dec: -1.86413562\n",
      "| epoch  88 |  1000/ 4361 batches | lr 0.000000 | ms/batch 400.79 | loss  2.74 | ppl    15.48 | acc     0.68 | train_ae_norm     1.00\n",
      "[88/200][1099/4361] Loss_D: 0.00079096 (Loss_D_real: 0.00026510 Loss_D_fake: 0.00052586) Loss_G: 0.40340844 Loss_Enh_Dec: -2.13334727\n",
      "| epoch  88 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  2.73 | ppl    15.35 | acc     0.68 | train_ae_norm     1.00\n",
      "[88/200][1199/4361] Loss_D: 0.00083678 (Loss_D_real: 0.00008877 Loss_D_fake: 0.00074801) Loss_G: 0.44361988 Loss_Enh_Dec: -2.08317542\n",
      "| epoch  88 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.72 | ppl    15.21 | acc     0.70 | train_ae_norm     1.00\n",
      "[88/200][1299/4361] Loss_D: 0.00247407 (Loss_D_real: 0.00169465 Loss_D_fake: 0.00077942) Loss_G: 0.37715653 Loss_Enh_Dec: -1.63426960\n",
      "| epoch  88 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.68 | loss  2.75 | ppl    15.66 | acc     0.70 | train_ae_norm     1.00\n",
      "[88/200][1399/4361] Loss_D: 0.00094565 (Loss_D_real: 0.00054833 Loss_D_fake: 0.00039732) Loss_G: 0.41194645 Loss_Enh_Dec: -1.59828269\n",
      "| epoch  88 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.45 | loss  2.76 | ppl    15.74 | acc     0.65 | train_ae_norm     1.00\n",
      "[88/200][1499/4361] Loss_D: 0.00213714 (Loss_D_real: 0.00114891 Loss_D_fake: 0.00098823) Loss_G: 0.37480286 Loss_Enh_Dec: -1.75392079\n",
      "| epoch  88 |  1500/ 4361 batches | lr 0.000000 | ms/batch 400.93 | loss  2.80 | ppl    16.46 | acc     0.64 | train_ae_norm     1.00\n",
      "[88/200][1599/4361] Loss_D: 0.00283083 (Loss_D_real: 0.00192969 Loss_D_fake: 0.00090114) Loss_G: 0.38122949 Loss_Enh_Dec: -1.58669841\n",
      "| epoch  88 |  1600/ 4361 batches | lr 0.000000 | ms/batch 400.18 | loss  2.78 | ppl    16.19 | acc     0.66 | train_ae_norm     1.00\n",
      "[88/200][1699/4361] Loss_D: 0.00136260 (Loss_D_real: 0.00034051 Loss_D_fake: 0.00102209) Loss_G: 0.38010508 Loss_Enh_Dec: -1.52924299\n",
      "| epoch  88 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  2.77 | ppl    15.98 | acc     0.66 | train_ae_norm     1.00\n",
      "[88/200][1799/4361] Loss_D: 0.00215216 (Loss_D_real: 0.00088095 Loss_D_fake: 0.00127122) Loss_G: 0.36293888 Loss_Enh_Dec: -2.19029093\n",
      "| epoch  88 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.63 | loss  2.71 | ppl    15.03 | acc     0.69 | train_ae_norm     1.00\n",
      "[88/200][1899/4361] Loss_D: 0.00157318 (Loss_D_real: 0.00059640 Loss_D_fake: 0.00097679) Loss_G: 0.35769382 Loss_Enh_Dec: -1.98947430\n",
      "| epoch  88 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.65 | loss  2.77 | ppl    15.89 | acc     0.72 | train_ae_norm     1.00\n",
      "[88/200][1999/4361] Loss_D: 0.00101236 (Loss_D_real: 0.00004991 Loss_D_fake: 0.00096245) Loss_G: 0.41519070 Loss_Enh_Dec: -1.45841968\n",
      "| epoch  88 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.30 | loss  2.69 | ppl    14.77 | acc     0.71 | train_ae_norm     1.00\n",
      "[88/200][2099/4361] Loss_D: 0.00138611 (Loss_D_real: 0.00007234 Loss_D_fake: 0.00131377) Loss_G: 0.35773182 Loss_Enh_Dec: -1.40975893\n",
      "| epoch  88 |  2100/ 4361 batches | lr 0.000000 | ms/batch 399.70 | loss  2.72 | ppl    15.12 | acc     0.70 | train_ae_norm     1.00\n",
      "[88/200][2199/4361] Loss_D: 0.00085328 (Loss_D_real: 0.00004507 Loss_D_fake: 0.00080822) Loss_G: 0.37519959 Loss_Enh_Dec: -1.15464520\n",
      "| epoch  88 |  2200/ 4361 batches | lr 0.000000 | ms/batch 399.23 | loss  2.72 | ppl    15.24 | acc     0.70 | train_ae_norm     1.00\n",
      "[88/200][2299/4361] Loss_D: 0.00183923 (Loss_D_real: 0.00024360 Loss_D_fake: 0.00159563) Loss_G: 0.36716196 Loss_Enh_Dec: -1.31299317\n",
      "| epoch  88 |  2300/ 4361 batches | lr 0.000000 | ms/batch 399.50 | loss  2.70 | ppl    14.84 | acc     0.70 | train_ae_norm     1.00\n",
      "[88/200][2399/4361] Loss_D: 0.00058498 (Loss_D_real: 0.00005750 Loss_D_fake: 0.00052749) Loss_G: 0.42959195 Loss_Enh_Dec: -1.64488828\n",
      "| epoch  88 |  2400/ 4361 batches | lr 0.000000 | ms/batch 399.58 | loss  2.69 | ppl    14.76 | acc     0.67 | train_ae_norm     1.00\n",
      "[88/200][2499/4361] Loss_D: 0.00100722 (Loss_D_real: 0.00014666 Loss_D_fake: 0.00086056) Loss_G: 0.38641617 Loss_Enh_Dec: -1.58587289\n",
      "| epoch  88 |  2500/ 4361 batches | lr 0.000000 | ms/batch 399.82 | loss  2.75 | ppl    15.64 | acc     0.69 | train_ae_norm     1.00\n",
      "[88/200][2599/4361] Loss_D: 0.00157838 (Loss_D_real: 0.00022500 Loss_D_fake: 0.00135338) Loss_G: 0.38262144 Loss_Enh_Dec: -1.16156042\n",
      "| epoch  88 |  2600/ 4361 batches | lr 0.000000 | ms/batch 399.37 | loss  2.70 | ppl    14.91 | acc     0.66 | train_ae_norm     1.00\n",
      "[88/200][2699/4361] Loss_D: 0.00142414 (Loss_D_real: 0.00025842 Loss_D_fake: 0.00116572) Loss_G: 0.35729003 Loss_Enh_Dec: -1.82506204\n",
      "| epoch  88 |  2700/ 4361 batches | lr 0.000000 | ms/batch 400.49 | loss  2.71 | ppl    15.00 | acc     0.69 | train_ae_norm     1.00\n",
      "[88/200][2799/4361] Loss_D: 0.00068095 (Loss_D_real: 0.00010057 Loss_D_fake: 0.00058039) Loss_G: 0.41243893 Loss_Enh_Dec: -1.15829277\n",
      "| epoch  88 |  2800/ 4361 batches | lr 0.000000 | ms/batch 399.69 | loss  2.66 | ppl    14.32 | acc     0.67 | train_ae_norm     1.00\n",
      "[88/200][2899/4361] Loss_D: 0.00088890 (Loss_D_real: 0.00005539 Loss_D_fake: 0.00083351) Loss_G: 0.47439948 Loss_Enh_Dec: -1.67061639\n",
      "| epoch  88 |  2900/ 4361 batches | lr 0.000000 | ms/batch 399.69 | loss  2.67 | ppl    14.49 | acc     0.70 | train_ae_norm     1.00\n",
      "[88/200][2999/4361] Loss_D: 0.00351115 (Loss_D_real: 0.00287706 Loss_D_fake: 0.00063409) Loss_G: 0.57382166 Loss_Enh_Dec: -1.10069740\n",
      "| epoch  88 |  3000/ 4361 batches | lr 0.000000 | ms/batch 399.82 | loss  2.70 | ppl    14.84 | acc     0.69 | train_ae_norm     1.00\n",
      "[88/200][3099/4361] Loss_D: 0.00112977 (Loss_D_real: 0.00050015 Loss_D_fake: 0.00062962) Loss_G: 0.43359494 Loss_Enh_Dec: -1.26719844\n",
      "| epoch  88 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.49 | loss  2.69 | ppl    14.80 | acc     0.69 | train_ae_norm     1.00\n",
      "[88/200][3199/4361] Loss_D: 0.00298284 (Loss_D_real: 0.00180628 Loss_D_fake: 0.00117656) Loss_G: 0.39717385 Loss_Enh_Dec: -1.01133311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  88 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.57 | loss  2.74 | ppl    15.41 | acc     0.69 | train_ae_norm     1.00\n",
      "[88/200][3299/4361] Loss_D: 0.00168630 (Loss_D_real: 0.00078958 Loss_D_fake: 0.00089672) Loss_G: 0.39045131 Loss_Enh_Dec: -1.44241047\n",
      "| epoch  88 |  3300/ 4361 batches | lr 0.000000 | ms/batch 400.53 | loss  2.72 | ppl    15.18 | acc     0.70 | train_ae_norm     1.00\n",
      "[88/200][3399/4361] Loss_D: 0.00494011 (Loss_D_real: 0.00401631 Loss_D_fake: 0.00092380) Loss_G: 0.44585228 Loss_Enh_Dec: -0.74332750\n",
      "| epoch  88 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.44 | loss  2.70 | ppl    14.92 | acc     0.66 | train_ae_norm     1.00\n",
      "[88/200][3499/4361] Loss_D: 0.00289140 (Loss_D_real: 0.00171940 Loss_D_fake: 0.00117201) Loss_G: 0.53996032 Loss_Enh_Dec: -1.08110809\n",
      "| epoch  88 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.64 | ppl    13.96 | acc     0.69 | train_ae_norm     1.00\n",
      "[88/200][3599/4361] Loss_D: 0.00083251 (Loss_D_real: 0.00018779 Loss_D_fake: 0.00064473) Loss_G: 0.41812631 Loss_Enh_Dec: -0.83210051\n",
      "| epoch  88 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.65 | ppl    14.17 | acc     0.71 | train_ae_norm     1.00\n",
      "[88/200][3699/4361] Loss_D: 0.00073642 (Loss_D_real: 0.00027346 Loss_D_fake: 0.00046296) Loss_G: 0.42761275 Loss_Enh_Dec: -1.23197818\n",
      "| epoch  88 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.46 | loss  2.67 | ppl    14.51 | acc     0.65 | train_ae_norm     1.00\n",
      "[88/200][3799/4361] Loss_D: 0.00150736 (Loss_D_real: 0.00110877 Loss_D_fake: 0.00039859) Loss_G: 0.40149036 Loss_Enh_Dec: -1.32431185\n",
      "| epoch  88 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.70 | ppl    14.85 | acc     0.73 | train_ae_norm     1.00\n",
      "[88/200][3899/4361] Loss_D: 0.01447869 (Loss_D_real: 0.01424575 Loss_D_fake: 0.00023294) Loss_G: 0.42910352 Loss_Enh_Dec: -0.79984629\n",
      "| epoch  88 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.90 | loss  2.69 | ppl    14.77 | acc     0.68 | train_ae_norm     1.00\n",
      "[88/200][3999/4361] Loss_D: 0.00230251 (Loss_D_real: 0.00203966 Loss_D_fake: 0.00026285) Loss_G: 0.43271485 Loss_Enh_Dec: -1.04440868\n",
      "| epoch  88 |  4000/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  2.70 | ppl    14.85 | acc     0.68 | train_ae_norm     1.00\n",
      "[88/200][4099/4361] Loss_D: 0.00548018 (Loss_D_real: 0.00185569 Loss_D_fake: 0.00362449) Loss_G: 0.39056394 Loss_Enh_Dec: -1.06001306\n",
      "| epoch  88 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.71 | loss  2.67 | ppl    14.44 | acc     0.69 | train_ae_norm     1.00\n",
      "[88/200][4199/4361] Loss_D: 0.00228066 (Loss_D_real: 0.00172192 Loss_D_fake: 0.00055875) Loss_G: 0.43558899 Loss_Enh_Dec: -1.46001768\n",
      "| epoch  88 |  4200/ 4361 batches | lr 0.000000 | ms/batch 400.64 | loss  2.70 | ppl    14.94 | acc     0.73 | train_ae_norm     1.00\n",
      "[88/200][4299/4361] Loss_D: 0.00086858 (Loss_D_real: 0.00057678 Loss_D_fake: 0.00029180) Loss_G: 0.49405071 Loss_Enh_Dec: -0.79856151\n",
      "| epoch  88 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  2.69 | ppl    14.77 | acc     0.72 | train_ae_norm     1.00\n",
      "| end of epoch  88 | time: 1849.71s | test loss  2.72 | test ppl 15.13 | acc 0.723\n",
      "bleu_self:  [8.12499998e-01 7.50002794e-04 7.50099212e-05 2.37357743e-05\n",
      " 1.19140333e-05]\n",
      "bleu_test:  [9.58333332e-01 7.50006234e-04 7.50125086e-05 2.37394059e-05\n",
      " 1.19184970e-05]\n",
      "bleu_self: [0.81250000,0.00075000,0.00007501,0.00002374,0.00001191]\n",
      "bleu_test: [0.95833333,0.00075001,0.00007501,0.00002374,0.00001192]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 89 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:48.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:18.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.699\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.497\n",
      "  Test Loss: 4.398\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  89 |     0/ 4361 batches | lr 0.000000 | ms/batch 858.85 | loss  0.03 | ppl     1.03 | acc     0.72 | train_ae_norm     1.00\n",
      "[89/200][99/4361] Loss_D: 0.00044900 (Loss_D_real: 0.00009572 Loss_D_fake: 0.00035328) Loss_G: 0.42787179 Loss_Enh_Dec: -1.47716773\n",
      "| epoch  89 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  2.69 | ppl    14.80 | acc     0.65 | train_ae_norm     1.00\n",
      "[89/200][199/4361] Loss_D: 0.00034729 (Loss_D_real: 0.00003814 Loss_D_fake: 0.00030916) Loss_G: 0.48501545 Loss_Enh_Dec: -1.55352056\n",
      "| epoch  89 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  2.72 | ppl    15.15 | acc     0.70 | train_ae_norm     1.00\n",
      "[89/200][299/4361] Loss_D: 0.00175817 (Loss_D_real: 0.00050227 Loss_D_fake: 0.00125590) Loss_G: 0.40648851 Loss_Enh_Dec: -1.65519941\n",
      "| epoch  89 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  2.70 | ppl    14.93 | acc     0.68 | train_ae_norm     1.00\n",
      "[89/200][399/4361] Loss_D: 0.00158080 (Loss_D_real: 0.00124951 Loss_D_fake: 0.00033129) Loss_G: 0.46464872 Loss_Enh_Dec: -1.96029568\n",
      "| epoch  89 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  2.62 | ppl    13.80 | acc     0.72 | train_ae_norm     1.00\n",
      "[89/200][499/4361] Loss_D: 0.00148375 (Loss_D_real: 0.00030948 Loss_D_fake: 0.00117427) Loss_G: 0.47317225 Loss_Enh_Dec: -1.73512936\n",
      "| epoch  89 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.82 | loss  2.70 | ppl    14.82 | acc     0.70 | train_ae_norm     1.00\n",
      "[89/200][599/4361] Loss_D: 0.00526755 (Loss_D_real: 0.00522548 Loss_D_fake: 0.00004207) Loss_G: 0.63727987 Loss_Enh_Dec: -1.75537038\n",
      "| epoch  89 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  2.64 | ppl    14.08 | acc     0.66 | train_ae_norm     1.00\n",
      "[89/200][699/4361] Loss_D: 0.00037236 (Loss_D_real: 0.00011540 Loss_D_fake: 0.00025697) Loss_G: 0.49592930 Loss_Enh_Dec: -1.44358313\n",
      "| epoch  89 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  2.70 | ppl    14.89 | acc     0.68 | train_ae_norm     1.00\n",
      "[89/200][799/4361] Loss_D: 0.01168288 (Loss_D_real: 0.01018619 Loss_D_fake: 0.00149669) Loss_G: 0.48583531 Loss_Enh_Dec: -1.81299675\n",
      "| epoch  89 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  2.67 | ppl    14.46 | acc     0.69 | train_ae_norm     1.00\n",
      "[89/200][899/4361] Loss_D: 0.00055670 (Loss_D_real: 0.00036561 Loss_D_fake: 0.00019109) Loss_G: 0.39814702 Loss_Enh_Dec: -1.56871116\n",
      "| epoch  89 |   900/ 4361 batches | lr 0.000000 | ms/batch 402.33 | loss  2.69 | ppl    14.69 | acc     0.73 | train_ae_norm     1.00\n",
      "[89/200][999/4361] Loss_D: 0.00038461 (Loss_D_real: 0.00014718 Loss_D_fake: 0.00023743) Loss_G: 0.41152641 Loss_Enh_Dec: -2.09274292\n",
      "| epoch  89 |  1000/ 4361 batches | lr 0.000000 | ms/batch 400.77 | loss  2.68 | ppl    14.55 | acc     0.70 | train_ae_norm     1.00\n",
      "[89/200][1099/4361] Loss_D: 0.00103347 (Loss_D_real: 0.00050901 Loss_D_fake: 0.00052446) Loss_G: 0.39828810 Loss_Enh_Dec: -1.65135157\n",
      "| epoch  89 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.67 | ppl    14.48 | acc     0.67 | train_ae_norm     1.00\n",
      "[89/200][1199/4361] Loss_D: 0.00136693 (Loss_D_real: 0.00003451 Loss_D_fake: 0.00133242) Loss_G: 0.45715362 Loss_Enh_Dec: -1.50793159\n",
      "| epoch  89 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.57 | loss  2.68 | ppl    14.56 | acc     0.72 | train_ae_norm     1.00\n",
      "[89/200][1299/4361] Loss_D: 0.00115931 (Loss_D_real: 0.00004613 Loss_D_fake: 0.00111318) Loss_G: 0.43981454 Loss_Enh_Dec: -1.72453010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  89 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.71 | loss  2.71 | ppl    15.05 | acc     0.68 | train_ae_norm     1.00\n",
      "[89/200][1399/4361] Loss_D: 0.00183113 (Loss_D_real: 0.00067535 Loss_D_fake: 0.00115578) Loss_G: 0.49638534 Loss_Enh_Dec: -2.02757740\n",
      "| epoch  89 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.88 | loss  2.70 | ppl    14.94 | acc     0.65 | train_ae_norm     1.00\n",
      "[89/200][1499/4361] Loss_D: 0.00134998 (Loss_D_real: 0.00088608 Loss_D_fake: 0.00046390) Loss_G: 0.64774132 Loss_Enh_Dec: -1.84383893\n",
      "| epoch  89 |  1500/ 4361 batches | lr 0.000000 | ms/batch 406.18 | loss  2.77 | ppl    15.96 | acc     0.67 | train_ae_norm     1.00\n",
      "[89/200][1599/4361] Loss_D: 0.00234585 (Loss_D_real: 0.00185219 Loss_D_fake: 0.00049365) Loss_G: 0.50479472 Loss_Enh_Dec: -1.55673921\n",
      "| epoch  89 |  1600/ 4361 batches | lr 0.000000 | ms/batch 400.86 | loss  2.73 | ppl    15.26 | acc     0.67 | train_ae_norm     1.00\n",
      "[89/200][1699/4361] Loss_D: 0.00042908 (Loss_D_real: 0.00006097 Loss_D_fake: 0.00036812) Loss_G: 0.43248329 Loss_Enh_Dec: -1.89100957\n",
      "| epoch  89 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.63 | loss  2.70 | ppl    14.85 | acc     0.68 | train_ae_norm     1.00\n",
      "[89/200][1799/4361] Loss_D: 0.00186319 (Loss_D_real: 0.00014556 Loss_D_fake: 0.00171762) Loss_G: 0.45064202 Loss_Enh_Dec: -1.61068630\n",
      "| epoch  89 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.85 | loss  2.67 | ppl    14.38 | acc     0.69 | train_ae_norm     1.00\n",
      "[89/200][1899/4361] Loss_D: 0.00519390 (Loss_D_real: 0.00038107 Loss_D_fake: 0.00481283) Loss_G: 0.61869615 Loss_Enh_Dec: -1.73901558\n",
      "| epoch  89 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.08 | loss  2.72 | ppl    15.22 | acc     0.70 | train_ae_norm     1.00\n",
      "[89/200][1999/4361] Loss_D: 0.00945078 (Loss_D_real: 0.00890156 Loss_D_fake: 0.00054922) Loss_G: 0.42464551 Loss_Enh_Dec: -2.20446277\n",
      "| epoch  89 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  2.67 | ppl    14.38 | acc     0.70 | train_ae_norm     1.00\n",
      "[89/200][2099/4361] Loss_D: 0.00055039 (Loss_D_real: 0.00008667 Loss_D_fake: 0.00046372) Loss_G: 0.44549868 Loss_Enh_Dec: -2.02263212\n",
      "| epoch  89 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  2.69 | ppl    14.75 | acc     0.71 | train_ae_norm     1.00\n",
      "[89/200][2199/4361] Loss_D: 0.00194552 (Loss_D_real: 0.00136436 Loss_D_fake: 0.00058115) Loss_G: 0.41669288 Loss_Enh_Dec: -2.27852941\n",
      "| epoch  89 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  2.69 | ppl    14.74 | acc     0.70 | train_ae_norm     1.00\n",
      "[89/200][2299/4361] Loss_D: 0.00825529 (Loss_D_real: 0.00512509 Loss_D_fake: 0.00313019) Loss_G: 0.51674670 Loss_Enh_Dec: -1.92683947\n",
      "| epoch  89 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  2.67 | ppl    14.46 | acc     0.72 | train_ae_norm     1.00\n",
      "[89/200][2399/4361] Loss_D: 0.00044393 (Loss_D_real: 0.00010888 Loss_D_fake: 0.00033505) Loss_G: 0.42253867 Loss_Enh_Dec: -1.76624620\n",
      "| epoch  89 |  2400/ 4361 batches | lr 0.000000 | ms/batch 400.75 | loss  2.67 | ppl    14.49 | acc     0.66 | train_ae_norm     1.00\n",
      "[89/200][2499/4361] Loss_D: 0.00189468 (Loss_D_real: 0.00020223 Loss_D_fake: 0.00169246) Loss_G: 0.64388579 Loss_Enh_Dec: -1.89029682\n",
      "| epoch  89 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  2.72 | ppl    15.20 | acc     0.69 | train_ae_norm     1.00\n",
      "[89/200][2599/4361] Loss_D: 0.00100334 (Loss_D_real: 0.00038573 Loss_D_fake: 0.00061761) Loss_G: 0.43540764 Loss_Enh_Dec: -2.09251642\n",
      "| epoch  89 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.92 | loss  2.67 | ppl    14.50 | acc     0.68 | train_ae_norm     1.00\n",
      "[89/200][2699/4361] Loss_D: 0.00073563 (Loss_D_real: 0.00016885 Loss_D_fake: 0.00056677) Loss_G: 0.44346580 Loss_Enh_Dec: -2.11562896\n",
      "| epoch  89 |  2700/ 4361 batches | lr 0.000000 | ms/batch 400.85 | loss  2.68 | ppl    14.54 | acc     0.69 | train_ae_norm     1.00\n",
      "[89/200][2799/4361] Loss_D: 0.00189260 (Loss_D_real: 0.00143410 Loss_D_fake: 0.00045850) Loss_G: 0.40845188 Loss_Enh_Dec: -2.36429763\n",
      "| epoch  89 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  2.64 | ppl    14.02 | acc     0.68 | train_ae_norm     1.00\n",
      "[89/200][2899/4361] Loss_D: 0.00059576 (Loss_D_real: 0.00008808 Loss_D_fake: 0.00050768) Loss_G: 0.43221387 Loss_Enh_Dec: -2.40822673\n",
      "| epoch  89 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.89 | loss  2.67 | ppl    14.40 | acc     0.68 | train_ae_norm     1.00\n",
      "[89/200][2999/4361] Loss_D: 0.00064334 (Loss_D_real: 0.00025366 Loss_D_fake: 0.00038968) Loss_G: 0.47361380 Loss_Enh_Dec: -2.43449640\n",
      "| epoch  89 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.44 | loss  2.65 | ppl    14.15 | acc     0.69 | train_ae_norm     1.00\n",
      "[89/200][3099/4361] Loss_D: 0.00222044 (Loss_D_real: 0.00017313 Loss_D_fake: 0.00204731) Loss_G: 0.40868616 Loss_Enh_Dec: -2.25721812\n",
      "| epoch  89 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.69 | loss  2.67 | ppl    14.43 | acc     0.70 | train_ae_norm     1.00\n",
      "[89/200][3199/4361] Loss_D: 0.01035485 (Loss_D_real: 0.00770180 Loss_D_fake: 0.00265305) Loss_G: 0.49480754 Loss_Enh_Dec: -1.89501572\n",
      "| epoch  89 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  2.72 | ppl    15.19 | acc     0.68 | train_ae_norm     1.00\n",
      "[89/200][3299/4361] Loss_D: 0.00058556 (Loss_D_real: 0.00019645 Loss_D_fake: 0.00038911) Loss_G: 0.45234948 Loss_Enh_Dec: -1.96387756\n",
      "| epoch  89 |  3300/ 4361 batches | lr 0.000000 | ms/batch 400.77 | loss  2.72 | ppl    15.19 | acc     0.69 | train_ae_norm     1.00\n",
      "[89/200][3399/4361] Loss_D: 0.00167183 (Loss_D_real: 0.00066471 Loss_D_fake: 0.00100712) Loss_G: 0.42628089 Loss_Enh_Dec: -2.00771260\n",
      "| epoch  89 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.81 | loss  2.71 | ppl    14.96 | acc     0.66 | train_ae_norm     1.00\n",
      "[89/200][3499/4361] Loss_D: 0.00113422 (Loss_D_real: 0.00081972 Loss_D_fake: 0.00031450) Loss_G: 0.44080025 Loss_Enh_Dec: -2.15640712\n",
      "| epoch  89 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.81 | loss  2.64 | ppl    14.06 | acc     0.67 | train_ae_norm     1.00\n",
      "[89/200][3599/4361] Loss_D: 0.00120228 (Loss_D_real: 0.00042499 Loss_D_fake: 0.00077730) Loss_G: 0.47864124 Loss_Enh_Dec: -2.14936829\n",
      "| epoch  89 |  3600/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  2.66 | ppl    14.36 | acc     0.71 | train_ae_norm     1.00\n",
      "[89/200][3699/4361] Loss_D: 0.00189528 (Loss_D_real: 0.00115841 Loss_D_fake: 0.00073688) Loss_G: 0.41332006 Loss_Enh_Dec: -2.20606232\n",
      "| epoch  89 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  2.69 | ppl    14.74 | acc     0.66 | train_ae_norm     1.00\n",
      "[89/200][3799/4361] Loss_D: 0.00101842 (Loss_D_real: 0.00087739 Loss_D_fake: 0.00014103) Loss_G: 0.61310250 Loss_Enh_Dec: -1.99482501\n",
      "| epoch  89 |  3800/ 4361 batches | lr 0.000000 | ms/batch 400.12 | loss  2.70 | ppl    14.88 | acc     0.73 | train_ae_norm     1.00\n",
      "[89/200][3899/4361] Loss_D: 0.00032723 (Loss_D_real: 0.00013207 Loss_D_fake: 0.00019516) Loss_G: 0.51417089 Loss_Enh_Dec: -2.01004076\n",
      "| epoch  89 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.89 | loss  2.70 | ppl    14.95 | acc     0.66 | train_ae_norm     1.00\n",
      "[89/200][3999/4361] Loss_D: 0.79400229 (Loss_D_real: 0.02291048 Loss_D_fake: 0.77109182) Loss_G: 0.90603447 Loss_Enh_Dec: -1.55849755\n",
      "| epoch  89 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  2.71 | ppl    15.02 | acc     0.69 | train_ae_norm     1.00\n",
      "[89/200][4099/4361] Loss_D: 0.00347129 (Loss_D_real: 0.00284948 Loss_D_fake: 0.00062181) Loss_G: 0.44876519 Loss_Enh_Dec: -1.71665406\n",
      "| epoch  89 |  4100/ 4361 batches | lr 0.000000 | ms/batch 399.97 | loss  2.65 | ppl    14.15 | acc     0.68 | train_ae_norm     1.00\n",
      "[89/200][4199/4361] Loss_D: 0.00160504 (Loss_D_real: 0.00018178 Loss_D_fake: 0.00142326) Loss_G: 0.45014426 Loss_Enh_Dec: -1.75394404\n",
      "| epoch  89 |  4200/ 4361 batches | lr 0.000000 | ms/batch 400.91 | loss  2.70 | ppl    14.91 | acc     0.72 | train_ae_norm     1.00\n",
      "[89/200][4299/4361] Loss_D: 0.01020983 (Loss_D_real: 0.00026072 Loss_D_fake: 0.00994911) Loss_G: 0.44685632 Loss_Enh_Dec: -1.95366943\n",
      "| epoch  89 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.67 | loss  2.66 | ppl    14.30 | acc     0.70 | train_ae_norm     1.00\n",
      "| end of epoch  89 | time: 1850.93s | test loss  2.67 | test ppl 14.47 | acc 0.728\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 90 / 200 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:18.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.701\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.497\n",
      "  Test Loss: 4.237\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  90 |     0/ 4361 batches | lr 0.000000 | ms/batch 860.13 | loss  0.02 | ppl     1.02 | acc     0.71 | train_ae_norm     1.00\n",
      "[90/200][99/4361] Loss_D: 0.00142129 (Loss_D_real: 0.00020581 Loss_D_fake: 0.00121548) Loss_G: 0.47649595 Loss_Enh_Dec: -1.67862892\n",
      "| epoch  90 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  2.66 | ppl    14.33 | acc     0.65 | train_ae_norm     1.00\n",
      "[90/200][199/4361] Loss_D: 0.00146002 (Loss_D_real: 0.00107873 Loss_D_fake: 0.00038129) Loss_G: 0.43958455 Loss_Enh_Dec: -1.94760442\n",
      "| epoch  90 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.67 | ppl    14.48 | acc     0.69 | train_ae_norm     1.00\n",
      "[90/200][299/4361] Loss_D: 0.00070097 (Loss_D_real: 0.00056472 Loss_D_fake: 0.00013625) Loss_G: 0.51050138 Loss_Enh_Dec: -2.14221454\n",
      "| epoch  90 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  2.68 | ppl    14.58 | acc     0.66 | train_ae_norm     1.00\n",
      "[90/200][399/4361] Loss_D: 0.00085810 (Loss_D_real: 0.00034001 Loss_D_fake: 0.00051809) Loss_G: 0.42315874 Loss_Enh_Dec: -2.29242229\n",
      "| epoch  90 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  2.61 | ppl    13.59 | acc     0.71 | train_ae_norm     1.00\n",
      "[90/200][499/4361] Loss_D: 0.00151208 (Loss_D_real: 0.00108093 Loss_D_fake: 0.00043115) Loss_G: 0.46448031 Loss_Enh_Dec: -1.79225194\n",
      "| epoch  90 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.54 | loss  2.67 | ppl    14.38 | acc     0.70 | train_ae_norm     1.00\n",
      "[90/200][599/4361] Loss_D: 0.00215883 (Loss_D_real: 0.00114930 Loss_D_fake: 0.00100952) Loss_G: 0.47765055 Loss_Enh_Dec: -2.10463095\n",
      "| epoch  90 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  2.62 | ppl    13.70 | acc     0.66 | train_ae_norm     1.00\n",
      "[90/200][699/4361] Loss_D: 0.01584904 (Loss_D_real: 0.01504719 Loss_D_fake: 0.00080186) Loss_G: 0.44387314 Loss_Enh_Dec: -1.88634610\n",
      "| epoch  90 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.66 | ppl    14.36 | acc     0.69 | train_ae_norm     1.00\n",
      "[90/200][799/4361] Loss_D: 0.00938346 (Loss_D_real: 0.00935116 Loss_D_fake: 0.00003229) Loss_G: 0.89711058 Loss_Enh_Dec: -1.93726659\n",
      "| epoch  90 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.41 | loss  2.65 | ppl    14.19 | acc     0.68 | train_ae_norm     1.00\n",
      "[90/200][899/4361] Loss_D: 0.00303121 (Loss_D_real: 0.00240903 Loss_D_fake: 0.00062218) Loss_G: 0.53964567 Loss_Enh_Dec: -1.64697587\n",
      "| epoch  90 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  2.66 | ppl    14.24 | acc     0.71 | train_ae_norm     1.00\n",
      "[90/200][999/4361] Loss_D: 0.00945553 (Loss_D_real: 0.00926977 Loss_D_fake: 0.00018577) Loss_G: 0.49314114 Loss_Enh_Dec: -1.30747330\n",
      "| epoch  90 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  2.65 | ppl    14.17 | acc     0.72 | train_ae_norm     1.00\n",
      "[90/200][1099/4361] Loss_D: 0.00106291 (Loss_D_real: 0.00041488 Loss_D_fake: 0.00064803) Loss_G: 0.44781706 Loss_Enh_Dec: -1.42609096\n",
      "| epoch  90 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.64 | loss  2.66 | ppl    14.23 | acc     0.68 | train_ae_norm     1.00\n",
      "[90/200][1199/4361] Loss_D: 0.00088588 (Loss_D_real: 0.00036477 Loss_D_fake: 0.00052110) Loss_G: 0.42784548 Loss_Enh_Dec: -1.78510058\n",
      "| epoch  90 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  2.66 | ppl    14.25 | acc     0.72 | train_ae_norm     1.00\n",
      "[90/200][1299/4361] Loss_D: 0.00226614 (Loss_D_real: 0.00055675 Loss_D_fake: 0.00170939) Loss_G: 0.39003959 Loss_Enh_Dec: -2.13767004\n",
      "| epoch  90 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  2.67 | ppl    14.45 | acc     0.67 | train_ae_norm     1.00\n",
      "[90/200][1399/4361] Loss_D: 0.00287809 (Loss_D_real: 0.00045483 Loss_D_fake: 0.00242326) Loss_G: 0.40580454 Loss_Enh_Dec: -2.17983055\n",
      "| epoch  90 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.05 | loss  2.66 | ppl    14.28 | acc     0.67 | train_ae_norm     1.00\n",
      "[90/200][1499/4361] Loss_D: 0.00090451 (Loss_D_real: 0.00000675 Loss_D_fake: 0.00089777) Loss_G: 0.43376970 Loss_Enh_Dec: -2.23895192\n",
      "| epoch  90 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  2.72 | ppl    15.25 | acc     0.69 | train_ae_norm     1.00\n",
      "[90/200][1599/4361] Loss_D: 0.00340042 (Loss_D_real: 0.00026541 Loss_D_fake: 0.00313501) Loss_G: 0.38733280 Loss_Enh_Dec: -1.75368714\n",
      "| epoch  90 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  2.69 | ppl    14.70 | acc     0.70 | train_ae_norm     1.00\n",
      "[90/200][1699/4361] Loss_D: 0.01310202 (Loss_D_real: 0.01196907 Loss_D_fake: 0.00113295) Loss_G: 0.38472173 Loss_Enh_Dec: -1.81095624\n",
      "| epoch  90 |  1700/ 4361 batches | lr 0.000000 | ms/batch 402.51 | loss  2.67 | ppl    14.38 | acc     0.69 | train_ae_norm     1.00\n",
      "[90/200][1799/4361] Loss_D: 0.00105780 (Loss_D_real: 0.00014322 Loss_D_fake: 0.00091458) Loss_G: 0.41034564 Loss_Enh_Dec: -1.45365012\n",
      "| epoch  90 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  2.67 | ppl    14.49 | acc     0.71 | train_ae_norm     1.00\n",
      "[90/200][1899/4361] Loss_D: 0.00367542 (Loss_D_real: 0.00313000 Loss_D_fake: 0.00054542) Loss_G: 0.54725868 Loss_Enh_Dec: -2.02710533\n",
      "| epoch  90 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.71 | ppl    15.00 | acc     0.73 | train_ae_norm     1.00\n",
      "[90/200][1999/4361] Loss_D: 0.00170844 (Loss_D_real: 0.00070731 Loss_D_fake: 0.00100113) Loss_G: 0.44464961 Loss_Enh_Dec: -1.76941490\n",
      "| epoch  90 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  2.66 | ppl    14.35 | acc     0.71 | train_ae_norm     1.00\n",
      "[90/200][2099/4361] Loss_D: 0.00207467 (Loss_D_real: 0.00023166 Loss_D_fake: 0.00184301) Loss_G: 0.37086549 Loss_Enh_Dec: -1.63497984\n",
      "| epoch  90 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  2.69 | ppl    14.66 | acc     0.71 | train_ae_norm     1.00\n",
      "[90/200][2199/4361] Loss_D: 0.00638495 (Loss_D_real: 0.00575693 Loss_D_fake: 0.00062801) Loss_G: 0.40322122 Loss_Enh_Dec: -2.10640955\n",
      "| epoch  90 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.67 | ppl    14.40 | acc     0.69 | train_ae_norm     1.00\n",
      "[90/200][2299/4361] Loss_D: 0.00207104 (Loss_D_real: 0.00174688 Loss_D_fake: 0.00032416) Loss_G: 0.59121168 Loss_Enh_Dec: -1.68235195\n",
      "| epoch  90 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.79 | loss  2.66 | ppl    14.29 | acc     0.71 | train_ae_norm     1.00\n",
      "[90/200][2399/4361] Loss_D: 0.00176568 (Loss_D_real: 0.00041009 Loss_D_fake: 0.00135559) Loss_G: 0.43783346 Loss_Enh_Dec: -0.93986434\n",
      "| epoch  90 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  2.67 | ppl    14.37 | acc     0.67 | train_ae_norm     1.00\n",
      "[90/200][2499/4361] Loss_D: 0.00124729 (Loss_D_real: 0.00052811 Loss_D_fake: 0.00071918) Loss_G: 0.44665644 Loss_Enh_Dec: -1.32719767\n",
      "| epoch  90 |  2500/ 4361 batches | lr 0.000000 | ms/batch 400.37 | loss  2.70 | ppl    14.94 | acc     0.70 | train_ae_norm     1.00\n",
      "[90/200][2599/4361] Loss_D: 0.00207597 (Loss_D_real: 0.00142595 Loss_D_fake: 0.00065002) Loss_G: 0.41266146 Loss_Enh_Dec: -1.10657573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  90 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.59 | loss  2.67 | ppl    14.47 | acc     0.67 | train_ae_norm     1.00\n",
      "[90/200][2699/4361] Loss_D: 0.00085378 (Loss_D_real: 0.00010551 Loss_D_fake: 0.00074827) Loss_G: 0.42767248 Loss_Enh_Dec: -1.60355461\n",
      "| epoch  90 |  2700/ 4361 batches | lr 0.000000 | ms/batch 400.22 | loss  2.69 | ppl    14.72 | acc     0.67 | train_ae_norm     1.00\n",
      "[90/200][2799/4361] Loss_D: 0.00114945 (Loss_D_real: 0.00047611 Loss_D_fake: 0.00067334) Loss_G: 0.41524535 Loss_Enh_Dec: -1.42181170\n",
      "| epoch  90 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.64 | ppl    14.04 | acc     0.68 | train_ae_norm     1.00\n",
      "[90/200][2899/4361] Loss_D: 0.00067672 (Loss_D_real: 0.00021374 Loss_D_fake: 0.00046299) Loss_G: 0.41763464 Loss_Enh_Dec: -1.51839089\n",
      "| epoch  90 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  2.67 | ppl    14.37 | acc     0.68 | train_ae_norm     1.00\n",
      "[90/200][2999/4361] Loss_D: 0.00037883 (Loss_D_real: 0.00027354 Loss_D_fake: 0.00010529) Loss_G: 0.75311965 Loss_Enh_Dec: -1.27866960\n",
      "| epoch  90 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  2.68 | ppl    14.52 | acc     0.69 | train_ae_norm     1.00\n",
      "[90/200][3099/4361] Loss_D: 0.00178613 (Loss_D_real: 0.00065755 Loss_D_fake: 0.00112857) Loss_G: 0.50509506 Loss_Enh_Dec: -1.64434206\n",
      "| epoch  90 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  2.69 | ppl    14.77 | acc     0.69 | train_ae_norm     1.00\n",
      "[90/200][3199/4361] Loss_D: 0.00204957 (Loss_D_real: 0.00166239 Loss_D_fake: 0.00038718) Loss_G: 0.44793454 Loss_Enh_Dec: -1.73557591\n",
      "| epoch  90 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.56 | loss  2.71 | ppl    14.96 | acc     0.70 | train_ae_norm     1.00\n",
      "[90/200][3299/4361] Loss_D: 0.00153838 (Loss_D_real: 0.00068638 Loss_D_fake: 0.00085200) Loss_G: 0.42818424 Loss_Enh_Dec: -1.17339587\n",
      "| epoch  90 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  2.71 | ppl    14.99 | acc     0.68 | train_ae_norm     1.00\n",
      "[90/200][3399/4361] Loss_D: 0.01594780 (Loss_D_real: 0.01364289 Loss_D_fake: 0.00230492) Loss_G: 0.59686083 Loss_Enh_Dec: -1.48922956\n",
      "| epoch  90 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.88 | loss  2.69 | ppl    14.73 | acc     0.65 | train_ae_norm     1.00\n",
      "[90/200][3499/4361] Loss_D: 0.00204391 (Loss_D_real: 0.00125330 Loss_D_fake: 0.00079061) Loss_G: 0.46505976 Loss_Enh_Dec: -1.77205563\n",
      "| epoch  90 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.39 | loss  2.62 | ppl    13.68 | acc     0.68 | train_ae_norm     1.00\n",
      "[90/200][3599/4361] Loss_D: 0.00191992 (Loss_D_real: 0.00127791 Loss_D_fake: 0.00064201) Loss_G: 0.39461061 Loss_Enh_Dec: -1.43165171\n",
      "| epoch  90 |  3600/ 4361 batches | lr 0.000000 | ms/batch 400.54 | loss  2.65 | ppl    14.11 | acc     0.73 | train_ae_norm     1.00\n",
      "[90/200][3699/4361] Loss_D: 0.01436928 (Loss_D_real: 0.01396018 Loss_D_fake: 0.00040911) Loss_G: 0.44838580 Loss_Enh_Dec: -1.47151983\n",
      "| epoch  90 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.91 | loss  2.67 | ppl    14.38 | acc     0.68 | train_ae_norm     1.00\n",
      "[90/200][3799/4361] Loss_D: 0.00340670 (Loss_D_real: 0.00209276 Loss_D_fake: 0.00131393) Loss_G: 0.39180753 Loss_Enh_Dec: -1.35899639\n",
      "| epoch  90 |  3800/ 4361 batches | lr 0.000000 | ms/batch 400.92 | loss  2.68 | ppl    14.63 | acc     0.73 | train_ae_norm     1.00\n",
      "[90/200][3899/4361] Loss_D: 0.00061549 (Loss_D_real: 0.00020904 Loss_D_fake: 0.00040645) Loss_G: 0.43267891 Loss_Enh_Dec: -1.61231315\n",
      "| epoch  90 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  2.67 | ppl    14.50 | acc     0.67 | train_ae_norm     1.00\n",
      "[90/200][3999/4361] Loss_D: 0.00171487 (Loss_D_real: 0.00061417 Loss_D_fake: 0.00110070) Loss_G: 0.43129227 Loss_Enh_Dec: -1.68386304\n",
      "| epoch  90 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  2.67 | ppl    14.47 | acc     0.71 | train_ae_norm     1.00\n",
      "[90/200][4099/4361] Loss_D: 0.00114451 (Loss_D_real: 0.00028306 Loss_D_fake: 0.00086145) Loss_G: 0.41814795 Loss_Enh_Dec: -1.38240850\n",
      "| epoch  90 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  2.64 | ppl    14.02 | acc     0.67 | train_ae_norm     1.00\n",
      "[90/200][4199/4361] Loss_D: 0.00148155 (Loss_D_real: 0.00142766 Loss_D_fake: 0.00005389) Loss_G: 0.73985547 Loss_Enh_Dec: -1.35072255\n",
      "| epoch  90 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  2.66 | ppl    14.37 | acc     0.73 | train_ae_norm     1.00\n",
      "[90/200][4299/4361] Loss_D: 0.00052207 (Loss_D_real: 0.00017493 Loss_D_fake: 0.00034714) Loss_G: 0.48606154 Loss_Enh_Dec: -1.73535657\n",
      "| epoch  90 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.51 | loss  2.64 | ppl    14.06 | acc     0.70 | train_ae_norm     1.00\n",
      "| end of epoch  90 | time: 1850.83s | test loss  2.64 | test ppl 14.03 | acc 0.731\n",
      "bleu_self:  [2.81250000e-01 2.65165043e-01 2.36235197e-01 4.71537602e-05\n",
      " 1.79316644e-05]\n",
      "bleu_test:  [8.95833333e-01 2.09872505e-08 2.50418040e-08 4.45541480e-08\n",
      " 6.32235740e-08]\n",
      "bleu_self: [0.28125000,0.26516504,0.23623520,0.00004715,0.00001793]\n",
      "bleu_test: [0.89583333,0.00000002,0.00000003,0.00000004,0.00000006]\n",
      "New saving model: epoch 090.\n",
      "Saving models to ./results/yahoo_merge_assigned_results\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 91 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:18.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.704\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.502\n",
      "  Test Loss: 4.297\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  91 |     0/ 4361 batches | lr 0.000000 | ms/batch 861.28 | loss  0.02 | ppl     1.02 | acc     0.74 | train_ae_norm     1.00\n",
      "[91/200][99/4361] Loss_D: 0.00130239 (Loss_D_real: 0.00061143 Loss_D_fake: 0.00069096) Loss_G: 0.46538430 Loss_Enh_Dec: -1.69363654\n",
      "| epoch  91 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  2.63 | ppl    13.90 | acc     0.67 | train_ae_norm     1.00\n",
      "[91/200][199/4361] Loss_D: 0.00432858 (Loss_D_real: 0.00273889 Loss_D_fake: 0.00158970) Loss_G: 0.43093976 Loss_Enh_Dec: -1.54768431\n",
      "| epoch  91 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  2.67 | ppl    14.44 | acc     0.70 | train_ae_norm     1.00\n",
      "[91/200][299/4361] Loss_D: 0.03682213 (Loss_D_real: 0.03557476 Loss_D_fake: 0.00124738) Loss_G: 0.42052442 Loss_Enh_Dec: -1.72592437\n",
      "| epoch  91 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.49 | loss  2.68 | ppl    14.52 | acc     0.66 | train_ae_norm     1.00\n",
      "[91/200][399/4361] Loss_D: 0.00036834 (Loss_D_real: 0.00011125 Loss_D_fake: 0.00025710) Loss_G: 0.44804668 Loss_Enh_Dec: -1.52456880\n",
      "| epoch  91 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  2.57 | ppl    13.06 | acc     0.70 | train_ae_norm     1.00\n",
      "[91/200][499/4361] Loss_D: 0.00169571 (Loss_D_real: 0.00089205 Loss_D_fake: 0.00080367) Loss_G: 0.48058635 Loss_Enh_Dec: -1.73983991\n",
      "| epoch  91 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.60 | loss  2.64 | ppl    14.00 | acc     0.71 | train_ae_norm     1.00\n",
      "[91/200][599/4361] Loss_D: 0.00119031 (Loss_D_real: 0.00013088 Loss_D_fake: 0.00105943) Loss_G: 0.43131313 Loss_Enh_Dec: -1.72801137\n",
      "| epoch  91 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  2.60 | ppl    13.51 | acc     0.67 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[91/200][699/4361] Loss_D: 0.00025710 (Loss_D_real: 0.00004327 Loss_D_fake: 0.00021383) Loss_G: 0.40289411 Loss_Enh_Dec: -2.01493335\n",
      "| epoch  91 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.39 | loss  2.63 | ppl    13.94 | acc     0.69 | train_ae_norm     1.00\n",
      "[91/200][799/4361] Loss_D: 0.00132985 (Loss_D_real: 0.00087456 Loss_D_fake: 0.00045528) Loss_G: 0.42928830 Loss_Enh_Dec: -2.11668181\n",
      "| epoch  91 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.92 | loss  2.62 | ppl    13.68 | acc     0.69 | train_ae_norm     1.00\n",
      "[91/200][899/4361] Loss_D: 0.00259481 (Loss_D_real: 0.00206058 Loss_D_fake: 0.00053423) Loss_G: 0.39943409 Loss_Enh_Dec: -2.08570004\n",
      "| epoch  91 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  2.63 | ppl    13.93 | acc     0.71 | train_ae_norm     1.00\n",
      "[91/200][999/4361] Loss_D: 0.03531375 (Loss_D_real: 0.03477272 Loss_D_fake: 0.00054104) Loss_G: 0.74561441 Loss_Enh_Dec: -2.10493970\n",
      "| epoch  91 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  2.63 | ppl    13.82 | acc     0.70 | train_ae_norm     1.00\n",
      "[91/200][1099/4361] Loss_D: 0.00028683 (Loss_D_real: 0.00011750 Loss_D_fake: 0.00016933) Loss_G: 0.55874521 Loss_Enh_Dec: -2.07360911\n",
      "| epoch  91 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  2.61 | ppl    13.63 | acc     0.69 | train_ae_norm     1.00\n",
      "[91/200][1199/4361] Loss_D: 0.00126419 (Loss_D_real: 0.00099221 Loss_D_fake: 0.00027197) Loss_G: 0.50774592 Loss_Enh_Dec: -1.89083064\n",
      "| epoch  91 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.71 | loss  2.61 | ppl    13.58 | acc     0.71 | train_ae_norm     1.00\n",
      "[91/200][1299/4361] Loss_D: 0.00083767 (Loss_D_real: 0.00031882 Loss_D_fake: 0.00051885) Loss_G: 0.42240128 Loss_Enh_Dec: -1.77700865\n",
      "| epoch  91 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.64 | ppl    14.04 | acc     0.69 | train_ae_norm     1.00\n",
      "[91/200][1399/4361] Loss_D: 0.00050225 (Loss_D_real: 0.00006296 Loss_D_fake: 0.00043928) Loss_G: 0.44689757 Loss_Enh_Dec: -1.85562730\n",
      "| epoch  91 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.69 | loss  2.63 | ppl    13.85 | acc     0.66 | train_ae_norm     1.00\n",
      "[91/200][1499/4361] Loss_D: 0.00058304 (Loss_D_real: 0.00008139 Loss_D_fake: 0.00050165) Loss_G: 0.42534265 Loss_Enh_Dec: -1.72182930\n",
      "| epoch  91 |  1500/ 4361 batches | lr 0.000000 | ms/batch 400.61 | loss  2.71 | ppl    14.98 | acc     0.70 | train_ae_norm     1.00\n",
      "[91/200][1599/4361] Loss_D: 0.00180953 (Loss_D_real: 0.00044102 Loss_D_fake: 0.00136851) Loss_G: 0.40155679 Loss_Enh_Dec: -2.25429392\n",
      "| epoch  91 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  2.67 | ppl    14.47 | acc     0.70 | train_ae_norm     1.00\n",
      "[91/200][1699/4361] Loss_D: 0.00154568 (Loss_D_real: 0.00036067 Loss_D_fake: 0.00118501) Loss_G: 0.39118743 Loss_Enh_Dec: -1.46814501\n",
      "| epoch  91 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.62 | loss  2.65 | ppl    14.18 | acc     0.67 | train_ae_norm     1.00\n",
      "[91/200][1799/4361] Loss_D: 0.00091150 (Loss_D_real: 0.00050392 Loss_D_fake: 0.00040758) Loss_G: 0.45331812 Loss_Enh_Dec: -1.25576127\n",
      "| epoch  91 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  2.63 | ppl    13.90 | acc     0.71 | train_ae_norm     1.00\n",
      "[91/200][1899/4361] Loss_D: 0.00303348 (Loss_D_real: 0.00238560 Loss_D_fake: 0.00064788) Loss_G: 0.41094404 Loss_Enh_Dec: -1.93833351\n",
      "| epoch  91 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  2.67 | ppl    14.47 | acc     0.71 | train_ae_norm     1.00\n",
      "[91/200][1999/4361] Loss_D: 0.00958006 (Loss_D_real: 0.00901108 Loss_D_fake: 0.00056898) Loss_G: 0.42042646 Loss_Enh_Dec: -1.87985170\n",
      "| epoch  91 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  2.63 | ppl    13.84 | acc     0.71 | train_ae_norm     1.00\n",
      "[91/200][2099/4361] Loss_D: 0.00059862 (Loss_D_real: 0.00023230 Loss_D_fake: 0.00036632) Loss_G: 0.41817671 Loss_Enh_Dec: -1.99627817\n",
      "| epoch  91 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.42 | loss  2.64 | ppl    14.03 | acc     0.72 | train_ae_norm     1.00\n",
      "[91/200][2199/4361] Loss_D: 0.05941284 (Loss_D_real: 0.05876373 Loss_D_fake: 0.00064912) Loss_G: 0.40321213 Loss_Enh_Dec: -1.85759926\n",
      "| epoch  91 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  2.65 | ppl    14.14 | acc     0.69 | train_ae_norm     1.00\n",
      "[91/200][2299/4361] Loss_D: 0.00504656 (Loss_D_real: 0.00140295 Loss_D_fake: 0.00364361) Loss_G: 0.40920505 Loss_Enh_Dec: -2.20700908\n",
      "| epoch  91 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  2.64 | ppl    14.00 | acc     0.71 | train_ae_norm     1.00\n",
      "[91/200][2399/4361] Loss_D: 0.00169001 (Loss_D_real: 0.00025684 Loss_D_fake: 0.00143317) Loss_G: 0.38633004 Loss_Enh_Dec: -2.02050138\n",
      "| epoch  91 |  2400/ 4361 batches | lr 0.000000 | ms/batch 400.88 | loss  2.64 | ppl    14.06 | acc     0.66 | train_ae_norm     1.00\n",
      "[91/200][2499/4361] Loss_D: 0.00390401 (Loss_D_real: 0.00258395 Loss_D_fake: 0.00132006) Loss_G: 0.39871478 Loss_Enh_Dec: -1.98809934\n",
      "| epoch  91 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  2.69 | ppl    14.68 | acc     0.71 | train_ae_norm     1.00\n",
      "[91/200][2599/4361] Loss_D: 0.00423101 (Loss_D_real: 0.00101459 Loss_D_fake: 0.00321642) Loss_G: 0.41515943 Loss_Enh_Dec: -2.54997826\n",
      "| epoch  91 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  2.65 | ppl    14.17 | acc     0.67 | train_ae_norm     1.00\n",
      "[91/200][2699/4361] Loss_D: 0.00081393 (Loss_D_real: 0.00030390 Loss_D_fake: 0.00051003) Loss_G: 0.41812554 Loss_Enh_Dec: -2.15410447\n",
      "| epoch  91 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  2.63 | ppl    13.88 | acc     0.70 | train_ae_norm     1.00\n",
      "[91/200][2799/4361] Loss_D: 0.00114136 (Loss_D_real: 0.00026690 Loss_D_fake: 0.00087446) Loss_G: 0.45413151 Loss_Enh_Dec: -2.00251842\n",
      "| epoch  91 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.21 | loss  2.60 | ppl    13.40 | acc     0.70 | train_ae_norm     1.00\n",
      "[91/200][2899/4361] Loss_D: 0.00334283 (Loss_D_real: 0.00125518 Loss_D_fake: 0.00208765) Loss_G: 0.39361778 Loss_Enh_Dec: -2.51616120\n",
      "| epoch  91 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  2.63 | ppl    13.88 | acc     0.70 | train_ae_norm     1.00\n",
      "[91/200][2999/4361] Loss_D: 0.00111312 (Loss_D_real: 0.00030676 Loss_D_fake: 0.00080636) Loss_G: 0.37328115 Loss_Enh_Dec: -2.11284804\n",
      "| epoch  91 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  2.64 | ppl    14.01 | acc     0.69 | train_ae_norm     1.00\n",
      "[91/200][3099/4361] Loss_D: 0.00067169 (Loss_D_real: 0.00007126 Loss_D_fake: 0.00060043) Loss_G: 0.44761059 Loss_Enh_Dec: -1.85244203\n",
      "| epoch  91 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  2.64 | ppl    13.96 | acc     0.69 | train_ae_norm     1.00\n",
      "[91/200][3199/4361] Loss_D: 0.00075411 (Loss_D_real: 0.00012180 Loss_D_fake: 0.00063231) Loss_G: 0.43045256 Loss_Enh_Dec: -1.99433124\n",
      "| epoch  91 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.67 | ppl    14.44 | acc     0.71 | train_ae_norm     1.00\n",
      "[91/200][3299/4361] Loss_D: 0.00271158 (Loss_D_real: 0.00128122 Loss_D_fake: 0.00143036) Loss_G: 0.37277198 Loss_Enh_Dec: -2.35077214\n",
      "| epoch  91 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  2.65 | ppl    14.11 | acc     0.70 | train_ae_norm     1.00\n",
      "[91/200][3399/4361] Loss_D: 0.00514194 (Loss_D_real: 0.00458907 Loss_D_fake: 0.00055286) Loss_G: 0.62300557 Loss_Enh_Dec: -1.27644610\n",
      "| epoch  91 |  3400/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  2.63 | ppl    13.84 | acc     0.67 | train_ae_norm     1.00\n",
      "[91/200][3499/4361] Loss_D: 0.03713766 (Loss_D_real: 0.03295380 Loss_D_fake: 0.00418387) Loss_G: 0.38739690 Loss_Enh_Dec: -1.61780059\n",
      "| epoch  91 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  2.57 | ppl    13.04 | acc     0.68 | train_ae_norm     1.00\n",
      "[91/200][3599/4361] Loss_D: 0.00114621 (Loss_D_real: 0.00023445 Loss_D_fake: 0.00091176) Loss_G: 0.37510967 Loss_Enh_Dec: -1.52884126\n",
      "| epoch  91 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  2.59 | ppl    13.32 | acc     0.73 | train_ae_norm     1.00\n",
      "[91/200][3699/4361] Loss_D: 0.00222392 (Loss_D_real: 0.00100679 Loss_D_fake: 0.00121713) Loss_G: 0.39253646 Loss_Enh_Dec: -1.41603506\n",
      "| epoch  91 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  2.62 | ppl    13.70 | acc     0.67 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[91/200][3799/4361] Loss_D: 0.00090263 (Loss_D_real: 0.00029595 Loss_D_fake: 0.00060668) Loss_G: 0.37556493 Loss_Enh_Dec: -1.85284042\n",
      "| epoch  91 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.63 | ppl    13.85 | acc     0.76 | train_ae_norm     1.00\n",
      "[91/200][3899/4361] Loss_D: 0.00058407 (Loss_D_real: 0.00013624 Loss_D_fake: 0.00044783) Loss_G: 0.37252975 Loss_Enh_Dec: -1.47857130\n",
      "| epoch  91 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.61 | ppl    13.61 | acc     0.67 | train_ae_norm     1.00\n",
      "[91/200][3999/4361] Loss_D: 0.00309210 (Loss_D_real: 0.00246921 Loss_D_fake: 0.00062289) Loss_G: 0.41099721 Loss_Enh_Dec: -1.70518363\n",
      "| epoch  91 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  2.61 | ppl    13.67 | acc     0.70 | train_ae_norm     1.00\n",
      "[91/200][4099/4361] Loss_D: 0.00122437 (Loss_D_real: 0.00067132 Loss_D_fake: 0.00055305) Loss_G: 0.43291458 Loss_Enh_Dec: -1.69517362\n",
      "| epoch  91 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.88 | loss  2.62 | ppl    13.72 | acc     0.70 | train_ae_norm     1.00\n",
      "[91/200][4199/4361] Loss_D: 0.00774376 (Loss_D_real: 0.00676377 Loss_D_fake: 0.00098000) Loss_G: 0.39809528 Loss_Enh_Dec: -1.88394415\n",
      "| epoch  91 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  2.63 | ppl    13.87 | acc     0.73 | train_ae_norm     1.00\n",
      "[91/200][4299/4361] Loss_D: 0.00185970 (Loss_D_real: 0.00110003 Loss_D_fake: 0.00075967) Loss_G: 0.45832092 Loss_Enh_Dec: -1.88621557\n",
      "| epoch  91 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.59 | ppl    13.29 | acc     0.68 | train_ae_norm     1.00\n",
      "| end of epoch  91 | time: 1851.13s | test loss  2.64 | test ppl 14.08 | acc 0.732\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 92 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.699\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.492\n",
      "  Test Loss: 4.491\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  92 |     0/ 4361 batches | lr 0.000000 | ms/batch 864.00 | loss  0.02 | ppl     1.02 | acc     0.75 | train_ae_norm     1.00\n",
      "[92/200][99/4361] Loss_D: 0.00131927 (Loss_D_real: 0.00023534 Loss_D_fake: 0.00108392) Loss_G: 0.37092853 Loss_Enh_Dec: -1.85714340\n",
      "| epoch  92 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.36 | loss  2.61 | ppl    13.58 | acc     0.66 | train_ae_norm     1.00\n",
      "[92/200][199/4361] Loss_D: 0.00028199 (Loss_D_real: 0.00015041 Loss_D_fake: 0.00013157) Loss_G: 0.45612127 Loss_Enh_Dec: -1.86348474\n",
      "| epoch  92 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  2.64 | ppl    14.08 | acc     0.70 | train_ae_norm     1.00\n",
      "[92/200][299/4361] Loss_D: 0.00109066 (Loss_D_real: 0.00046023 Loss_D_fake: 0.00063042) Loss_G: 0.46734831 Loss_Enh_Dec: -1.40524983\n",
      "| epoch  92 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  2.65 | ppl    14.15 | acc     0.66 | train_ae_norm     1.00\n",
      "[92/200][399/4361] Loss_D: 0.00086579 (Loss_D_real: 0.00038652 Loss_D_fake: 0.00047928) Loss_G: 0.42632809 Loss_Enh_Dec: -2.11869693\n",
      "| epoch  92 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  2.58 | ppl    13.13 | acc     0.69 | train_ae_norm     1.00\n",
      "[92/200][499/4361] Loss_D: 0.00211490 (Loss_D_real: 0.00191044 Loss_D_fake: 0.00020446) Loss_G: 0.51102436 Loss_Enh_Dec: -1.88859594\n",
      "| epoch  92 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  2.65 | ppl    14.11 | acc     0.71 | train_ae_norm     1.00\n",
      "[92/200][599/4361] Loss_D: 0.00114429 (Loss_D_real: 0.00027750 Loss_D_fake: 0.00086679) Loss_G: 0.37349281 Loss_Enh_Dec: -1.40178680\n",
      "| epoch  92 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  2.60 | ppl    13.40 | acc     0.68 | train_ae_norm     1.00\n",
      "[92/200][699/4361] Loss_D: 0.00768435 (Loss_D_real: 0.00047335 Loss_D_fake: 0.00721100) Loss_G: 0.83015102 Loss_Enh_Dec: -1.55741596\n",
      "| epoch  92 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.62 | ppl    13.75 | acc     0.69 | train_ae_norm     1.00\n",
      "[92/200][799/4361] Loss_D: 0.00122725 (Loss_D_real: 0.00052681 Loss_D_fake: 0.00070044) Loss_G: 0.43435621 Loss_Enh_Dec: -1.80340219\n",
      "| epoch  92 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  2.60 | ppl    13.41 | acc     0.71 | train_ae_norm     1.00\n",
      "[92/200][899/4361] Loss_D: 0.00234542 (Loss_D_real: 0.00139803 Loss_D_fake: 0.00094739) Loss_G: 0.42898628 Loss_Enh_Dec: -1.61358666\n",
      "| epoch  92 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  2.62 | ppl    13.74 | acc     0.71 | train_ae_norm     1.00\n",
      "[92/200][999/4361] Loss_D: 0.00319018 (Loss_D_real: 0.00268124 Loss_D_fake: 0.00050894) Loss_G: 0.41824275 Loss_Enh_Dec: -1.70867348\n",
      "| epoch  92 |  1000/ 4361 batches | lr 0.000000 | ms/batch 402.62 | loss  2.61 | ppl    13.62 | acc     0.69 | train_ae_norm     1.00\n",
      "[92/200][1099/4361] Loss_D: 0.00337867 (Loss_D_real: 0.00150381 Loss_D_fake: 0.00187486) Loss_G: 0.41710120 Loss_Enh_Dec: -1.13553941\n",
      "| epoch  92 |  1100/ 4361 batches | lr 0.000000 | ms/batch 402.16 | loss  2.63 | ppl    13.81 | acc     0.68 | train_ae_norm     1.00\n",
      "[92/200][1199/4361] Loss_D: 0.00193589 (Loss_D_real: 0.00069963 Loss_D_fake: 0.00123625) Loss_G: 0.46592140 Loss_Enh_Dec: -1.63581967\n",
      "| epoch  92 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  2.62 | ppl    13.78 | acc     0.72 | train_ae_norm     1.00\n",
      "[92/200][1299/4361] Loss_D: 0.00081045 (Loss_D_real: 0.00030415 Loss_D_fake: 0.00050630) Loss_G: 0.43087229 Loss_Enh_Dec: -1.33026087\n",
      "| epoch  92 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  2.64 | ppl    13.95 | acc     0.68 | train_ae_norm     1.00\n",
      "[92/200][1399/4361] Loss_D: 0.00182170 (Loss_D_real: 0.00090168 Loss_D_fake: 0.00092001) Loss_G: 0.41026926 Loss_Enh_Dec: -1.36549139\n",
      "| epoch  92 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  2.63 | ppl    13.89 | acc     0.65 | train_ae_norm     1.00\n",
      "[92/200][1499/4361] Loss_D: 0.00831065 (Loss_D_real: 0.00228781 Loss_D_fake: 0.00602285) Loss_G: 0.47570372 Loss_Enh_Dec: -1.44649494\n",
      "| epoch  92 |  1500/ 4361 batches | lr 0.000000 | ms/batch 402.07 | loss  2.68 | ppl    14.55 | acc     0.65 | train_ae_norm     1.00\n",
      "[92/200][1599/4361] Loss_D: 0.00205591 (Loss_D_real: 0.00160610 Loss_D_fake: 0.00044981) Loss_G: 0.43937126 Loss_Enh_Dec: -1.66474462\n",
      "| epoch  92 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.63 | ppl    13.88 | acc     0.70 | train_ae_norm     1.00\n",
      "[92/200][1699/4361] Loss_D: 0.00416552 (Loss_D_real: 0.00285247 Loss_D_fake: 0.00131305) Loss_G: 0.41610813 Loss_Enh_Dec: -1.89628983\n",
      "| epoch  92 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.62 | ppl    13.68 | acc     0.67 | train_ae_norm     1.00\n",
      "[92/200][1799/4361] Loss_D: 0.00059289 (Loss_D_real: 0.00017795 Loss_D_fake: 0.00041493) Loss_G: 0.41572928 Loss_Enh_Dec: -1.57281685\n",
      "| epoch  92 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  2.59 | ppl    13.33 | acc     0.69 | train_ae_norm     1.00\n",
      "[92/200][1899/4361] Loss_D: 0.00549891 (Loss_D_real: 0.00484189 Loss_D_fake: 0.00065702) Loss_G: 0.43030950 Loss_Enh_Dec: -1.76248515\n",
      "| epoch  92 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  2.65 | ppl    14.11 | acc     0.71 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[92/200][1999/4361] Loss_D: 0.00721913 (Loss_D_real: 0.00616967 Loss_D_fake: 0.00104946) Loss_G: 0.44478771 Loss_Enh_Dec: -1.56524265\n",
      "| epoch  92 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  2.59 | ppl    13.30 | acc     0.74 | train_ae_norm     1.00\n",
      "[92/200][2099/4361] Loss_D: 0.00072418 (Loss_D_real: 0.00016241 Loss_D_fake: 0.00056177) Loss_G: 0.38803121 Loss_Enh_Dec: -1.90672207\n",
      "| epoch  92 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.64 | ppl    14.00 | acc     0.70 | train_ae_norm     1.00\n",
      "[92/200][2199/4361] Loss_D: 0.00152267 (Loss_D_real: 0.00017523 Loss_D_fake: 0.00134744) Loss_G: 0.39165398 Loss_Enh_Dec: -1.83383775\n",
      "| epoch  92 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  2.60 | ppl    13.50 | acc     0.72 | train_ae_norm     1.00\n",
      "[92/200][2299/4361] Loss_D: 0.00280501 (Loss_D_real: 0.00211830 Loss_D_fake: 0.00068670) Loss_G: 0.53209203 Loss_Enh_Dec: -1.90366614\n",
      "| epoch  92 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  2.60 | ppl    13.40 | acc     0.74 | train_ae_norm     1.00\n",
      "[92/200][2399/4361] Loss_D: 0.00588930 (Loss_D_real: 0.00043003 Loss_D_fake: 0.00545926) Loss_G: 0.38972488 Loss_Enh_Dec: -1.64865077\n",
      "| epoch  92 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.58 | ppl    13.25 | acc     0.66 | train_ae_norm     1.00\n",
      "[92/200][2499/4361] Loss_D: 0.00434200 (Loss_D_real: 0.00068091 Loss_D_fake: 0.00366109) Loss_G: 0.43373546 Loss_Enh_Dec: -1.75339568\n",
      "| epoch  92 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  2.61 | ppl    13.62 | acc     0.74 | train_ae_norm     1.00\n",
      "[92/200][2599/4361] Loss_D: 0.00351098 (Loss_D_real: 0.00221661 Loss_D_fake: 0.00129438) Loss_G: 0.40955564 Loss_Enh_Dec: -1.51149023\n",
      "| epoch  92 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.57 | ppl    13.06 | acc     0.69 | train_ae_norm     1.00\n",
      "[92/200][2699/4361] Loss_D: 0.00202814 (Loss_D_real: 0.00069059 Loss_D_fake: 0.00133755) Loss_G: 0.41267619 Loss_Enh_Dec: -1.77575803\n",
      "| epoch  92 |  2700/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  2.60 | ppl    13.46 | acc     0.69 | train_ae_norm     1.00\n",
      "[92/200][2799/4361] Loss_D: 0.01160235 (Loss_D_real: 0.00974712 Loss_D_fake: 0.00185523) Loss_G: 0.57695556 Loss_Enh_Dec: -1.93964994\n",
      "| epoch  92 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  2.55 | ppl    12.78 | acc     0.67 | train_ae_norm     1.00\n",
      "[92/200][2899/4361] Loss_D: 0.00230593 (Loss_D_real: 0.00175269 Loss_D_fake: 0.00055324) Loss_G: 0.65740603 Loss_Enh_Dec: -1.82253730\n",
      "| epoch  92 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.72 | loss  2.59 | ppl    13.27 | acc     0.70 | train_ae_norm     1.00\n",
      "[92/200][2999/4361] Loss_D: 0.00077884 (Loss_D_real: 0.00007419 Loss_D_fake: 0.00070465) Loss_G: 0.49168086 Loss_Enh_Dec: -1.87659192\n",
      "| epoch  92 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  2.59 | ppl    13.39 | acc     0.69 | train_ae_norm     1.00\n",
      "[92/200][3099/4361] Loss_D: 0.00138242 (Loss_D_real: 0.00013477 Loss_D_fake: 0.00124765) Loss_G: 0.42918044 Loss_Enh_Dec: -1.82828867\n",
      "| epoch  92 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.45 | loss  2.58 | ppl    13.19 | acc     0.68 | train_ae_norm     1.00\n",
      "[92/200][3199/4361] Loss_D: 0.00202010 (Loss_D_real: 0.00140983 Loss_D_fake: 0.00061027) Loss_G: 0.54477960 Loss_Enh_Dec: -1.48170745\n",
      "| epoch  92 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  2.61 | ppl    13.62 | acc     0.72 | train_ae_norm     1.00\n",
      "[92/200][3299/4361] Loss_D: 0.00526362 (Loss_D_real: 0.00410983 Loss_D_fake: 0.00115380) Loss_G: 0.44364840 Loss_Enh_Dec: -1.78299904\n",
      "| epoch  92 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  2.60 | ppl    13.53 | acc     0.72 | train_ae_norm     1.00\n",
      "[92/200][3399/4361] Loss_D: 0.00141587 (Loss_D_real: 0.00061211 Loss_D_fake: 0.00080376) Loss_G: 0.40213624 Loss_Enh_Dec: -1.48244309\n",
      "| epoch  92 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.93 | loss  2.59 | ppl    13.31 | acc     0.69 | train_ae_norm     1.00\n",
      "[92/200][3499/4361] Loss_D: 0.00537498 (Loss_D_real: 0.00061378 Loss_D_fake: 0.00476120) Loss_G: 0.61266470 Loss_Enh_Dec: -1.41596854\n",
      "| epoch  92 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  2.53 | ppl    12.60 | acc     0.69 | train_ae_norm     1.00\n",
      "[92/200][3599/4361] Loss_D: 0.00232379 (Loss_D_real: 0.00103685 Loss_D_fake: 0.00128695) Loss_G: 0.39398023 Loss_Enh_Dec: -1.20263374\n",
      "| epoch  92 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.56 | ppl    12.96 | acc     0.74 | train_ae_norm     1.00\n",
      "[92/200][3699/4361] Loss_D: 0.01061232 (Loss_D_real: 0.00896048 Loss_D_fake: 0.00165183) Loss_G: 0.45372000 Loss_Enh_Dec: -1.53999984\n",
      "| epoch  92 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  2.58 | ppl    13.23 | acc     0.67 | train_ae_norm     1.00\n",
      "[92/200][3799/4361] Loss_D: 0.00327767 (Loss_D_real: 0.00146990 Loss_D_fake: 0.00180777) Loss_G: 0.43043566 Loss_Enh_Dec: -1.34091139\n",
      "| epoch  92 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  2.60 | ppl    13.46 | acc     0.74 | train_ae_norm     1.00\n",
      "[92/200][3899/4361] Loss_D: 0.00233058 (Loss_D_real: 0.00045171 Loss_D_fake: 0.00187887) Loss_G: 0.47162175 Loss_Enh_Dec: -1.42773592\n",
      "| epoch  92 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  2.61 | ppl    13.56 | acc     0.67 | train_ae_norm     1.00\n",
      "[92/200][3999/4361] Loss_D: 0.00138294 (Loss_D_real: 0.00029289 Loss_D_fake: 0.00109005) Loss_G: 0.44179687 Loss_Enh_Dec: -1.30991685\n",
      "| epoch  92 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  2.59 | ppl    13.35 | acc     0.70 | train_ae_norm     1.00\n",
      "[92/200][4099/4361] Loss_D: 0.00119655 (Loss_D_real: 0.00045648 Loss_D_fake: 0.00074007) Loss_G: 0.46892467 Loss_Enh_Dec: -1.31062567\n",
      "| epoch  92 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  2.56 | ppl    12.91 | acc     0.69 | train_ae_norm     1.00\n",
      "[92/200][4199/4361] Loss_D: 0.00735272 (Loss_D_real: 0.00665239 Loss_D_fake: 0.00070033) Loss_G: 0.48180056 Loss_Enh_Dec: -1.39453697\n",
      "| epoch  92 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  2.60 | ppl    13.49 | acc     0.72 | train_ae_norm     1.00\n",
      "[92/200][4299/4361] Loss_D: 0.00329539 (Loss_D_real: 0.00012411 Loss_D_fake: 0.00317128) Loss_G: 0.41055736 Loss_Enh_Dec: -1.55268586\n",
      "| epoch  92 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.56 | ppl    12.99 | acc     0.71 | train_ae_norm     1.00\n",
      "| end of epoch  92 | time: 1852.83s | test loss  2.58 | test ppl 13.20 | acc 0.735\n",
      "bleu_self:  [2.70833333e-01 5.84559150e-02 4.95160534e-07 1.67416354e-09\n",
      " 6.63870358e-10]\n",
      "bleu_test:  [7.26388889e-01 1.33985562e-01 9.81831623e-07 3.57603117e-09\n",
      " 2.49921863e-09]\n",
      "bleu_self: [0.27083333,0.05845591,0.00000050,0.00000000,0.00000000]\n",
      "bleu_test: [0.72638889,0.13398556,0.00000098,0.00000000,0.00000000]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 93 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.701\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.475\n",
      "  Test Loss: 4.480\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  93 |     0/ 4361 batches | lr 0.000000 | ms/batch 862.09 | loss  0.02 | ppl     1.02 | acc     0.73 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[93/200][99/4361] Loss_D: 0.00100621 (Loss_D_real: 0.00031328 Loss_D_fake: 0.00069292) Loss_G: 0.44798884 Loss_Enh_Dec: -0.91273397\n",
      "| epoch  93 |   100/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.58 | ppl    13.18 | acc     0.67 | train_ae_norm     1.00\n",
      "[93/200][199/4361] Loss_D: 0.00170971 (Loss_D_real: 0.00008891 Loss_D_fake: 0.00162080) Loss_G: 0.42756349 Loss_Enh_Dec: -0.89913809\n",
      "| epoch  93 |   200/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  2.60 | ppl    13.50 | acc     0.70 | train_ae_norm     1.00\n",
      "[93/200][299/4361] Loss_D: 0.00319001 (Loss_D_real: 0.00237107 Loss_D_fake: 0.00081894) Loss_G: 0.53997195 Loss_Enh_Dec: -1.68985617\n",
      "| epoch  93 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  2.60 | ppl    13.50 | acc     0.66 | train_ae_norm     1.00\n",
      "[93/200][399/4361] Loss_D: 0.02383589 (Loss_D_real: 0.02277983 Loss_D_fake: 0.00105606) Loss_G: 0.45067191 Loss_Enh_Dec: -1.66027677\n",
      "| epoch  93 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.53 | ppl    12.61 | acc     0.71 | train_ae_norm     1.00\n",
      "[93/200][499/4361] Loss_D: 0.00144634 (Loss_D_real: 0.00084212 Loss_D_fake: 0.00060422) Loss_G: 0.42872533 Loss_Enh_Dec: -1.69168556\n",
      "| epoch  93 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  2.59 | ppl    13.34 | acc     0.71 | train_ae_norm     1.00\n",
      "[93/200][599/4361] Loss_D: 0.00197693 (Loss_D_real: 0.00088375 Loss_D_fake: 0.00109318) Loss_G: 0.49671784 Loss_Enh_Dec: -1.76108098\n",
      "| epoch  93 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.35 | loss  2.53 | ppl    12.53 | acc     0.67 | train_ae_norm     1.00\n",
      "[93/200][699/4361] Loss_D: 0.00260780 (Loss_D_real: 0.00046831 Loss_D_fake: 0.00213949) Loss_G: 0.48755190 Loss_Enh_Dec: -1.28750348\n",
      "| epoch  93 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.57 | ppl    13.10 | acc     0.71 | train_ae_norm     1.00\n",
      "[93/200][799/4361] Loss_D: 0.00182897 (Loss_D_real: 0.00121877 Loss_D_fake: 0.00061021) Loss_G: 0.42355013 Loss_Enh_Dec: -1.91563308\n",
      "| epoch  93 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  2.57 | ppl    13.05 | acc     0.69 | train_ae_norm     1.00\n",
      "[93/200][899/4361] Loss_D: 0.00169629 (Loss_D_real: 0.00021422 Loss_D_fake: 0.00148207) Loss_G: 0.44601211 Loss_Enh_Dec: -2.29486346\n",
      "| epoch  93 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  2.59 | ppl    13.28 | acc     0.72 | train_ae_norm     1.00\n",
      "[93/200][999/4361] Loss_D: 0.00127284 (Loss_D_real: 0.00074017 Loss_D_fake: 0.00053266) Loss_G: 0.44081908 Loss_Enh_Dec: -1.62525010\n",
      "| epoch  93 |  1000/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  2.58 | ppl    13.14 | acc     0.72 | train_ae_norm     1.00\n",
      "[93/200][1099/4361] Loss_D: 0.00086408 (Loss_D_real: 0.00028295 Loss_D_fake: 0.00058113) Loss_G: 0.43791133 Loss_Enh_Dec: -2.29327703\n",
      "| epoch  93 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  2.57 | ppl    13.04 | acc     0.69 | train_ae_norm     1.00\n",
      "[93/200][1199/4361] Loss_D: 0.00375227 (Loss_D_real: 0.00291813 Loss_D_fake: 0.00083414) Loss_G: 0.41307893 Loss_Enh_Dec: -2.36194849\n",
      "| epoch  93 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  2.57 | ppl    13.11 | acc     0.72 | train_ae_norm     1.00\n",
      "[93/200][1299/4361] Loss_D: 0.01956422 (Loss_D_real: 0.01902428 Loss_D_fake: 0.00053994) Loss_G: 0.45578289 Loss_Enh_Dec: -2.30264258\n",
      "| epoch  93 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.54 | loss  2.59 | ppl    13.32 | acc     0.71 | train_ae_norm     1.00\n",
      "[93/200][1399/4361] Loss_D: 0.00053752 (Loss_D_real: 0.00007321 Loss_D_fake: 0.00046431) Loss_G: 0.39916703 Loss_Enh_Dec: -2.38256264\n",
      "| epoch  93 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.76 | loss  2.57 | ppl    13.08 | acc     0.66 | train_ae_norm     1.00\n",
      "[93/200][1499/4361] Loss_D: 0.00087211 (Loss_D_real: 0.00022127 Loss_D_fake: 0.00065084) Loss_G: 0.42964688 Loss_Enh_Dec: -2.08568263\n",
      "| epoch  93 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.06 | loss  2.62 | ppl    13.80 | acc     0.69 | train_ae_norm     1.00\n",
      "[93/200][1599/4361] Loss_D: 0.00083774 (Loss_D_real: 0.00016623 Loss_D_fake: 0.00067151) Loss_G: 0.51553571 Loss_Enh_Dec: -1.87348938\n",
      "| epoch  93 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  2.59 | ppl    13.35 | acc     0.71 | train_ae_norm     1.00\n",
      "[93/200][1699/4361] Loss_D: 0.00188251 (Loss_D_real: 0.00123839 Loss_D_fake: 0.00064412) Loss_G: 0.54228926 Loss_Enh_Dec: -1.80687940\n",
      "| epoch  93 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  2.57 | ppl    13.11 | acc     0.69 | train_ae_norm     1.00\n",
      "[93/200][1799/4361] Loss_D: 0.00375995 (Loss_D_real: 0.00166606 Loss_D_fake: 0.00209389) Loss_G: 0.39380461 Loss_Enh_Dec: -1.65550804\n",
      "| epoch  93 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  2.56 | ppl    12.92 | acc     0.71 | train_ae_norm     1.00\n",
      "[93/200][1899/4361] Loss_D: 0.00077961 (Loss_D_real: 0.00021253 Loss_D_fake: 0.00056708) Loss_G: 0.41570416 Loss_Enh_Dec: -1.46111417\n",
      "| epoch  93 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  2.58 | ppl    13.26 | acc     0.71 | train_ae_norm     1.00\n",
      "[93/200][1999/4361] Loss_D: 0.00316187 (Loss_D_real: 0.00142338 Loss_D_fake: 0.00173848) Loss_G: 0.46880922 Loss_Enh_Dec: -1.99161947\n",
      "| epoch  93 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  2.56 | ppl    12.88 | acc     0.72 | train_ae_norm     1.00\n",
      "[93/200][2099/4361] Loss_D: 0.00100597 (Loss_D_real: 0.00010260 Loss_D_fake: 0.00090337) Loss_G: 0.39824453 Loss_Enh_Dec: -1.60459578\n",
      "| epoch  93 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.08 | loss  2.57 | ppl    13.03 | acc     0.73 | train_ae_norm     1.00\n",
      "[93/200][2199/4361] Loss_D: 0.00531338 (Loss_D_real: 0.00032552 Loss_D_fake: 0.00498786) Loss_G: 0.41733381 Loss_Enh_Dec: -2.16310287\n",
      "| epoch  93 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.93 | loss  2.56 | ppl    12.92 | acc     0.71 | train_ae_norm     1.00\n",
      "[93/200][2299/4361] Loss_D: 0.00113912 (Loss_D_real: 0.00067073 Loss_D_fake: 0.00046839) Loss_G: 0.44667679 Loss_Enh_Dec: -2.02463746\n",
      "| epoch  93 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.84 | loss  2.55 | ppl    12.83 | acc     0.73 | train_ae_norm     1.00\n",
      "[93/200][2399/4361] Loss_D: 0.00099880 (Loss_D_real: 0.00061279 Loss_D_fake: 0.00038601) Loss_G: 0.41373044 Loss_Enh_Dec: -1.77983403\n",
      "| epoch  93 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  2.57 | ppl    13.03 | acc     0.68 | train_ae_norm     1.00\n",
      "[93/200][2499/4361] Loss_D: 0.02041850 (Loss_D_real: 0.01948320 Loss_D_fake: 0.00093530) Loss_G: 0.41872421 Loss_Enh_Dec: -1.63734365\n",
      "| epoch  93 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  2.60 | ppl    13.42 | acc     0.73 | train_ae_norm     1.00\n",
      "[93/200][2599/4361] Loss_D: 0.00143988 (Loss_D_real: 0.00047216 Loss_D_fake: 0.00096771) Loss_G: 0.43933764 Loss_Enh_Dec: -1.79894769\n",
      "| epoch  93 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.72 | loss  2.56 | ppl    12.96 | acc     0.66 | train_ae_norm     1.00\n",
      "[93/200][2699/4361] Loss_D: 0.00954116 (Loss_D_real: 0.00876872 Loss_D_fake: 0.00077244) Loss_G: 0.50664538 Loss_Enh_Dec: -1.71972311\n",
      "| epoch  93 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  2.58 | ppl    13.23 | acc     0.70 | train_ae_norm     1.00\n",
      "[93/200][2799/4361] Loss_D: 0.00082746 (Loss_D_real: 0.00017534 Loss_D_fake: 0.00065213) Loss_G: 0.42235595 Loss_Enh_Dec: -2.04133391\n",
      "| epoch  93 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  2.54 | ppl    12.68 | acc     0.69 | train_ae_norm     1.00\n",
      "[93/200][2899/4361] Loss_D: 0.02104927 (Loss_D_real: 0.01996077 Loss_D_fake: 0.00108850) Loss_G: 0.45735270 Loss_Enh_Dec: -1.84675789\n",
      "| epoch  93 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.52 | loss  2.57 | ppl    13.06 | acc     0.70 | train_ae_norm     1.00\n",
      "[93/200][2999/4361] Loss_D: 0.00079141 (Loss_D_real: 0.00049468 Loss_D_fake: 0.00029672) Loss_G: 0.64952308 Loss_Enh_Dec: -1.71589398\n",
      "| epoch  93 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.06 | loss  2.59 | ppl    13.30 | acc     0.70 | train_ae_norm     1.00\n",
      "[93/200][3099/4361] Loss_D: 0.01068927 (Loss_D_real: 0.01028547 Loss_D_fake: 0.00040380) Loss_G: 0.47650263 Loss_Enh_Dec: -1.95654035\n",
      "| epoch  93 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.47 | loss  2.59 | ppl    13.35 | acc     0.69 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[93/200][3199/4361] Loss_D: 0.00138449 (Loss_D_real: 0.00087226 Loss_D_fake: 0.00051223) Loss_G: 0.43682519 Loss_Enh_Dec: -2.04803443\n",
      "| epoch  93 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  2.63 | ppl    13.82 | acc     0.69 | train_ae_norm     1.00\n",
      "[93/200][3299/4361] Loss_D: 0.00234391 (Loss_D_real: 0.00199403 Loss_D_fake: 0.00034988) Loss_G: 0.42759266 Loss_Enh_Dec: -2.05707383\n",
      "| epoch  93 |  3300/ 4361 batches | lr 0.000000 | ms/batch 400.17 | loss  2.62 | ppl    13.71 | acc     0.70 | train_ae_norm     1.00\n",
      "[93/200][3399/4361] Loss_D: 0.00286670 (Loss_D_real: 0.00248742 Loss_D_fake: 0.00037927) Loss_G: 0.42757568 Loss_Enh_Dec: -1.58951402\n",
      "| epoch  93 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  2.61 | ppl    13.62 | acc     0.69 | train_ae_norm     1.00\n",
      "[93/200][3499/4361] Loss_D: 0.00145008 (Loss_D_real: 0.00005518 Loss_D_fake: 0.00139490) Loss_G: 0.43547311 Loss_Enh_Dec: -1.82835853\n",
      "| epoch  93 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  2.54 | ppl    12.74 | acc     0.68 | train_ae_norm     1.00\n",
      "[93/200][3599/4361] Loss_D: 0.00496912 (Loss_D_real: 0.00452049 Loss_D_fake: 0.00044863) Loss_G: 0.50404829 Loss_Enh_Dec: -1.67178762\n",
      "| epoch  93 |  3600/ 4361 batches | lr 0.000000 | ms/batch 400.37 | loss  2.57 | ppl    13.12 | acc     0.73 | train_ae_norm     1.00\n",
      "[93/200][3699/4361] Loss_D: 0.00115282 (Loss_D_real: 0.00064930 Loss_D_fake: 0.00050353) Loss_G: 0.41128302 Loss_Enh_Dec: -1.65721214\n",
      "| epoch  93 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.72 | loss  2.59 | ppl    13.33 | acc     0.67 | train_ae_norm     1.00\n",
      "[93/200][3799/4361] Loss_D: 0.00199464 (Loss_D_real: 0.00076678 Loss_D_fake: 0.00122785) Loss_G: 0.41382000 Loss_Enh_Dec: -1.70009673\n",
      "| epoch  93 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.59 | ppl    13.34 | acc     0.75 | train_ae_norm     1.00\n",
      "[93/200][3899/4361] Loss_D: 0.00187335 (Loss_D_real: 0.00129028 Loss_D_fake: 0.00058307) Loss_G: 0.45946565 Loss_Enh_Dec: -1.68056381\n",
      "| epoch  93 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  2.63 | ppl    13.90 | acc     0.67 | train_ae_norm     1.00\n",
      "[93/200][3999/4361] Loss_D: 0.00036989 (Loss_D_real: 0.00036897 Loss_D_fake: 0.00000092) Loss_G: 0.99364012 Loss_Enh_Dec: -1.47486532\n",
      "| epoch  93 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.91 | loss  2.87 | ppl    17.65 | acc     0.70 | train_ae_norm     1.00\n",
      "[93/200][4099/4361] Loss_D: 0.00183849 (Loss_D_real: 0.00060033 Loss_D_fake: 0.00123816) Loss_G: 0.49321824 Loss_Enh_Dec: -1.76796710\n",
      "| epoch  93 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.59 | ppl    13.34 | acc     0.69 | train_ae_norm     1.00\n",
      "[93/200][4199/4361] Loss_D: 0.00374914 (Loss_D_real: 0.00313690 Loss_D_fake: 0.00061224) Loss_G: 0.42294174 Loss_Enh_Dec: -1.71841085\n",
      "| epoch  93 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  2.64 | ppl    13.99 | acc     0.72 | train_ae_norm     1.00\n",
      "[93/200][4299/4361] Loss_D: 0.00707699 (Loss_D_real: 0.00222473 Loss_D_fake: 0.00485226) Loss_G: 0.36265984 Loss_Enh_Dec: -1.84589601\n",
      "| epoch  93 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.06 | loss  2.58 | ppl    13.25 | acc     0.70 | train_ae_norm     1.00\n",
      "| end of epoch  93 | time: 1850.99s | test loss  2.61 | test ppl 13.60 | acc 0.734\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 94 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.701\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.480\n",
      "  Test Loss: 4.382\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  94 |     0/ 4361 batches | lr 0.000000 | ms/batch 866.74 | loss  0.02 | ppl     1.02 | acc     0.72 | train_ae_norm     1.00\n",
      "[94/200][99/4361] Loss_D: 0.00084785 (Loss_D_real: 0.00013488 Loss_D_fake: 0.00071297) Loss_G: 0.49336463 Loss_Enh_Dec: -1.69936049\n",
      "| epoch  94 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  2.60 | ppl    13.42 | acc     0.66 | train_ae_norm     1.00\n",
      "[94/200][199/4361] Loss_D: 0.00276938 (Loss_D_real: 0.00241997 Loss_D_fake: 0.00034942) Loss_G: 0.47999334 Loss_Enh_Dec: -1.80744874\n",
      "| epoch  94 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  2.62 | ppl    13.76 | acc     0.71 | train_ae_norm     1.00\n",
      "[94/200][299/4361] Loss_D: 0.00099096 (Loss_D_real: 0.00015557 Loss_D_fake: 0.00083539) Loss_G: 0.43974695 Loss_Enh_Dec: -1.85524547\n",
      "| epoch  94 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  2.63 | ppl    13.92 | acc     0.67 | train_ae_norm     1.00\n",
      "[94/200][399/4361] Loss_D: 0.00329432 (Loss_D_real: 0.00271463 Loss_D_fake: 0.00057970) Loss_G: 0.48792228 Loss_Enh_Dec: -1.39047325\n",
      "| epoch  94 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  2.54 | ppl    12.72 | acc     0.71 | train_ae_norm     1.00\n",
      "[94/200][499/4361] Loss_D: 0.00196957 (Loss_D_real: 0.00097711 Loss_D_fake: 0.00099246) Loss_G: 0.38518977 Loss_Enh_Dec: -1.74131668\n",
      "| epoch  94 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.64 | ppl    14.05 | acc     0.71 | train_ae_norm     1.00\n",
      "[94/200][599/4361] Loss_D: 0.00134136 (Loss_D_real: 0.00108899 Loss_D_fake: 0.00025237) Loss_G: 0.76064682 Loss_Enh_Dec: -1.34755886\n",
      "| epoch  94 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.59 | ppl    13.35 | acc     0.66 | train_ae_norm     1.00\n",
      "[94/200][699/4361] Loss_D: 0.00334023 (Loss_D_real: 0.00088329 Loss_D_fake: 0.00245694) Loss_G: 0.46406785 Loss_Enh_Dec: -1.63613248\n",
      "| epoch  94 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  2.63 | ppl    13.87 | acc     0.69 | train_ae_norm     1.00\n",
      "[94/200][799/4361] Loss_D: 0.00194952 (Loss_D_real: 0.00135968 Loss_D_fake: 0.00058984) Loss_G: 0.46937272 Loss_Enh_Dec: -1.72811472\n",
      "| epoch  94 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.78 | loss  2.61 | ppl    13.54 | acc     0.70 | train_ae_norm     1.00\n",
      "[94/200][899/4361] Loss_D: 0.03304070 (Loss_D_real: 0.03167945 Loss_D_fake: 0.00136126) Loss_G: 0.44643983 Loss_Enh_Dec: -1.79025769\n",
      "| epoch  94 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.62 | ppl    13.67 | acc     0.71 | train_ae_norm     1.00\n",
      "[94/200][999/4361] Loss_D: 0.00187120 (Loss_D_real: 0.00034879 Loss_D_fake: 0.00152241) Loss_G: 0.40884066 Loss_Enh_Dec: -2.22161412\n",
      "| epoch  94 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  2.59 | ppl    13.28 | acc     0.70 | train_ae_norm     1.00\n",
      "[94/200][1099/4361] Loss_D: 0.00206886 (Loss_D_real: 0.00022630 Loss_D_fake: 0.00184256) Loss_G: 0.43259269 Loss_Enh_Dec: -2.20735788\n",
      "| epoch  94 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.58 | ppl    13.21 | acc     0.71 | train_ae_norm     1.00\n",
      "[94/200][1199/4361] Loss_D: 0.00411170 (Loss_D_real: 0.00315625 Loss_D_fake: 0.00095545) Loss_G: 0.39659813 Loss_Enh_Dec: -1.43267596\n",
      "| epoch  94 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.77 | loss  2.58 | ppl    13.16 | acc     0.73 | train_ae_norm     1.00\n",
      "[94/200][1299/4361] Loss_D: 0.34992501 (Loss_D_real: 0.34816498 Loss_D_fake: 0.00176002) Loss_G: 0.36600897 Loss_Enh_Dec: -2.08579707\n",
      "| epoch  94 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.91 | loss  2.61 | ppl    13.60 | acc     0.70 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[94/200][1399/4361] Loss_D: 0.00143557 (Loss_D_real: 0.00018032 Loss_D_fake: 0.00125525) Loss_G: 0.45380297 Loss_Enh_Dec: -2.20290732\n",
      "| epoch  94 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.88 | loss  2.61 | ppl    13.54 | acc     0.67 | train_ae_norm     1.00\n",
      "[94/200][1499/4361] Loss_D: 0.01727665 (Loss_D_real: 0.01680906 Loss_D_fake: 0.00046759) Loss_G: 0.47265139 Loss_Enh_Dec: -2.26281166\n",
      "| epoch  94 |  1500/ 4361 batches | lr 0.000000 | ms/batch 400.89 | loss  2.65 | ppl    14.13 | acc     0.67 | train_ae_norm     1.00\n",
      "[94/200][1599/4361] Loss_D: 0.00080241 (Loss_D_real: 0.00004934 Loss_D_fake: 0.00075306) Loss_G: 0.36816320 Loss_Enh_Dec: -1.86881983\n",
      "| epoch  94 |  1600/ 4361 batches | lr 0.000000 | ms/batch 400.88 | loss  2.62 | ppl    13.68 | acc     0.70 | train_ae_norm     1.00\n",
      "[94/200][1699/4361] Loss_D: 0.00440707 (Loss_D_real: 0.00147710 Loss_D_fake: 0.00292997) Loss_G: 0.38950238 Loss_Enh_Dec: -1.68640363\n",
      "| epoch  94 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.05 | loss  2.59 | ppl    13.37 | acc     0.67 | train_ae_norm     1.00\n",
      "[94/200][1799/4361] Loss_D: 0.00144837 (Loss_D_real: 0.00042328 Loss_D_fake: 0.00102509) Loss_G: 0.41423845 Loss_Enh_Dec: -1.87398839\n",
      "| epoch  94 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  2.59 | ppl    13.29 | acc     0.70 | train_ae_norm     1.00\n",
      "[94/200][1899/4361] Loss_D: 0.00701891 (Loss_D_real: 0.00062136 Loss_D_fake: 0.00639756) Loss_G: 0.46560928 Loss_Enh_Dec: -1.97368848\n",
      "| epoch  94 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  2.63 | ppl    13.89 | acc     0.74 | train_ae_norm     1.00\n",
      "[94/200][1999/4361] Loss_D: 0.00183502 (Loss_D_real: 0.00035002 Loss_D_fake: 0.00148501) Loss_G: 0.50037783 Loss_Enh_Dec: -1.65249145\n",
      "| epoch  94 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  2.59 | ppl    13.31 | acc     0.72 | train_ae_norm     1.00\n",
      "[94/200][2099/4361] Loss_D: 0.00180594 (Loss_D_real: 0.00023637 Loss_D_fake: 0.00156957) Loss_G: 0.42345753 Loss_Enh_Dec: -1.77857137\n",
      "| epoch  94 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.91 | loss  2.62 | ppl    13.78 | acc     0.71 | train_ae_norm     1.00\n",
      "[94/200][2199/4361] Loss_D: 0.00075929 (Loss_D_real: 0.00010298 Loss_D_fake: 0.00065631) Loss_G: 0.54617566 Loss_Enh_Dec: -1.66529202\n",
      "| epoch  94 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  2.62 | ppl    13.71 | acc     0.71 | train_ae_norm     1.00\n",
      "[94/200][2299/4361] Loss_D: 0.00074406 (Loss_D_real: 0.00019047 Loss_D_fake: 0.00055359) Loss_G: 0.44995919 Loss_Enh_Dec: -1.57220519\n",
      "| epoch  94 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  2.61 | ppl    13.66 | acc     0.71 | train_ae_norm     1.00\n",
      "[94/200][2399/4361] Loss_D: 0.00298732 (Loss_D_real: 0.00243575 Loss_D_fake: 0.00055157) Loss_G: 0.42821088 Loss_Enh_Dec: -1.92794156\n",
      "| epoch  94 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.63 | ppl    13.92 | acc     0.66 | train_ae_norm     1.00\n",
      "[94/200][2499/4361] Loss_D: 0.00124492 (Loss_D_real: 0.00062646 Loss_D_fake: 0.00061845) Loss_G: 0.37888536 Loss_Enh_Dec: -1.88349271\n",
      "| epoch  94 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  2.66 | ppl    14.29 | acc     0.73 | train_ae_norm     1.00\n",
      "[94/200][2599/4361] Loss_D: 0.00883984 (Loss_D_real: 0.00378220 Loss_D_fake: 0.00505764) Loss_G: 0.38915882 Loss_Enh_Dec: -2.05676413\n",
      "| epoch  94 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.88 | loss  2.63 | ppl    13.88 | acc     0.65 | train_ae_norm     1.00\n",
      "[94/200][2699/4361] Loss_D: 0.00355701 (Loss_D_real: 0.00081113 Loss_D_fake: 0.00274588) Loss_G: 0.38544104 Loss_Enh_Dec: -2.30183268\n",
      "| epoch  94 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.64 | ppl    13.97 | acc     0.68 | train_ae_norm     1.00\n",
      "[94/200][2799/4361] Loss_D: 0.00195161 (Loss_D_real: 0.00075248 Loss_D_fake: 0.00119913) Loss_G: 0.45824528 Loss_Enh_Dec: -2.04229593\n",
      "| epoch  94 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  2.61 | ppl    13.61 | acc     0.68 | train_ae_norm     1.00\n",
      "[94/200][2899/4361] Loss_D: 0.00551476 (Loss_D_real: 0.00010347 Loss_D_fake: 0.00541129) Loss_G: 0.38299385 Loss_Enh_Dec: -1.54042470\n",
      "| epoch  94 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  2.61 | ppl    13.62 | acc     0.66 | train_ae_norm     1.00\n",
      "[94/200][2999/4361] Loss_D: 0.00312290 (Loss_D_real: 0.00258950 Loss_D_fake: 0.00053339) Loss_G: 0.38742682 Loss_Enh_Dec: -1.84584868\n",
      "| epoch  94 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  2.62 | ppl    13.78 | acc     0.70 | train_ae_norm     1.00\n",
      "[94/200][3099/4361] Loss_D: 0.00816584 (Loss_D_real: 0.00652841 Loss_D_fake: 0.00163742) Loss_G: 0.36364889 Loss_Enh_Dec: -2.14782310\n",
      "| epoch  94 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.82 | loss  2.61 | ppl    13.57 | acc     0.68 | train_ae_norm     1.00\n",
      "[94/200][3199/4361] Loss_D: 0.00842589 (Loss_D_real: 0.00510736 Loss_D_fake: 0.00331853) Loss_G: 0.39125106 Loss_Enh_Dec: -1.99120235\n",
      "| epoch  94 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  2.67 | ppl    14.40 | acc     0.72 | train_ae_norm     1.00\n",
      "[94/200][3299/4361] Loss_D: 0.04232374 (Loss_D_real: 0.03980314 Loss_D_fake: 0.00252061) Loss_G: 0.40961152 Loss_Enh_Dec: -2.15259004\n",
      "| epoch  94 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  2.65 | ppl    14.17 | acc     0.69 | train_ae_norm     1.00\n",
      "[94/200][3399/4361] Loss_D: 0.00164728 (Loss_D_real: 0.00038300 Loss_D_fake: 0.00126427) Loss_G: 0.38787398 Loss_Enh_Dec: -2.25350261\n",
      "| epoch  94 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  2.64 | ppl    13.95 | acc     0.68 | train_ae_norm     1.00\n",
      "[94/200][3499/4361] Loss_D: 0.00306811 (Loss_D_real: 0.00257559 Loss_D_fake: 0.00049252) Loss_G: 0.47782931 Loss_Enh_Dec: -1.74079406\n",
      "| epoch  94 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.62 | loss  2.57 | ppl    13.07 | acc     0.68 | train_ae_norm     1.00\n",
      "[94/200][3599/4361] Loss_D: 0.00130016 (Loss_D_real: 0.00029771 Loss_D_fake: 0.00100245) Loss_G: 0.45731124 Loss_Enh_Dec: -2.17392874\n",
      "| epoch  94 |  3600/ 4361 batches | lr 0.000000 | ms/batch 400.60 | loss  2.60 | ppl    13.48 | acc     0.72 | train_ae_norm     1.00\n",
      "[94/200][3699/4361] Loss_D: 0.00332264 (Loss_D_real: 0.00083115 Loss_D_fake: 0.00249149) Loss_G: 0.42695138 Loss_Enh_Dec: -1.92899442\n",
      "| epoch  94 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  2.61 | ppl    13.57 | acc     0.65 | train_ae_norm     1.00\n",
      "[94/200][3799/4361] Loss_D: 0.00327800 (Loss_D_real: 0.00182006 Loss_D_fake: 0.00145794) Loss_G: 0.40296292 Loss_Enh_Dec: -2.09333158\n",
      "| epoch  94 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.61 | ppl    13.60 | acc     0.74 | train_ae_norm     1.00\n",
      "[94/200][3899/4361] Loss_D: 0.01764126 (Loss_D_real: 0.01717433 Loss_D_fake: 0.00046693) Loss_G: 0.49707595 Loss_Enh_Dec: -2.07221389\n",
      "| epoch  94 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.63 | ppl    13.91 | acc     0.64 | train_ae_norm     1.00\n",
      "[94/200][3999/4361] Loss_D: 0.00648146 (Loss_D_real: 0.00399269 Loss_D_fake: 0.00248876) Loss_G: 0.38333747 Loss_Enh_Dec: -1.83010733\n",
      "| epoch  94 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.33 | loss  2.62 | ppl    13.80 | acc     0.70 | train_ae_norm     1.00\n",
      "[94/200][4099/4361] Loss_D: 0.02899602 (Loss_D_real: 0.02828161 Loss_D_fake: 0.00071442) Loss_G: 0.44461641 Loss_Enh_Dec: -1.90487480\n",
      "| epoch  94 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.58 | ppl    13.22 | acc     0.70 | train_ae_norm     1.00\n",
      "[94/200][4199/4361] Loss_D: 0.00178262 (Loss_D_real: 0.00027821 Loss_D_fake: 0.00150442) Loss_G: 0.39768597 Loss_Enh_Dec: -2.13812518\n",
      "| epoch  94 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  2.62 | ppl    13.80 | acc     0.74 | train_ae_norm     1.00\n",
      "[94/200][4299/4361] Loss_D: 0.00256847 (Loss_D_real: 0.00104669 Loss_D_fake: 0.00152178) Loss_G: 0.65119976 Loss_Enh_Dec: -2.05010200\n",
      "| epoch  94 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  2.59 | ppl    13.36 | acc     0.71 | train_ae_norm     1.00\n",
      "| end of epoch  94 | time: 1851.68s | test loss  2.61 | test ppl 13.54 | acc 0.736\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 95 / 200 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.701\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.482\n",
      "  Test Loss: 4.482\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  95 |     0/ 4361 batches | lr 0.000000 | ms/batch 863.93 | loss  0.02 | ppl     1.02 | acc     0.73 | train_ae_norm     1.00\n",
      "[95/200][99/4361] Loss_D: 0.00064673 (Loss_D_real: 0.00020424 Loss_D_fake: 0.00044249) Loss_G: 0.44917297 Loss_Enh_Dec: -1.67519307\n",
      "| epoch  95 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  2.58 | ppl    13.20 | acc     0.66 | train_ae_norm     1.00\n",
      "[95/200][199/4361] Loss_D: 0.00376646 (Loss_D_real: 0.00310163 Loss_D_fake: 0.00066483) Loss_G: 0.47887704 Loss_Enh_Dec: -2.02920794\n",
      "| epoch  95 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  2.61 | ppl    13.57 | acc     0.72 | train_ae_norm     1.00\n",
      "[95/200][299/4361] Loss_D: 0.00171670 (Loss_D_real: 0.00006398 Loss_D_fake: 0.00165272) Loss_G: 0.37363473 Loss_Enh_Dec: -1.78990483\n",
      "| epoch  95 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.80 | loss  2.61 | ppl    13.56 | acc     0.63 | train_ae_norm     1.00\n",
      "[95/200][399/4361] Loss_D: 0.00325788 (Loss_D_real: 0.00060461 Loss_D_fake: 0.00265327) Loss_G: 0.50250959 Loss_Enh_Dec: -1.87582099\n",
      "| epoch  95 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  2.51 | ppl    12.35 | acc     0.71 | train_ae_norm     1.00\n",
      "[95/200][499/4361] Loss_D: 0.00270546 (Loss_D_real: 0.00045770 Loss_D_fake: 0.00224776) Loss_G: 0.37134939 Loss_Enh_Dec: -1.82761252\n",
      "| epoch  95 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.61 | loss  2.57 | ppl    13.08 | acc     0.71 | train_ae_norm     1.00\n",
      "[95/200][599/4361] Loss_D: 0.00568529 (Loss_D_real: 0.00028751 Loss_D_fake: 0.00539778) Loss_G: 0.40644595 Loss_Enh_Dec: -2.07801795\n",
      "| epoch  95 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.45 | loss  2.55 | ppl    12.78 | acc     0.65 | train_ae_norm     1.00\n",
      "[95/200][699/4361] Loss_D: 0.00142198 (Loss_D_real: 0.00028269 Loss_D_fake: 0.00113929) Loss_G: 0.45357552 Loss_Enh_Dec: -2.36337352\n",
      "| epoch  95 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  2.57 | ppl    13.02 | acc     0.71 | train_ae_norm     1.00\n",
      "[95/200][799/4361] Loss_D: 0.00329221 (Loss_D_real: 0.00029734 Loss_D_fake: 0.00299487) Loss_G: 0.45964271 Loss_Enh_Dec: -1.93218887\n",
      "| epoch  95 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  2.56 | ppl    12.99 | acc     0.70 | train_ae_norm     1.00\n",
      "[95/200][899/4361] Loss_D: 0.00683665 (Loss_D_real: 0.00620325 Loss_D_fake: 0.00063340) Loss_G: 0.37845421 Loss_Enh_Dec: -1.75158238\n",
      "| epoch  95 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.80 | loss  2.56 | ppl    12.99 | acc     0.74 | train_ae_norm     1.00\n",
      "[95/200][999/4361] Loss_D: 0.01865753 (Loss_D_real: 0.01673155 Loss_D_fake: 0.00192598) Loss_G: 0.36558414 Loss_Enh_Dec: -1.46542251\n",
      "| epoch  95 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  2.56 | ppl    12.94 | acc     0.74 | train_ae_norm     1.00\n",
      "[95/200][1099/4361] Loss_D: 0.00366590 (Loss_D_real: 0.00156310 Loss_D_fake: 0.00210279) Loss_G: 0.39911017 Loss_Enh_Dec: -1.65781975\n",
      "| epoch  95 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.93 | loss  2.55 | ppl    12.84 | acc     0.67 | train_ae_norm     1.00\n",
      "[95/200][1199/4361] Loss_D: 0.00103898 (Loss_D_real: 0.00071916 Loss_D_fake: 0.00031981) Loss_G: 0.60108775 Loss_Enh_Dec: -1.97405517\n",
      "| epoch  95 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.51 | loss  2.57 | ppl    13.07 | acc     0.73 | train_ae_norm     1.00\n",
      "[95/200][1299/4361] Loss_D: 0.00059408 (Loss_D_real: 0.00014656 Loss_D_fake: 0.00044752) Loss_G: 0.40256879 Loss_Enh_Dec: -1.55613661\n",
      "| epoch  95 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.34 | loss  2.59 | ppl    13.31 | acc     0.70 | train_ae_norm     1.00\n",
      "[95/200][1399/4361] Loss_D: 0.00302548 (Loss_D_real: 0.00222874 Loss_D_fake: 0.00079674) Loss_G: 0.39833409 Loss_Enh_Dec: -1.71694529\n",
      "| epoch  95 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.68 | loss  2.59 | ppl    13.33 | acc     0.67 | train_ae_norm     1.00\n",
      "[95/200][1499/4361] Loss_D: 0.00312255 (Loss_D_real: 0.00181527 Loss_D_fake: 0.00130728) Loss_G: 0.38724515 Loss_Enh_Dec: -1.80084419\n",
      "| epoch  95 |  1500/ 4361 batches | lr 0.000000 | ms/batch 400.64 | loss  2.65 | ppl    14.10 | acc     0.66 | train_ae_norm     1.00\n",
      "[95/200][1599/4361] Loss_D: 0.00232541 (Loss_D_real: 0.00134152 Loss_D_fake: 0.00098389) Loss_G: 0.42812473 Loss_Enh_Dec: -1.91439784\n",
      "| epoch  95 |  1600/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.62 | ppl    13.74 | acc     0.72 | train_ae_norm     1.00\n",
      "[95/200][1699/4361] Loss_D: 0.00218019 (Loss_D_real: 0.00009194 Loss_D_fake: 0.00208824) Loss_G: 0.43494049 Loss_Enh_Dec: -1.97602904\n",
      "| epoch  95 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  2.58 | ppl    13.14 | acc     0.68 | train_ae_norm     1.00\n",
      "[95/200][1799/4361] Loss_D: 0.00214159 (Loss_D_real: 0.00128997 Loss_D_fake: 0.00085162) Loss_G: 0.37870321 Loss_Enh_Dec: -1.78766823\n",
      "| epoch  95 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  2.56 | ppl    12.92 | acc     0.69 | train_ae_norm     1.00\n",
      "[95/200][1899/4361] Loss_D: 0.00566972 (Loss_D_real: 0.00386267 Loss_D_fake: 0.00180705) Loss_G: 0.37283525 Loss_Enh_Dec: -1.79747808\n",
      "| epoch  95 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.57 | loss  2.60 | ppl    13.42 | acc     0.72 | train_ae_norm     1.00\n",
      "[95/200][1999/4361] Loss_D: 0.00109738 (Loss_D_real: 0.00069488 Loss_D_fake: 0.00040251) Loss_G: 0.48150015 Loss_Enh_Dec: -1.67391860\n",
      "| epoch  95 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.68 | loss  2.57 | ppl    13.05 | acc     0.70 | train_ae_norm     1.00\n",
      "[95/200][2099/4361] Loss_D: 0.00518200 (Loss_D_real: 0.00457215 Loss_D_fake: 0.00060985) Loss_G: 0.37592456 Loss_Enh_Dec: -1.28534305\n",
      "| epoch  95 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.11 | loss  2.59 | ppl    13.27 | acc     0.70 | train_ae_norm     1.00\n",
      "[95/200][2199/4361] Loss_D: 0.00865776 (Loss_D_real: 0.00676428 Loss_D_fake: 0.00189347) Loss_G: 0.43550545 Loss_Enh_Dec: -1.61034667\n",
      "| epoch  95 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.20 | loss  2.59 | ppl    13.31 | acc     0.69 | train_ae_norm     1.00\n",
      "[95/200][2299/4361] Loss_D: 0.00523781 (Loss_D_real: 0.00459336 Loss_D_fake: 0.00064445) Loss_G: 0.46203572 Loss_Enh_Dec: -1.88104403\n",
      "| epoch  95 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.25 | loss  2.56 | ppl    12.97 | acc     0.72 | train_ae_norm     1.00\n",
      "[95/200][2399/4361] Loss_D: 0.00125578 (Loss_D_real: 0.00010865 Loss_D_fake: 0.00114713) Loss_G: 0.41734096 Loss_Enh_Dec: -1.86815834\n",
      "| epoch  95 |  2400/ 4361 batches | lr 0.000000 | ms/batch 400.31 | loss  2.57 | ppl    13.04 | acc     0.67 | train_ae_norm     1.00\n",
      "[95/200][2499/4361] Loss_D: 0.00838177 (Loss_D_real: 0.00620759 Loss_D_fake: 0.00217418) Loss_G: 0.34352928 Loss_Enh_Dec: -2.07465029\n",
      "| epoch  95 |  2500/ 4361 batches | lr 0.000000 | ms/batch 400.73 | loss  2.60 | ppl    13.53 | acc     0.72 | train_ae_norm     1.00\n",
      "[95/200][2599/4361] Loss_D: 0.00114863 (Loss_D_real: 0.00028385 Loss_D_fake: 0.00086478) Loss_G: 0.40579319 Loss_Enh_Dec: -1.81673968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  95 |  2600/ 4361 batches | lr 0.000000 | ms/batch 399.73 | loss  2.60 | ppl    13.42 | acc     0.68 | train_ae_norm     1.00\n",
      "[95/200][2699/4361] Loss_D: 0.00122314 (Loss_D_real: 0.00102283 Loss_D_fake: 0.00020031) Loss_G: 0.56636399 Loss_Enh_Dec: -1.50110006\n",
      "| epoch  95 |  2700/ 4361 batches | lr 0.000000 | ms/batch 400.42 | loss  2.59 | ppl    13.28 | acc     0.69 | train_ae_norm     1.00\n",
      "[95/200][2799/4361] Loss_D: 0.00257567 (Loss_D_real: 0.00109648 Loss_D_fake: 0.00147919) Loss_G: 0.39942202 Loss_Enh_Dec: -1.61154866\n",
      "| epoch  95 |  2800/ 4361 batches | lr 0.000000 | ms/batch 399.99 | loss  2.56 | ppl    12.94 | acc     0.68 | train_ae_norm     1.00\n",
      "[95/200][2899/4361] Loss_D: 0.00330744 (Loss_D_real: 0.00112226 Loss_D_fake: 0.00218518) Loss_G: 0.59268636 Loss_Enh_Dec: -1.91775167\n",
      "| epoch  95 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.44 | loss  2.56 | ppl    12.95 | acc     0.69 | train_ae_norm     1.00\n",
      "[95/200][2999/4361] Loss_D: 0.00214040 (Loss_D_real: 0.00169451 Loss_D_fake: 0.00044590) Loss_G: 0.51879781 Loss_Enh_Dec: -1.44076836\n",
      "| epoch  95 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.58 | ppl    13.23 | acc     0.70 | train_ae_norm     1.00\n",
      "[95/200][3099/4361] Loss_D: 0.00242006 (Loss_D_real: 0.00149836 Loss_D_fake: 0.00092170) Loss_G: 0.39354187 Loss_Enh_Dec: -1.65409088\n",
      "| epoch  95 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.76 | loss  2.58 | ppl    13.18 | acc     0.69 | train_ae_norm     1.00\n",
      "[95/200][3199/4361] Loss_D: 0.00072576 (Loss_D_real: 0.00005743 Loss_D_fake: 0.00066834) Loss_G: 0.43270627 Loss_Enh_Dec: -1.82919693\n",
      "| epoch  95 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.50 | loss  2.61 | ppl    13.58 | acc     0.71 | train_ae_norm     1.00\n",
      "[95/200][3299/4361] Loss_D: 0.00148803 (Loss_D_real: 0.00004443 Loss_D_fake: 0.00144359) Loss_G: 0.40230224 Loss_Enh_Dec: -1.63745618\n",
      "| epoch  95 |  3300/ 4361 batches | lr 0.000000 | ms/batch 400.80 | loss  2.59 | ppl    13.39 | acc     0.71 | train_ae_norm     1.00\n",
      "[95/200][3399/4361] Loss_D: 0.00409371 (Loss_D_real: 0.00197205 Loss_D_fake: 0.00212166) Loss_G: 0.41240689 Loss_Enh_Dec: -1.47553587\n",
      "| epoch  95 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.56 | ppl    13.00 | acc     0.70 | train_ae_norm     1.00\n",
      "[95/200][3499/4361] Loss_D: 0.00190138 (Loss_D_real: 0.00029621 Loss_D_fake: 0.00160518) Loss_G: 0.57347006 Loss_Enh_Dec: -1.48021579\n",
      "| epoch  95 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.08 | loss  2.53 | ppl    12.50 | acc     0.69 | train_ae_norm     1.00\n",
      "[95/200][3599/4361] Loss_D: 0.00806240 (Loss_D_real: 0.00543151 Loss_D_fake: 0.00263089) Loss_G: 0.60207015 Loss_Enh_Dec: -1.13353026\n",
      "| epoch  95 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  2.55 | ppl    12.82 | acc     0.71 | train_ae_norm     1.00\n",
      "[95/200][3699/4361] Loss_D: 0.00391104 (Loss_D_real: 0.00025581 Loss_D_fake: 0.00365523) Loss_G: 0.40895262 Loss_Enh_Dec: -1.60719073\n",
      "| epoch  95 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  2.58 | ppl    13.25 | acc     0.68 | train_ae_norm     1.00\n",
      "[95/200][3799/4361] Loss_D: 0.01241378 (Loss_D_real: 0.01084353 Loss_D_fake: 0.00157024) Loss_G: 0.41572505 Loss_Enh_Dec: -1.41546953\n",
      "| epoch  95 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  2.59 | ppl    13.30 | acc     0.74 | train_ae_norm     1.00\n",
      "[95/200][3899/4361] Loss_D: 0.00300155 (Loss_D_real: 0.00010162 Loss_D_fake: 0.00289994) Loss_G: 0.38096824 Loss_Enh_Dec: -1.29360914\n",
      "| epoch  95 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.68 | loss  2.59 | ppl    13.33 | acc     0.67 | train_ae_norm     1.00\n",
      "[95/200][3999/4361] Loss_D: 0.00302758 (Loss_D_real: 0.00190267 Loss_D_fake: 0.00112492) Loss_G: 0.44313121 Loss_Enh_Dec: -1.50710559\n",
      "| epoch  95 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  2.57 | ppl    13.09 | acc     0.71 | train_ae_norm     1.00\n",
      "[95/200][4099/4361] Loss_D: 0.00785829 (Loss_D_real: 0.00628222 Loss_D_fake: 0.00157607) Loss_G: 0.42985994 Loss_Enh_Dec: -2.01834178\n",
      "| epoch  95 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  2.56 | ppl    12.87 | acc     0.69 | train_ae_norm     1.00\n",
      "[95/200][4199/4361] Loss_D: 0.00230690 (Loss_D_real: 0.00049941 Loss_D_fake: 0.00180749) Loss_G: 0.34386349 Loss_Enh_Dec: -1.76775253\n",
      "| epoch  95 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  2.59 | ppl    13.37 | acc     0.74 | train_ae_norm     1.00\n",
      "[95/200][4299/4361] Loss_D: 0.01103927 (Loss_D_real: 0.00913410 Loss_D_fake: 0.00190517) Loss_G: 0.45147187 Loss_Enh_Dec: -1.64715946\n",
      "| epoch  95 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.55 | ppl    12.76 | acc     0.71 | train_ae_norm     1.00\n",
      "| end of epoch  95 | time: 1850.25s | test loss  2.56 | test ppl 12.89 | acc 0.740\n",
      "bleu_self:  [1.59375000e-01 5.77250801e-09 2.19308279e-11 8.08956606e-11\n",
      " 3.52440021e-10]\n",
      "bleu_test:  [7.87500000e-01 1.25000013e-01 1.25003888e-06 7.03023486e-07\n",
      " 4.98044045e-07]\n",
      "bleu_self: [0.15937500,0.00000001,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.78750000,0.12500001,0.00000125,0.00000070,0.00000050]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 96 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.699\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.485\n",
      "  Test Loss: 4.542\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  96 |     0/ 4361 batches | lr 0.000000 | ms/batch 863.75 | loss  0.02 | ppl     1.02 | acc     0.72 | train_ae_norm     1.00\n",
      "[96/200][99/4361] Loss_D: 0.50409955 (Loss_D_real: 0.00053364 Loss_D_fake: 0.50356591) Loss_G: 0.58722895 Loss_Enh_Dec: -1.25157690\n",
      "| epoch  96 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  2.55 | ppl    12.79 | acc     0.66 | train_ae_norm     1.00\n",
      "[96/200][199/4361] Loss_D: 0.00272746 (Loss_D_real: 0.00101983 Loss_D_fake: 0.00170764) Loss_G: 0.46283856 Loss_Enh_Dec: -1.46396732\n",
      "| epoch  96 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  2.57 | ppl    13.09 | acc     0.71 | train_ae_norm     1.00\n",
      "[96/200][299/4361] Loss_D: 0.00053740 (Loss_D_real: 0.00016706 Loss_D_fake: 0.00037034) Loss_G: 0.46441451 Loss_Enh_Dec: -0.74789208\n",
      "| epoch  96 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  2.57 | ppl    13.08 | acc     0.68 | train_ae_norm     1.00\n",
      "[96/200][399/4361] Loss_D: 0.00793351 (Loss_D_real: 0.00699678 Loss_D_fake: 0.00093673) Loss_G: 0.54752517 Loss_Enh_Dec: -1.46110833\n",
      "| epoch  96 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  2.48 | ppl    11.97 | acc     0.71 | train_ae_norm     1.00\n",
      "[96/200][499/4361] Loss_D: 0.00129362 (Loss_D_real: 0.00076915 Loss_D_fake: 0.00052448) Loss_G: 0.47812232 Loss_Enh_Dec: -1.24849594\n",
      "| epoch  96 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  2.56 | ppl    12.90 | acc     0.72 | train_ae_norm     1.00\n",
      "[96/200][599/4361] Loss_D: 0.01206349 (Loss_D_real: 0.01028231 Loss_D_fake: 0.00178118) Loss_G: 0.44560561 Loss_Enh_Dec: -1.19669902\n",
      "| epoch  96 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  2.52 | ppl    12.49 | acc     0.67 | train_ae_norm     1.00\n",
      "[96/200][699/4361] Loss_D: 0.00059480 (Loss_D_real: 0.00008883 Loss_D_fake: 0.00050598) Loss_G: 0.43233758 Loss_Enh_Dec: -1.81846619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  96 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  2.56 | ppl    12.94 | acc     0.70 | train_ae_norm     1.00\n",
      "[96/200][799/4361] Loss_D: 0.01364515 (Loss_D_real: 0.00039431 Loss_D_fake: 0.01325084) Loss_G: 0.39060709 Loss_Enh_Dec: -1.44902575\n",
      "| epoch  96 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  2.54 | ppl    12.62 | acc     0.70 | train_ae_norm     1.00\n",
      "[96/200][899/4361] Loss_D: 0.00360384 (Loss_D_real: 0.00014755 Loss_D_fake: 0.00345628) Loss_G: 0.54663020 Loss_Enh_Dec: -0.64996177\n",
      "| epoch  96 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  2.54 | ppl    12.72 | acc     0.74 | train_ae_norm     1.00\n",
      "[96/200][999/4361] Loss_D: 0.00081681 (Loss_D_real: 0.00035478 Loss_D_fake: 0.00046203) Loss_G: 0.49161202 Loss_Enh_Dec: -1.29678023\n",
      "| epoch  96 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.52 | ppl    12.42 | acc     0.74 | train_ae_norm     1.00\n",
      "[96/200][1099/4361] Loss_D: 0.00099549 (Loss_D_real: 0.00052106 Loss_D_fake: 0.00047443) Loss_G: 0.41094708 Loss_Enh_Dec: -1.43122363\n",
      "| epoch  96 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  2.53 | ppl    12.54 | acc     0.70 | train_ae_norm     1.00\n",
      "[96/200][1199/4361] Loss_D: 0.00279286 (Loss_D_real: 0.00022700 Loss_D_fake: 0.00256585) Loss_G: 0.39726883 Loss_Enh_Dec: -1.99940038\n",
      "| epoch  96 |  1200/ 4361 batches | lr 0.000000 | ms/batch 402.65 | loss  2.53 | ppl    12.59 | acc     0.71 | train_ae_norm     1.00\n",
      "[96/200][1299/4361] Loss_D: 0.00430051 (Loss_D_real: 0.00067270 Loss_D_fake: 0.00362781) Loss_G: 0.37473127 Loss_Enh_Dec: -1.42840981\n",
      "| epoch  96 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  2.56 | ppl    12.89 | acc     0.71 | train_ae_norm     1.00\n",
      "[96/200][1399/4361] Loss_D: 0.01950153 (Loss_D_real: 0.01327984 Loss_D_fake: 0.00622169) Loss_G: 0.42682835 Loss_Enh_Dec: -1.80518436\n",
      "| epoch  96 |  1400/ 4361 batches | lr 0.000000 | ms/batch 402.51 | loss  2.55 | ppl    12.86 | acc     0.66 | train_ae_norm     1.00\n",
      "[96/200][1499/4361] Loss_D: 0.00231576 (Loss_D_real: 0.00028816 Loss_D_fake: 0.00202760) Loss_G: 0.38852268 Loss_Enh_Dec: -1.97571266\n",
      "| epoch  96 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.62 | ppl    13.69 | acc     0.68 | train_ae_norm     1.00\n",
      "[96/200][1599/4361] Loss_D: 0.00154160 (Loss_D_real: 0.00007995 Loss_D_fake: 0.00146165) Loss_G: 0.47936621 Loss_Enh_Dec: -1.81883907\n",
      "| epoch  96 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  2.60 | ppl    13.51 | acc     0.69 | train_ae_norm     1.00\n",
      "[96/200][1699/4361] Loss_D: 0.00202541 (Loss_D_real: 0.00086677 Loss_D_fake: 0.00115864) Loss_G: 0.46336767 Loss_Enh_Dec: -2.15443993\n",
      "| epoch  96 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  2.56 | ppl    12.93 | acc     0.70 | train_ae_norm     1.00\n",
      "[96/200][1799/4361] Loss_D: 0.00231767 (Loss_D_real: 0.00041503 Loss_D_fake: 0.00190264) Loss_G: 0.44404292 Loss_Enh_Dec: -1.56280994\n",
      "| epoch  96 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  2.55 | ppl    12.76 | acc     0.71 | train_ae_norm     1.00\n",
      "[96/200][1899/4361] Loss_D: 0.06316150 (Loss_D_real: 0.01765244 Loss_D_fake: 0.04550906) Loss_G: 0.77207947 Loss_Enh_Dec: -1.87819695\n",
      "| epoch  96 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  2.59 | ppl    13.35 | acc     0.74 | train_ae_norm     1.00\n",
      "[96/200][1999/4361] Loss_D: 0.00073427 (Loss_D_real: 0.00009826 Loss_D_fake: 0.00063601) Loss_G: 0.42069507 Loss_Enh_Dec: -1.83227313\n",
      "| epoch  96 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  2.55 | ppl    12.81 | acc     0.72 | train_ae_norm     1.00\n",
      "[96/200][2099/4361] Loss_D: 0.00272650 (Loss_D_real: 0.00060103 Loss_D_fake: 0.00212547) Loss_G: 0.34850159 Loss_Enh_Dec: -0.72738117\n",
      "| epoch  96 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  2.58 | ppl    13.17 | acc     0.72 | train_ae_norm     1.00\n",
      "[96/200][2199/4361] Loss_D: 0.00239624 (Loss_D_real: 0.00127753 Loss_D_fake: 0.00111871) Loss_G: 0.41271600 Loss_Enh_Dec: -1.60486031\n",
      "| epoch  96 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.83 | loss  2.57 | ppl    13.04 | acc     0.70 | train_ae_norm     1.00\n",
      "[96/200][2299/4361] Loss_D: 0.10679679 (Loss_D_real: 0.00720757 Loss_D_fake: 0.09958921) Loss_G: 0.76386184 Loss_Enh_Dec: -1.26159704\n",
      "| epoch  96 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  2.55 | ppl    12.84 | acc     0.72 | train_ae_norm     1.00\n",
      "[96/200][2399/4361] Loss_D: 0.00165853 (Loss_D_real: 0.00030662 Loss_D_fake: 0.00135191) Loss_G: 0.37529394 Loss_Enh_Dec: -1.92594564\n",
      "| epoch  96 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.57 | ppl    13.05 | acc     0.67 | train_ae_norm     1.00\n",
      "[96/200][2499/4361] Loss_D: 0.00392698 (Loss_D_real: 0.00241574 Loss_D_fake: 0.00151124) Loss_G: 0.34674302 Loss_Enh_Dec: -1.38232458\n",
      "| epoch  96 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  2.60 | ppl    13.42 | acc     0.72 | train_ae_norm     1.00\n",
      "[96/200][2599/4361] Loss_D: 0.01129695 (Loss_D_real: 0.00695850 Loss_D_fake: 0.00433845) Loss_G: 0.38618028 Loss_Enh_Dec: -1.73060501\n",
      "| epoch  96 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  2.56 | ppl    12.98 | acc     0.67 | train_ae_norm     1.00\n",
      "[96/200][2699/4361] Loss_D: 0.00361979 (Loss_D_real: 0.00135321 Loss_D_fake: 0.00226658) Loss_G: 0.80071908 Loss_Enh_Dec: -0.76645291\n",
      "| epoch  96 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  2.57 | ppl    13.04 | acc     0.67 | train_ae_norm     1.00\n",
      "[96/200][2799/4361] Loss_D: 0.00458277 (Loss_D_real: 0.00156712 Loss_D_fake: 0.00301565) Loss_G: 0.37528482 Loss_Enh_Dec: -0.86113614\n",
      "| epoch  96 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  2.53 | ppl    12.57 | acc     0.67 | train_ae_norm     1.00\n",
      "[96/200][2899/4361] Loss_D: 0.00273448 (Loss_D_real: 0.00078165 Loss_D_fake: 0.00195283) Loss_G: 0.38255969 Loss_Enh_Dec: -1.06955647\n",
      "| epoch  96 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  2.55 | ppl    12.78 | acc     0.70 | train_ae_norm     1.00\n",
      "[96/200][2999/4361] Loss_D: 0.00512848 (Loss_D_real: 0.00355582 Loss_D_fake: 0.00157266) Loss_G: 0.32751361 Loss_Enh_Dec: -1.21925724\n",
      "| epoch  96 |  3000/ 4361 batches | lr 0.000000 | ms/batch 402.05 | loss  2.57 | ppl    13.08 | acc     0.70 | train_ae_norm     1.00\n",
      "[96/200][3099/4361] Loss_D: 0.00390057 (Loss_D_real: 0.00071317 Loss_D_fake: 0.00318740) Loss_G: 0.36190507 Loss_Enh_Dec: -1.38537860\n",
      "| epoch  96 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  2.56 | ppl    12.88 | acc     0.71 | train_ae_norm     1.00\n",
      "[96/200][3199/4361] Loss_D: 0.03308639 (Loss_D_real: 0.02328781 Loss_D_fake: 0.00979857) Loss_G: 0.60836583 Loss_Enh_Dec: -1.27784085\n",
      "| epoch  96 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.79 | loss  2.60 | ppl    13.49 | acc     0.72 | train_ae_norm     1.00\n",
      "[96/200][3299/4361] Loss_D: 0.00850343 (Loss_D_real: 0.00010386 Loss_D_fake: 0.00839957) Loss_G: 0.43230662 Loss_Enh_Dec: -1.63210297\n",
      "| epoch  96 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  2.60 | ppl    13.48 | acc     0.72 | train_ae_norm     1.00\n",
      "[96/200][3399/4361] Loss_D: 0.00214282 (Loss_D_real: 0.00019522 Loss_D_fake: 0.00194760) Loss_G: 0.34423786 Loss_Enh_Dec: -1.33256209\n",
      "| epoch  96 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.58 | ppl    13.19 | acc     0.66 | train_ae_norm     1.00\n",
      "[96/200][3499/4361] Loss_D: 0.00978619 (Loss_D_real: 0.00800139 Loss_D_fake: 0.00178481) Loss_G: 0.43848997 Loss_Enh_Dec: -1.76330113\n",
      "| epoch  96 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  2.54 | ppl    12.63 | acc     0.68 | train_ae_norm     1.00\n",
      "[96/200][3599/4361] Loss_D: 0.02553306 (Loss_D_real: 0.02507496 Loss_D_fake: 0.00045810) Loss_G: 0.45118514 Loss_Enh_Dec: -2.12974715\n",
      "| epoch  96 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  2.56 | ppl    12.94 | acc     0.73 | train_ae_norm     1.00\n",
      "[96/200][3699/4361] Loss_D: 0.00295855 (Loss_D_real: 0.00151475 Loss_D_fake: 0.00144381) Loss_G: 0.38744125 Loss_Enh_Dec: -2.02029109\n",
      "| epoch  96 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  2.57 | ppl    13.11 | acc     0.68 | train_ae_norm     1.00\n",
      "[96/200][3799/4361] Loss_D: 0.00568898 (Loss_D_real: 0.00019333 Loss_D_fake: 0.00549565) Loss_G: 0.38633838 Loss_Enh_Dec: -1.77031076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  96 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  2.59 | ppl    13.32 | acc     0.75 | train_ae_norm     1.00\n",
      "[96/200][3899/4361] Loss_D: 0.00112086 (Loss_D_real: 0.00008225 Loss_D_fake: 0.00103861) Loss_G: 0.36581856 Loss_Enh_Dec: -1.55840409\n",
      "| epoch  96 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  2.60 | ppl    13.47 | acc     0.68 | train_ae_norm     1.00\n",
      "[96/200][3999/4361] Loss_D: 0.01730241 (Loss_D_real: 0.01376008 Loss_D_fake: 0.00354233) Loss_G: 0.35498413 Loss_Enh_Dec: -1.82192695\n",
      "| epoch  96 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  2.59 | ppl    13.37 | acc     0.70 | train_ae_norm     1.00\n",
      "[96/200][4099/4361] Loss_D: 0.02119307 (Loss_D_real: 0.00028908 Loss_D_fake: 0.02090399) Loss_G: 0.35408983 Loss_Enh_Dec: -2.30016780\n",
      "| epoch  96 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.55 | ppl    12.82 | acc     0.70 | train_ae_norm     1.00\n",
      "[96/200][4199/4361] Loss_D: 0.02544983 (Loss_D_real: 0.02449324 Loss_D_fake: 0.00095660) Loss_G: 0.67126578 Loss_Enh_Dec: -1.75518608\n",
      "| epoch  96 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  2.60 | ppl    13.41 | acc     0.73 | train_ae_norm     1.00\n",
      "[96/200][4299/4361] Loss_D: 0.09951640 (Loss_D_real: 0.01017708 Loss_D_fake: 0.08933932) Loss_G: 0.41018650 Loss_Enh_Dec: -1.58617735\n",
      "| epoch  96 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  2.57 | ppl    13.12 | acc     0.72 | train_ae_norm     1.00\n",
      "| end of epoch  96 | time: 1853.35s | test loss  2.58 | test ppl 13.18 | acc 0.738\n",
      "bleu_self:  [2.64460439e-01 8.51613100e-02 7.65120233e-07 4.92656283e-09\n",
      " 3.77838947e-09]\n",
      "bleu_test:  [7.86160714e-01 8.58608510e-02 6.70612836e-07 2.41301924e-08\n",
      " 3.14580563e-08]\n",
      "bleu_self: [0.26446044,0.08516131,0.00000077,0.00000000,0.00000000]\n",
      "bleu_test: [0.78616071,0.08586085,0.00000067,0.00000002,0.00000003]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 97 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.699\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.492\n",
      "  Test Loss: 4.481\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  97 |     0/ 4361 batches | lr 0.000000 | ms/batch 865.87 | loss  0.02 | ppl     1.02 | acc     0.71 | train_ae_norm     1.00\n",
      "[97/200][99/4361] Loss_D: 0.01956722 (Loss_D_real: 0.00104128 Loss_D_fake: 0.01852594) Loss_G: 0.24202044 Loss_Enh_Dec: -1.00853348\n",
      "| epoch  97 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  2.56 | ppl    12.91 | acc     0.67 | train_ae_norm     1.00\n",
      "[97/200][199/4361] Loss_D: 0.06420080 (Loss_D_real: 0.05293625 Loss_D_fake: 0.01126455) Loss_G: 0.23945761 Loss_Enh_Dec: -0.98022443\n",
      "| epoch  97 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  2.59 | ppl    13.37 | acc     0.70 | train_ae_norm     1.00\n",
      "[97/200][299/4361] Loss_D: 0.01405608 (Loss_D_real: 0.00551062 Loss_D_fake: 0.00854546) Loss_G: 0.25073975 Loss_Enh_Dec: -1.31091988\n",
      "| epoch  97 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  2.59 | ppl    13.33 | acc     0.65 | train_ae_norm     1.00\n",
      "[97/200][399/4361] Loss_D: 0.00771198 (Loss_D_real: 0.00169518 Loss_D_fake: 0.00601680) Loss_G: 0.26702192 Loss_Enh_Dec: -1.38178742\n",
      "| epoch  97 |   400/ 4361 batches | lr 0.000000 | ms/batch 402.13 | loss  2.50 | ppl    12.14 | acc     0.72 | train_ae_norm     1.00\n",
      "[97/200][499/4361] Loss_D: 0.00837468 (Loss_D_real: 0.00390021 Loss_D_fake: 0.00447447) Loss_G: 0.27520919 Loss_Enh_Dec: -0.95027620\n",
      "| epoch  97 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  2.58 | ppl    13.16 | acc     0.72 | train_ae_norm     1.00\n",
      "[97/200][599/4361] Loss_D: 0.00677164 (Loss_D_real: 0.00073932 Loss_D_fake: 0.00603232) Loss_G: 0.26729313 Loss_Enh_Dec: -1.07827032\n",
      "| epoch  97 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.73 | loss  2.51 | ppl    12.27 | acc     0.68 | train_ae_norm     1.00\n",
      "[97/200][699/4361] Loss_D: 0.00726815 (Loss_D_real: 0.00147028 Loss_D_fake: 0.00579787) Loss_G: 0.27212664 Loss_Enh_Dec: -1.03208518\n",
      "| epoch  97 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.56 | ppl    12.88 | acc     0.70 | train_ae_norm     1.00\n",
      "[97/200][799/4361] Loss_D: 0.00947105 (Loss_D_real: 0.00073380 Loss_D_fake: 0.00873725) Loss_G: 0.25534472 Loss_Enh_Dec: -1.30276525\n",
      "| epoch  97 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  2.55 | ppl    12.86 | acc     0.72 | train_ae_norm     1.00\n",
      "[97/200][899/4361] Loss_D: 0.00651818 (Loss_D_real: 0.00159437 Loss_D_fake: 0.00492380) Loss_G: 0.28522748 Loss_Enh_Dec: -1.42964101\n",
      "| epoch  97 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  2.56 | ppl    12.89 | acc     0.72 | train_ae_norm     1.00\n",
      "[97/200][999/4361] Loss_D: 0.01535334 (Loss_D_real: 0.00528780 Loss_D_fake: 0.01006554) Loss_G: 0.26956305 Loss_Enh_Dec: -1.61497104\n",
      "| epoch  97 |  1000/ 4361 batches | lr 0.000000 | ms/batch 402.29 | loss  2.54 | ppl    12.62 | acc     0.74 | train_ae_norm     1.00\n",
      "[97/200][1099/4361] Loss_D: 0.00752398 (Loss_D_real: 0.00321191 Loss_D_fake: 0.00431207) Loss_G: 0.27956772 Loss_Enh_Dec: -1.25052702\n",
      "| epoch  97 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.81 | loss  2.52 | ppl    12.45 | acc     0.68 | train_ae_norm     1.00\n",
      "[97/200][1199/4361] Loss_D: 0.00822986 (Loss_D_real: 0.00188108 Loss_D_fake: 0.00634879) Loss_G: 0.27228999 Loss_Enh_Dec: -1.48383415\n",
      "| epoch  97 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  2.53 | ppl    12.53 | acc     0.73 | train_ae_norm     1.00\n",
      "[97/200][1299/4361] Loss_D: 0.00626477 (Loss_D_real: 0.00268963 Loss_D_fake: 0.00357514) Loss_G: 0.28067333 Loss_Enh_Dec: -1.73033845\n",
      "| epoch  97 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  2.56 | ppl    12.90 | acc     0.69 | train_ae_norm     1.00\n",
      "[97/200][1399/4361] Loss_D: 0.00423106 (Loss_D_real: 0.00064164 Loss_D_fake: 0.00358941) Loss_G: 0.30174428 Loss_Enh_Dec: -1.37607193\n",
      "| epoch  97 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  2.55 | ppl    12.81 | acc     0.66 | train_ae_norm     1.00\n",
      "[97/200][1499/4361] Loss_D: 0.00560377 (Loss_D_real: 0.00064766 Loss_D_fake: 0.00495610) Loss_G: 0.27980793 Loss_Enh_Dec: -2.02273345\n",
      "| epoch  97 |  1500/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  2.59 | ppl    13.30 | acc     0.69 | train_ae_norm     1.00\n",
      "[97/200][1599/4361] Loss_D: 0.01653946 (Loss_D_real: 0.01267874 Loss_D_fake: 0.00386072) Loss_G: 0.28631830 Loss_Enh_Dec: -1.93195117\n",
      "| epoch  97 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  2.56 | ppl    12.91 | acc     0.70 | train_ae_norm     1.00\n",
      "[97/200][1699/4361] Loss_D: 0.00436114 (Loss_D_real: 0.00068091 Loss_D_fake: 0.00368022) Loss_G: 0.29254508 Loss_Enh_Dec: -2.26505256\n",
      "| epoch  97 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  2.52 | ppl    12.43 | acc     0.69 | train_ae_norm     1.00\n",
      "[97/200][1799/4361] Loss_D: 0.00493269 (Loss_D_real: 0.00120063 Loss_D_fake: 0.00373206) Loss_G: 0.28270826 Loss_Enh_Dec: -2.12746096\n",
      "| epoch  97 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  2.52 | ppl    12.47 | acc     0.68 | train_ae_norm     1.00\n",
      "[97/200][1899/4361] Loss_D: 0.00472505 (Loss_D_real: 0.00174476 Loss_D_fake: 0.00298029) Loss_G: 0.29872152 Loss_Enh_Dec: -2.01093698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  97 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.57 | ppl    13.07 | acc     0.73 | train_ae_norm     1.00\n",
      "[97/200][1999/4361] Loss_D: 0.01860021 (Loss_D_real: 0.01448065 Loss_D_fake: 0.00411957) Loss_G: 0.29027259 Loss_Enh_Dec: -1.90706730\n",
      "| epoch  97 |  2000/ 4361 batches | lr 0.000000 | ms/batch 402.28 | loss  2.54 | ppl    12.62 | acc     0.72 | train_ae_norm     1.00\n",
      "[97/200][2099/4361] Loss_D: 0.02464624 (Loss_D_real: 0.02174893 Loss_D_fake: 0.00289731) Loss_G: 0.31210738 Loss_Enh_Dec: -2.23331714\n",
      "| epoch  97 |  2100/ 4361 batches | lr 0.000000 | ms/batch 402.04 | loss  2.54 | ppl    12.70 | acc     0.74 | train_ae_norm     1.00\n",
      "[97/200][2199/4361] Loss_D: 0.00304904 (Loss_D_real: 0.00017583 Loss_D_fake: 0.00287321) Loss_G: 0.31927365 Loss_Enh_Dec: -1.83857727\n",
      "| epoch  97 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.54 | ppl    12.64 | acc     0.70 | train_ae_norm     1.00\n",
      "[97/200][2299/4361] Loss_D: 0.00192326 (Loss_D_real: 0.00024146 Loss_D_fake: 0.00168180) Loss_G: 0.33168140 Loss_Enh_Dec: -2.29838037\n",
      "| epoch  97 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  2.51 | ppl    12.33 | acc     0.71 | train_ae_norm     1.00\n",
      "[97/200][2399/4361] Loss_D: 0.00425611 (Loss_D_real: 0.00172879 Loss_D_fake: 0.00252732) Loss_G: 0.31676108 Loss_Enh_Dec: -2.02571869\n",
      "| epoch  97 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  2.54 | ppl    12.68 | acc     0.65 | train_ae_norm     1.00\n",
      "[97/200][2499/4361] Loss_D: 0.00489202 (Loss_D_real: 0.00153476 Loss_D_fake: 0.00335725) Loss_G: 0.33144823 Loss_Enh_Dec: -1.83920383\n",
      "| epoch  97 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  2.57 | ppl    13.06 | acc     0.69 | train_ae_norm     1.00\n",
      "[97/200][2599/4361] Loss_D: 0.00232434 (Loss_D_real: 0.00036102 Loss_D_fake: 0.00196333) Loss_G: 0.31449327 Loss_Enh_Dec: -2.16684079\n",
      "| epoch  97 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.55 | ppl    12.83 | acc     0.67 | train_ae_norm     1.00\n",
      "[97/200][2699/4361] Loss_D: 0.00619950 (Loss_D_real: 0.00277162 Loss_D_fake: 0.00342789) Loss_G: 0.30516300 Loss_Enh_Dec: -2.24375582\n",
      "| epoch  97 |  2700/ 4361 batches | lr 0.000000 | ms/batch 402.65 | loss  2.56 | ppl    12.94 | acc     0.69 | train_ae_norm     1.00\n",
      "[97/200][2799/4361] Loss_D: 0.00329181 (Loss_D_real: 0.00025325 Loss_D_fake: 0.00303856) Loss_G: 0.33121908 Loss_Enh_Dec: -1.82877815\n",
      "| epoch  97 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  2.52 | ppl    12.47 | acc     0.70 | train_ae_norm     1.00\n",
      "[97/200][2899/4361] Loss_D: 0.01588517 (Loss_D_real: 0.01453921 Loss_D_fake: 0.00134596) Loss_G: 0.36030185 Loss_Enh_Dec: -2.25266504\n",
      "| epoch  97 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.53 | ppl    12.53 | acc     0.71 | train_ae_norm     1.00\n",
      "[97/200][2999/4361] Loss_D: 0.00378048 (Loss_D_real: 0.00020174 Loss_D_fake: 0.00357874) Loss_G: 0.35593668 Loss_Enh_Dec: -2.05192709\n",
      "| epoch  97 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.62 | loss  2.55 | ppl    12.83 | acc     0.71 | train_ae_norm     1.00\n",
      "[97/200][3099/4361] Loss_D: 0.00359588 (Loss_D_real: 0.00159610 Loss_D_fake: 0.00199978) Loss_G: 0.37582603 Loss_Enh_Dec: -1.74370277\n",
      "| epoch  97 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  2.57 | ppl    13.01 | acc     0.69 | train_ae_norm     1.00\n",
      "[97/200][3199/4361] Loss_D: 0.00294418 (Loss_D_real: 0.00211699 Loss_D_fake: 0.00082719) Loss_G: 0.39239919 Loss_Enh_Dec: -2.14653969\n",
      "| epoch  97 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.51 | loss  2.58 | ppl    13.24 | acc     0.69 | train_ae_norm     1.00\n",
      "[97/200][3299/4361] Loss_D: 0.00854469 (Loss_D_real: 0.00013653 Loss_D_fake: 0.00840816) Loss_G: 0.34747526 Loss_Enh_Dec: -2.43483567\n",
      "| epoch  97 |  3300/ 4361 batches | lr 0.000000 | ms/batch 400.39 | loss  2.57 | ppl    13.13 | acc     0.72 | train_ae_norm     1.00\n",
      "[97/200][3399/4361] Loss_D: 0.00406714 (Loss_D_real: 0.00155170 Loss_D_fake: 0.00251544) Loss_G: 0.31933516 Loss_Enh_Dec: -2.28442240\n",
      "| epoch  97 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.12 | loss  2.55 | ppl    12.86 | acc     0.66 | train_ae_norm     1.00\n",
      "[97/200][3499/4361] Loss_D: 0.00290731 (Loss_D_real: 0.00042377 Loss_D_fake: 0.00248354) Loss_G: 0.33357874 Loss_Enh_Dec: -2.36827230\n",
      "| epoch  97 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.69 | loss  2.48 | ppl    11.99 | acc     0.69 | train_ae_norm     1.00\n",
      "[97/200][3599/4361] Loss_D: 0.00271010 (Loss_D_real: 0.00067192 Loss_D_fake: 0.00203818) Loss_G: 0.33739829 Loss_Enh_Dec: -2.52145362\n",
      "| epoch  97 |  3600/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  2.49 | ppl    12.11 | acc     0.72 | train_ae_norm     1.00\n",
      "[97/200][3699/4361] Loss_D: 0.00285004 (Loss_D_real: 0.00130278 Loss_D_fake: 0.00154725) Loss_G: 0.35809204 Loss_Enh_Dec: -2.54740405\n",
      "| epoch  97 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.06 | loss  2.53 | ppl    12.52 | acc     0.68 | train_ae_norm     1.00\n",
      "[97/200][3799/4361] Loss_D: 0.00252717 (Loss_D_real: 0.00071361 Loss_D_fake: 0.00181356) Loss_G: 0.34649679 Loss_Enh_Dec: -2.20154357\n",
      "| epoch  97 |  3800/ 4361 batches | lr 0.000000 | ms/batch 400.17 | loss  2.51 | ppl    12.32 | acc     0.76 | train_ae_norm     1.00\n",
      "[97/200][3899/4361] Loss_D: 0.00161504 (Loss_D_real: 0.00017285 Loss_D_fake: 0.00144220) Loss_G: 0.37414646 Loss_Enh_Dec: -2.56864357\n",
      "| epoch  97 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.10 | loss  2.54 | ppl    12.65 | acc     0.71 | train_ae_norm     1.00\n",
      "[97/200][3999/4361] Loss_D: 0.00414611 (Loss_D_real: 0.00245380 Loss_D_fake: 0.00169231) Loss_G: 0.34848729 Loss_Enh_Dec: -2.50703692\n",
      "| epoch  97 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.75 | loss  2.52 | ppl    12.49 | acc     0.72 | train_ae_norm     1.00\n",
      "[97/200][4099/4361] Loss_D: 0.00444050 (Loss_D_real: 0.00208182 Loss_D_fake: 0.00235868) Loss_G: 0.31626073 Loss_Enh_Dec: -2.64533591\n",
      "| epoch  97 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.10 | loss  2.48 | ppl    11.96 | acc     0.71 | train_ae_norm     1.00\n",
      "[97/200][4199/4361] Loss_D: 0.00215377 (Loss_D_real: 0.00015848 Loss_D_fake: 0.00199530) Loss_G: 0.31687808 Loss_Enh_Dec: -2.52219152\n",
      "| epoch  97 |  4200/ 4361 batches | lr 0.000000 | ms/batch 400.89 | loss  2.53 | ppl    12.49 | acc     0.72 | train_ae_norm     1.00\n",
      "[97/200][4299/4361] Loss_D: 0.00307026 (Loss_D_real: 0.00075264 Loss_D_fake: 0.00231761) Loss_G: 0.31756136 Loss_Enh_Dec: -2.49278021\n",
      "| epoch  97 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.90 | loss  2.49 | ppl    12.10 | acc     0.74 | train_ae_norm     1.00\n",
      "| end of epoch  97 | time: 1851.97s | test loss  2.55 | test ppl 12.82 | acc 0.742\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 98 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:18.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.699\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.480\n",
      "  Test Loss: 4.537\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  98 |     0/ 4361 batches | lr 0.000000 | ms/batch 861.08 | loss  0.02 | ppl     1.02 | acc     0.74 | train_ae_norm     1.00\n",
      "[98/200][99/4361] Loss_D: 0.00151400 (Loss_D_real: 0.00003866 Loss_D_fake: 0.00147534) Loss_G: 0.33986279 Loss_Enh_Dec: -2.34091783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  98 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.50 | ppl    12.14 | acc     0.66 | train_ae_norm     1.00\n",
      "[98/200][199/4361] Loss_D: 0.00172637 (Loss_D_real: 0.00032781 Loss_D_fake: 0.00139857) Loss_G: 0.31568703 Loss_Enh_Dec: -2.28040457\n",
      "| epoch  98 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  2.51 | ppl    12.36 | acc     0.74 | train_ae_norm     1.00\n",
      "[98/200][299/4361] Loss_D: 0.05797193 (Loss_D_real: 0.05532369 Loss_D_fake: 0.00264825) Loss_G: 0.31031474 Loss_Enh_Dec: -2.26971126\n",
      "| epoch  98 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.69 | loss  2.54 | ppl    12.64 | acc     0.69 | train_ae_norm     1.00\n",
      "[98/200][399/4361] Loss_D: 0.00205217 (Loss_D_real: 0.00053764 Loss_D_fake: 0.00151453) Loss_G: 0.32594261 Loss_Enh_Dec: -2.73519278\n",
      "| epoch  98 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  2.45 | ppl    11.60 | acc     0.74 | train_ae_norm     1.00\n",
      "[98/200][499/4361] Loss_D: 0.00245116 (Loss_D_real: 0.00024156 Loss_D_fake: 0.00220960) Loss_G: 0.30158815 Loss_Enh_Dec: -2.62196231\n",
      "| epoch  98 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.86 | loss  2.51 | ppl    12.31 | acc     0.70 | train_ae_norm     1.00\n",
      "[98/200][599/4361] Loss_D: 0.00615818 (Loss_D_real: 0.00390902 Loss_D_fake: 0.00224916) Loss_G: 0.31216535 Loss_Enh_Dec: -2.41678548\n",
      "| epoch  98 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  2.48 | ppl    12.00 | acc     0.71 | train_ae_norm     1.00\n",
      "[98/200][699/4361] Loss_D: 0.01143414 (Loss_D_real: 0.01087152 Loss_D_fake: 0.00056262) Loss_G: 0.57601041 Loss_Enh_Dec: -2.64041018\n",
      "| epoch  98 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.51 | ppl    12.29 | acc     0.70 | train_ae_norm     1.00\n",
      "[98/200][799/4361] Loss_D: 0.00037350 (Loss_D_real: 0.00003786 Loss_D_fake: 0.00033564) Loss_G: 0.46616015 Loss_Enh_Dec: -2.56228328\n",
      "| epoch  98 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  2.50 | ppl    12.12 | acc     0.71 | train_ae_norm     1.00\n",
      "[98/200][899/4361] Loss_D: 0.00367227 (Loss_D_real: 0.00254882 Loss_D_fake: 0.00112345) Loss_G: 0.53832853 Loss_Enh_Dec: -2.47179270\n",
      "| epoch  98 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  2.51 | ppl    12.30 | acc     0.74 | train_ae_norm     1.00\n",
      "[98/200][999/4361] Loss_D: 0.00050131 (Loss_D_real: 0.00023191 Loss_D_fake: 0.00026940) Loss_G: 0.45541802 Loss_Enh_Dec: -2.41084194\n",
      "| epoch  98 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.50 | ppl    12.17 | acc     0.73 | train_ae_norm     1.00\n",
      "[98/200][1099/4361] Loss_D: 0.00087239 (Loss_D_real: 0.00010057 Loss_D_fake: 0.00077182) Loss_G: 0.43722582 Loss_Enh_Dec: -2.37337875\n",
      "| epoch  98 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  2.48 | ppl    11.95 | acc     0.70 | train_ae_norm     1.00\n",
      "[98/200][1199/4361] Loss_D: 0.00086451 (Loss_D_real: 0.00036270 Loss_D_fake: 0.00050181) Loss_G: 0.45628691 Loss_Enh_Dec: -2.16573548\n",
      "| epoch  98 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.90 | loss  2.50 | ppl    12.17 | acc     0.74 | train_ae_norm     1.00\n",
      "[98/200][1299/4361] Loss_D: 0.00113382 (Loss_D_real: 0.00023463 Loss_D_fake: 0.00089920) Loss_G: 0.40741092 Loss_Enh_Dec: -2.42922425\n",
      "| epoch  98 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  2.54 | ppl    12.62 | acc     0.70 | train_ae_norm     1.00\n",
      "[98/200][1399/4361] Loss_D: 0.00265508 (Loss_D_real: 0.00037199 Loss_D_fake: 0.00228310) Loss_G: 0.40794283 Loss_Enh_Dec: -2.41850638\n",
      "| epoch  98 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  2.53 | ppl    12.61 | acc     0.68 | train_ae_norm     1.00\n",
      "[98/200][1499/4361] Loss_D: 0.00171794 (Loss_D_real: 0.00037603 Loss_D_fake: 0.00134191) Loss_G: 0.40400347 Loss_Enh_Dec: -1.98111570\n",
      "| epoch  98 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.61 | ppl    13.62 | acc     0.68 | train_ae_norm     1.00\n",
      "[98/200][1599/4361] Loss_D: 0.00174449 (Loss_D_real: 0.00011270 Loss_D_fake: 0.00163180) Loss_G: 0.39767861 Loss_Enh_Dec: -2.38713765\n",
      "| epoch  98 |  1600/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  2.56 | ppl    12.99 | acc     0.71 | train_ae_norm     1.00\n",
      "[98/200][1699/4361] Loss_D: 0.00229336 (Loss_D_real: 0.00085906 Loss_D_fake: 0.00143431) Loss_G: 0.46572390 Loss_Enh_Dec: -2.52058339\n",
      "| epoch  98 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.53 | ppl    12.58 | acc     0.70 | train_ae_norm     1.00\n",
      "[98/200][1799/4361] Loss_D: 0.00180429 (Loss_D_real: 0.00147597 Loss_D_fake: 0.00032832) Loss_G: 0.55898875 Loss_Enh_Dec: -1.84100044\n",
      "| epoch  98 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  2.53 | ppl    12.54 | acc     0.68 | train_ae_norm     1.00\n",
      "[98/200][1899/4361] Loss_D: 0.00252683 (Loss_D_real: 0.00158772 Loss_D_fake: 0.00093910) Loss_G: 0.40263852 Loss_Enh_Dec: -2.08313632\n",
      "| epoch  98 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.57 | ppl    13.03 | acc     0.73 | train_ae_norm     1.00\n",
      "[98/200][1999/4361] Loss_D: 0.00099926 (Loss_D_real: 0.00057665 Loss_D_fake: 0.00042261) Loss_G: 0.41915247 Loss_Enh_Dec: -2.20456672\n",
      "| epoch  98 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  2.53 | ppl    12.51 | acc     0.72 | train_ae_norm     1.00\n",
      "[98/200][2099/4361] Loss_D: 0.00139852 (Loss_D_real: 0.00108307 Loss_D_fake: 0.00031545) Loss_G: 0.42050454 Loss_Enh_Dec: -2.29966044\n",
      "| epoch  98 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  2.54 | ppl    12.73 | acc     0.73 | train_ae_norm     1.00\n",
      "[98/200][2199/4361] Loss_D: 0.00799963 (Loss_D_real: 0.00763400 Loss_D_fake: 0.00036564) Loss_G: 0.45672470 Loss_Enh_Dec: -2.21713614\n",
      "| epoch  98 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.54 | ppl    12.73 | acc     0.71 | train_ae_norm     1.00\n",
      "[98/200][2299/4361] Loss_D: 0.00184218 (Loss_D_real: 0.00042481 Loss_D_fake: 0.00141737) Loss_G: 0.39382085 Loss_Enh_Dec: -1.76619649\n",
      "| epoch  98 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  2.52 | ppl    12.48 | acc     0.71 | train_ae_norm     1.00\n",
      "[98/200][2399/4361] Loss_D: 0.00226245 (Loss_D_real: 0.00002697 Loss_D_fake: 0.00223548) Loss_G: 0.37104541 Loss_Enh_Dec: -2.18318486\n",
      "| epoch  98 |  2400/ 4361 batches | lr 0.000000 | ms/batch 400.76 | loss  2.56 | ppl    12.88 | acc     0.67 | train_ae_norm     1.00\n",
      "[98/200][2499/4361] Loss_D: 0.00165485 (Loss_D_real: 0.00024644 Loss_D_fake: 0.00140842) Loss_G: 0.39629889 Loss_Enh_Dec: -2.08715320\n",
      "| epoch  98 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  2.56 | ppl    12.94 | acc     0.72 | train_ae_norm     1.00\n",
      "[98/200][2599/4361] Loss_D: 0.00144664 (Loss_D_real: 0.00098442 Loss_D_fake: 0.00046222) Loss_G: 0.47181055 Loss_Enh_Dec: -2.32844520\n",
      "| epoch  98 |  2600/ 4361 batches | lr 0.000000 | ms/batch 402.28 | loss  2.55 | ppl    12.78 | acc     0.67 | train_ae_norm     1.00\n",
      "[98/200][2699/4361] Loss_D: 0.00163523 (Loss_D_real: 0.00056832 Loss_D_fake: 0.00106691) Loss_G: 0.40972254 Loss_Enh_Dec: -2.28563285\n",
      "| epoch  98 |  2700/ 4361 batches | lr 0.000000 | ms/batch 402.03 | loss  2.56 | ppl    12.89 | acc     0.68 | train_ae_norm     1.00\n",
      "[98/200][2799/4361] Loss_D: 0.00061206 (Loss_D_real: 0.00006880 Loss_D_fake: 0.00054327) Loss_G: 0.42282706 Loss_Enh_Dec: -2.25957847\n",
      "| epoch  98 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  2.48 | ppl    11.97 | acc     0.68 | train_ae_norm     1.00\n",
      "[98/200][2899/4361] Loss_D: 0.00166396 (Loss_D_real: 0.00040588 Loss_D_fake: 0.00125808) Loss_G: 0.38202530 Loss_Enh_Dec: -2.50815725\n",
      "| epoch  98 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.53 | loss  2.52 | ppl    12.39 | acc     0.70 | train_ae_norm     1.00\n",
      "[98/200][2999/4361] Loss_D: 0.00324587 (Loss_D_real: 0.00004880 Loss_D_fake: 0.00319707) Loss_G: 0.40870854 Loss_Enh_Dec: -2.39039302\n",
      "| epoch  98 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  2.53 | ppl    12.50 | acc     0.69 | train_ae_norm     1.00\n",
      "[98/200][3099/4361] Loss_D: 0.00126472 (Loss_D_real: 0.00076778 Loss_D_fake: 0.00049694) Loss_G: 0.40129986 Loss_Enh_Dec: -2.59414935\n",
      "| epoch  98 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  2.54 | ppl    12.62 | acc     0.70 | train_ae_norm     1.00\n",
      "[98/200][3199/4361] Loss_D: 0.00991439 (Loss_D_real: 0.00107535 Loss_D_fake: 0.00883903) Loss_G: 0.65598679 Loss_Enh_Dec: -2.21770382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  98 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  2.57 | ppl    13.07 | acc     0.72 | train_ae_norm     1.00\n",
      "[98/200][3299/4361] Loss_D: 0.00087151 (Loss_D_real: 0.00008650 Loss_D_fake: 0.00078502) Loss_G: 0.38587606 Loss_Enh_Dec: -2.27394009\n",
      "| epoch  98 |  3300/ 4361 batches | lr 0.000000 | ms/batch 400.60 | loss  2.55 | ppl    12.80 | acc     0.72 | train_ae_norm     1.00\n",
      "[98/200][3399/4361] Loss_D: 0.00995665 (Loss_D_real: 0.00520732 Loss_D_fake: 0.00474933) Loss_G: 0.40704814 Loss_Enh_Dec: -2.41788077\n",
      "| epoch  98 |  3400/ 4361 batches | lr 0.000000 | ms/batch 402.04 | loss  2.55 | ppl    12.76 | acc     0.66 | train_ae_norm     1.00\n",
      "[98/200][3499/4361] Loss_D: 0.00199017 (Loss_D_real: 0.00137349 Loss_D_fake: 0.00061668) Loss_G: 0.60444200 Loss_Enh_Dec: -1.92926061\n",
      "| epoch  98 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  2.49 | ppl    12.07 | acc     0.71 | train_ae_norm     1.00\n",
      "[98/200][3599/4361] Loss_D: 0.00103631 (Loss_D_real: 0.00015868 Loss_D_fake: 0.00087763) Loss_G: 0.42256624 Loss_Enh_Dec: -2.19421148\n",
      "| epoch  98 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  2.51 | ppl    12.30 | acc     0.75 | train_ae_norm     1.00\n",
      "[98/200][3699/4361] Loss_D: 0.00262224 (Loss_D_real: 0.00142352 Loss_D_fake: 0.00119871) Loss_G: 0.38137576 Loss_Enh_Dec: -2.16420627\n",
      "| epoch  98 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.63 | loss  2.53 | ppl    12.61 | acc     0.67 | train_ae_norm     1.00\n",
      "[98/200][3799/4361] Loss_D: 0.00090598 (Loss_D_real: 0.00054321 Loss_D_fake: 0.00036276) Loss_G: 0.43453193 Loss_Enh_Dec: -1.45889616\n",
      "| epoch  98 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  2.53 | ppl    12.57 | acc     0.74 | train_ae_norm     1.00\n",
      "[98/200][3899/4361] Loss_D: 0.00058475 (Loss_D_real: 0.00012244 Loss_D_fake: 0.00046231) Loss_G: 0.42744207 Loss_Enh_Dec: -1.46448469\n",
      "| epoch  98 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  2.54 | ppl    12.62 | acc     0.68 | train_ae_norm     1.00\n",
      "[98/200][3999/4361] Loss_D: 0.00179149 (Loss_D_real: 0.00040935 Loss_D_fake: 0.00138214) Loss_G: 0.41081062 Loss_Enh_Dec: -0.93103856\n",
      "| epoch  98 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  2.53 | ppl    12.55 | acc     0.72 | train_ae_norm     1.00\n",
      "[98/200][4099/4361] Loss_D: 0.00157327 (Loss_D_real: 0.00091882 Loss_D_fake: 0.00065444) Loss_G: 0.42987195 Loss_Enh_Dec: -1.46001744\n",
      "| epoch  98 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  2.51 | ppl    12.29 | acc     0.68 | train_ae_norm     1.00\n",
      "[98/200][4199/4361] Loss_D: 0.00077829 (Loss_D_real: 0.00030616 Loss_D_fake: 0.00047213) Loss_G: 0.40839797 Loss_Enh_Dec: -2.02551723\n",
      "| epoch  98 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  2.56 | ppl    12.88 | acc     0.71 | train_ae_norm     1.00\n",
      "[98/200][4299/4361] Loss_D: 0.00599703 (Loss_D_real: 0.00519325 Loss_D_fake: 0.00080378) Loss_G: 0.44323921 Loss_Enh_Dec: -2.17718649\n",
      "| epoch  98 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  2.50 | ppl    12.23 | acc     0.71 | train_ae_norm     1.00\n",
      "| end of epoch  98 | time: 1852.20s | test loss  2.57 | test ppl 13.09 | acc 0.742\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 99 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.706\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.477\n",
      "  Test Loss: 4.114\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  99 |     0/ 4361 batches | lr 0.000000 | ms/batch 861.74 | loss  0.02 | ppl     1.02 | acc     0.72 | train_ae_norm     1.00\n",
      "[99/200][99/4361] Loss_D: 0.00259447 (Loss_D_real: 0.00212161 Loss_D_fake: 0.00047286) Loss_G: 0.46437883 Loss_Enh_Dec: -2.10393858\n",
      "| epoch  99 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  2.50 | ppl    12.17 | acc     0.64 | train_ae_norm     1.00\n",
      "[99/200][199/4361] Loss_D: 0.00187918 (Loss_D_real: 0.00075528 Loss_D_fake: 0.00112390) Loss_G: 0.43021327 Loss_Enh_Dec: -2.53330874\n",
      "| epoch  99 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  2.53 | ppl    12.56 | acc     0.73 | train_ae_norm     1.00\n",
      "[99/200][299/4361] Loss_D: 0.00268829 (Loss_D_real: 0.00159521 Loss_D_fake: 0.00109307) Loss_G: 0.42876917 Loss_Enh_Dec: -2.01537943\n",
      "| epoch  99 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  2.54 | ppl    12.68 | acc     0.66 | train_ae_norm     1.00\n",
      "[99/200][399/4361] Loss_D: 0.00136462 (Loss_D_real: 0.00012083 Loss_D_fake: 0.00124378) Loss_G: 0.37850395 Loss_Enh_Dec: -2.34068274\n",
      "| epoch  99 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  2.46 | ppl    11.71 | acc     0.71 | train_ae_norm     1.00\n",
      "[99/200][499/4361] Loss_D: 0.00071943 (Loss_D_real: 0.00019686 Loss_D_fake: 0.00052258) Loss_G: 0.39593062 Loss_Enh_Dec: -2.26481223\n",
      "| epoch  99 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.54 | ppl    12.66 | acc     0.71 | train_ae_norm     1.00\n",
      "[99/200][599/4361] Loss_D: 0.00165215 (Loss_D_real: 0.00083476 Loss_D_fake: 0.00081739) Loss_G: 0.42443505 Loss_Enh_Dec: -2.39814520\n",
      "| epoch  99 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  2.48 | ppl    11.96 | acc     0.70 | train_ae_norm     1.00\n",
      "[99/200][699/4361] Loss_D: 0.00273327 (Loss_D_real: 0.00119958 Loss_D_fake: 0.00153369) Loss_G: 0.68546140 Loss_Enh_Dec: -2.23229384\n",
      "| epoch  99 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  2.53 | ppl    12.59 | acc     0.72 | train_ae_norm     1.00\n",
      "[99/200][799/4361] Loss_D: 0.00129566 (Loss_D_real: 0.00018880 Loss_D_fake: 0.00110686) Loss_G: 0.49366996 Loss_Enh_Dec: -2.29180145\n",
      "| epoch  99 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  2.53 | ppl    12.52 | acc     0.69 | train_ae_norm     1.00\n",
      "[99/200][899/4361] Loss_D: 0.00137228 (Loss_D_real: 0.00048913 Loss_D_fake: 0.00088315) Loss_G: 0.45134708 Loss_Enh_Dec: -2.25108242\n",
      "| epoch  99 |   900/ 4361 batches | lr 0.000000 | ms/batch 402.24 | loss  2.51 | ppl    12.36 | acc     0.72 | train_ae_norm     1.00\n",
      "[99/200][999/4361] Loss_D: 0.00273276 (Loss_D_real: 0.00059879 Loss_D_fake: 0.00213397) Loss_G: 0.39747405 Loss_Enh_Dec: -1.89176083\n",
      "| epoch  99 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  2.52 | ppl    12.40 | acc     0.71 | train_ae_norm     1.00\n",
      "[99/200][1099/4361] Loss_D: 0.00148482 (Loss_D_real: 0.00096817 Loss_D_fake: 0.00051665) Loss_G: 0.39360997 Loss_Enh_Dec: -2.34848189\n",
      "| epoch  99 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  2.48 | ppl    11.93 | acc     0.71 | train_ae_norm     1.00\n",
      "[99/200][1199/4361] Loss_D: 0.00312775 (Loss_D_real: 0.00223487 Loss_D_fake: 0.00089288) Loss_G: 0.38674641 Loss_Enh_Dec: -2.33396029\n",
      "| epoch  99 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.78 | loss  2.51 | ppl    12.27 | acc     0.73 | train_ae_norm     1.00\n",
      "[99/200][1299/4361] Loss_D: 0.00371286 (Loss_D_real: 0.00030496 Loss_D_fake: 0.00340790) Loss_G: 0.40540576 Loss_Enh_Dec: -2.14821386\n",
      "| epoch  99 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.08 | loss  2.53 | ppl    12.59 | acc     0.71 | train_ae_norm     1.00\n",
      "[99/200][1399/4361] Loss_D: 0.00455351 (Loss_D_real: 0.00380125 Loss_D_fake: 0.00075226) Loss_G: 0.43930015 Loss_Enh_Dec: -2.05493712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  99 |  1400/ 4361 batches | lr 0.000000 | ms/batch 402.34 | loss  2.50 | ppl    12.24 | acc     0.68 | train_ae_norm     1.00\n",
      "[99/200][1499/4361] Loss_D: 0.00086851 (Loss_D_real: 0.00013472 Loss_D_fake: 0.00073379) Loss_G: 0.40206346 Loss_Enh_Dec: -2.47551632\n",
      "| epoch  99 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.57 | ppl    13.10 | acc     0.70 | train_ae_norm     1.00\n",
      "[99/200][1599/4361] Loss_D: 0.00210768 (Loss_D_real: 0.00012217 Loss_D_fake: 0.00198552) Loss_G: 0.44476911 Loss_Enh_Dec: -2.35220695\n",
      "| epoch  99 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  2.55 | ppl    12.82 | acc     0.70 | train_ae_norm     1.00\n",
      "[99/200][1699/4361] Loss_D: 0.00108008 (Loss_D_real: 0.00033696 Loss_D_fake: 0.00074311) Loss_G: 0.39445138 Loss_Enh_Dec: -2.24843097\n",
      "| epoch  99 |  1700/ 4361 batches | lr 0.000000 | ms/batch 402.03 | loss  2.51 | ppl    12.35 | acc     0.68 | train_ae_norm     1.00\n",
      "[99/200][1799/4361] Loss_D: 0.00187291 (Loss_D_real: 0.00034902 Loss_D_fake: 0.00152389) Loss_G: 0.35211381 Loss_Enh_Dec: -2.28683138\n",
      "| epoch  99 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  2.48 | ppl    11.97 | acc     0.70 | train_ae_norm     1.00\n",
      "[99/200][1899/4361] Loss_D: 0.00144312 (Loss_D_real: 0.00044782 Loss_D_fake: 0.00099529) Loss_G: 0.42009518 Loss_Enh_Dec: -2.30311489\n",
      "| epoch  99 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  2.53 | ppl    12.60 | acc     0.74 | train_ae_norm     1.00\n",
      "[99/200][1999/4361] Loss_D: 0.00195439 (Loss_D_real: 0.00116343 Loss_D_fake: 0.00079097) Loss_G: 0.38085106 Loss_Enh_Dec: -2.49735379\n",
      "| epoch  99 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  2.49 | ppl    12.06 | acc     0.72 | train_ae_norm     1.00\n",
      "[99/200][2099/4361] Loss_D: 0.00167298 (Loss_D_real: 0.00017663 Loss_D_fake: 0.00149635) Loss_G: 0.43265039 Loss_Enh_Dec: -2.11976933\n",
      "| epoch  99 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  2.52 | ppl    12.45 | acc     0.74 | train_ae_norm     1.00\n",
      "[99/200][2199/4361] Loss_D: 0.00136765 (Loss_D_real: 0.00039573 Loss_D_fake: 0.00097191) Loss_G: 0.37099561 Loss_Enh_Dec: -1.76977181\n",
      "| epoch  99 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.91 | loss  2.52 | ppl    12.49 | acc     0.71 | train_ae_norm     1.00\n",
      "[99/200][2299/4361] Loss_D: 0.00285476 (Loss_D_real: 0.00063275 Loss_D_fake: 0.00222201) Loss_G: 0.37021968 Loss_Enh_Dec: -2.34724784\n",
      "| epoch  99 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  2.52 | ppl    12.44 | acc     0.72 | train_ae_norm     1.00\n",
      "[99/200][2399/4361] Loss_D: 0.00287842 (Loss_D_real: 0.00173672 Loss_D_fake: 0.00114170) Loss_G: 0.39719248 Loss_Enh_Dec: -2.25670934\n",
      "| epoch  99 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  2.51 | ppl    12.33 | acc     0.66 | train_ae_norm     1.00\n",
      "[99/200][2499/4361] Loss_D: 0.00885813 (Loss_D_real: 0.00812344 Loss_D_fake: 0.00073469) Loss_G: 0.47621855 Loss_Enh_Dec: -2.00677848\n",
      "| epoch  99 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  2.55 | ppl    12.77 | acc     0.71 | train_ae_norm     1.00\n",
      "[99/200][2599/4361] Loss_D: 0.00151988 (Loss_D_real: 0.00105197 Loss_D_fake: 0.00046791) Loss_G: 0.81622761 Loss_Enh_Dec: -2.07222366\n",
      "| epoch  99 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  2.51 | ppl    12.25 | acc     0.69 | train_ae_norm     1.00\n",
      "[99/200][2699/4361] Loss_D: 0.00325171 (Loss_D_real: 0.00228918 Loss_D_fake: 0.00096253) Loss_G: 0.44121382 Loss_Enh_Dec: -2.26946449\n",
      "| epoch  99 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  2.50 | ppl    12.21 | acc     0.70 | train_ae_norm     1.00\n",
      "[99/200][2799/4361] Loss_D: 0.01791741 (Loss_D_real: 0.00024422 Loss_D_fake: 0.01767319) Loss_G: 0.44799757 Loss_Enh_Dec: -2.03357482\n",
      "| epoch  99 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.71 | loss  2.47 | ppl    11.82 | acc     0.71 | train_ae_norm     1.00\n",
      "[99/200][2899/4361] Loss_D: 0.00189626 (Loss_D_real: 0.00145164 Loss_D_fake: 0.00044462) Loss_G: 0.48003823 Loss_Enh_Dec: -2.05923963\n",
      "| epoch  99 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  2.49 | ppl    12.09 | acc     0.71 | train_ae_norm     1.00\n",
      "[99/200][2999/4361] Loss_D: 0.00505836 (Loss_D_real: 0.00225718 Loss_D_fake: 0.00280118) Loss_G: 0.38731328 Loss_Enh_Dec: -2.01421380\n",
      "| epoch  99 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  2.51 | ppl    12.25 | acc     0.70 | train_ae_norm     1.00\n",
      "[99/200][3099/4361] Loss_D: 0.00229952 (Loss_D_real: 0.00104442 Loss_D_fake: 0.00125509) Loss_G: 0.42149431 Loss_Enh_Dec: -2.13253999\n",
      "| epoch  99 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  2.52 | ppl    12.41 | acc     0.70 | train_ae_norm     1.00\n",
      "[99/200][3199/4361] Loss_D: 0.00262097 (Loss_D_real: 0.00139412 Loss_D_fake: 0.00122685) Loss_G: 0.50092947 Loss_Enh_Dec: -2.10164046\n",
      "| epoch  99 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.68 | loss  2.54 | ppl    12.64 | acc     0.71 | train_ae_norm     1.00\n",
      "[99/200][3299/4361] Loss_D: 0.00220566 (Loss_D_real: 0.00097741 Loss_D_fake: 0.00122826) Loss_G: 0.47285071 Loss_Enh_Dec: -1.90300453\n",
      "| epoch  99 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  2.54 | ppl    12.74 | acc     0.69 | train_ae_norm     1.00\n",
      "[99/200][3399/4361] Loss_D: 0.00328989 (Loss_D_real: 0.00035156 Loss_D_fake: 0.00293833) Loss_G: 0.45627460 Loss_Enh_Dec: -2.11668754\n",
      "| epoch  99 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  2.50 | ppl    12.17 | acc     0.69 | train_ae_norm     1.00\n",
      "[99/200][3499/4361] Loss_D: 0.00683152 (Loss_D_real: 0.00491016 Loss_D_fake: 0.00192136) Loss_G: 0.36747998 Loss_Enh_Dec: -1.85720670\n",
      "| epoch  99 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.44 | ppl    11.45 | acc     0.71 | train_ae_norm     1.00\n",
      "[99/200][3599/4361] Loss_D: 0.01867187 (Loss_D_real: 0.00533203 Loss_D_fake: 0.01333984) Loss_G: 0.37173802 Loss_Enh_Dec: -1.90969646\n",
      "| epoch  99 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.44 | ppl    11.51 | acc     0.74 | train_ae_norm     1.00\n",
      "[99/200][3699/4361] Loss_D: 0.00890202 (Loss_D_real: 0.00155099 Loss_D_fake: 0.00735103) Loss_G: 0.54750216 Loss_Enh_Dec: -1.86299062\n",
      "| epoch  99 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.84 | loss  2.47 | ppl    11.85 | acc     0.69 | train_ae_norm     1.00\n",
      "[99/200][3799/4361] Loss_D: 0.00934989 (Loss_D_real: 0.00855198 Loss_D_fake: 0.00079792) Loss_G: 0.54793882 Loss_Enh_Dec: -2.10463428\n",
      "| epoch  99 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  2.46 | ppl    11.74 | acc     0.74 | train_ae_norm     1.00\n",
      "[99/200][3899/4361] Loss_D: 0.02040569 (Loss_D_real: 0.01874499 Loss_D_fake: 0.00166071) Loss_G: 0.43440825 Loss_Enh_Dec: -2.28374696\n",
      "| epoch  99 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.48 | ppl    11.96 | acc     0.69 | train_ae_norm     1.00\n",
      "[99/200][3999/4361] Loss_D: 0.00356994 (Loss_D_real: 0.00279099 Loss_D_fake: 0.00077895) Loss_G: 0.50597984 Loss_Enh_Dec: -1.84333706\n",
      "| epoch  99 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.47 | ppl    11.86 | acc     0.71 | train_ae_norm     1.00\n",
      "[99/200][4099/4361] Loss_D: 0.00072760 (Loss_D_real: 0.00028782 Loss_D_fake: 0.00043978) Loss_G: 0.40190879 Loss_Enh_Dec: -2.05155015\n",
      "| epoch  99 |  4100/ 4361 batches | lr 0.000000 | ms/batch 402.04 | loss  2.44 | ppl    11.51 | acc     0.70 | train_ae_norm     1.00\n",
      "[99/200][4199/4361] Loss_D: 0.00178803 (Loss_D_real: 0.00114751 Loss_D_fake: 0.00064052) Loss_G: 0.40229374 Loss_Enh_Dec: -2.26966596\n",
      "| epoch  99 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.47 | ppl    11.79 | acc     0.74 | train_ae_norm     1.00\n",
      "[99/200][4299/4361] Loss_D: 0.01724546 (Loss_D_real: 0.01631261 Loss_D_fake: 0.00093285) Loss_G: 0.38805664 Loss_Enh_Dec: -1.97405946\n",
      "| epoch  99 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.08 | loss  2.44 | ppl    11.46 | acc     0.73 | train_ae_norm     1.00\n",
      "| end of epoch  99 | time: 1852.89s | test loss  2.49 | test ppl 12.07 | acc 0.748\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 100 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.702\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.495\n",
      "  Test Loss: 4.209\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 100 |     0/ 4361 batches | lr 0.000000 | ms/batch 864.08 | loss  0.02 | ppl     1.02 | acc     0.76 | train_ae_norm     1.00\n",
      "[100/200][99/4361] Loss_D: 0.00158365 (Loss_D_real: 0.00014110 Loss_D_fake: 0.00144255) Loss_G: 0.38569337 Loss_Enh_Dec: -2.16518712\n",
      "| epoch 100 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  2.47 | ppl    11.79 | acc     0.69 | train_ae_norm     1.00\n",
      "[100/200][199/4361] Loss_D: 0.00293950 (Loss_D_real: 0.00045062 Loss_D_fake: 0.00248888) Loss_G: 0.37631616 Loss_Enh_Dec: -1.99666560\n",
      "| epoch 100 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  2.48 | ppl    11.90 | acc     0.71 | train_ae_norm     1.00\n",
      "[100/200][299/4361] Loss_D: 0.00206673 (Loss_D_real: 0.00071327 Loss_D_fake: 0.00135346) Loss_G: 0.48380119 Loss_Enh_Dec: -1.87772024\n",
      "| epoch 100 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.49 | ppl    12.04 | acc     0.67 | train_ae_norm     1.00\n",
      "[100/200][399/4361] Loss_D: 0.00332704 (Loss_D_real: 0.00291672 Loss_D_fake: 0.00041032) Loss_G: 0.41074076 Loss_Enh_Dec: -2.11236000\n",
      "| epoch 100 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.39 | ppl    10.86 | acc     0.72 | train_ae_norm     1.00\n",
      "[100/200][499/4361] Loss_D: 0.00280357 (Loss_D_real: 0.00123223 Loss_D_fake: 0.00157134) Loss_G: 0.37836552 Loss_Enh_Dec: -1.94500196\n",
      "| epoch 100 |   500/ 4361 batches | lr 0.000000 | ms/batch 402.16 | loss  2.46 | ppl    11.71 | acc     0.75 | train_ae_norm     1.00\n",
      "[100/200][599/4361] Loss_D: 0.00273202 (Loss_D_real: 0.00145684 Loss_D_fake: 0.00127518) Loss_G: 0.40026054 Loss_Enh_Dec: -1.91930485\n",
      "| epoch 100 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  2.43 | ppl    11.32 | acc     0.68 | train_ae_norm     1.00\n",
      "[100/200][699/4361] Loss_D: 0.00107238 (Loss_D_real: 0.00024486 Loss_D_fake: 0.00082752) Loss_G: 0.40340015 Loss_Enh_Dec: -1.97741759\n",
      "| epoch 100 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  2.47 | ppl    11.85 | acc     0.69 | train_ae_norm     1.00\n",
      "[100/200][799/4361] Loss_D: 0.05243663 (Loss_D_real: 0.05132499 Loss_D_fake: 0.00111164) Loss_G: 0.41999575 Loss_Enh_Dec: -1.29138064\n",
      "| epoch 100 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  2.45 | ppl    11.55 | acc     0.69 | train_ae_norm     1.00\n",
      "[100/200][899/4361] Loss_D: 0.00110557 (Loss_D_real: 0.00035158 Loss_D_fake: 0.00075399) Loss_G: 0.48168489 Loss_Enh_Dec: -1.68351066\n",
      "| epoch 100 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  2.45 | ppl    11.55 | acc     0.75 | train_ae_norm     1.00\n",
      "[100/200][999/4361] Loss_D: 0.01061993 (Loss_D_real: 0.00041655 Loss_D_fake: 0.01020337) Loss_G: 0.40493318 Loss_Enh_Dec: -1.82803404\n",
      "| epoch 100 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  2.45 | ppl    11.60 | acc     0.72 | train_ae_norm     1.00\n",
      "[100/200][1099/4361] Loss_D: 0.40410471 (Loss_D_real: 0.00032082 Loss_D_fake: 0.40378389) Loss_G: 0.89928305 Loss_Enh_Dec: -1.48867357\n",
      "| epoch 100 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  2.45 | ppl    11.56 | acc     0.71 | train_ae_norm     1.00\n",
      "[100/200][1199/4361] Loss_D: 0.00356116 (Loss_D_real: 0.00285159 Loss_D_fake: 0.00070957) Loss_G: 0.49507305 Loss_Enh_Dec: -2.13072085\n",
      "| epoch 100 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  2.45 | ppl    11.57 | acc     0.73 | train_ae_norm     1.00\n",
      "[100/200][1299/4361] Loss_D: 0.00204710 (Loss_D_real: 0.00056019 Loss_D_fake: 0.00148691) Loss_G: 0.37793919 Loss_Enh_Dec: -1.81861150\n",
      "| epoch 100 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  2.48 | ppl    11.96 | acc     0.72 | train_ae_norm     1.00\n",
      "[100/200][1399/4361] Loss_D: 0.00600993 (Loss_D_real: 0.00494382 Loss_D_fake: 0.00106612) Loss_G: 0.39105567 Loss_Enh_Dec: -1.61097872\n",
      "| epoch 100 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.36 | loss  2.46 | ppl    11.70 | acc     0.65 | train_ae_norm     1.00\n",
      "[100/200][1499/4361] Loss_D: 0.00127637 (Loss_D_real: 0.00019521 Loss_D_fake: 0.00108116) Loss_G: 0.38572845 Loss_Enh_Dec: -1.88499153\n",
      "| epoch 100 |  1500/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.52 | ppl    12.43 | acc     0.70 | train_ae_norm     1.00\n",
      "[100/200][1599/4361] Loss_D: 0.04322921 (Loss_D_real: 0.04221616 Loss_D_fake: 0.00101305) Loss_G: 0.39452904 Loss_Enh_Dec: -1.77440834\n",
      "| epoch 100 |  1600/ 4361 batches | lr 0.000000 | ms/batch 400.77 | loss  2.49 | ppl    12.01 | acc     0.70 | train_ae_norm     1.00\n",
      "[100/200][1699/4361] Loss_D: 0.00440008 (Loss_D_real: 0.00218259 Loss_D_fake: 0.00221749) Loss_G: 0.52817595 Loss_Enh_Dec: -1.91159284\n",
      "| epoch 100 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.36 | loss  2.44 | ppl    11.51 | acc     0.68 | train_ae_norm     1.00\n",
      "[100/200][1799/4361] Loss_D: 0.01162855 (Loss_D_real: 0.01008148 Loss_D_fake: 0.00154707) Loss_G: 0.36511907 Loss_Enh_Dec: -2.35187578\n",
      "| epoch 100 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.78 | loss  2.44 | ppl    11.42 | acc     0.72 | train_ae_norm     1.00\n",
      "[100/200][1899/4361] Loss_D: 0.00159432 (Loss_D_real: 0.00087175 Loss_D_fake: 0.00072257) Loss_G: 0.50316232 Loss_Enh_Dec: -2.08864212\n",
      "| epoch 100 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  2.53 | ppl    12.50 | acc     0.74 | train_ae_norm     1.00\n",
      "[100/200][1999/4361] Loss_D: 0.00808479 (Loss_D_real: 0.00337265 Loss_D_fake: 0.00471214) Loss_G: 0.48198080 Loss_Enh_Dec: -1.84507430\n",
      "| epoch 100 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  2.46 | ppl    11.67 | acc     0.73 | train_ae_norm     1.00\n",
      "[100/200][2099/4361] Loss_D: 0.00597105 (Loss_D_real: 0.00531597 Loss_D_fake: 0.00065508) Loss_G: 0.40084296 Loss_Enh_Dec: -1.23008275\n",
      "| epoch 100 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.32 | loss  2.51 | ppl    12.25 | acc     0.74 | train_ae_norm     1.00\n",
      "[100/200][2199/4361] Loss_D: 0.00554814 (Loss_D_real: 0.00421225 Loss_D_fake: 0.00133589) Loss_G: 0.40100574 Loss_Enh_Dec: -2.05808711\n",
      "| epoch 100 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.31 | loss  2.46 | ppl    11.65 | acc     0.74 | train_ae_norm     1.00\n",
      "[100/200][2299/4361] Loss_D: 0.00475205 (Loss_D_real: 0.00068935 Loss_D_fake: 0.00406270) Loss_G: 0.37899181 Loss_Enh_Dec: -2.28680873\n",
      "| epoch 100 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  2.46 | ppl    11.73 | acc     0.73 | train_ae_norm     1.00\n",
      "[100/200][2399/4361] Loss_D: 0.00197083 (Loss_D_real: 0.00045480 Loss_D_fake: 0.00151603) Loss_G: 0.42772350 Loss_Enh_Dec: -2.43861198\n",
      "| epoch 100 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.49 | ppl    12.01 | acc     0.70 | train_ae_norm     1.00\n",
      "[100/200][2499/4361] Loss_D: 0.00136784 (Loss_D_real: 0.00083415 Loss_D_fake: 0.00053369) Loss_G: 0.41143876 Loss_Enh_Dec: -2.32536912\n",
      "| epoch 100 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  2.51 | ppl    12.32 | acc     0.72 | train_ae_norm     1.00\n",
      "[100/200][2599/4361] Loss_D: 0.00278250 (Loss_D_real: 0.00023025 Loss_D_fake: 0.00255225) Loss_G: 0.36557385 Loss_Enh_Dec: -2.10947084\n",
      "| epoch 100 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  2.47 | ppl    11.88 | acc     0.67 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100/200][2699/4361] Loss_D: 0.00867874 (Loss_D_real: 0.00147750 Loss_D_fake: 0.00720124) Loss_G: 0.52741718 Loss_Enh_Dec: -1.99378991\n",
      "| epoch 100 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  2.49 | ppl    12.10 | acc     0.72 | train_ae_norm     1.00\n",
      "[100/200][2799/4361] Loss_D: 0.00129497 (Loss_D_real: 0.00021167 Loss_D_fake: 0.00108330) Loss_G: 0.39756289 Loss_Enh_Dec: -2.00987768\n",
      "| epoch 100 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  2.44 | ppl    11.49 | acc     0.70 | train_ae_norm     1.00\n",
      "[100/200][2899/4361] Loss_D: 0.00208243 (Loss_D_real: 0.00085926 Loss_D_fake: 0.00122317) Loss_G: 0.44615650 Loss_Enh_Dec: -1.94558358\n",
      "| epoch 100 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  2.47 | ppl    11.78 | acc     0.71 | train_ae_norm     1.00\n",
      "[100/200][2999/4361] Loss_D: 0.00094984 (Loss_D_real: 0.00019024 Loss_D_fake: 0.00075959) Loss_G: 0.45296031 Loss_Enh_Dec: -1.49374413\n",
      "| epoch 100 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  2.46 | ppl    11.69 | acc     0.69 | train_ae_norm     1.00\n",
      "[100/200][3099/4361] Loss_D: 0.00134891 (Loss_D_real: 0.00039819 Loss_D_fake: 0.00095073) Loss_G: 0.40874705 Loss_Enh_Dec: -2.09423423\n",
      "| epoch 100 |  3100/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  2.46 | ppl    11.72 | acc     0.72 | train_ae_norm     1.00\n",
      "[100/200][3199/4361] Loss_D: 0.00192879 (Loss_D_real: 0.00024908 Loss_D_fake: 0.00167971) Loss_G: 0.42875147 Loss_Enh_Dec: -1.64433694\n",
      "| epoch 100 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  2.50 | ppl    12.19 | acc     0.73 | train_ae_norm     1.00\n",
      "[100/200][3299/4361] Loss_D: 0.00113052 (Loss_D_real: 0.00030487 Loss_D_fake: 0.00082565) Loss_G: 0.38843125 Loss_Enh_Dec: -2.10885715\n",
      "| epoch 100 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  2.52 | ppl    12.45 | acc     0.72 | train_ae_norm     1.00\n",
      "[100/200][3399/4361] Loss_D: 0.00071671 (Loss_D_real: 0.00043090 Loss_D_fake: 0.00028581) Loss_G: 0.46033722 Loss_Enh_Dec: -1.85780394\n",
      "| epoch 100 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  2.48 | ppl    11.95 | acc     0.66 | train_ae_norm     1.00\n",
      "[100/200][3499/4361] Loss_D: 0.00058956 (Loss_D_real: 0.00037112 Loss_D_fake: 0.00021844) Loss_G: 0.56821090 Loss_Enh_Dec: -1.95768631\n",
      "| epoch 100 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  2.41 | ppl    11.17 | acc     0.69 | train_ae_norm     1.00\n",
      "[100/200][3599/4361] Loss_D: 0.00110303 (Loss_D_real: 0.00023464 Loss_D_fake: 0.00086839) Loss_G: 0.43980727 Loss_Enh_Dec: -1.79001415\n",
      "| epoch 100 |  3600/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.45 | ppl    11.54 | acc     0.74 | train_ae_norm     1.00\n",
      "[100/200][3699/4361] Loss_D: 0.00153453 (Loss_D_real: 0.00041950 Loss_D_fake: 0.00111503) Loss_G: 0.63243908 Loss_Enh_Dec: -1.33792913\n",
      "| epoch 100 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.46 | ppl    11.70 | acc     0.69 | train_ae_norm     1.00\n",
      "[100/200][3799/4361] Loss_D: 0.00189680 (Loss_D_real: 0.00120232 Loss_D_fake: 0.00069448) Loss_G: 0.52689493 Loss_Enh_Dec: -1.34665024\n",
      "| epoch 100 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.46 | ppl    11.76 | acc     0.73 | train_ae_norm     1.00\n",
      "[100/200][3899/4361] Loss_D: 0.00487797 (Loss_D_real: 0.00374149 Loss_D_fake: 0.00113648) Loss_G: 0.39339277 Loss_Enh_Dec: -1.90336120\n",
      "| epoch 100 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  2.46 | ppl    11.69 | acc     0.69 | train_ae_norm     1.00\n",
      "[100/200][3999/4361] Loss_D: 0.00336703 (Loss_D_real: 0.00140842 Loss_D_fake: 0.00195861) Loss_G: 0.44551927 Loss_Enh_Dec: -1.51238239\n",
      "| epoch 100 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.83 | loss  2.45 | ppl    11.59 | acc     0.71 | train_ae_norm     1.00\n",
      "[100/200][4099/4361] Loss_D: 0.00171906 (Loss_D_real: 0.00084042 Loss_D_fake: 0.00087865) Loss_G: 0.51829612 Loss_Enh_Dec: -1.28268719\n",
      "| epoch 100 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.44 | ppl    11.51 | acc     0.71 | train_ae_norm     1.00\n",
      "[100/200][4199/4361] Loss_D: 0.00366914 (Loss_D_real: 0.00283088 Loss_D_fake: 0.00083825) Loss_G: 0.40398255 Loss_Enh_Dec: -1.91481197\n",
      "| epoch 100 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.48 | ppl    11.99 | acc     0.74 | train_ae_norm     1.00\n",
      "[100/200][4299/4361] Loss_D: 0.00132615 (Loss_D_real: 0.00040709 Loss_D_fake: 0.00091906) Loss_G: 0.45652637 Loss_Enh_Dec: -1.96012771\n",
      "| epoch 100 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.59 | ppl    13.34 | acc     0.68 | train_ae_norm     1.00\n",
      "| end of epoch 100 | time: 1851.76s | test loss  2.55 | test ppl 12.81 | acc 0.744\n",
      "bleu_self:  [3.61122452e-01 9.94433348e-02 1.01252698e-06 1.51283222e-08\n",
      " 2.18364540e-08]\n",
      "bleu_test:  [7.78869047e-01 1.80901710e-01 1.25050688e-03 1.25001795e-04\n",
      " 3.13990867e-05]\n",
      "bleu_self: [0.36112245,0.09944333,0.00000101,0.00000002,0.00000002]\n",
      "bleu_test: [0.77886905,0.18090171,0.00125051,0.00012500,0.00003140]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 101 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.700\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.490\n",
      "  Test Loss: 4.341\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 101 |     0/ 4361 batches | lr 0.000000 | ms/batch 864.54 | loss  0.02 | ppl     1.02 | acc     0.75 | train_ae_norm     1.00\n",
      "[101/200][99/4361] Loss_D: 0.01332117 (Loss_D_real: 0.01271966 Loss_D_fake: 0.00060151) Loss_G: 0.42388925 Loss_Enh_Dec: -2.07759643\n",
      "| epoch 101 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  2.47 | ppl    11.81 | acc     0.67 | train_ae_norm     1.00\n",
      "[101/200][199/4361] Loss_D: 0.01494688 (Loss_D_real: 0.01385787 Loss_D_fake: 0.00108901) Loss_G: 0.56688040 Loss_Enh_Dec: -1.85344470\n",
      "| epoch 101 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  2.47 | ppl    11.83 | acc     0.75 | train_ae_norm     1.00\n",
      "[101/200][299/4361] Loss_D: 0.00273330 (Loss_D_real: 0.00230674 Loss_D_fake: 0.00042656) Loss_G: 0.46083900 Loss_Enh_Dec: -1.66885459\n",
      "| epoch 101 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.75 | loss  2.47 | ppl    11.80 | acc     0.67 | train_ae_norm     1.00\n",
      "[101/200][399/4361] Loss_D: 0.00154861 (Loss_D_real: 0.00042143 Loss_D_fake: 0.00112718) Loss_G: 0.42417726 Loss_Enh_Dec: -1.38472617\n",
      "| epoch 101 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  2.38 | ppl    10.80 | acc     0.72 | train_ae_norm     1.00\n",
      "[101/200][499/4361] Loss_D: 0.00317326 (Loss_D_real: 0.00184899 Loss_D_fake: 0.00132427) Loss_G: 0.39345843 Loss_Enh_Dec: -1.59717500\n",
      "| epoch 101 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  2.46 | ppl    11.70 | acc     0.74 | train_ae_norm     1.00\n",
      "[101/200][599/4361] Loss_D: 0.00484807 (Loss_D_real: 0.00348189 Loss_D_fake: 0.00136618) Loss_G: 0.39581344 Loss_Enh_Dec: -2.03452992\n",
      "| epoch 101 |   600/ 4361 batches | lr 0.000000 | ms/batch 402.20 | loss  2.41 | ppl    11.13 | acc     0.71 | train_ae_norm     1.00\n",
      "[101/200][699/4361] Loss_D: 0.00094917 (Loss_D_real: 0.00016134 Loss_D_fake: 0.00078783) Loss_G: 0.40687734 Loss_Enh_Dec: -1.53815842\n",
      "| epoch 101 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  2.45 | ppl    11.61 | acc     0.72 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101/200][799/4361] Loss_D: 0.00349057 (Loss_D_real: 0.00132558 Loss_D_fake: 0.00216499) Loss_G: 0.38310048 Loss_Enh_Dec: -1.23781538\n",
      "| epoch 101 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  2.42 | ppl    11.30 | acc     0.71 | train_ae_norm     1.00\n",
      "[101/200][899/4361] Loss_D: 0.00051820 (Loss_D_real: 0.00026279 Loss_D_fake: 0.00025541) Loss_G: 0.44871065 Loss_Enh_Dec: -1.63282418\n",
      "| epoch 101 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  2.46 | ppl    11.74 | acc     0.74 | train_ae_norm     1.00\n",
      "[101/200][999/4361] Loss_D: 0.00490474 (Loss_D_real: 0.00411815 Loss_D_fake: 0.00078658) Loss_G: 0.45831117 Loss_Enh_Dec: -1.52390003\n",
      "| epoch 101 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  2.43 | ppl    11.38 | acc     0.75 | train_ae_norm     1.00\n",
      "[101/200][1099/4361] Loss_D: 0.00231530 (Loss_D_real: 0.00181660 Loss_D_fake: 0.00049870) Loss_G: 0.46979371 Loss_Enh_Dec: -1.60872233\n",
      "| epoch 101 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.45 | ppl    11.54 | acc     0.70 | train_ae_norm     1.00\n",
      "[101/200][1199/4361] Loss_D: 0.00278026 (Loss_D_real: 0.00082721 Loss_D_fake: 0.00195305) Loss_G: 0.38343826 Loss_Enh_Dec: -2.19058442\n",
      "| epoch 101 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  2.45 | ppl    11.62 | acc     0.75 | train_ae_norm     1.00\n",
      "[101/200][1299/4361] Loss_D: 0.00460185 (Loss_D_real: 0.00188042 Loss_D_fake: 0.00272143) Loss_G: 0.42021400 Loss_Enh_Dec: -2.50696111\n",
      "| epoch 101 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.93 | loss  2.49 | ppl    12.02 | acc     0.73 | train_ae_norm     1.00\n",
      "[101/200][1399/4361] Loss_D: 0.00263866 (Loss_D_real: 0.00054266 Loss_D_fake: 0.00209600) Loss_G: 0.47543746 Loss_Enh_Dec: -2.44671082\n",
      "| epoch 101 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.93 | loss  2.47 | ppl    11.83 | acc     0.68 | train_ae_norm     1.00\n",
      "[101/200][1499/4361] Loss_D: 0.00227210 (Loss_D_real: 0.00007147 Loss_D_fake: 0.00220063) Loss_G: 0.43751842 Loss_Enh_Dec: -2.00593114\n",
      "| epoch 101 |  1500/ 4361 batches | lr 0.000000 | ms/batch 400.56 | loss  2.53 | ppl    12.52 | acc     0.67 | train_ae_norm     1.00\n",
      "[101/200][1599/4361] Loss_D: 0.00234110 (Loss_D_real: 0.00019595 Loss_D_fake: 0.00214514) Loss_G: 0.45161745 Loss_Enh_Dec: -2.25784421\n",
      "| epoch 101 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.49 | ppl    12.09 | acc     0.70 | train_ae_norm     1.00\n",
      "[101/200][1699/4361] Loss_D: 0.00202775 (Loss_D_real: 0.00003534 Loss_D_fake: 0.00199241) Loss_G: 0.43518430 Loss_Enh_Dec: -2.20193243\n",
      "| epoch 101 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.39 | loss  2.44 | ppl    11.51 | acc     0.70 | train_ae_norm     1.00\n",
      "[101/200][1799/4361] Loss_D: 0.00246488 (Loss_D_real: 0.00015344 Loss_D_fake: 0.00231145) Loss_G: 0.42722163 Loss_Enh_Dec: -2.22598577\n",
      "| epoch 101 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  2.46 | ppl    11.69 | acc     0.71 | train_ae_norm     1.00\n",
      "[101/200][1899/4361] Loss_D: 0.00196966 (Loss_D_real: 0.00011474 Loss_D_fake: 0.00185492) Loss_G: 0.51734507 Loss_Enh_Dec: -1.60381210\n",
      "| epoch 101 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.91 | loss  2.52 | ppl    12.38 | acc     0.72 | train_ae_norm     1.00\n",
      "[101/200][1999/4361] Loss_D: 0.00094210 (Loss_D_real: 0.00007259 Loss_D_fake: 0.00086952) Loss_G: 0.44270220 Loss_Enh_Dec: -2.25072169\n",
      "| epoch 101 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  2.45 | ppl    11.63 | acc     0.73 | train_ae_norm     1.00\n",
      "[101/200][2099/4361] Loss_D: 0.00295230 (Loss_D_real: 0.00103414 Loss_D_fake: 0.00191816) Loss_G: 0.40815052 Loss_Enh_Dec: -2.37270355\n",
      "| epoch 101 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  2.49 | ppl    12.05 | acc     0.75 | train_ae_norm     1.00\n",
      "[101/200][2199/4361] Loss_D: 0.00223093 (Loss_D_real: 0.00021575 Loss_D_fake: 0.00201519) Loss_G: 0.44991952 Loss_Enh_Dec: -2.45501876\n",
      "| epoch 101 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  2.45 | ppl    11.62 | acc     0.72 | train_ae_norm     1.00\n",
      "[101/200][2299/4361] Loss_D: 0.00175745 (Loss_D_real: 0.00053612 Loss_D_fake: 0.00122133) Loss_G: 0.39756438 Loss_Enh_Dec: -1.97531343\n",
      "| epoch 101 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  2.44 | ppl    11.48 | acc     0.74 | train_ae_norm     1.00\n",
      "[101/200][2399/4361] Loss_D: 0.00059975 (Loss_D_real: 0.00047598 Loss_D_fake: 0.00012377) Loss_G: 0.68833828 Loss_Enh_Dec: -1.97389185\n",
      "| epoch 101 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  2.45 | ppl    11.58 | acc     0.70 | train_ae_norm     1.00\n",
      "[101/200][2499/4361] Loss_D: 0.00199275 (Loss_D_real: 0.00119544 Loss_D_fake: 0.00079731) Loss_G: 0.42016616 Loss_Enh_Dec: -2.30474687\n",
      "| epoch 101 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  2.47 | ppl    11.86 | acc     0.73 | train_ae_norm     1.00\n",
      "[101/200][2599/4361] Loss_D: 0.00090040 (Loss_D_real: 0.00031283 Loss_D_fake: 0.00058757) Loss_G: 0.44143429 Loss_Enh_Dec: -2.04158902\n",
      "| epoch 101 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  2.45 | ppl    11.55 | acc     0.69 | train_ae_norm     1.00\n",
      "[101/200][2699/4361] Loss_D: 0.00124925 (Loss_D_real: 0.00044661 Loss_D_fake: 0.00080264) Loss_G: 0.41074380 Loss_Enh_Dec: -1.82693386\n",
      "| epoch 101 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.45 | ppl    11.62 | acc     0.71 | train_ae_norm     1.00\n",
      "[101/200][2799/4361] Loss_D: 0.00478620 (Loss_D_real: 0.00062275 Loss_D_fake: 0.00416345) Loss_G: 0.40204555 Loss_Enh_Dec: -2.20775723\n",
      "| epoch 101 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.58 | loss  2.40 | ppl    11.05 | acc     0.71 | train_ae_norm     1.00\n",
      "[101/200][2899/4361] Loss_D: 0.00298185 (Loss_D_real: 0.00153119 Loss_D_fake: 0.00145066) Loss_G: 0.48991194 Loss_Enh_Dec: -2.28764558\n",
      "| epoch 101 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  2.43 | ppl    11.34 | acc     0.70 | train_ae_norm     1.00\n",
      "[101/200][2999/4361] Loss_D: 0.00068395 (Loss_D_real: 0.00006071 Loss_D_fake: 0.00062324) Loss_G: 0.42036015 Loss_Enh_Dec: -1.95618713\n",
      "| epoch 101 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.42 | ppl    11.28 | acc     0.68 | train_ae_norm     1.00\n",
      "[101/200][3099/4361] Loss_D: 0.00278940 (Loss_D_real: 0.00071600 Loss_D_fake: 0.00207340) Loss_G: 0.40335441 Loss_Enh_Dec: -2.25156832\n",
      "| epoch 101 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  2.44 | ppl    11.49 | acc     0.71 | train_ae_norm     1.00\n",
      "[101/200][3199/4361] Loss_D: 0.00232536 (Loss_D_real: 0.00195644 Loss_D_fake: 0.00036892) Loss_G: 0.43821144 Loss_Enh_Dec: -1.77971804\n",
      "| epoch 101 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.47 | ppl    11.80 | acc     0.73 | train_ae_norm     1.00\n",
      "[101/200][3299/4361] Loss_D: 0.00148417 (Loss_D_real: 0.00144417 Loss_D_fake: 0.00004000) Loss_G: 0.72148108 Loss_Enh_Dec: -1.57808912\n",
      "| epoch 101 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.47 | ppl    11.78 | acc     0.72 | train_ae_norm     1.00\n",
      "[101/200][3399/4361] Loss_D: 0.00254244 (Loss_D_real: 0.00172854 Loss_D_fake: 0.00081390) Loss_G: 0.66253716 Loss_Enh_Dec: -1.63264549\n",
      "| epoch 101 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  2.43 | ppl    11.38 | acc     0.68 | train_ae_norm     1.00\n",
      "[101/200][3499/4361] Loss_D: 0.00140253 (Loss_D_real: 0.00048233 Loss_D_fake: 0.00092020) Loss_G: 0.47237697 Loss_Enh_Dec: -1.49840391\n",
      "| epoch 101 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  2.40 | ppl    11.01 | acc     0.71 | train_ae_norm     1.00\n",
      "[101/200][3599/4361] Loss_D: 0.00107844 (Loss_D_real: 0.00010995 Loss_D_fake: 0.00096849) Loss_G: 0.40551710 Loss_Enh_Dec: -1.74610603\n",
      "| epoch 101 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.40 | ppl    11.05 | acc     0.73 | train_ae_norm     1.00\n",
      "[101/200][3699/4361] Loss_D: 0.00105360 (Loss_D_real: 0.00015722 Loss_D_fake: 0.00089638) Loss_G: 0.39292756 Loss_Enh_Dec: -1.70279205\n",
      "| epoch 101 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  2.43 | ppl    11.38 | acc     0.69 | train_ae_norm     1.00\n",
      "[101/200][3799/4361] Loss_D: 0.00078212 (Loss_D_real: 0.00023060 Loss_D_fake: 0.00055152) Loss_G: 0.44100171 Loss_Enh_Dec: -2.31663036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 101 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  2.44 | ppl    11.47 | acc     0.74 | train_ae_norm     1.00\n",
      "[101/200][3899/4361] Loss_D: 0.00064274 (Loss_D_real: 0.00003030 Loss_D_fake: 0.00061244) Loss_G: 0.41434059 Loss_Enh_Dec: -2.07133055\n",
      "| epoch 101 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  2.44 | ppl    11.47 | acc     0.69 | train_ae_norm     1.00\n",
      "[101/200][3999/4361] Loss_D: 0.00101235 (Loss_D_real: 0.00024287 Loss_D_fake: 0.00076948) Loss_G: 0.41088316 Loss_Enh_Dec: -2.16722941\n",
      "| epoch 101 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  2.43 | ppl    11.31 | acc     0.71 | train_ae_norm     1.00\n",
      "[101/200][4099/4361] Loss_D: 0.00303978 (Loss_D_real: 0.00112163 Loss_D_fake: 0.00191814) Loss_G: 0.39137918 Loss_Enh_Dec: -2.41504264\n",
      "| epoch 101 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  2.40 | ppl    10.98 | acc     0.73 | train_ae_norm     1.00\n",
      "[101/200][4199/4361] Loss_D: 0.00241937 (Loss_D_real: 0.00120257 Loss_D_fake: 0.00121681) Loss_G: 0.39615306 Loss_Enh_Dec: -2.15826416\n",
      "| epoch 101 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  2.45 | ppl    11.61 | acc     0.74 | train_ae_norm     1.00\n",
      "[101/200][4299/4361] Loss_D: 0.00057952 (Loss_D_real: 0.00005318 Loss_D_fake: 0.00052634) Loss_G: 0.45654851 Loss_Enh_Dec: -1.84321654\n",
      "| epoch 101 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  2.43 | ppl    11.38 | acc     0.72 | train_ae_norm     1.00\n",
      "| end of epoch 101 | time: 1852.64s | test loss  2.48 | test ppl 11.92 | acc 0.752\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 102 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.699\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.490\n",
      "  Test Loss: 4.383\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 102 |     0/ 4361 batches | lr 0.000000 | ms/batch 862.33 | loss  0.02 | ppl     1.02 | acc     0.76 | train_ae_norm     1.00\n",
      "[102/200][99/4361] Loss_D: 0.00884451 (Loss_D_real: 0.00612518 Loss_D_fake: 0.00271934) Loss_G: 0.35308501 Loss_Enh_Dec: -1.70206380\n",
      "| epoch 102 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.43 | ppl    11.31 | acc     0.67 | train_ae_norm     1.00\n",
      "[102/200][199/4361] Loss_D: 0.00257193 (Loss_D_real: 0.00096394 Loss_D_fake: 0.00160798) Loss_G: 0.42023960 Loss_Enh_Dec: -1.10427463\n",
      "| epoch 102 |   200/ 4361 batches | lr 0.000000 | ms/batch 402.37 | loss  2.44 | ppl    11.50 | acc     0.72 | train_ae_norm     1.00\n",
      "[102/200][299/4361] Loss_D: 0.00119615 (Loss_D_real: 0.00029284 Loss_D_fake: 0.00090331) Loss_G: 0.43385392 Loss_Enh_Dec: -1.40541637\n",
      "| epoch 102 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  2.49 | ppl    12.03 | acc     0.66 | train_ae_norm     1.00\n",
      "[102/200][399/4361] Loss_D: 0.00297798 (Loss_D_real: 0.00074208 Loss_D_fake: 0.00223590) Loss_G: 0.42280847 Loss_Enh_Dec: -1.75255454\n",
      "| epoch 102 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.39 | ppl    10.86 | acc     0.72 | train_ae_norm     1.00\n",
      "[102/200][499/4361] Loss_D: 0.00120547 (Loss_D_real: 0.00015685 Loss_D_fake: 0.00104862) Loss_G: 0.38321808 Loss_Enh_Dec: -1.49942684\n",
      "| epoch 102 |   500/ 4361 batches | lr 0.000000 | ms/batch 402.15 | loss  2.45 | ppl    11.61 | acc     0.73 | train_ae_norm     1.00\n",
      "[102/200][599/4361] Loss_D: 0.00178942 (Loss_D_real: 0.00051477 Loss_D_fake: 0.00127465) Loss_G: 0.40888295 Loss_Enh_Dec: -1.57529020\n",
      "| epoch 102 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  2.41 | ppl    11.15 | acc     0.69 | train_ae_norm     1.00\n",
      "[102/200][699/4361] Loss_D: 0.00172974 (Loss_D_real: 0.00099501 Loss_D_fake: 0.00073473) Loss_G: 0.43684503 Loss_Enh_Dec: -1.83989036\n",
      "| epoch 102 |   700/ 4361 batches | lr 0.000000 | ms/batch 402.02 | loss  2.44 | ppl    11.49 | acc     0.73 | train_ae_norm     1.00\n",
      "[102/200][799/4361] Loss_D: 0.00958392 (Loss_D_real: 0.00801118 Loss_D_fake: 0.00157274) Loss_G: 0.42916107 Loss_Enh_Dec: -1.23640132\n",
      "| epoch 102 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  2.43 | ppl    11.32 | acc     0.69 | train_ae_norm     1.00\n",
      "[102/200][899/4361] Loss_D: 0.00379729 (Loss_D_real: 0.00051161 Loss_D_fake: 0.00328568) Loss_G: 0.44537574 Loss_Enh_Dec: -1.53457367\n",
      "| epoch 102 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  2.45 | ppl    11.57 | acc     0.75 | train_ae_norm     1.00\n",
      "[102/200][999/4361] Loss_D: 0.00513897 (Loss_D_real: 0.00398596 Loss_D_fake: 0.00115301) Loss_G: 0.47220016 Loss_Enh_Dec: -1.07819307\n",
      "| epoch 102 |  1000/ 4361 batches | lr 0.000000 | ms/batch 431.73 | loss  2.43 | ppl    11.34 | acc     0.73 | train_ae_norm     1.00\n",
      "[102/200][1099/4361] Loss_D: 0.00126481 (Loss_D_real: 0.00006362 Loss_D_fake: 0.00120119) Loss_G: 0.41894075 Loss_Enh_Dec: -2.54859233\n",
      "| epoch 102 |  1100/ 4361 batches | lr 0.000000 | ms/batch 437.37 | loss  2.42 | ppl    11.24 | acc     0.70 | train_ae_norm     1.00\n",
      "[102/200][1199/4361] Loss_D: 0.00510894 (Loss_D_real: 0.00225626 Loss_D_fake: 0.00285268) Loss_G: 0.45918629 Loss_Enh_Dec: -1.71160889\n",
      "| epoch 102 |  1200/ 4361 batches | lr 0.000000 | ms/batch 407.98 | loss  2.41 | ppl    11.16 | acc     0.73 | train_ae_norm     1.00\n",
      "[102/200][1299/4361] Loss_D: 0.01599234 (Loss_D_real: 0.00031987 Loss_D_fake: 0.01567247) Loss_G: 0.41220626 Loss_Enh_Dec: -1.17576158\n",
      "| epoch 102 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.45 | ppl    11.62 | acc     0.74 | train_ae_norm     1.00\n",
      "[102/200][1399/4361] Loss_D: 0.00484567 (Loss_D_real: 0.00222912 Loss_D_fake: 0.00261655) Loss_G: 0.60109675 Loss_Enh_Dec: -1.15846717\n",
      "| epoch 102 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  2.42 | ppl    11.21 | acc     0.69 | train_ae_norm     1.00\n",
      "[102/200][1499/4361] Loss_D: 0.00271608 (Loss_D_real: 0.00166201 Loss_D_fake: 0.00105407) Loss_G: 0.40793964 Loss_Enh_Dec: -1.78049982\n",
      "| epoch 102 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.47 | ppl    11.85 | acc     0.69 | train_ae_norm     1.00\n",
      "[102/200][1599/4361] Loss_D: 0.00160610 (Loss_D_real: 0.00025633 Loss_D_fake: 0.00134977) Loss_G: 0.36594883 Loss_Enh_Dec: -1.77305496\n",
      "| epoch 102 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  2.45 | ppl    11.54 | acc     0.71 | train_ae_norm     1.00\n",
      "[102/200][1699/4361] Loss_D: 0.03034490 (Loss_D_real: 0.02808310 Loss_D_fake: 0.00226180) Loss_G: 0.36021361 Loss_Enh_Dec: -1.48980713\n",
      "| epoch 102 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  2.41 | ppl    11.18 | acc     0.70 | train_ae_norm     1.00\n",
      "[102/200][1799/4361] Loss_D: 0.00662248 (Loss_D_real: 0.00486039 Loss_D_fake: 0.00176208) Loss_G: 0.39583951 Loss_Enh_Dec: -1.92911148\n",
      "| epoch 102 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.40 | ppl    11.04 | acc     0.71 | train_ae_norm     1.00\n",
      "[102/200][1899/4361] Loss_D: 0.00551953 (Loss_D_real: 0.00396452 Loss_D_fake: 0.00155501) Loss_G: 0.36665753 Loss_Enh_Dec: -2.23385978\n",
      "| epoch 102 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.45 | loss  2.46 | ppl    11.76 | acc     0.73 | train_ae_norm     1.00\n",
      "[102/200][1999/4361] Loss_D: 0.00233048 (Loss_D_real: 0.00172480 Loss_D_fake: 0.00060568) Loss_G: 0.41829395 Loss_Enh_Dec: -1.05876529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 102 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.65 | loss  2.40 | ppl    11.03 | acc     0.73 | train_ae_norm     1.00\n",
      "[102/200][2099/4361] Loss_D: 0.00127741 (Loss_D_real: 0.00056580 Loss_D_fake: 0.00071160) Loss_G: 0.38079411 Loss_Enh_Dec: -1.55986917\n",
      "| epoch 102 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.88 | loss  2.44 | ppl    11.48 | acc     0.75 | train_ae_norm     1.00\n",
      "[102/200][2199/4361] Loss_D: 0.00662809 (Loss_D_real: 0.00339115 Loss_D_fake: 0.00323694) Loss_G: 0.40770203 Loss_Enh_Dec: -1.84873283\n",
      "| epoch 102 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.34 | loss  2.42 | ppl    11.29 | acc     0.74 | train_ae_norm     1.00\n",
      "[102/200][2299/4361] Loss_D: 0.00079867 (Loss_D_real: 0.00020056 Loss_D_fake: 0.00059812) Loss_G: 0.54776841 Loss_Enh_Dec: -2.07946134\n",
      "| epoch 102 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.59 | loss  2.42 | ppl    11.21 | acc     0.72 | train_ae_norm     1.00\n",
      "[102/200][2399/4361] Loss_D: 0.02176580 (Loss_D_real: 0.02106472 Loss_D_fake: 0.00070108) Loss_G: 0.42589647 Loss_Enh_Dec: -2.18334937\n",
      "| epoch 102 |  2400/ 4361 batches | lr 0.000000 | ms/batch 400.41 | loss  2.44 | ppl    11.45 | acc     0.68 | train_ae_norm     1.00\n",
      "[102/200][2499/4361] Loss_D: 0.00371242 (Loss_D_real: 0.00139842 Loss_D_fake: 0.00231400) Loss_G: 0.41656014 Loss_Enh_Dec: -2.11983228\n",
      "| epoch 102 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.47 | ppl    11.84 | acc     0.72 | train_ae_norm     1.00\n",
      "[102/200][2599/4361] Loss_D: 0.00446577 (Loss_D_real: 0.00089421 Loss_D_fake: 0.00357156) Loss_G: 0.41588965 Loss_Enh_Dec: -2.15075040\n",
      "| epoch 102 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  2.43 | ppl    11.39 | acc     0.67 | train_ae_norm     1.00\n",
      "[102/200][2699/4361] Loss_D: 0.00070248 (Loss_D_real: 0.00069002 Loss_D_fake: 0.00001245) Loss_G: 0.66810495 Loss_Enh_Dec: -2.31530428\n",
      "| epoch 102 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  2.44 | ppl    11.47 | acc     0.70 | train_ae_norm     1.00\n",
      "[102/200][2799/4361] Loss_D: 0.00314096 (Loss_D_real: 0.00017911 Loss_D_fake: 0.00296185) Loss_G: 0.40677643 Loss_Enh_Dec: -1.97424150\n",
      "| epoch 102 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  2.42 | ppl    11.21 | acc     0.71 | train_ae_norm     1.00\n",
      "[102/200][2899/4361] Loss_D: 0.00053678 (Loss_D_real: 0.00003843 Loss_D_fake: 0.00049835) Loss_G: 0.40834269 Loss_Enh_Dec: -2.10740948\n",
      "| epoch 102 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.82 | loss  2.44 | ppl    11.49 | acc     0.70 | train_ae_norm     1.00\n",
      "[102/200][2999/4361] Loss_D: 0.00305847 (Loss_D_real: 0.00010911 Loss_D_fake: 0.00294936) Loss_G: 0.42538768 Loss_Enh_Dec: -2.03328991\n",
      "| epoch 102 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  2.45 | ppl    11.58 | acc     0.71 | train_ae_norm     1.00\n",
      "[102/200][3099/4361] Loss_D: 0.00295964 (Loss_D_real: 0.00210933 Loss_D_fake: 0.00085031) Loss_G: 0.47872621 Loss_Enh_Dec: -2.10383272\n",
      "| epoch 102 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  2.46 | ppl    11.68 | acc     0.71 | train_ae_norm     1.00\n",
      "[102/200][3199/4361] Loss_D: 0.00090405 (Loss_D_real: 0.00018901 Loss_D_fake: 0.00071504) Loss_G: 0.37361082 Loss_Enh_Dec: -2.27201295\n",
      "| epoch 102 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  2.47 | ppl    11.82 | acc     0.74 | train_ae_norm     1.00\n",
      "[102/200][3299/4361] Loss_D: 0.07176784 (Loss_D_real: 0.07137199 Loss_D_fake: 0.00039585) Loss_G: 0.43654448 Loss_Enh_Dec: -1.73808348\n",
      "| epoch 102 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  2.48 | ppl    12.00 | acc     0.70 | train_ae_norm     1.00\n",
      "[102/200][3399/4361] Loss_D: 0.01543710 (Loss_D_real: 0.01285838 Loss_D_fake: 0.00257873) Loss_G: 0.41878924 Loss_Enh_Dec: -1.40542912\n",
      "| epoch 102 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  2.48 | ppl    11.90 | acc     0.68 | train_ae_norm     1.00\n",
      "[102/200][3499/4361] Loss_D: 0.00639143 (Loss_D_real: 0.00356910 Loss_D_fake: 0.00282232) Loss_G: 0.40098867 Loss_Enh_Dec: -2.01778197\n",
      "| epoch 102 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  2.43 | ppl    11.32 | acc     0.68 | train_ae_norm     1.00\n",
      "[102/200][3599/4361] Loss_D: 0.03472785 (Loss_D_real: 0.03329813 Loss_D_fake: 0.00142971) Loss_G: 0.40751123 Loss_Enh_Dec: -2.26613665\n",
      "| epoch 102 |  3600/ 4361 batches | lr 0.000000 | ms/batch 402.27 | loss  2.45 | ppl    11.61 | acc     0.72 | train_ae_norm     1.00\n",
      "[102/200][3699/4361] Loss_D: 0.01023351 (Loss_D_real: 0.00872733 Loss_D_fake: 0.00150619) Loss_G: 0.35869732 Loss_Enh_Dec: -2.25388193\n",
      "| epoch 102 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  2.45 | ppl    11.61 | acc     0.67 | train_ae_norm     1.00\n",
      "[102/200][3799/4361] Loss_D: 0.01737519 (Loss_D_real: 0.00960705 Loss_D_fake: 0.00776814) Loss_G: 0.39067155 Loss_Enh_Dec: -2.56268978\n",
      "| epoch 102 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  2.46 | ppl    11.66 | acc     0.73 | train_ae_norm     1.00\n",
      "[102/200][3899/4361] Loss_D: 0.02260697 (Loss_D_real: 0.01823453 Loss_D_fake: 0.00437244) Loss_G: 0.32768160 Loss_Enh_Dec: -2.22110200\n",
      "| epoch 102 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  2.45 | ppl    11.64 | acc     0.69 | train_ae_norm     1.00\n",
      "[102/200][3999/4361] Loss_D: 0.00350753 (Loss_D_real: 0.00055590 Loss_D_fake: 0.00295163) Loss_G: 0.37951013 Loss_Enh_Dec: -1.68186116\n",
      "| epoch 102 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  2.43 | ppl    11.38 | acc     0.70 | train_ae_norm     1.00\n",
      "[102/200][4099/4361] Loss_D: 0.00300227 (Loss_D_real: 0.00014833 Loss_D_fake: 0.00285394) Loss_G: 0.39179981 Loss_Enh_Dec: -2.00370765\n",
      "| epoch 102 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.59 | loss  2.41 | ppl    11.10 | acc     0.71 | train_ae_norm     1.00\n",
      "[102/200][4199/4361] Loss_D: 0.00430158 (Loss_D_real: 0.00094118 Loss_D_fake: 0.00336040) Loss_G: 0.35714328 Loss_Enh_Dec: -2.67433476\n",
      "| epoch 102 |  4200/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  2.45 | ppl    11.61 | acc     0.74 | train_ae_norm     1.00\n",
      "[102/200][4299/4361] Loss_D: 0.00571909 (Loss_D_real: 0.00156134 Loss_D_fake: 0.00415775) Loss_G: 0.36061808 Loss_Enh_Dec: -2.01595211\n",
      "| epoch 102 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.42 | ppl    11.21 | acc     0.72 | train_ae_norm     1.00\n",
      "| end of epoch 102 | time: 1859.67s | test loss  2.47 | test ppl 11.86 | acc 0.752\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 103 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.699\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.485\n",
      "  Test Loss: 4.378\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 103 |     0/ 4361 batches | lr 0.000000 | ms/batch 865.90 | loss  0.02 | ppl     1.02 | acc     0.75 | train_ae_norm     1.00\n",
      "[103/200][299/4361] Loss_D: 0.01825023 (Loss_D_real: 0.01516248 Loss_D_fake: 0.00308775) Loss_G: 0.39974171 Loss_Enh_Dec: -2.19215584\n",
      "| epoch 103 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  2.46 | ppl    11.68 | acc     0.68 | train_ae_norm     1.00\n",
      "[103/200][399/4361] Loss_D: 0.00304221 (Loss_D_real: 0.00192515 Loss_D_fake: 0.00111707) Loss_G: 0.40898639 Loss_Enh_Dec: -1.33341050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 103 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  2.36 | ppl    10.57 | acc     0.71 | train_ae_norm     1.00\n",
      "[103/200][499/4361] Loss_D: 0.00979594 (Loss_D_real: 0.00004919 Loss_D_fake: 0.00974675) Loss_G: 0.36343017 Loss_Enh_Dec: -2.09943938\n",
      "| epoch 103 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  2.44 | ppl    11.49 | acc     0.72 | train_ae_norm     1.00\n",
      "[103/200][599/4361] Loss_D: 0.00615161 (Loss_D_real: 0.00525400 Loss_D_fake: 0.00089761) Loss_G: 0.45150396 Loss_Enh_Dec: -1.88142228\n",
      "| epoch 103 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  2.40 | ppl    11.00 | acc     0.71 | train_ae_norm     1.00\n",
      "[103/200][699/4361] Loss_D: 0.00301995 (Loss_D_real: 0.00144290 Loss_D_fake: 0.00157705) Loss_G: 0.41107589 Loss_Enh_Dec: -2.31865311\n",
      "| epoch 103 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.33 | loss  2.45 | ppl    11.58 | acc     0.71 | train_ae_norm     1.00\n",
      "[103/200][799/4361] Loss_D: 0.00171465 (Loss_D_real: 0.00070357 Loss_D_fake: 0.00101108) Loss_G: 0.39562735 Loss_Enh_Dec: -1.77075505\n",
      "| epoch 103 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.55 | loss  2.42 | ppl    11.27 | acc     0.71 | train_ae_norm     1.00\n",
      "[103/200][899/4361] Loss_D: 0.01681758 (Loss_D_real: 0.01451190 Loss_D_fake: 0.00230568) Loss_G: 0.34720460 Loss_Enh_Dec: -2.01951647\n",
      "| epoch 103 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  2.44 | ppl    11.49 | acc     0.75 | train_ae_norm     1.00\n",
      "[103/200][999/4361] Loss_D: 0.02659162 (Loss_D_real: 0.02523239 Loss_D_fake: 0.00135922) Loss_G: 0.39700380 Loss_Enh_Dec: -2.40187979\n",
      "| epoch 103 |  1000/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.43 | ppl    11.39 | acc     0.72 | train_ae_norm     1.00\n",
      "[103/200][1099/4361] Loss_D: 0.01345914 (Loss_D_real: 0.00045009 Loss_D_fake: 0.01300905) Loss_G: 0.40676862 Loss_Enh_Dec: -1.52515018\n",
      "| epoch 103 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  2.45 | ppl    11.56 | acc     0.69 | train_ae_norm     1.00\n",
      "[103/200][1199/4361] Loss_D: 0.05458501 (Loss_D_real: 0.05045453 Loss_D_fake: 0.00413048) Loss_G: 0.82380247 Loss_Enh_Dec: -1.80106068\n",
      "| epoch 103 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.35 | loss  2.46 | ppl    11.66 | acc     0.73 | train_ae_norm     1.00\n",
      "[103/200][1299/4361] Loss_D: 0.00092069 (Loss_D_real: 0.00024457 Loss_D_fake: 0.00067612) Loss_G: 0.66547203 Loss_Enh_Dec: -1.88341260\n",
      "| epoch 103 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  2.48 | ppl    11.95 | acc     0.73 | train_ae_norm     1.00\n",
      "[103/200][1399/4361] Loss_D: 0.00409568 (Loss_D_real: 0.00191871 Loss_D_fake: 0.00217697) Loss_G: 0.37045792 Loss_Enh_Dec: -2.09612799\n",
      "| epoch 103 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  2.46 | ppl    11.66 | acc     0.69 | train_ae_norm     1.00\n",
      "[103/200][1499/4361] Loss_D: 0.01378118 (Loss_D_real: 0.00394588 Loss_D_fake: 0.00983530) Loss_G: 0.42817622 Loss_Enh_Dec: -1.75387561\n",
      "| epoch 103 |  1500/ 4361 batches | lr 0.000000 | ms/batch 400.35 | loss  2.50 | ppl    12.13 | acc     0.68 | train_ae_norm     1.00\n",
      "[103/200][1599/4361] Loss_D: 0.02188876 (Loss_D_real: 0.01760528 Loss_D_fake: 0.00428348) Loss_G: 0.44143000 Loss_Enh_Dec: -1.47188139\n",
      "| epoch 103 |  1600/ 4361 batches | lr 0.000000 | ms/batch 400.84 | loss  2.47 | ppl    11.81 | acc     0.69 | train_ae_norm     1.00\n",
      "[103/200][1699/4361] Loss_D: 0.00416664 (Loss_D_real: 0.00144734 Loss_D_fake: 0.00271930) Loss_G: 0.34937054 Loss_Enh_Dec: -2.21463752\n",
      "| epoch 103 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.68 | loss  2.42 | ppl    11.30 | acc     0.71 | train_ae_norm     1.00\n",
      "[103/200][1799/4361] Loss_D: 0.00547754 (Loss_D_real: 0.00061404 Loss_D_fake: 0.00486350) Loss_G: 0.35465929 Loss_Enh_Dec: -1.76545167\n",
      "| epoch 103 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  2.40 | ppl    11.07 | acc     0.71 | train_ae_norm     1.00\n",
      "[103/200][1899/4361] Loss_D: 0.00272971 (Loss_D_real: 0.00145206 Loss_D_fake: 0.00127766) Loss_G: 0.53778851 Loss_Enh_Dec: -1.68877375\n",
      "| epoch 103 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.80 | loss  2.45 | ppl    11.62 | acc     0.74 | train_ae_norm     1.00\n",
      "[103/200][1999/4361] Loss_D: 0.00207845 (Loss_D_real: 0.00028876 Loss_D_fake: 0.00178969) Loss_G: 0.35870859 Loss_Enh_Dec: -1.78698337\n",
      "| epoch 103 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.41 | ppl    11.11 | acc     0.73 | train_ae_norm     1.00\n",
      "[103/200][2099/4361] Loss_D: 0.00481492 (Loss_D_real: 0.00199677 Loss_D_fake: 0.00281815) Loss_G: 0.32887253 Loss_Enh_Dec: -1.95763171\n",
      "| epoch 103 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  2.45 | ppl    11.53 | acc     0.75 | train_ae_norm     1.00\n",
      "[103/200][2199/4361] Loss_D: 0.00528192 (Loss_D_real: 0.00174219 Loss_D_fake: 0.00353973) Loss_G: 0.33591723 Loss_Enh_Dec: -2.32153201\n",
      "| epoch 103 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.54 | loss  2.41 | ppl    11.11 | acc     0.72 | train_ae_norm     1.00\n",
      "[103/200][2299/4361] Loss_D: 0.00580314 (Loss_D_real: 0.00107003 Loss_D_fake: 0.00473311) Loss_G: 0.36202335 Loss_Enh_Dec: -1.96651578\n",
      "| epoch 103 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.91 | loss  2.42 | ppl    11.27 | acc     0.72 | train_ae_norm     1.00\n",
      "[103/200][2399/4361] Loss_D: 0.00444678 (Loss_D_real: 0.00052853 Loss_D_fake: 0.00391824) Loss_G: 0.35740337 Loss_Enh_Dec: -1.88718760\n",
      "| epoch 103 |  2400/ 4361 batches | lr 0.000000 | ms/batch 400.74 | loss  2.42 | ppl    11.23 | acc     0.69 | train_ae_norm     1.00\n",
      "[103/200][2499/4361] Loss_D: 0.00290783 (Loss_D_real: 0.00084771 Loss_D_fake: 0.00206012) Loss_G: 0.36191356 Loss_Enh_Dec: -1.90122032\n",
      "| epoch 103 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  2.44 | ppl    11.51 | acc     0.72 | train_ae_norm     1.00\n",
      "[103/200][2599/4361] Loss_D: 0.00238572 (Loss_D_real: 0.00028295 Loss_D_fake: 0.00210277) Loss_G: 0.35806009 Loss_Enh_Dec: -2.03927302\n",
      "| epoch 103 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.41 | loss  2.40 | ppl    10.99 | acc     0.68 | train_ae_norm     1.00\n",
      "[103/200][2699/4361] Loss_D: 0.00292639 (Loss_D_real: 0.00048400 Loss_D_fake: 0.00244239) Loss_G: 0.35429284 Loss_Enh_Dec: -1.94916284\n",
      "| epoch 103 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  2.40 | ppl    11.06 | acc     0.70 | train_ae_norm     1.00\n",
      "[103/200][2799/4361] Loss_D: 0.00335572 (Loss_D_real: 0.00144167 Loss_D_fake: 0.00191406) Loss_G: 0.35645196 Loss_Enh_Dec: -2.57816505\n",
      "| epoch 103 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.89 | loss  2.36 | ppl    10.56 | acc     0.70 | train_ae_norm     1.00\n",
      "[103/200][2899/4361] Loss_D: 0.03313410 (Loss_D_real: 0.00177869 Loss_D_fake: 0.03135541) Loss_G: 0.48574463 Loss_Enh_Dec: -1.36296308\n",
      "| epoch 103 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.51 | loss  2.38 | ppl    10.78 | acc     0.72 | train_ae_norm     1.00\n",
      "[103/200][2999/4361] Loss_D: 0.00364303 (Loss_D_real: 0.00116200 Loss_D_fake: 0.00248103) Loss_G: 0.37614265 Loss_Enh_Dec: -1.66284406\n",
      "| epoch 103 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  2.40 | ppl    11.02 | acc     0.71 | train_ae_norm     1.00\n",
      "[103/200][3099/4361] Loss_D: 0.00188389 (Loss_D_real: 0.00098951 Loss_D_fake: 0.00089439) Loss_G: 0.44514504 Loss_Enh_Dec: -1.59305680\n",
      "| epoch 103 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.91 | loss  2.39 | ppl    10.96 | acc     0.70 | train_ae_norm     1.00\n",
      "[103/200][3199/4361] Loss_D: 0.00599279 (Loss_D_real: 0.00032896 Loss_D_fake: 0.00566384) Loss_G: 0.44262668 Loss_Enh_Dec: -1.84199584\n",
      "| epoch 103 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.81 | loss  2.41 | ppl    11.19 | acc     0.74 | train_ae_norm     1.00\n",
      "[103/200][3299/4361] Loss_D: 0.00332624 (Loss_D_real: 0.00150187 Loss_D_fake: 0.00182437) Loss_G: 0.37653336 Loss_Enh_Dec: -1.87278485\n",
      "| epoch 103 |  3300/ 4361 batches | lr 0.000000 | ms/batch 400.88 | loss  2.42 | ppl    11.30 | acc     0.73 | train_ae_norm     1.00\n",
      "[103/200][3399/4361] Loss_D: 0.00495705 (Loss_D_real: 0.00298944 Loss_D_fake: 0.00196761) Loss_G: 0.41153309 Loss_Enh_Dec: -1.71151638\n",
      "| epoch 103 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.50 | loss  2.43 | ppl    11.38 | acc     0.70 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[103/200][3499/4361] Loss_D: 0.00221945 (Loss_D_real: 0.00012018 Loss_D_fake: 0.00209927) Loss_G: 0.44205004 Loss_Enh_Dec: -1.79065025\n",
      "| epoch 103 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.36 | loss  2.40 | ppl    10.97 | acc     0.68 | train_ae_norm     1.00\n",
      "[103/200][3599/4361] Loss_D: 0.01706174 (Loss_D_real: 0.01599052 Loss_D_fake: 0.00107122) Loss_G: 0.39909098 Loss_Enh_Dec: -1.79095268\n",
      "| epoch 103 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  2.39 | ppl    10.92 | acc     0.73 | train_ae_norm     1.00\n",
      "[103/200][3699/4361] Loss_D: 0.00652185 (Loss_D_real: 0.00012956 Loss_D_fake: 0.00639229) Loss_G: 0.38558337 Loss_Enh_Dec: -1.71096575\n",
      "| epoch 103 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.40 | loss  2.42 | ppl    11.21 | acc     0.70 | train_ae_norm     1.00\n",
      "[103/200][3799/4361] Loss_D: 0.00321141 (Loss_D_real: 0.00027617 Loss_D_fake: 0.00293524) Loss_G: 0.39580646 Loss_Enh_Dec: -2.07810187\n",
      "| epoch 103 |  3800/ 4361 batches | lr 0.000000 | ms/batch 400.91 | loss  2.41 | ppl    11.16 | acc     0.76 | train_ae_norm     1.00\n",
      "[103/200][3899/4361] Loss_D: 0.00753998 (Loss_D_real: 0.00594420 Loss_D_fake: 0.00159578) Loss_G: 0.40899140 Loss_Enh_Dec: -1.76190591\n",
      "| epoch 103 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.61 | loss  2.42 | ppl    11.30 | acc     0.70 | train_ae_norm     1.00\n",
      "[103/200][3999/4361] Loss_D: 0.00517270 (Loss_D_real: 0.00251560 Loss_D_fake: 0.00265710) Loss_G: 0.45710826 Loss_Enh_Dec: -1.44215763\n",
      "| epoch 103 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  2.42 | ppl    11.19 | acc     0.72 | train_ae_norm     1.00\n",
      "[103/200][4099/4361] Loss_D: 0.01490390 (Loss_D_real: 0.00399610 Loss_D_fake: 0.01090780) Loss_G: 0.41369343 Loss_Enh_Dec: -1.54991913\n",
      "| epoch 103 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.88 | loss  2.40 | ppl    10.97 | acc     0.69 | train_ae_norm     1.00\n",
      "[103/200][4199/4361] Loss_D: 0.01078550 (Loss_D_real: 0.00925495 Loss_D_fake: 0.00153055) Loss_G: 0.47462639 Loss_Enh_Dec: -1.69190633\n",
      "| epoch 103 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  2.43 | ppl    11.31 | acc     0.75 | train_ae_norm     1.00\n",
      "[103/200][4299/4361] Loss_D: 0.04282697 (Loss_D_real: 0.04157688 Loss_D_fake: 0.00125009) Loss_G: 0.35518780 Loss_Enh_Dec: -1.63208735\n",
      "| epoch 103 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.41 | ppl    11.14 | acc     0.71 | train_ae_norm     1.00\n",
      "| end of epoch 103 | time: 1850.75s | test loss  2.45 | test ppl 11.59 | acc 0.755\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 104 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.698\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.485\n",
      "  Test Loss: 4.466\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 104 |     0/ 4361 batches | lr 0.000000 | ms/batch 867.17 | loss  0.02 | ppl     1.02 | acc     0.77 | train_ae_norm     1.00\n",
      "[104/200][99/4361] Loss_D: 0.00253285 (Loss_D_real: 0.00107040 Loss_D_fake: 0.00146246) Loss_G: 0.34556291 Loss_Enh_Dec: -2.01180887\n",
      "| epoch 104 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.41 | ppl    11.09 | acc     0.67 | train_ae_norm     1.00\n",
      "[104/200][199/4361] Loss_D: 0.00319248 (Loss_D_real: 0.00266301 Loss_D_fake: 0.00052947) Loss_G: 0.44368887 Loss_Enh_Dec: -1.97893715\n",
      "| epoch 104 |   200/ 4361 batches | lr 0.000000 | ms/batch 400.83 | loss  2.44 | ppl    11.42 | acc     0.73 | train_ae_norm     1.00\n",
      "[104/200][299/4361] Loss_D: 0.00389312 (Loss_D_real: 0.00344026 Loss_D_fake: 0.00045287) Loss_G: 0.42807713 Loss_Enh_Dec: -2.22868204\n",
      "| epoch 104 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  2.47 | ppl    11.88 | acc     0.67 | train_ae_norm     1.00\n",
      "[104/200][399/4361] Loss_D: 0.00107069 (Loss_D_real: 0.00014404 Loss_D_fake: 0.00092665) Loss_G: 0.46794280 Loss_Enh_Dec: -1.95685041\n",
      "| epoch 104 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  2.37 | ppl    10.65 | acc     0.72 | train_ae_norm     1.00\n",
      "[104/200][499/4361] Loss_D: 0.00676377 (Loss_D_real: 0.00597431 Loss_D_fake: 0.00078947) Loss_G: 0.39927039 Loss_Enh_Dec: -2.00009751\n",
      "| epoch 104 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.22 | loss  2.45 | ppl    11.59 | acc     0.75 | train_ae_norm     1.00\n",
      "[104/200][599/4361] Loss_D: 0.00453251 (Loss_D_real: 0.00344366 Loss_D_fake: 0.00108885) Loss_G: 0.66894811 Loss_Enh_Dec: -2.01410460\n",
      "| epoch 104 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.82 | loss  2.42 | ppl    11.30 | acc     0.69 | train_ae_norm     1.00\n",
      "[104/200][699/4361] Loss_D: 0.00110681 (Loss_D_real: 0.00042233 Loss_D_fake: 0.00068448) Loss_G: 0.48393092 Loss_Enh_Dec: -2.10975456\n",
      "| epoch 104 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  2.46 | ppl    11.69 | acc     0.72 | train_ae_norm     1.00\n",
      "[104/200][799/4361] Loss_D: 0.00194983 (Loss_D_real: 0.00122639 Loss_D_fake: 0.00072344) Loss_G: 0.40429583 Loss_Enh_Dec: -1.90773773\n",
      "| epoch 104 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.81 | loss  2.45 | ppl    11.61 | acc     0.71 | train_ae_norm     1.00\n",
      "[104/200][899/4361] Loss_D: 0.00115331 (Loss_D_real: 0.00009049 Loss_D_fake: 0.00106282) Loss_G: 0.41527382 Loss_Enh_Dec: -1.98511207\n",
      "| epoch 104 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.66 | loss  2.44 | ppl    11.52 | acc     0.74 | train_ae_norm     1.00\n",
      "[104/200][999/4361] Loss_D: 0.00133219 (Loss_D_real: 0.00038392 Loss_D_fake: 0.00094827) Loss_G: 0.47489461 Loss_Enh_Dec: -2.06828499\n",
      "| epoch 104 |  1000/ 4361 batches | lr 0.000000 | ms/batch 400.36 | loss  2.44 | ppl    11.43 | acc     0.73 | train_ae_norm     1.00\n",
      "[104/200][1099/4361] Loss_D: 0.00206936 (Loss_D_real: 0.00086827 Loss_D_fake: 0.00120109) Loss_G: 0.52900916 Loss_Enh_Dec: -1.30693209\n",
      "| epoch 104 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  2.42 | ppl    11.25 | acc     0.71 | train_ae_norm     1.00\n",
      "[104/200][1199/4361] Loss_D: 0.00400261 (Loss_D_real: 0.00163116 Loss_D_fake: 0.00237145) Loss_G: 0.39157864 Loss_Enh_Dec: -1.61980820\n",
      "| epoch 104 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  2.46 | ppl    11.73 | acc     0.74 | train_ae_norm     1.00\n",
      "[104/200][1299/4361] Loss_D: 0.03254625 (Loss_D_real: 0.03151451 Loss_D_fake: 0.00103174) Loss_G: 0.38451457 Loss_Enh_Dec: -1.49674106\n",
      "| epoch 104 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  2.50 | ppl    12.14 | acc     0.71 | train_ae_norm     1.00\n",
      "[104/200][1399/4361] Loss_D: 0.00507472 (Loss_D_real: 0.00464663 Loss_D_fake: 0.00042809) Loss_G: 0.43153438 Loss_Enh_Dec: -1.99567115\n",
      "| epoch 104 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  2.58 | ppl    13.19 | acc     0.59 | train_ae_norm     1.00\n",
      "[104/200][1499/4361] Loss_D: 0.00184261 (Loss_D_real: 0.00077469 Loss_D_fake: 0.00106791) Loss_G: 0.44153139 Loss_Enh_Dec: -2.24012733\n",
      "| epoch 104 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  2.72 | ppl    15.13 | acc     0.71 | train_ae_norm     1.00\n",
      "[104/200][1599/4361] Loss_D: 0.00328917 (Loss_D_real: 0.00038953 Loss_D_fake: 0.00289964) Loss_G: 0.51883858 Loss_Enh_Dec: -2.30194521\n",
      "| epoch 104 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  2.53 | ppl    12.57 | acc     0.69 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104/200][1699/4361] Loss_D: 0.00135893 (Loss_D_real: 0.00083499 Loss_D_fake: 0.00052394) Loss_G: 0.40400329 Loss_Enh_Dec: -2.12154627\n",
      "| epoch 104 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.46 | ppl    11.74 | acc     0.67 | train_ae_norm     1.00\n",
      "[104/200][1799/4361] Loss_D: 0.00181788 (Loss_D_real: 0.00128901 Loss_D_fake: 0.00052887) Loss_G: 0.47481933 Loss_Enh_Dec: -2.05382776\n",
      "| epoch 104 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.41 | ppl    11.09 | acc     0.69 | train_ae_norm     1.00\n",
      "[104/200][1899/4361] Loss_D: 0.00261856 (Loss_D_real: 0.00165356 Loss_D_fake: 0.00096500) Loss_G: 0.46198541 Loss_Enh_Dec: -2.07578254\n",
      "| epoch 104 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.83 | loss  2.47 | ppl    11.86 | acc     0.72 | train_ae_norm     1.00\n",
      "[104/200][1999/4361] Loss_D: 0.00859509 (Loss_D_real: 0.00772179 Loss_D_fake: 0.00087329) Loss_G: 0.52414298 Loss_Enh_Dec: -2.12763119\n",
      "| epoch 104 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.43 | ppl    11.34 | acc     0.75 | train_ae_norm     1.00\n",
      "[104/200][2099/4361] Loss_D: 0.00304855 (Loss_D_real: 0.00122895 Loss_D_fake: 0.00181960) Loss_G: 0.45274934 Loss_Enh_Dec: -2.04166412\n",
      "| epoch 104 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  2.44 | ppl    11.43 | acc     0.75 | train_ae_norm     1.00\n",
      "[104/200][2199/4361] Loss_D: 0.00958272 (Loss_D_real: 0.00865126 Loss_D_fake: 0.00093146) Loss_G: 0.48184538 Loss_Enh_Dec: -1.84542298\n",
      "| epoch 104 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  2.42 | ppl    11.27 | acc     0.73 | train_ae_norm     1.00\n",
      "[104/200][2299/4361] Loss_D: 0.00478586 (Loss_D_real: 0.00350613 Loss_D_fake: 0.00127973) Loss_G: 0.38649547 Loss_Enh_Dec: -2.12582994\n",
      "| epoch 104 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.42 | ppl    11.24 | acc     0.72 | train_ae_norm     1.00\n",
      "[104/200][2399/4361] Loss_D: 0.03035658 (Loss_D_real: 0.01758067 Loss_D_fake: 0.01277591) Loss_G: 0.41484815 Loss_Enh_Dec: -2.00920272\n",
      "| epoch 104 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  2.41 | ppl    11.12 | acc     0.69 | train_ae_norm     1.00\n",
      "[104/200][2499/4361] Loss_D: 0.01143397 (Loss_D_real: 0.00703128 Loss_D_fake: 0.00440269) Loss_G: 0.37574127 Loss_Enh_Dec: -2.08641505\n",
      "| epoch 104 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  2.44 | ppl    11.50 | acc     0.74 | train_ae_norm     1.00\n",
      "[104/200][2599/4361] Loss_D: 0.05619578 (Loss_D_real: 0.05374301 Loss_D_fake: 0.00245277) Loss_G: 0.45874387 Loss_Enh_Dec: -1.78072166\n",
      "| epoch 104 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.43 | ppl    11.31 | acc     0.68 | train_ae_norm     1.00\n",
      "[104/200][2699/4361] Loss_D: 0.00435073 (Loss_D_real: 0.00425887 Loss_D_fake: 0.00009186) Loss_G: 0.76005787 Loss_Enh_Dec: -2.09912181\n",
      "| epoch 104 |  2700/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  2.43 | ppl    11.38 | acc     0.70 | train_ae_norm     1.00\n",
      "[104/200][2799/4361] Loss_D: 0.00492966 (Loss_D_real: 0.00009908 Loss_D_fake: 0.00483058) Loss_G: 0.37694007 Loss_Enh_Dec: -2.09273648\n",
      "| epoch 104 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  2.40 | ppl    11.03 | acc     0.70 | train_ae_norm     1.00\n",
      "[104/200][2899/4361] Loss_D: 0.00272630 (Loss_D_real: 0.00024291 Loss_D_fake: 0.00248339) Loss_G: 0.41751263 Loss_Enh_Dec: -1.81303155\n",
      "| epoch 104 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  2.41 | ppl    11.18 | acc     0.71 | train_ae_norm     1.00\n",
      "[104/200][2999/4361] Loss_D: 0.00263250 (Loss_D_real: 0.00021283 Loss_D_fake: 0.00241967) Loss_G: 0.36770427 Loss_Enh_Dec: -1.54110181\n",
      "| epoch 104 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  2.43 | ppl    11.35 | acc     0.69 | train_ae_norm     1.00\n",
      "[104/200][3099/4361] Loss_D: 0.00631507 (Loss_D_real: 0.00283402 Loss_D_fake: 0.00348106) Loss_G: 0.39388549 Loss_Enh_Dec: -1.62558401\n",
      "| epoch 104 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.44 | ppl    11.43 | acc     0.69 | train_ae_norm     1.00\n",
      "[104/200][3199/4361] Loss_D: 0.00691201 (Loss_D_real: 0.00482840 Loss_D_fake: 0.00208361) Loss_G: 0.46854869 Loss_Enh_Dec: -1.44639575\n",
      "| epoch 104 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  2.48 | ppl    11.93 | acc     0.71 | train_ae_norm     1.00\n",
      "[104/200][3299/4361] Loss_D: 0.00317287 (Loss_D_real: 0.00196199 Loss_D_fake: 0.00121089) Loss_G: 0.39020622 Loss_Enh_Dec: -1.51738012\n",
      "| epoch 104 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  2.48 | ppl    11.93 | acc     0.71 | train_ae_norm     1.00\n",
      "[104/200][3399/4361] Loss_D: 0.00567214 (Loss_D_real: 0.00031831 Loss_D_fake: 0.00535383) Loss_G: 0.32953709 Loss_Enh_Dec: -1.71862221\n",
      "| epoch 104 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.84 | loss  2.46 | ppl    11.69 | acc     0.70 | train_ae_norm     1.00\n",
      "[104/200][3499/4361] Loss_D: 0.00341259 (Loss_D_real: 0.00178888 Loss_D_fake: 0.00162371) Loss_G: 0.34422585 Loss_Enh_Dec: -1.18325806\n",
      "| epoch 104 |  3500/ 4361 batches | lr 0.000000 | ms/batch 402.04 | loss  2.43 | ppl    11.35 | acc     0.69 | train_ae_norm     1.00\n",
      "[104/200][3599/4361] Loss_D: 0.00082345 (Loss_D_real: 0.00044470 Loss_D_fake: 0.00037875) Loss_G: 0.52915508 Loss_Enh_Dec: -1.70888805\n",
      "| epoch 104 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  2.45 | ppl    11.56 | acc     0.73 | train_ae_norm     1.00\n",
      "[104/200][3699/4361] Loss_D: 0.00320544 (Loss_D_real: 0.00066154 Loss_D_fake: 0.00254390) Loss_G: 0.38548622 Loss_Enh_Dec: -1.40039587\n",
      "| epoch 104 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  2.46 | ppl    11.74 | acc     0.67 | train_ae_norm     1.00\n",
      "[104/200][3799/4361] Loss_D: 0.00268183 (Loss_D_real: 0.00002736 Loss_D_fake: 0.00265447) Loss_G: 0.38150689 Loss_Enh_Dec: -1.72807682\n",
      "| epoch 104 |  3800/ 4361 batches | lr 0.000000 | ms/batch 402.48 | loss  2.48 | ppl    11.90 | acc     0.76 | train_ae_norm     1.00\n",
      "[104/200][3899/4361] Loss_D: 0.01433510 (Loss_D_real: 0.01430774 Loss_D_fake: 0.00002736) Loss_G: 0.65925437 Loss_Enh_Dec: -1.95926630\n",
      "| epoch 104 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  2.46 | ppl    11.67 | acc     0.68 | train_ae_norm     1.00\n",
      "[104/200][3999/4361] Loss_D: 0.00682453 (Loss_D_real: 0.00390346 Loss_D_fake: 0.00292107) Loss_G: 0.36987075 Loss_Enh_Dec: -1.87603724\n",
      "| epoch 104 |  4000/ 4361 batches | lr 0.000000 | ms/batch 402.17 | loss  2.46 | ppl    11.67 | acc     0.71 | train_ae_norm     1.00\n",
      "[104/200][4099/4361] Loss_D: 0.00292208 (Loss_D_real: 0.00130584 Loss_D_fake: 0.00161624) Loss_G: 0.37767050 Loss_Enh_Dec: -2.04791617\n",
      "| epoch 104 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  2.43 | ppl    11.34 | acc     0.72 | train_ae_norm     1.00\n",
      "[104/200][4199/4361] Loss_D: 0.00402923 (Loss_D_real: 0.00241648 Loss_D_fake: 0.00161275) Loss_G: 0.44649217 Loss_Enh_Dec: -1.74439621\n",
      "| epoch 104 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  2.45 | ppl    11.60 | acc     0.74 | train_ae_norm     1.00\n",
      "[104/200][4299/4361] Loss_D: 0.00515286 (Loss_D_real: 0.00379591 Loss_D_fake: 0.00135694) Loss_G: 0.39032862 Loss_Enh_Dec: -1.20071745\n",
      "| epoch 104 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  2.44 | ppl    11.48 | acc     0.73 | train_ae_norm     1.00\n",
      "| end of epoch 104 | time: 1852.67s | test loss  2.45 | test ppl 11.57 | acc 0.753\n",
      "bleu_self:  [4.28659364e-01 1.71678513e-01 1.43493612e-06 4.50010940e-09\n",
      " 2.56644782e-09]\n",
      "bleu_test:  [8.77008928e-01 1.52580632e-01 9.65535807e-07 2.51409665e-09\n",
      " 7.81153937e-11]\n",
      "bleu_self: [0.42865936,0.17167851,0.00000143,0.00000000,0.00000000]\n",
      "bleu_test: [0.87700893,0.15258063,0.00000097,0.00000000,0.00000000]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 105 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.697\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.485\n",
      "  Test Loss: 4.487\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 105 |     0/ 4361 batches | lr 0.000000 | ms/batch 866.04 | loss  0.02 | ppl     1.02 | acc     0.74 | train_ae_norm     1.00\n",
      "[105/200][99/4361] Loss_D: 0.01631919 (Loss_D_real: 0.01246710 Loss_D_fake: 0.00385208) Loss_G: 0.35044572 Loss_Enh_Dec: -1.54336727\n",
      "| epoch 105 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  2.44 | ppl    11.51 | acc     0.67 | train_ae_norm     1.00\n",
      "[105/200][199/4361] Loss_D: 0.01163985 (Loss_D_real: 0.01093717 Loss_D_fake: 0.00070268) Loss_G: 0.35405141 Loss_Enh_Dec: -1.51857305\n",
      "| epoch 105 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  2.48 | ppl    11.96 | acc     0.72 | train_ae_norm     1.00\n",
      "[105/200][299/4361] Loss_D: 0.00182706 (Loss_D_real: 0.00143808 Loss_D_fake: 0.00038898) Loss_G: 0.41107282 Loss_Enh_Dec: -1.82150638\n",
      "| epoch 105 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.83 | loss  2.48 | ppl    11.91 | acc     0.66 | train_ae_norm     1.00\n",
      "[105/200][399/4361] Loss_D: 0.00644371 (Loss_D_real: 0.00422743 Loss_D_fake: 0.00221628) Loss_G: 0.34595433 Loss_Enh_Dec: -1.85421371\n",
      "| epoch 105 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.92 | loss  2.39 | ppl    10.95 | acc     0.71 | train_ae_norm     1.00\n",
      "[105/200][499/4361] Loss_D: 0.00226060 (Loss_D_real: 0.00036732 Loss_D_fake: 0.00189328) Loss_G: 0.46762896 Loss_Enh_Dec: -2.01977158\n",
      "| epoch 105 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.85 | loss  2.46 | ppl    11.72 | acc     0.72 | train_ae_norm     1.00\n",
      "[105/200][599/4361] Loss_D: 0.00276279 (Loss_D_real: 0.00203687 Loss_D_fake: 0.00072593) Loss_G: 0.53429794 Loss_Enh_Dec: -1.89937782\n",
      "| epoch 105 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.41 | ppl    11.15 | acc     0.70 | train_ae_norm     1.00\n",
      "[105/200][699/4361] Loss_D: 0.00197118 (Loss_D_real: 0.00046983 Loss_D_fake: 0.00150136) Loss_G: 0.45427799 Loss_Enh_Dec: -1.97363889\n",
      "| epoch 105 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  2.46 | ppl    11.73 | acc     0.71 | train_ae_norm     1.00\n",
      "[105/200][799/4361] Loss_D: 0.00188171 (Loss_D_real: 0.00099146 Loss_D_fake: 0.00089025) Loss_G: 0.38141793 Loss_Enh_Dec: -1.89419556\n",
      "| epoch 105 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  2.45 | ppl    11.59 | acc     0.73 | train_ae_norm     1.00\n",
      "[105/200][899/4361] Loss_D: 0.02912792 (Loss_D_real: 0.02131114 Loss_D_fake: 0.00781678) Loss_G: 0.82483834 Loss_Enh_Dec: -1.62665904\n",
      "| epoch 105 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  2.46 | ppl    11.73 | acc     0.73 | train_ae_norm     1.00\n",
      "[105/200][999/4361] Loss_D: 0.00585876 (Loss_D_real: 0.00396095 Loss_D_fake: 0.00189781) Loss_G: 0.50836051 Loss_Enh_Dec: -1.61392212\n",
      "| epoch 105 |  1000/ 4361 batches | lr 0.000000 | ms/batch 400.89 | loss  2.47 | ppl    11.79 | acc     0.72 | train_ae_norm     1.00\n",
      "[105/200][1099/4361] Loss_D: 0.00610313 (Loss_D_real: 0.00024923 Loss_D_fake: 0.00585390) Loss_G: 0.33068073 Loss_Enh_Dec: -1.60966551\n",
      "| epoch 105 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  2.46 | ppl    11.66 | acc     0.68 | train_ae_norm     1.00\n",
      "[105/200][1199/4361] Loss_D: 0.00403634 (Loss_D_real: 0.00010247 Loss_D_fake: 0.00393386) Loss_G: 0.41425404 Loss_Enh_Dec: -1.50434911\n",
      "| epoch 105 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.49 | ppl    12.01 | acc     0.72 | train_ae_norm     1.00\n",
      "[105/200][1299/4361] Loss_D: 0.00998079 (Loss_D_real: 0.00797669 Loss_D_fake: 0.00200410) Loss_G: 0.34927192 Loss_Enh_Dec: -1.65469587\n",
      "| epoch 105 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  2.49 | ppl    12.07 | acc     0.71 | train_ae_norm     1.00\n",
      "[105/200][1399/4361] Loss_D: 0.00176032 (Loss_D_real: 0.00015021 Loss_D_fake: 0.00161011) Loss_G: 0.38579312 Loss_Enh_Dec: -1.81545091\n",
      "| epoch 105 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.50 | ppl    12.13 | acc     0.64 | train_ae_norm     1.00\n",
      "[105/200][1499/4361] Loss_D: 0.00329228 (Loss_D_real: 0.00091568 Loss_D_fake: 0.00237660) Loss_G: 0.36794677 Loss_Enh_Dec: -1.29315555\n",
      "| epoch 105 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  2.52 | ppl    12.39 | acc     0.69 | train_ae_norm     1.00\n",
      "[105/200][1599/4361] Loss_D: 0.00349646 (Loss_D_real: 0.00060302 Loss_D_fake: 0.00289345) Loss_G: 0.49806720 Loss_Enh_Dec: -1.53703487\n",
      "| epoch 105 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.50 | ppl    12.23 | acc     0.69 | train_ae_norm     1.00\n",
      "[105/200][1699/4361] Loss_D: 0.00460835 (Loss_D_real: 0.00159988 Loss_D_fake: 0.00300847) Loss_G: 0.33411917 Loss_Enh_Dec: -2.04647350\n",
      "| epoch 105 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  2.46 | ppl    11.70 | acc     0.69 | train_ae_norm     1.00\n",
      "[105/200][1799/4361] Loss_D: 0.00438275 (Loss_D_real: 0.00028801 Loss_D_fake: 0.00409474) Loss_G: 0.34967017 Loss_Enh_Dec: -0.54849982\n",
      "| epoch 105 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  2.44 | ppl    11.47 | acc     0.71 | train_ae_norm     1.00\n",
      "[105/200][1899/4361] Loss_D: 0.00264912 (Loss_D_real: 0.00036705 Loss_D_fake: 0.00228207) Loss_G: 0.41482130 Loss_Enh_Dec: -1.28422761\n",
      "| epoch 105 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  2.48 | ppl    11.95 | acc     0.71 | train_ae_norm     1.00\n",
      "[105/200][1999/4361] Loss_D: 0.01195241 (Loss_D_real: 0.01044583 Loss_D_fake: 0.00150657) Loss_G: 0.44863826 Loss_Enh_Dec: -2.31348419\n",
      "| epoch 105 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  2.45 | ppl    11.58 | acc     0.72 | train_ae_norm     1.00\n",
      "[105/200][2099/4361] Loss_D: 0.00268300 (Loss_D_real: 0.00211106 Loss_D_fake: 0.00057193) Loss_G: 0.69930804 Loss_Enh_Dec: -1.78367317\n",
      "| epoch 105 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  2.51 | ppl    12.26 | acc     0.75 | train_ae_norm     1.00\n",
      "[105/200][2199/4361] Loss_D: 0.00374067 (Loss_D_real: 0.00263307 Loss_D_fake: 0.00110760) Loss_G: 0.41763297 Loss_Enh_Dec: -1.99901736\n",
      "| epoch 105 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.76 | loss  2.47 | ppl    11.78 | acc     0.71 | train_ae_norm     1.00\n",
      "[105/200][2299/4361] Loss_D: 0.00191850 (Loss_D_real: 0.00010984 Loss_D_fake: 0.00180866) Loss_G: 0.39999703 Loss_Enh_Dec: -1.61432040\n",
      "| epoch 105 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  2.45 | ppl    11.63 | acc     0.76 | train_ae_norm     1.00\n",
      "[105/200][2399/4361] Loss_D: 0.00174430 (Loss_D_real: 0.00027365 Loss_D_fake: 0.00147065) Loss_G: 0.40020639 Loss_Enh_Dec: -1.31164539\n",
      "| epoch 105 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.08 | loss  2.46 | ppl    11.66 | acc     0.68 | train_ae_norm     1.00\n",
      "[105/200][2499/4361] Loss_D: 0.00088940 (Loss_D_real: 0.00003641 Loss_D_fake: 0.00085299) Loss_G: 0.41890955 Loss_Enh_Dec: -1.33619153\n",
      "| epoch 105 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.48 | ppl    11.94 | acc     0.72 | train_ae_norm     1.00\n",
      "[105/200][2599/4361] Loss_D: 0.00498762 (Loss_D_real: 0.00011009 Loss_D_fake: 0.00487753) Loss_G: 0.41383806 Loss_Enh_Dec: -1.33217490\n",
      "| epoch 105 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  2.46 | ppl    11.74 | acc     0.66 | train_ae_norm     1.00\n",
      "[105/200][2699/4361] Loss_D: 0.00498917 (Loss_D_real: 0.00351637 Loss_D_fake: 0.00147280) Loss_G: 0.47474247 Loss_Enh_Dec: -1.77168012\n",
      "| epoch 105 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.06 | loss  2.50 | ppl    12.13 | acc     0.68 | train_ae_norm     1.00\n",
      "[105/200][2799/4361] Loss_D: 0.00244936 (Loss_D_real: 0.00034446 Loss_D_fake: 0.00210490) Loss_G: 0.40172806 Loss_Enh_Dec: -1.64906728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 105 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.65 | loss  2.45 | ppl    11.54 | acc     0.67 | train_ae_norm     1.00\n",
      "[105/200][2899/4361] Loss_D: 0.00340655 (Loss_D_real: 0.00168264 Loss_D_fake: 0.00172391) Loss_G: 0.40967304 Loss_Enh_Dec: -1.99641883\n",
      "| epoch 105 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.83 | loss  2.51 | ppl    12.31 | acc     0.70 | train_ae_norm     1.00\n",
      "[105/200][2999/4361] Loss_D: 0.00078262 (Loss_D_real: 0.00020202 Loss_D_fake: 0.00058060) Loss_G: 0.45226827 Loss_Enh_Dec: -1.34564650\n",
      "| epoch 105 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.57 | loss  2.49 | ppl    12.04 | acc     0.68 | train_ae_norm     1.00\n",
      "[105/200][3099/4361] Loss_D: 0.00109765 (Loss_D_real: 0.00006202 Loss_D_fake: 0.00103563) Loss_G: 0.45683509 Loss_Enh_Dec: -1.25658214\n",
      "| epoch 105 |  3100/ 4361 batches | lr 0.000000 | ms/batch 399.62 | loss  2.50 | ppl    12.20 | acc     0.69 | train_ae_norm     1.00\n",
      "[105/200][3199/4361] Loss_D: 0.01271699 (Loss_D_real: 0.01114076 Loss_D_fake: 0.00157623) Loss_G: 0.43025279 Loss_Enh_Dec: -1.03842580\n",
      "| epoch 105 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.51 | ppl    12.32 | acc     0.70 | train_ae_norm     1.00\n",
      "[105/200][3299/4361] Loss_D: 0.00245278 (Loss_D_real: 0.00014798 Loss_D_fake: 0.00230480) Loss_G: 0.42131153 Loss_Enh_Dec: -0.79740804\n",
      "| epoch 105 |  3300/ 4361 batches | lr 0.000000 | ms/batch 400.36 | loss  2.53 | ppl    12.60 | acc     0.69 | train_ae_norm     1.00\n",
      "[105/200][3399/4361] Loss_D: 0.00174693 (Loss_D_real: 0.00033024 Loss_D_fake: 0.00141669) Loss_G: 0.37086916 Loss_Enh_Dec: -0.98135036\n",
      "| epoch 105 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.51 | ppl    12.25 | acc     0.68 | train_ae_norm     1.00\n",
      "[105/200][3499/4361] Loss_D: 0.00075700 (Loss_D_real: 0.00016376 Loss_D_fake: 0.00059324) Loss_G: 0.44567403 Loss_Enh_Dec: -1.22224867\n",
      "| epoch 105 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.60 | loss  2.46 | ppl    11.69 | acc     0.69 | train_ae_norm     1.00\n",
      "[105/200][3599/4361] Loss_D: 0.00130059 (Loss_D_real: 0.00018553 Loss_D_fake: 0.00111507) Loss_G: 0.59065408 Loss_Enh_Dec: -1.59273612\n",
      "| epoch 105 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  2.45 | ppl    11.62 | acc     0.71 | train_ae_norm     1.00\n",
      "[105/200][3699/4361] Loss_D: 0.00117116 (Loss_D_real: 0.00059137 Loss_D_fake: 0.00057980) Loss_G: 0.46394578 Loss_Enh_Dec: -0.95107043\n",
      "| epoch 105 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  2.45 | ppl    11.62 | acc     0.69 | train_ae_norm     1.00\n",
      "[105/200][3799/4361] Loss_D: 0.00175819 (Loss_D_real: 0.00029438 Loss_D_fake: 0.00146381) Loss_G: 0.38050801 Loss_Enh_Dec: -1.16275775\n",
      "| epoch 105 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  2.47 | ppl    11.83 | acc     0.75 | train_ae_norm     1.00\n",
      "[105/200][3899/4361] Loss_D: 0.00066948 (Loss_D_real: 0.00018179 Loss_D_fake: 0.00048769) Loss_G: 0.52554005 Loss_Enh_Dec: -1.22435367\n",
      "| epoch 105 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.79 | loss  2.45 | ppl    11.63 | acc     0.68 | train_ae_norm     1.00\n",
      "[105/200][3999/4361] Loss_D: 0.00172012 (Loss_D_real: 0.00068319 Loss_D_fake: 0.00103694) Loss_G: 0.45272627 Loss_Enh_Dec: -1.02548170\n",
      "| epoch 105 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  2.45 | ppl    11.55 | acc     0.72 | train_ae_norm     1.00\n",
      "[105/200][4099/4361] Loss_D: 0.00290443 (Loss_D_real: 0.00267681 Loss_D_fake: 0.00022762) Loss_G: 0.51666510 Loss_Enh_Dec: -0.82852709\n",
      "| epoch 105 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  2.41 | ppl    11.14 | acc     0.71 | train_ae_norm     1.00\n",
      "[105/200][4199/4361] Loss_D: 0.00149455 (Loss_D_real: 0.00031107 Loss_D_fake: 0.00118348) Loss_G: 0.44929311 Loss_Enh_Dec: -0.92113030\n",
      "| epoch 105 |  4200/ 4361 batches | lr 0.000000 | ms/batch 402.04 | loss  2.44 | ppl    11.50 | acc     0.75 | train_ae_norm     1.00\n",
      "[105/200][4299/4361] Loss_D: 0.00120434 (Loss_D_real: 0.00021184 Loss_D_fake: 0.00099250) Loss_G: 0.45448285 Loss_Enh_Dec: -1.08254850\n",
      "| epoch 105 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  2.42 | ppl    11.27 | acc     0.71 | train_ae_norm     1.00\n",
      "| end of epoch 105 | time: 1851.75s | test loss  2.43 | test ppl 11.31 | acc 0.758\n",
      "bleu_self:  [4.94792869e-01 2.50047194e-01 7.60122020e-04 7.63547694e-05\n",
      " 1.94536221e-05]\n",
      "bleu_test:  [6.69642857e-01 1.50888358e-01 1.62707335e-06 6.12211838e-07\n",
      " 4.66110879e-07]\n",
      "bleu_self: [0.49479287,0.25004719,0.00076012,0.00007635,0.00001945]\n",
      "bleu_test: [0.66964286,0.15088836,0.00000163,0.00000061,0.00000047]\n",
      "New saving model: epoch 105.\n",
      "Saving models to ./results/yahoo_merge_assigned_results\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 106 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.700\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.485\n",
      "  Test Loss: 4.485\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 106 |     0/ 4361 batches | lr 0.000000 | ms/batch 861.75 | loss  0.02 | ppl     1.02 | acc     0.77 | train_ae_norm     1.00\n",
      "[106/200][99/4361] Loss_D: 0.00226216 (Loss_D_real: 0.00021146 Loss_D_fake: 0.00205069) Loss_G: 0.43801412 Loss_Enh_Dec: -1.39765680\n",
      "| epoch 106 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  2.40 | ppl    10.97 | acc     0.67 | train_ae_norm     1.00\n",
      "[106/200][199/4361] Loss_D: 0.00206274 (Loss_D_real: 0.00098961 Loss_D_fake: 0.00107313) Loss_G: 0.39282155 Loss_Enh_Dec: -1.97544920\n",
      "| epoch 106 |   200/ 4361 batches | lr 0.000000 | ms/batch 400.65 | loss  2.42 | ppl    11.29 | acc     0.73 | train_ae_norm     1.00\n",
      "[106/200][299/4361] Loss_D: 0.00211117 (Loss_D_real: 0.00013975 Loss_D_fake: 0.00197141) Loss_G: 0.44021851 Loss_Enh_Dec: -1.05740535\n",
      "| epoch 106 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  2.45 | ppl    11.64 | acc     0.68 | train_ae_norm     1.00\n",
      "[106/200][399/4361] Loss_D: 0.00147598 (Loss_D_real: 0.00145562 Loss_D_fake: 0.00002036) Loss_G: 0.90206349 Loss_Enh_Dec: -1.46526027\n",
      "| epoch 106 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.66 | loss  2.35 | ppl    10.52 | acc     0.72 | train_ae_norm     1.00\n",
      "[106/200][499/4361] Loss_D: 0.00245134 (Loss_D_real: 0.00119837 Loss_D_fake: 0.00125297) Loss_G: 0.51295710 Loss_Enh_Dec: -1.20760953\n",
      "| epoch 106 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  2.43 | ppl    11.33 | acc     0.71 | train_ae_norm     1.00\n",
      "[106/200][599/4361] Loss_D: 0.00147336 (Loss_D_real: 0.00040890 Loss_D_fake: 0.00106446) Loss_G: 0.41615534 Loss_Enh_Dec: -0.73718405\n",
      "| epoch 106 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.63 | loss  2.37 | ppl    10.69 | acc     0.71 | train_ae_norm     1.00\n",
      "[106/200][699/4361] Loss_D: 0.00107637 (Loss_D_real: 0.00018897 Loss_D_fake: 0.00088740) Loss_G: 0.38817269 Loss_Enh_Dec: -1.61468470\n",
      "| epoch 106 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  2.41 | ppl    11.10 | acc     0.71 | train_ae_norm     1.00\n",
      "[106/200][799/4361] Loss_D: 0.02540212 (Loss_D_real: 0.02519741 Loss_D_fake: 0.00020471) Loss_G: 0.43849269 Loss_Enh_Dec: -1.39740634\n",
      "| epoch 106 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  2.42 | ppl    11.19 | acc     0.72 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[106/200][899/4361] Loss_D: 0.00239739 (Loss_D_real: 0.00049621 Loss_D_fake: 0.00190118) Loss_G: 0.53430533 Loss_Enh_Dec: -1.27450550\n",
      "| epoch 106 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  2.43 | ppl    11.35 | acc     0.76 | train_ae_norm     1.00\n",
      "[106/200][999/4361] Loss_D: 0.00136950 (Loss_D_real: 0.00061031 Loss_D_fake: 0.00075919) Loss_G: 0.41648361 Loss_Enh_Dec: -1.53239024\n",
      "| epoch 106 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  2.44 | ppl    11.50 | acc     0.73 | train_ae_norm     1.00\n",
      "[106/200][1099/4361] Loss_D: 0.00238541 (Loss_D_real: 0.00027768 Loss_D_fake: 0.00210774) Loss_G: 0.43397322 Loss_Enh_Dec: -1.35386658\n",
      "| epoch 106 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  2.40 | ppl    11.00 | acc     0.69 | train_ae_norm     1.00\n",
      "[106/200][1199/4361] Loss_D: 0.00306884 (Loss_D_real: 0.00102777 Loss_D_fake: 0.00204107) Loss_G: 0.40429497 Loss_Enh_Dec: -1.86890030\n",
      "| epoch 106 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  2.41 | ppl    11.14 | acc     0.73 | train_ae_norm     1.00\n",
      "[106/200][1299/4361] Loss_D: 0.00143668 (Loss_D_real: 0.00094133 Loss_D_fake: 0.00049535) Loss_G: 0.49147302 Loss_Enh_Dec: -2.15441132\n",
      "| epoch 106 |  1300/ 4361 batches | lr 0.000000 | ms/batch 402.11 | loss  2.43 | ppl    11.40 | acc     0.71 | train_ae_norm     1.00\n",
      "[106/200][1399/4361] Loss_D: 0.00101318 (Loss_D_real: 0.00045360 Loss_D_fake: 0.00055959) Loss_G: 0.43597975 Loss_Enh_Dec: -2.17161989\n",
      "| epoch 106 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  2.43 | ppl    11.32 | acc     0.66 | train_ae_norm     1.00\n",
      "[106/200][1499/4361] Loss_D: 0.00164887 (Loss_D_real: 0.00023005 Loss_D_fake: 0.00141883) Loss_G: 0.40815791 Loss_Enh_Dec: -2.22032475\n",
      "| epoch 106 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  2.47 | ppl    11.83 | acc     0.69 | train_ae_norm     1.00\n",
      "[106/200][1599/4361] Loss_D: 0.00393613 (Loss_D_real: 0.00354373 Loss_D_fake: 0.00039240) Loss_G: 0.41567713 Loss_Enh_Dec: -2.34573054\n",
      "| epoch 106 |  1600/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  2.46 | ppl    11.69 | acc     0.71 | train_ae_norm     1.00\n",
      "[106/200][1699/4361] Loss_D: 0.00098367 (Loss_D_real: 0.00013978 Loss_D_fake: 0.00084390) Loss_G: 0.48625293 Loss_Enh_Dec: -1.96297228\n",
      "| epoch 106 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.42 | ppl    11.26 | acc     0.70 | train_ae_norm     1.00\n",
      "[106/200][1799/4361] Loss_D: 0.00092682 (Loss_D_real: 0.00023954 Loss_D_fake: 0.00068729) Loss_G: 0.50030512 Loss_Enh_Dec: -2.26652765\n",
      "| epoch 106 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.42 | ppl    11.30 | acc     0.73 | train_ae_norm     1.00\n",
      "[106/200][1899/4361] Loss_D: 0.00470968 (Loss_D_real: 0.00366224 Loss_D_fake: 0.00104744) Loss_G: 0.44546214 Loss_Enh_Dec: -2.26186585\n",
      "| epoch 106 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.93 | loss  2.48 | ppl    11.99 | acc     0.74 | train_ae_norm     1.00\n",
      "[106/200][1999/4361] Loss_D: 0.00091721 (Loss_D_real: 0.00022727 Loss_D_fake: 0.00068994) Loss_G: 0.70161164 Loss_Enh_Dec: -1.81208265\n",
      "| epoch 106 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  2.44 | ppl    11.49 | acc     0.73 | train_ae_norm     1.00\n",
      "[106/200][2099/4361] Loss_D: 0.00283512 (Loss_D_real: 0.00251965 Loss_D_fake: 0.00031547) Loss_G: 0.54202443 Loss_Enh_Dec: -2.04384589\n",
      "| epoch 106 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.73 | loss  2.43 | ppl    11.38 | acc     0.73 | train_ae_norm     1.00\n",
      "[106/200][2199/4361] Loss_D: 0.00557486 (Loss_D_real: 0.00464537 Loss_D_fake: 0.00092949) Loss_G: 0.44015715 Loss_Enh_Dec: -2.13238502\n",
      "| epoch 106 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.60 | loss  2.42 | ppl    11.27 | acc     0.73 | train_ae_norm     1.00\n",
      "[106/200][2299/4361] Loss_D: 0.00187158 (Loss_D_real: 0.00046049 Loss_D_fake: 0.00141109) Loss_G: 0.46005660 Loss_Enh_Dec: -1.93308473\n",
      "| epoch 106 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.89 | loss  2.40 | ppl    10.99 | acc     0.75 | train_ae_norm     1.00\n",
      "[106/200][2399/4361] Loss_D: 0.00096931 (Loss_D_real: 0.00008755 Loss_D_fake: 0.00088176) Loss_G: 0.42191458 Loss_Enh_Dec: -2.51646733\n",
      "| epoch 106 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.39 | ppl    10.92 | acc     0.68 | train_ae_norm     1.00\n",
      "[106/200][2499/4361] Loss_D: 0.00159951 (Loss_D_real: 0.00088839 Loss_D_fake: 0.00071111) Loss_G: 0.42253256 Loss_Enh_Dec: -2.38788867\n",
      "| epoch 106 |  2500/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  2.43 | ppl    11.36 | acc     0.71 | train_ae_norm     1.00\n",
      "[106/200][2599/4361] Loss_D: 0.00064265 (Loss_D_real: 0.00015707 Loss_D_fake: 0.00048558) Loss_G: 0.47602758 Loss_Enh_Dec: -1.85695231\n",
      "| epoch 106 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  2.40 | ppl    11.00 | acc     0.68 | train_ae_norm     1.00\n",
      "[106/200][2699/4361] Loss_D: 0.00204032 (Loss_D_real: 0.00004676 Loss_D_fake: 0.00199356) Loss_G: 0.39024666 Loss_Enh_Dec: -0.95371765\n",
      "| epoch 106 |  2700/ 4361 batches | lr 0.000000 | ms/batch 400.65 | loss  2.41 | ppl    11.11 | acc     0.71 | train_ae_norm     1.00\n",
      "[106/200][2799/4361] Loss_D: 0.00170649 (Loss_D_real: 0.00029517 Loss_D_fake: 0.00141132) Loss_G: 0.40584168 Loss_Enh_Dec: -2.11760521\n",
      "| epoch 106 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.82 | loss  2.38 | ppl    10.75 | acc     0.71 | train_ae_norm     1.00\n",
      "[106/200][2899/4361] Loss_D: 0.00097235 (Loss_D_real: 0.00023980 Loss_D_fake: 0.00073255) Loss_G: 0.40756130 Loss_Enh_Dec: -2.26748157\n",
      "| epoch 106 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.55 | loss  2.41 | ppl    11.12 | acc     0.71 | train_ae_norm     1.00\n",
      "[106/200][2999/4361] Loss_D: 0.00079951 (Loss_D_real: 0.00020698 Loss_D_fake: 0.00059254) Loss_G: 0.45909387 Loss_Enh_Dec: -1.93730032\n",
      "| epoch 106 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.64 | loss  2.43 | ppl    11.31 | acc     0.70 | train_ae_norm     1.00\n",
      "[106/200][3099/4361] Loss_D: 0.00230623 (Loss_D_real: 0.00036936 Loss_D_fake: 0.00193687) Loss_G: 0.43953228 Loss_Enh_Dec: -2.08097148\n",
      "| epoch 106 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.85 | loss  2.44 | ppl    11.48 | acc     0.72 | train_ae_norm     1.00\n",
      "[106/200][3199/4361] Loss_D: 0.00323512 (Loss_D_real: 0.00006503 Loss_D_fake: 0.00317009) Loss_G: 0.47106263 Loss_Enh_Dec: -1.56820571\n",
      "| epoch 106 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.47 | loss  2.46 | ppl    11.73 | acc     0.72 | train_ae_norm     1.00\n",
      "[106/200][3299/4361] Loss_D: 0.03187831 (Loss_D_real: 0.03159085 Loss_D_fake: 0.00028746) Loss_G: 0.52666140 Loss_Enh_Dec: -2.25131226\n",
      "| epoch 106 |  3300/ 4361 batches | lr 0.000000 | ms/batch 400.64 | loss  2.47 | ppl    11.79 | acc     0.69 | train_ae_norm     1.00\n",
      "[106/200][3399/4361] Loss_D: 0.00180993 (Loss_D_real: 0.00084685 Loss_D_fake: 0.00096309) Loss_G: 0.49994150 Loss_Enh_Dec: -2.57472920\n",
      "| epoch 106 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.38 | loss  2.45 | ppl    11.60 | acc     0.70 | train_ae_norm     1.00\n",
      "[106/200][3499/4361] Loss_D: 0.00367163 (Loss_D_real: 0.00043137 Loss_D_fake: 0.00324026) Loss_G: 0.43512130 Loss_Enh_Dec: -2.10853052\n",
      "| epoch 106 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.60 | loss  2.38 | ppl    10.77 | acc     0.68 | train_ae_norm     1.00\n",
      "[106/200][3599/4361] Loss_D: 0.00442360 (Loss_D_real: 0.00251916 Loss_D_fake: 0.00190444) Loss_G: 0.41528478 Loss_Enh_Dec: -2.03263664\n",
      "| epoch 106 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  2.38 | ppl    10.83 | acc     0.73 | train_ae_norm     1.00\n",
      "[106/200][3699/4361] Loss_D: 0.01193541 (Loss_D_real: 0.01122085 Loss_D_fake: 0.00071456) Loss_G: 0.45317036 Loss_Enh_Dec: -1.81315458\n",
      "| epoch 106 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  2.42 | ppl    11.28 | acc     0.68 | train_ae_norm     1.00\n",
      "[106/200][3799/4361] Loss_D: 0.00093699 (Loss_D_real: 0.00003818 Loss_D_fake: 0.00089880) Loss_G: 0.41102001 Loss_Enh_Dec: -2.43333745\n",
      "| epoch 106 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  2.43 | ppl    11.35 | acc     0.75 | train_ae_norm     1.00\n",
      "[106/200][3899/4361] Loss_D: 0.00203060 (Loss_D_real: 0.00022493 Loss_D_fake: 0.00180566) Loss_G: 0.41600925 Loss_Enh_Dec: -2.09159970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 106 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.61 | loss  2.42 | ppl    11.21 | acc     0.68 | train_ae_norm     1.00\n",
      "[106/200][3999/4361] Loss_D: 0.00198237 (Loss_D_real: 0.00036001 Loss_D_fake: 0.00162236) Loss_G: 0.44401285 Loss_Enh_Dec: -1.54953420\n",
      "| epoch 106 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.42 | ppl    11.23 | acc     0.71 | train_ae_norm     1.00\n",
      "[106/200][4099/4361] Loss_D: 0.00473859 (Loss_D_real: 0.00362290 Loss_D_fake: 0.00111570) Loss_G: 0.48240826 Loss_Enh_Dec: -1.56979322\n",
      "| epoch 106 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  2.40 | ppl    10.98 | acc     0.70 | train_ae_norm     1.00\n",
      "[106/200][4199/4361] Loss_D: 0.00110308 (Loss_D_real: 0.00060168 Loss_D_fake: 0.00050140) Loss_G: 0.48993322 Loss_Enh_Dec: -0.66119760\n",
      "| epoch 106 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  2.45 | ppl    11.54 | acc     0.73 | train_ae_norm     1.00\n",
      "[106/200][4299/4361] Loss_D: 0.00096368 (Loss_D_real: 0.00054107 Loss_D_fake: 0.00042261) Loss_G: 0.44598714 Loss_Enh_Dec: -0.86463118\n",
      "| epoch 106 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  2.40 | ppl    10.98 | acc     0.72 | train_ae_norm     1.00\n",
      "| end of epoch 106 | time: 1851.01s | test loss  2.41 | test ppl 11.16 | acc 0.759\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 107 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.698\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.490\n",
      "  Test Loss: 4.502\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 107 |     0/ 4361 batches | lr 0.000000 | ms/batch 867.73 | loss  0.02 | ppl     1.02 | acc     0.76 | train_ae_norm     1.00\n",
      "[107/200][99/4361] Loss_D: 0.00145777 (Loss_D_real: 0.00018495 Loss_D_fake: 0.00127282) Loss_G: 0.40488729 Loss_Enh_Dec: -1.56826055\n",
      "| epoch 107 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.15 | loss  2.38 | ppl    10.84 | acc     0.67 | train_ae_norm     1.00\n",
      "[107/200][199/4361] Loss_D: 0.00270961 (Loss_D_real: 0.00215984 Loss_D_fake: 0.00054977) Loss_G: 0.44443694 Loss_Enh_Dec: -0.99878675\n",
      "| epoch 107 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  2.42 | ppl    11.20 | acc     0.73 | train_ae_norm     1.00\n",
      "[107/200][299/4361] Loss_D: 0.00092334 (Loss_D_real: 0.00019550 Loss_D_fake: 0.00072784) Loss_G: 0.42592746 Loss_Enh_Dec: -1.38762057\n",
      "| epoch 107 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  2.44 | ppl    11.52 | acc     0.66 | train_ae_norm     1.00\n",
      "[107/200][399/4361] Loss_D: 0.00130769 (Loss_D_real: 0.00055436 Loss_D_fake: 0.00075333) Loss_G: 0.46992204 Loss_Enh_Dec: -2.09916544\n",
      "| epoch 107 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.37 | ppl    10.73 | acc     0.72 | train_ae_norm     1.00\n",
      "[107/200][499/4361] Loss_D: 0.00077041 (Loss_D_real: 0.00044775 Loss_D_fake: 0.00032266) Loss_G: 0.47069284 Loss_Enh_Dec: -2.46059680\n",
      "| epoch 107 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  2.43 | ppl    11.32 | acc     0.74 | train_ae_norm     1.00\n",
      "[107/200][599/4361] Loss_D: 0.00231930 (Loss_D_real: 0.00008827 Loss_D_fake: 0.00223102) Loss_G: 0.49701267 Loss_Enh_Dec: -2.41552210\n",
      "| epoch 107 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.71 | loss  2.37 | ppl    10.75 | acc     0.69 | train_ae_norm     1.00\n",
      "[107/200][699/4361] Loss_D: 0.02445598 (Loss_D_real: 0.02297290 Loss_D_fake: 0.00148308) Loss_G: 0.50558168 Loss_Enh_Dec: -2.39102674\n",
      "| epoch 107 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.52 | loss  2.40 | ppl    11.07 | acc     0.70 | train_ae_norm     1.00\n",
      "[107/200][799/4361] Loss_D: 0.00144218 (Loss_D_real: 0.00023593 Loss_D_fake: 0.00120625) Loss_G: 0.40900913 Loss_Enh_Dec: -2.40909481\n",
      "| epoch 107 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  2.40 | ppl    11.04 | acc     0.72 | train_ae_norm     1.00\n",
      "[107/200][899/4361] Loss_D: 0.00837823 (Loss_D_real: 0.00223101 Loss_D_fake: 0.00614722) Loss_G: 0.27447256 Loss_Enh_Dec: -2.06989312\n",
      "| epoch 107 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.79 | loss  2.40 | ppl    10.97 | acc     0.74 | train_ae_norm     1.00\n",
      "[107/200][999/4361] Loss_D: 0.00507912 (Loss_D_real: 0.00034805 Loss_D_fake: 0.00473107) Loss_G: 0.26718435 Loss_Enh_Dec: -1.69041789\n",
      "| epoch 107 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  2.37 | ppl    10.66 | acc     0.70 | train_ae_norm     1.00\n",
      "[107/200][1099/4361] Loss_D: 0.00451563 (Loss_D_real: 0.00023454 Loss_D_fake: 0.00428109) Loss_G: 0.28232726 Loss_Enh_Dec: -1.54640710\n",
      "| epoch 107 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  2.36 | ppl    10.54 | acc     0.71 | train_ae_norm     1.00\n",
      "[107/200][1199/4361] Loss_D: 0.00719234 (Loss_D_real: 0.00325361 Loss_D_fake: 0.00393873) Loss_G: 0.28802729 Loss_Enh_Dec: -1.62874949\n",
      "| epoch 107 |  1200/ 4361 batches | lr 0.000000 | ms/batch 402.45 | loss  2.36 | ppl    10.60 | acc     0.76 | train_ae_norm     1.00\n",
      "[107/200][1299/4361] Loss_D: 0.00614944 (Loss_D_real: 0.00307170 Loss_D_fake: 0.00307774) Loss_G: 0.29839298 Loss_Enh_Dec: -0.54590964\n",
      "| epoch 107 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  2.38 | ppl    10.79 | acc     0.72 | train_ae_norm     1.00\n",
      "[107/200][1399/4361] Loss_D: 0.00256262 (Loss_D_real: 0.00025676 Loss_D_fake: 0.00230586) Loss_G: 0.30735064 Loss_Enh_Dec: -1.32674670\n",
      "| epoch 107 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  2.36 | ppl    10.58 | acc     0.69 | train_ae_norm     1.00\n",
      "[107/200][1499/4361] Loss_D: 0.02896464 (Loss_D_real: 0.02664437 Loss_D_fake: 0.00232027) Loss_G: 0.30729333 Loss_Enh_Dec: -1.48441160\n",
      "| epoch 107 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  2.43 | ppl    11.31 | acc     0.71 | train_ae_norm     1.00\n",
      "[107/200][1599/4361] Loss_D: 0.00507851 (Loss_D_real: 0.00171197 Loss_D_fake: 0.00336653) Loss_G: 0.28973696 Loss_Enh_Dec: -1.23949647\n",
      "| epoch 107 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  2.39 | ppl    10.96 | acc     0.70 | train_ae_norm     1.00\n",
      "[107/200][1699/4361] Loss_D: 0.00362393 (Loss_D_real: 0.00129899 Loss_D_fake: 0.00232495) Loss_G: 0.31491205 Loss_Enh_Dec: -1.80332112\n",
      "| epoch 107 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  2.37 | ppl    10.71 | acc     0.71 | train_ae_norm     1.00\n",
      "[107/200][1799/4361] Loss_D: 0.00342250 (Loss_D_real: 0.00027505 Loss_D_fake: 0.00314745) Loss_G: 0.29328763 Loss_Enh_Dec: -2.37151456\n",
      "| epoch 107 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.65 | loss  2.34 | ppl    10.36 | acc     0.71 | train_ae_norm     1.00\n",
      "[107/200][1899/4361] Loss_D: 0.00448077 (Loss_D_real: 0.00018065 Loss_D_fake: 0.00430012) Loss_G: 0.27445954 Loss_Enh_Dec: -2.10170293\n",
      "| epoch 107 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  2.39 | ppl    10.90 | acc     0.72 | train_ae_norm     1.00\n",
      "[107/200][1999/4361] Loss_D: 0.00957902 (Loss_D_real: 0.00652904 Loss_D_fake: 0.00304998) Loss_G: 0.29134899 Loss_Enh_Dec: -1.56738746\n",
      "| epoch 107 |  2000/ 4361 batches | lr 0.000000 | ms/batch 402.72 | loss  2.33 | ppl    10.31 | acc     0.76 | train_ae_norm     1.00\n",
      "[107/200][2099/4361] Loss_D: 0.00303973 (Loss_D_real: 0.00073087 Loss_D_fake: 0.00230887) Loss_G: 0.31552240 Loss_Enh_Dec: -1.65470338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 107 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.37 | ppl    10.72 | acc     0.76 | train_ae_norm     1.00\n",
      "[107/200][2199/4361] Loss_D: 0.00327048 (Loss_D_real: 0.00008764 Loss_D_fake: 0.00318283) Loss_G: 0.28111944 Loss_Enh_Dec: -1.59662771\n",
      "| epoch 107 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  2.38 | ppl    10.77 | acc     0.72 | train_ae_norm     1.00\n",
      "[107/200][2299/4361] Loss_D: 0.00548466 (Loss_D_real: 0.00193556 Loss_D_fake: 0.00354909) Loss_G: 0.27852708 Loss_Enh_Dec: -2.18025565\n",
      "| epoch 107 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  2.37 | ppl    10.72 | acc     0.73 | train_ae_norm     1.00\n",
      "[107/200][2399/4361] Loss_D: 0.00326362 (Loss_D_real: 0.00025410 Loss_D_fake: 0.00300952) Loss_G: 0.29592511 Loss_Enh_Dec: -1.92829835\n",
      "| epoch 107 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  2.38 | ppl    10.84 | acc     0.68 | train_ae_norm     1.00\n",
      "[107/200][2499/4361] Loss_D: 0.00327922 (Loss_D_real: 0.00031192 Loss_D_fake: 0.00296730) Loss_G: 0.29877967 Loss_Enh_Dec: -1.43552125\n",
      "| epoch 107 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  2.40 | ppl    11.01 | acc     0.74 | train_ae_norm     1.00\n",
      "[107/200][2599/4361] Loss_D: 0.00639907 (Loss_D_real: 0.00377926 Loss_D_fake: 0.00261981) Loss_G: 0.29963121 Loss_Enh_Dec: -1.93333876\n",
      "| epoch 107 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.37 | ppl    10.71 | acc     0.68 | train_ae_norm     1.00\n",
      "[107/200][2699/4361] Loss_D: 0.00388763 (Loss_D_real: 0.00111824 Loss_D_fake: 0.00276939) Loss_G: 0.29940161 Loss_Enh_Dec: -2.06387305\n",
      "| epoch 107 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  2.36 | ppl    10.61 | acc     0.72 | train_ae_norm     1.00\n",
      "[107/200][2799/4361] Loss_D: 0.00229007 (Loss_D_real: 0.00025931 Loss_D_fake: 0.00203076) Loss_G: 0.31678885 Loss_Enh_Dec: -2.32721210\n",
      "| epoch 107 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  2.34 | ppl    10.39 | acc     0.70 | train_ae_norm     1.00\n",
      "[107/200][2899/4361] Loss_D: 0.00545395 (Loss_D_real: 0.00217240 Loss_D_fake: 0.00328155) Loss_G: 0.33595988 Loss_Enh_Dec: -2.26203299\n",
      "| epoch 107 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.79 | loss  2.38 | ppl    10.79 | acc     0.70 | train_ae_norm     1.00\n",
      "[107/200][2999/4361] Loss_D: 0.00307703 (Loss_D_real: 0.00059517 Loss_D_fake: 0.00248187) Loss_G: 0.31177264 Loss_Enh_Dec: -2.23980522\n",
      "| epoch 107 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.36 | loss  2.37 | ppl    10.66 | acc     0.71 | train_ae_norm     1.00\n",
      "[107/200][3099/4361] Loss_D: 0.03202444 (Loss_D_real: 0.02861260 Loss_D_fake: 0.00341185) Loss_G: 0.28502300 Loss_Enh_Dec: -2.52163911\n",
      "| epoch 107 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  2.37 | ppl    10.70 | acc     0.72 | train_ae_norm     1.00\n",
      "[107/200][3199/4361] Loss_D: 0.00393274 (Loss_D_real: 0.00010927 Loss_D_fake: 0.00382347) Loss_G: 0.30016556 Loss_Enh_Dec: -2.36638427\n",
      "| epoch 107 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.67 | loss  2.40 | ppl    11.06 | acc     0.75 | train_ae_norm     1.00\n",
      "[107/200][3299/4361] Loss_D: 0.00435675 (Loss_D_real: 0.00203146 Loss_D_fake: 0.00232530) Loss_G: 0.32380754 Loss_Enh_Dec: -2.06201291\n",
      "| epoch 107 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  2.39 | ppl    10.92 | acc     0.72 | train_ae_norm     1.00\n",
      "[107/200][3399/4361] Loss_D: 0.00433606 (Loss_D_real: 0.00109338 Loss_D_fake: 0.00324268) Loss_G: 0.29890057 Loss_Enh_Dec: -2.42172217\n",
      "| epoch 107 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.67 | loss  2.40 | ppl    11.00 | acc     0.70 | train_ae_norm     1.00\n",
      "[107/200][3499/4361] Loss_D: 0.00861325 (Loss_D_real: 0.00636647 Loss_D_fake: 0.00224677) Loss_G: 0.31198522 Loss_Enh_Dec: -1.61028123\n",
      "| epoch 107 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  2.34 | ppl    10.36 | acc     0.71 | train_ae_norm     1.00\n",
      "[107/200][3599/4361] Loss_D: 0.00448174 (Loss_D_real: 0.00062571 Loss_D_fake: 0.00385603) Loss_G: 0.28474194 Loss_Enh_Dec: -2.73087311\n",
      "| epoch 107 |  3600/ 4361 batches | lr 0.000000 | ms/batch 400.04 | loss  2.35 | ppl    10.50 | acc     0.74 | train_ae_norm     1.00\n",
      "[107/200][3699/4361] Loss_D: 0.00258816 (Loss_D_real: 0.00014219 Loss_D_fake: 0.00244598) Loss_G: 0.28692281 Loss_Enh_Dec: -2.61592269\n",
      "| epoch 107 |  3700/ 4361 batches | lr 0.000000 | ms/batch 399.63 | loss  2.36 | ppl    10.57 | acc     0.68 | train_ae_norm     1.00\n",
      "[107/200][3799/4361] Loss_D: 0.00295233 (Loss_D_real: 0.00004584 Loss_D_fake: 0.00290648) Loss_G: 0.30774963 Loss_Enh_Dec: -2.61751103\n",
      "| epoch 107 |  3800/ 4361 batches | lr 0.000000 | ms/batch 400.58 | loss  2.38 | ppl    10.86 | acc     0.76 | train_ae_norm     1.00\n",
      "[107/200][3899/4361] Loss_D: 0.00175051 (Loss_D_real: 0.00014834 Loss_D_fake: 0.00160216) Loss_G: 0.33581254 Loss_Enh_Dec: -2.70310402\n",
      "| epoch 107 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  2.37 | ppl    10.74 | acc     0.69 | train_ae_norm     1.00\n",
      "[107/200][3999/4361] Loss_D: 0.00232848 (Loss_D_real: 0.00009261 Loss_D_fake: 0.00223587) Loss_G: 0.31069991 Loss_Enh_Dec: -2.43598557\n",
      "| epoch 107 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.86 | loss  2.38 | ppl    10.76 | acc     0.73 | train_ae_norm     1.00\n",
      "[107/200][4099/4361] Loss_D: 0.00350185 (Loss_D_real: 0.00060570 Loss_D_fake: 0.00289615) Loss_G: 0.33925864 Loss_Enh_Dec: -2.61057711\n",
      "| epoch 107 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.60 | loss  2.34 | ppl    10.34 | acc     0.72 | train_ae_norm     1.00\n",
      "[107/200][4199/4361] Loss_D: 0.00335078 (Loss_D_real: 0.00177198 Loss_D_fake: 0.00157880) Loss_G: 0.31954679 Loss_Enh_Dec: -2.95262671\n",
      "| epoch 107 |  4200/ 4361 batches | lr 0.000000 | ms/batch 400.58 | loss  2.38 | ppl    10.78 | acc     0.74 | train_ae_norm     1.00\n",
      "[107/200][4299/4361] Loss_D: 0.00431493 (Loss_D_real: 0.00224599 Loss_D_fake: 0.00206893) Loss_G: 0.31357810 Loss_Enh_Dec: -2.99623561\n",
      "| epoch 107 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.73 | loss  2.34 | ppl    10.43 | acc     0.72 | train_ae_norm     1.00\n",
      "| end of epoch 107 | time: 1851.85s | test loss  2.41 | test ppl 11.14 | acc 0.760\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 108 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:48.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:18.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.700\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.502\n",
      "  Test Loss: 4.486\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 108 |     0/ 4361 batches | lr 0.000000 | ms/batch 858.73 | loss  0.02 | ppl     1.02 | acc     0.77 | train_ae_norm     1.00\n",
      "[108/200][99/4361] Loss_D: 0.00444594 (Loss_D_real: 0.00001775 Loss_D_fake: 0.00442819) Loss_G: 0.32708922 Loss_Enh_Dec: -3.05318332\n",
      "| epoch 108 |   100/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  2.35 | ppl    10.44 | acc     0.69 | train_ae_norm     1.00\n",
      "[108/200][199/4361] Loss_D: 0.00313003 (Loss_D_real: 0.00025300 Loss_D_fake: 0.00287704) Loss_G: 0.34991455 Loss_Enh_Dec: -2.89981246\n",
      "| epoch 108 |   200/ 4361 batches | lr 0.000000 | ms/batch 400.86 | loss  2.38 | ppl    10.78 | acc     0.71 | train_ae_norm     1.00\n",
      "[108/200][299/4361] Loss_D: 0.01348223 (Loss_D_real: 0.01182571 Loss_D_fake: 0.00165652) Loss_G: 0.32864285 Loss_Enh_Dec: -3.02206326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 108 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  2.40 | ppl    10.98 | acc     0.68 | train_ae_norm     1.00\n",
      "[108/200][399/4361] Loss_D: 0.00385543 (Loss_D_real: 0.00074209 Loss_D_fake: 0.00311334) Loss_G: 0.35973004 Loss_Enh_Dec: -2.94302487\n",
      "| epoch 108 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.55 | loss  2.28 | ppl     9.81 | acc     0.75 | train_ae_norm     1.00\n",
      "[108/200][499/4361] Loss_D: 0.00756113 (Loss_D_real: 0.00562077 Loss_D_fake: 0.00194037) Loss_G: 0.32542440 Loss_Enh_Dec: -2.87735653\n",
      "| epoch 108 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.23 | loss  2.35 | ppl    10.45 | acc     0.74 | train_ae_norm     1.00\n",
      "[108/200][599/4361] Loss_D: 0.00261558 (Loss_D_real: 0.00007722 Loss_D_fake: 0.00253835) Loss_G: 0.31636262 Loss_Enh_Dec: -2.86364412\n",
      "| epoch 108 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.23 | loss  2.31 | ppl    10.09 | acc     0.71 | train_ae_norm     1.00\n",
      "[108/200][699/4361] Loss_D: 0.00330024 (Loss_D_real: 0.00007521 Loss_D_fake: 0.00322503) Loss_G: 0.33108336 Loss_Enh_Dec: -2.07124853\n",
      "| epoch 108 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.42 | loss  2.38 | ppl    10.77 | acc     0.74 | train_ae_norm     1.00\n",
      "[108/200][799/4361] Loss_D: 0.00320671 (Loss_D_real: 0.00116404 Loss_D_fake: 0.00204267) Loss_G: 0.32784614 Loss_Enh_Dec: -1.86560905\n",
      "| epoch 108 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  2.37 | ppl    10.67 | acc     0.75 | train_ae_norm     1.00\n",
      "[108/200][899/4361] Loss_D: 0.00468757 (Loss_D_real: 0.00216114 Loss_D_fake: 0.00252643) Loss_G: 0.30893648 Loss_Enh_Dec: -2.63288951\n",
      "| epoch 108 |   900/ 4361 batches | lr 0.000000 | ms/batch 399.86 | loss  2.38 | ppl    10.79 | acc     0.75 | train_ae_norm     1.00\n",
      "[108/200][999/4361] Loss_D: 0.00281927 (Loss_D_real: 0.00040422 Loss_D_fake: 0.00241505) Loss_G: 0.31358495 Loss_Enh_Dec: -2.93176937\n",
      "| epoch 108 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  2.36 | ppl    10.62 | acc     0.73 | train_ae_norm     1.00\n",
      "[108/200][1099/4361] Loss_D: 0.00225263 (Loss_D_real: 0.00031384 Loss_D_fake: 0.00193879) Loss_G: 0.31675887 Loss_Enh_Dec: -2.98649836\n",
      "| epoch 108 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  2.36 | ppl    10.63 | acc     0.71 | train_ae_norm     1.00\n",
      "[108/200][1199/4361] Loss_D: 0.00132094 (Loss_D_real: 0.00028218 Loss_D_fake: 0.00103876) Loss_G: 0.54464889 Loss_Enh_Dec: -2.79367065\n",
      "| epoch 108 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  2.36 | ppl    10.54 | acc     0.71 | train_ae_norm     1.00\n",
      "[108/200][1299/4361] Loss_D: 0.00117949 (Loss_D_real: 0.00009168 Loss_D_fake: 0.00108781) Loss_G: 0.40540120 Loss_Enh_Dec: -2.78529119\n",
      "| epoch 108 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  2.37 | ppl    10.73 | acc     0.72 | train_ae_norm     1.00\n",
      "[108/200][1399/4361] Loss_D: 0.00171851 (Loss_D_real: 0.00014486 Loss_D_fake: 0.00157365) Loss_G: 0.45275947 Loss_Enh_Dec: -2.25053477\n",
      "| epoch 108 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.41 | loss  2.36 | ppl    10.55 | acc     0.67 | train_ae_norm     1.00\n",
      "[108/200][1499/4361] Loss_D: 0.00146099 (Loss_D_real: 0.00077118 Loss_D_fake: 0.00068981) Loss_G: 0.45075771 Loss_Enh_Dec: -2.30465531\n",
      "| epoch 108 |  1500/ 4361 batches | lr 0.000000 | ms/batch 400.65 | loss  2.41 | ppl    11.09 | acc     0.70 | train_ae_norm     1.00\n",
      "[108/200][1599/4361] Loss_D: 0.00189676 (Loss_D_real: 0.00115697 Loss_D_fake: 0.00073979) Loss_G: 0.43926698 Loss_Enh_Dec: -2.24431729\n",
      "| epoch 108 |  1600/ 4361 batches | lr 0.000000 | ms/batch 400.58 | loss  2.39 | ppl    10.96 | acc     0.71 | train_ae_norm     1.00\n",
      "[108/200][1699/4361] Loss_D: 0.00156020 (Loss_D_real: 0.00052337 Loss_D_fake: 0.00103683) Loss_G: 0.44878006 Loss_Enh_Dec: -2.31313539\n",
      "| epoch 108 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  2.35 | ppl    10.45 | acc     0.69 | train_ae_norm     1.00\n",
      "[108/200][1799/4361] Loss_D: 0.00121601 (Loss_D_real: 0.00055395 Loss_D_fake: 0.00066206) Loss_G: 0.43796387 Loss_Enh_Dec: -2.04094291\n",
      "| epoch 108 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  2.33 | ppl    10.27 | acc     0.71 | train_ae_norm     1.00\n",
      "[108/200][1899/4361] Loss_D: 0.00170273 (Loss_D_real: 0.00037546 Loss_D_fake: 0.00132728) Loss_G: 0.42974225 Loss_Enh_Dec: -1.67398453\n",
      "| epoch 108 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  2.37 | ppl    10.75 | acc     0.73 | train_ae_norm     1.00\n",
      "[108/200][1999/4361] Loss_D: 0.00153460 (Loss_D_real: 0.00005329 Loss_D_fake: 0.00148130) Loss_G: 0.45293427 Loss_Enh_Dec: -2.08178186\n",
      "| epoch 108 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.89 | loss  2.32 | ppl    10.21 | acc     0.74 | train_ae_norm     1.00\n",
      "[108/200][2099/4361] Loss_D: 0.00521297 (Loss_D_real: 0.00064126 Loss_D_fake: 0.00457171) Loss_G: 0.44603401 Loss_Enh_Dec: -2.00933337\n",
      "| epoch 108 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.08 | loss  2.36 | ppl    10.55 | acc     0.78 | train_ae_norm     1.00\n",
      "[108/200][2199/4361] Loss_D: 0.02081834 (Loss_D_real: 0.02014766 Loss_D_fake: 0.00067069) Loss_G: 0.43662998 Loss_Enh_Dec: -2.17776322\n",
      "| epoch 108 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  2.33 | ppl    10.26 | acc     0.75 | train_ae_norm     1.00\n",
      "[108/200][2299/4361] Loss_D: 0.00432790 (Loss_D_real: 0.00022167 Loss_D_fake: 0.00410623) Loss_G: 0.41648063 Loss_Enh_Dec: -2.17536902\n",
      "| epoch 108 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.54 | loss  2.33 | ppl    10.27 | acc     0.76 | train_ae_norm     1.00\n",
      "[108/200][2399/4361] Loss_D: 0.02641547 (Loss_D_real: 0.02458519 Loss_D_fake: 0.00183028) Loss_G: 0.41749907 Loss_Enh_Dec: -1.99608290\n",
      "| epoch 108 |  2400/ 4361 batches | lr 0.000000 | ms/batch 400.50 | loss  2.33 | ppl    10.28 | acc     0.68 | train_ae_norm     1.00\n",
      "[108/200][2499/4361] Loss_D: 0.00856068 (Loss_D_real: 0.00691179 Loss_D_fake: 0.00164888) Loss_G: 0.35681197 Loss_Enh_Dec: -2.00404382\n",
      "| epoch 108 |  2500/ 4361 batches | lr 0.000000 | ms/batch 400.81 | loss  2.36 | ppl    10.60 | acc     0.74 | train_ae_norm     1.00\n",
      "[108/200][2599/4361] Loss_D: 0.00439823 (Loss_D_real: 0.00122672 Loss_D_fake: 0.00317151) Loss_G: 0.44208214 Loss_Enh_Dec: -1.05732882\n",
      "| epoch 108 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.72 | loss  2.34 | ppl    10.39 | acc     0.69 | train_ae_norm     1.00\n",
      "[108/200][2699/4361] Loss_D: 0.00305655 (Loss_D_real: 0.00102009 Loss_D_fake: 0.00203646) Loss_G: 0.38757351 Loss_Enh_Dec: -2.38570404\n",
      "| epoch 108 |  2700/ 4361 batches | lr 0.000000 | ms/batch 400.38 | loss  2.36 | ppl    10.62 | acc     0.70 | train_ae_norm     1.00\n",
      "[108/200][2799/4361] Loss_D: 0.01180838 (Loss_D_real: 0.00495395 Loss_D_fake: 0.00685443) Loss_G: 0.42729527 Loss_Enh_Dec: -2.39963508\n",
      "| epoch 108 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.90 | loss  2.34 | ppl    10.38 | acc     0.71 | train_ae_norm     1.00\n",
      "[108/200][2899/4361] Loss_D: 0.00608428 (Loss_D_real: 0.00389316 Loss_D_fake: 0.00219111) Loss_G: 0.37334844 Loss_Enh_Dec: -2.50436068\n",
      "| epoch 108 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  2.37 | ppl    10.66 | acc     0.70 | train_ae_norm     1.00\n",
      "[108/200][2999/4361] Loss_D: 0.00253188 (Loss_D_real: 0.00096015 Loss_D_fake: 0.00157174) Loss_G: 0.37809500 Loss_Enh_Dec: -2.48617244\n",
      "| epoch 108 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.37 | ppl    10.68 | acc     0.70 | train_ae_norm     1.00\n",
      "[108/200][3099/4361] Loss_D: 0.01152543 (Loss_D_real: 0.01048128 Loss_D_fake: 0.00104415) Loss_G: 0.41192126 Loss_Enh_Dec: -2.57850623\n",
      "| epoch 108 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.21 | loss  2.40 | ppl    11.05 | acc     0.69 | train_ae_norm     1.00\n",
      "[108/200][3199/4361] Loss_D: 0.02398524 (Loss_D_real: 0.00019066 Loss_D_fake: 0.02379458) Loss_G: 0.41532207 Loss_Enh_Dec: -2.56563997\n",
      "| epoch 108 |  3200/ 4361 batches | lr 0.000000 | ms/batch 399.62 | loss  2.43 | ppl    11.34 | acc     0.70 | train_ae_norm     1.00\n",
      "[108/200][3299/4361] Loss_D: 0.00845216 (Loss_D_real: 0.00035474 Loss_D_fake: 0.00809743) Loss_G: 0.40394258 Loss_Enh_Dec: -2.42366338\n",
      "| epoch 108 |  3300/ 4361 batches | lr 0.000000 | ms/batch 400.20 | loss  2.44 | ppl    11.42 | acc     0.73 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[108/200][3399/4361] Loss_D: 0.04076076 (Loss_D_real: 0.03818241 Loss_D_fake: 0.00257835) Loss_G: 0.35596868 Loss_Enh_Dec: -2.70771289\n",
      "| epoch 108 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  2.42 | ppl    11.26 | acc     0.70 | train_ae_norm     1.00\n",
      "[108/200][3499/4361] Loss_D: 0.00310193 (Loss_D_real: 0.00033107 Loss_D_fake: 0.00277086) Loss_G: 0.38259983 Loss_Enh_Dec: -2.25973034\n",
      "| epoch 108 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.34 | loss  2.35 | ppl    10.47 | acc     0.72 | train_ae_norm     1.00\n",
      "[108/200][3599/4361] Loss_D: 0.00579562 (Loss_D_real: 0.00196387 Loss_D_fake: 0.00383174) Loss_G: 0.34346792 Loss_Enh_Dec: -2.22268128\n",
      "| epoch 108 |  3600/ 4361 batches | lr 0.000000 | ms/batch 399.81 | loss  2.37 | ppl    10.69 | acc     0.74 | train_ae_norm     1.00\n",
      "[108/200][3699/4361] Loss_D: 0.12739219 (Loss_D_real: 0.12524483 Loss_D_fake: 0.00214736) Loss_G: 0.36394596 Loss_Enh_Dec: -1.70548522\n",
      "| epoch 108 |  3700/ 4361 batches | lr 0.000000 | ms/batch 399.52 | loss  2.38 | ppl    10.85 | acc     0.70 | train_ae_norm     1.00\n",
      "[108/200][3799/4361] Loss_D: 0.25034168 (Loss_D_real: 0.00141226 Loss_D_fake: 0.24892943) Loss_G: 0.54122633 Loss_Enh_Dec: -2.21844149\n",
      "| epoch 108 |  3800/ 4361 batches | lr 0.000000 | ms/batch 399.31 | loss  2.42 | ppl    11.25 | acc     0.77 | train_ae_norm     1.00\n",
      "[108/200][3899/4361] Loss_D: 0.00261393 (Loss_D_real: 0.00049493 Loss_D_fake: 0.00211899) Loss_G: 0.37592074 Loss_Enh_Dec: -2.56982851\n",
      "| epoch 108 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.01 | loss  2.41 | ppl    11.12 | acc     0.70 | train_ae_norm     1.00\n",
      "[108/200][3999/4361] Loss_D: 0.00687348 (Loss_D_real: 0.00241359 Loss_D_fake: 0.00445988) Loss_G: 0.38717172 Loss_Enh_Dec: -2.53157115\n",
      "| epoch 108 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.76 | loss  2.41 | ppl    11.18 | acc     0.70 | train_ae_norm     1.00\n",
      "[108/200][4099/4361] Loss_D: 0.00209767 (Loss_D_real: 0.00090480 Loss_D_fake: 0.00119287) Loss_G: 0.74767524 Loss_Enh_Dec: -2.40289950\n",
      "| epoch 108 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.53 | loss  2.38 | ppl    10.76 | acc     0.75 | train_ae_norm     1.00\n",
      "[108/200][4199/4361] Loss_D: 0.00684103 (Loss_D_real: 0.00600325 Loss_D_fake: 0.00083777) Loss_G: 0.43488857 Loss_Enh_Dec: -2.27550316\n",
      "| epoch 108 |  4200/ 4361 batches | lr 0.000000 | ms/batch 399.95 | loss  2.43 | ppl    11.37 | acc     0.74 | train_ae_norm     1.00\n",
      "[108/200][4299/4361] Loss_D: 0.00265480 (Loss_D_real: 0.00027237 Loss_D_fake: 0.00238243) Loss_G: 0.38377830 Loss_Enh_Dec: -2.36592770\n",
      "| epoch 108 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.42 | ppl    11.28 | acc     0.74 | train_ae_norm     1.00\n",
      "| end of epoch 108 | time: 1848.57s | test loss  2.48 | test ppl 11.92 | acc 0.759\n",
      "bleu_self:  [3.21570843e-01 9.62641619e-02 7.73767094e-07 2.30131537e-09\n",
      " 7.34252430e-11]\n",
      "bleu_test:  [6.91305916e-01 1.48198129e-01 1.17393041e-06 3.54251673e-09\n",
      " 5.08215830e-10]\n",
      "bleu_self: [0.32157084,0.09626416,0.00000077,0.00000000,0.00000000]\n",
      "bleu_test: [0.69130592,0.14819813,0.00000117,0.00000000,0.00000000]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 109 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.698\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.497\n",
      "  Test Loss: 4.536\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 109 |     0/ 4361 batches | lr 0.000000 | ms/batch 866.61 | loss  0.02 | ppl     1.02 | acc     0.78 | train_ae_norm     1.00\n",
      "[109/200][99/4361] Loss_D: 0.00727989 (Loss_D_real: 0.00706510 Loss_D_fake: 0.00021479) Loss_G: 0.62445909 Loss_Enh_Dec: -2.07171535\n",
      "| epoch 109 |   100/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  2.40 | ppl    10.98 | acc     0.69 | train_ae_norm     1.00\n",
      "[109/200][199/4361] Loss_D: 0.00068440 (Loss_D_real: 0.00005037 Loss_D_fake: 0.00063403) Loss_G: 0.45155022 Loss_Enh_Dec: -2.41717386\n",
      "| epoch 109 |   200/ 4361 batches | lr 0.000000 | ms/batch 400.85 | loss  2.40 | ppl    11.06 | acc     0.74 | train_ae_norm     1.00\n",
      "[109/200][299/4361] Loss_D: 0.00518449 (Loss_D_real: 0.00104847 Loss_D_fake: 0.00413602) Loss_G: 0.33080938 Loss_Enh_Dec: -2.49574971\n",
      "| epoch 109 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.79 | loss  2.43 | ppl    11.32 | acc     0.67 | train_ae_norm     1.00\n",
      "[109/200][399/4361] Loss_D: 0.00102941 (Loss_D_real: 0.00006366 Loss_D_fake: 0.00096574) Loss_G: 0.38271394 Loss_Enh_Dec: -2.02036452\n",
      "| epoch 109 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.32 | ppl    10.17 | acc     0.73 | train_ae_norm     1.00\n",
      "[109/200][499/4361] Loss_D: 0.00121509 (Loss_D_real: 0.00018590 Loss_D_fake: 0.00102919) Loss_G: 0.42240477 Loss_Enh_Dec: -2.10640073\n",
      "| epoch 109 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  2.38 | ppl    10.79 | acc     0.75 | train_ae_norm     1.00\n",
      "[109/200][599/4361] Loss_D: 0.04123544 (Loss_D_real: 0.03834839 Loss_D_fake: 0.00288705) Loss_G: 0.39039156 Loss_Enh_Dec: -1.99853349\n",
      "| epoch 109 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  2.34 | ppl    10.43 | acc     0.70 | train_ae_norm     1.00\n",
      "[109/200][699/4361] Loss_D: 0.01052413 (Loss_D_real: 0.00742624 Loss_D_fake: 0.00309789) Loss_G: 0.33457428 Loss_Enh_Dec: -1.91518712\n",
      "| epoch 109 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  2.37 | ppl    10.74 | acc     0.74 | train_ae_norm     1.00\n",
      "[109/200][799/4361] Loss_D: 0.01065372 (Loss_D_real: 0.00013906 Loss_D_fake: 0.01051466) Loss_G: 0.43694314 Loss_Enh_Dec: -1.73690069\n",
      "| epoch 109 |   800/ 4361 batches | lr 0.000000 | ms/batch 402.06 | loss  2.34 | ppl    10.36 | acc     0.72 | train_ae_norm     1.00\n",
      "[109/200][899/4361] Loss_D: 0.01602496 (Loss_D_real: 0.01573915 Loss_D_fake: 0.00028581) Loss_G: 0.62416792 Loss_Enh_Dec: -1.90736294\n",
      "| epoch 109 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  2.36 | ppl    10.54 | acc     0.73 | train_ae_norm     1.00\n",
      "[109/200][999/4361] Loss_D: 0.02205111 (Loss_D_real: 0.01770712 Loss_D_fake: 0.00434398) Loss_G: 0.37006426 Loss_Enh_Dec: -1.89487875\n",
      "| epoch 109 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  2.34 | ppl    10.34 | acc     0.74 | train_ae_norm     1.00\n",
      "[109/200][1099/4361] Loss_D: 0.00497107 (Loss_D_real: 0.00174291 Loss_D_fake: 0.00322816) Loss_G: 0.35365698 Loss_Enh_Dec: -1.66576123\n",
      "| epoch 109 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.80 | loss  2.32 | ppl    10.21 | acc     0.70 | train_ae_norm     1.00\n",
      "[109/200][1199/4361] Loss_D: 0.01097928 (Loss_D_real: 0.00773724 Loss_D_fake: 0.00324204) Loss_G: 0.38894328 Loss_Enh_Dec: -1.91115463\n",
      "| epoch 109 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  2.31 | ppl    10.05 | acc     0.74 | train_ae_norm     1.00\n",
      "[109/200][1299/4361] Loss_D: 0.01507889 (Loss_D_real: 0.01322737 Loss_D_fake: 0.00185152) Loss_G: 0.39046502 Loss_Enh_Dec: -1.55772710\n",
      "| epoch 109 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  2.35 | ppl    10.44 | acc     0.73 | train_ae_norm     1.00\n",
      "[109/200][1399/4361] Loss_D: 0.00376149 (Loss_D_real: 0.00260737 Loss_D_fake: 0.00115412) Loss_G: 0.38123786 Loss_Enh_Dec: -2.35060668\n",
      "| epoch 109 |  1400/ 4361 batches | lr 0.000000 | ms/batch 402.14 | loss  2.33 | ppl    10.24 | acc     0.69 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[109/200][1499/4361] Loss_D: 0.01173508 (Loss_D_real: 0.01057265 Loss_D_fake: 0.00116243) Loss_G: 0.37850547 Loss_Enh_Dec: -2.01453424\n",
      "| epoch 109 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  2.39 | ppl    10.95 | acc     0.71 | train_ae_norm     1.00\n",
      "[109/200][1599/4361] Loss_D: 0.00467355 (Loss_D_real: 0.00101768 Loss_D_fake: 0.00365587) Loss_G: 0.34558126 Loss_Enh_Dec: -2.01305747\n",
      "| epoch 109 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  2.36 | ppl    10.57 | acc     0.72 | train_ae_norm     1.00\n",
      "[109/200][1699/4361] Loss_D: 0.00435103 (Loss_D_real: 0.00309984 Loss_D_fake: 0.00125119) Loss_G: 0.42998919 Loss_Enh_Dec: -2.24585581\n",
      "| epoch 109 |  1700/ 4361 batches | lr 0.000000 | ms/batch 402.58 | loss  2.33 | ppl    10.27 | acc     0.70 | train_ae_norm     1.00\n",
      "[109/200][1799/4361] Loss_D: 0.01755168 (Loss_D_real: 0.01501525 Loss_D_fake: 0.00253644) Loss_G: 0.34687290 Loss_Enh_Dec: -2.11283636\n",
      "| epoch 109 |  1800/ 4361 batches | lr 0.000000 | ms/batch 402.20 | loss  2.31 | ppl    10.11 | acc     0.72 | train_ae_norm     1.00\n",
      "[109/200][1899/4361] Loss_D: 0.02349032 (Loss_D_real: 0.02070905 Loss_D_fake: 0.00278126) Loss_G: 0.33776289 Loss_Enh_Dec: -2.12424088\n",
      "| epoch 109 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  2.38 | ppl    10.76 | acc     0.73 | train_ae_norm     1.00\n",
      "[109/200][1999/4361] Loss_D: 0.00273722 (Loss_D_real: 0.00036021 Loss_D_fake: 0.00237701) Loss_G: 0.34705791 Loss_Enh_Dec: -2.02725816\n",
      "| epoch 109 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  2.31 | ppl    10.07 | acc     0.76 | train_ae_norm     1.00\n",
      "[109/200][2099/4361] Loss_D: 0.00646318 (Loss_D_real: 0.00039406 Loss_D_fake: 0.00606913) Loss_G: 0.42296711 Loss_Enh_Dec: -1.67146230\n",
      "| epoch 109 |  2100/ 4361 batches | lr 0.000000 | ms/batch 402.17 | loss  2.35 | ppl    10.44 | acc     0.75 | train_ae_norm     1.00\n",
      "[109/200][2199/4361] Loss_D: 0.00432036 (Loss_D_real: 0.00185839 Loss_D_fake: 0.00246197) Loss_G: 0.52686375 Loss_Enh_Dec: -2.03548741\n",
      "| epoch 109 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  2.32 | ppl    10.20 | acc     0.74 | train_ae_norm     1.00\n",
      "[109/200][2299/4361] Loss_D: 0.00248738 (Loss_D_real: 0.00016586 Loss_D_fake: 0.00232152) Loss_G: 0.35891828 Loss_Enh_Dec: -1.62636971\n",
      "| epoch 109 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.33 | ppl    10.27 | acc     0.73 | train_ae_norm     1.00\n",
      "[109/200][2399/4361] Loss_D: 0.00190846 (Loss_D_real: 0.00026113 Loss_D_fake: 0.00164733) Loss_G: 0.33982617 Loss_Enh_Dec: -1.68170202\n",
      "| epoch 109 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.33 | ppl    10.26 | acc     0.69 | train_ae_norm     1.00\n",
      "[109/200][2499/4361] Loss_D: 0.01662444 (Loss_D_real: 0.00217610 Loss_D_fake: 0.01444834) Loss_G: 0.36151254 Loss_Enh_Dec: -1.63144779\n",
      "| epoch 109 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  2.37 | ppl    10.71 | acc     0.76 | train_ae_norm     1.00\n",
      "[109/200][2599/4361] Loss_D: 0.00216715 (Loss_D_real: 0.00185132 Loss_D_fake: 0.00031583) Loss_G: 0.55152810 Loss_Enh_Dec: -1.82285058\n",
      "| epoch 109 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.05 | loss  2.33 | ppl    10.24 | acc     0.69 | train_ae_norm     1.00\n",
      "[109/200][2699/4361] Loss_D: 0.00854557 (Loss_D_real: 0.00804657 Loss_D_fake: 0.00049901) Loss_G: 0.44575223 Loss_Enh_Dec: -1.44271016\n",
      "| epoch 109 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  2.34 | ppl    10.33 | acc     0.73 | train_ae_norm     1.00\n",
      "[109/200][2799/4361] Loss_D: 0.00906320 (Loss_D_real: 0.00384243 Loss_D_fake: 0.00522077) Loss_G: 0.69574881 Loss_Enh_Dec: -1.83695972\n",
      "| epoch 109 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  2.31 | ppl    10.08 | acc     0.70 | train_ae_norm     1.00\n",
      "[109/200][2899/4361] Loss_D: 0.00308103 (Loss_D_real: 0.00025470 Loss_D_fake: 0.00282634) Loss_G: 0.41314355 Loss_Enh_Dec: -1.66881752\n",
      "| epoch 109 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  2.32 | ppl    10.19 | acc     0.70 | train_ae_norm     1.00\n",
      "[109/200][2999/4361] Loss_D: 0.01542639 (Loss_D_real: 0.01484265 Loss_D_fake: 0.00058374) Loss_G: 0.40205166 Loss_Enh_Dec: -1.98148537\n",
      "| epoch 109 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.35 | ppl    10.47 | acc     0.71 | train_ae_norm     1.00\n",
      "[109/200][3099/4361] Loss_D: 0.00309410 (Loss_D_real: 0.00073522 Loss_D_fake: 0.00235887) Loss_G: 0.37515673 Loss_Enh_Dec: -1.61131477\n",
      "| epoch 109 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  2.35 | ppl    10.50 | acc     0.73 | train_ae_norm     1.00\n",
      "[109/200][3199/4361] Loss_D: 0.00222221 (Loss_D_real: 0.00028379 Loss_D_fake: 0.00193842) Loss_G: 0.40199605 Loss_Enh_Dec: -2.05665040\n",
      "| epoch 109 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  2.38 | ppl    10.75 | acc     0.74 | train_ae_norm     1.00\n",
      "[109/200][3299/4361] Loss_D: 0.12209594 (Loss_D_real: 0.11905268 Loss_D_fake: 0.00304326) Loss_G: 0.57446110 Loss_Enh_Dec: -2.28302312\n",
      "| epoch 109 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  2.36 | ppl    10.55 | acc     0.73 | train_ae_norm     1.00\n",
      "[109/200][3399/4361] Loss_D: 0.00179442 (Loss_D_real: 0.00090509 Loss_D_fake: 0.00088933) Loss_G: 0.40045223 Loss_Enh_Dec: -2.56795478\n",
      "| epoch 109 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  2.35 | ppl    10.48 | acc     0.70 | train_ae_norm     1.00\n",
      "[109/200][3499/4361] Loss_D: 0.00292743 (Loss_D_real: 0.00145700 Loss_D_fake: 0.00147043) Loss_G: 0.34683251 Loss_Enh_Dec: -2.36132574\n",
      "| epoch 109 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  2.29 | ppl     9.87 | acc     0.71 | train_ae_norm     1.00\n",
      "[109/200][3599/4361] Loss_D: 0.00393961 (Loss_D_real: 0.00216054 Loss_D_fake: 0.00177907) Loss_G: 0.37724075 Loss_Enh_Dec: -2.56229472\n",
      "| epoch 109 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  2.32 | ppl    10.13 | acc     0.73 | train_ae_norm     1.00\n",
      "[109/200][3699/4361] Loss_D: 0.00151273 (Loss_D_real: 0.00074456 Loss_D_fake: 0.00076816) Loss_G: 0.40490279 Loss_Enh_Dec: -2.45001960\n",
      "| epoch 109 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.31 | ppl    10.06 | acc     0.71 | train_ae_norm     1.00\n",
      "[109/200][3799/4361] Loss_D: 0.00373062 (Loss_D_real: 0.00068632 Loss_D_fake: 0.00304430) Loss_G: 0.35820389 Loss_Enh_Dec: -2.21184230\n",
      "| epoch 109 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  2.33 | ppl    10.28 | acc     0.77 | train_ae_norm     1.00\n",
      "[109/200][3899/4361] Loss_D: 0.00119891 (Loss_D_real: 0.00039768 Loss_D_fake: 0.00080123) Loss_G: 0.52110296 Loss_Enh_Dec: -2.34862185\n",
      "| epoch 109 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.34 | ppl    10.39 | acc     0.70 | train_ae_norm     1.00\n",
      "[109/200][3999/4361] Loss_D: 0.00217237 (Loss_D_real: 0.00090703 Loss_D_fake: 0.00126534) Loss_G: 0.34289190 Loss_Enh_Dec: -2.09107566\n",
      "| epoch 109 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.50 | loss  2.34 | ppl    10.35 | acc     0.71 | train_ae_norm     1.00\n",
      "[109/200][4099/4361] Loss_D: 0.00276024 (Loss_D_real: 0.00022483 Loss_D_fake: 0.00253541) Loss_G: 0.33184385 Loss_Enh_Dec: -2.12068629\n",
      "| epoch 109 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  2.30 | ppl    10.01 | acc     0.75 | train_ae_norm     1.00\n",
      "[109/200][4199/4361] Loss_D: 0.01709775 (Loss_D_real: 0.01483488 Loss_D_fake: 0.00226287) Loss_G: 0.43733913 Loss_Enh_Dec: -2.05851436\n",
      "| epoch 109 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  2.34 | ppl    10.40 | acc     0.75 | train_ae_norm     1.00\n",
      "[109/200][4299/4361] Loss_D: 0.05831721 (Loss_D_real: 0.05730570 Loss_D_fake: 0.00101151) Loss_G: 0.35132408 Loss_Enh_Dec: -2.45795727\n",
      "| epoch 109 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  2.31 | ppl    10.03 | acc     0.74 | train_ae_norm     1.00\n",
      "| end of epoch 109 | time: 1853.02s | test loss  2.37 | test ppl 10.73 | acc 0.765\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 110 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.699\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.497\n",
      "  Test Loss: 4.384\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 110 |     0/ 4361 batches | lr 0.000000 | ms/batch 861.79 | loss  0.02 | ppl     1.02 | acc     0.77 | train_ae_norm     1.00\n",
      "[110/200][99/4361] Loss_D: 0.00581840 (Loss_D_real: 0.00132764 Loss_D_fake: 0.00449075) Loss_G: 0.36448199 Loss_Enh_Dec: -2.64570689\n",
      "| epoch 110 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.31 | ppl    10.03 | acc     0.70 | train_ae_norm     1.00\n",
      "[110/200][199/4361] Loss_D: 0.07427770 (Loss_D_real: 0.00118811 Loss_D_fake: 0.07308959) Loss_G: 0.32213816 Loss_Enh_Dec: -2.26042819\n",
      "| epoch 110 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  2.32 | ppl    10.21 | acc     0.74 | train_ae_norm     1.00\n",
      "[110/200][299/4361] Loss_D: 0.00519231 (Loss_D_real: 0.00091186 Loss_D_fake: 0.00428045) Loss_G: 0.39870080 Loss_Enh_Dec: -2.20321465\n",
      "| epoch 110 |   300/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  2.34 | ppl    10.41 | acc     0.66 | train_ae_norm     1.00\n",
      "[110/200][399/4361] Loss_D: 0.00855335 (Loss_D_real: 0.00473199 Loss_D_fake: 0.00382136) Loss_G: 0.34203833 Loss_Enh_Dec: -2.04158258\n",
      "| epoch 110 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.25 | ppl     9.47 | acc     0.74 | train_ae_norm     1.00\n",
      "[110/200][499/4361] Loss_D: 0.00741901 (Loss_D_real: 0.00474589 Loss_D_fake: 0.00267311) Loss_G: 0.37035683 Loss_Enh_Dec: -2.26270747\n",
      "| epoch 110 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  2.30 | ppl     9.96 | acc     0.76 | train_ae_norm     1.00\n",
      "[110/200][599/4361] Loss_D: 0.00405251 (Loss_D_real: 0.00296724 Loss_D_fake: 0.00108527) Loss_G: 0.39337865 Loss_Enh_Dec: -2.42295718\n",
      "| epoch 110 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  2.28 | ppl     9.74 | acc     0.70 | train_ae_norm     1.00\n",
      "[110/200][699/4361] Loss_D: 0.00228888 (Loss_D_real: 0.00097360 Loss_D_fake: 0.00131528) Loss_G: 0.36041775 Loss_Enh_Dec: -2.12131929\n",
      "| epoch 110 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.32 | ppl    10.13 | acc     0.72 | train_ae_norm     1.00\n",
      "[110/200][799/4361] Loss_D: 0.00361194 (Loss_D_real: 0.00171664 Loss_D_fake: 0.00189530) Loss_G: 0.50579894 Loss_Enh_Dec: -2.01864314\n",
      "| epoch 110 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  2.30 | ppl    10.00 | acc     0.70 | train_ae_norm     1.00\n",
      "[110/200][899/4361] Loss_D: 0.00220941 (Loss_D_real: 0.00030934 Loss_D_fake: 0.00190007) Loss_G: 0.37576252 Loss_Enh_Dec: -2.08419108\n",
      "| epoch 110 |   900/ 4361 batches | lr 0.000000 | ms/batch 402.00 | loss  2.35 | ppl    10.51 | acc     0.75 | train_ae_norm     1.00\n",
      "[110/200][999/4361] Loss_D: 0.17751244 (Loss_D_real: 0.17526731 Loss_D_fake: 0.00224514) Loss_G: 0.37856224 Loss_Enh_Dec: -1.78338647\n",
      "| epoch 110 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  2.33 | ppl    10.24 | acc     0.71 | train_ae_norm     1.00\n",
      "[110/200][1099/4361] Loss_D: 0.01966931 (Loss_D_real: 0.00163554 Loss_D_fake: 0.01803377) Loss_G: 0.44347402 Loss_Enh_Dec: -1.82886851\n",
      "| epoch 110 |  1100/ 4361 batches | lr 0.000000 | ms/batch 402.16 | loss  2.32 | ppl    10.13 | acc     0.71 | train_ae_norm     1.00\n",
      "[110/200][1199/4361] Loss_D: 0.00821102 (Loss_D_real: 0.00285817 Loss_D_fake: 0.00535285) Loss_G: 0.36190686 Loss_Enh_Dec: -2.06397820\n",
      "| epoch 110 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.84 | loss  2.32 | ppl    10.16 | acc     0.76 | train_ae_norm     1.00\n",
      "[110/200][1299/4361] Loss_D: 0.00221987 (Loss_D_real: 0.00019776 Loss_D_fake: 0.00202211) Loss_G: 0.36581275 Loss_Enh_Dec: -2.35865593\n",
      "| epoch 110 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.34 | ppl    10.41 | acc     0.74 | train_ae_norm     1.00\n",
      "[110/200][1399/4361] Loss_D: 0.00452149 (Loss_D_real: 0.00301212 Loss_D_fake: 0.00150937) Loss_G: 0.44761974 Loss_Enh_Dec: -2.17035532\n",
      "| epoch 110 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  2.34 | ppl    10.40 | acc     0.70 | train_ae_norm     1.00\n",
      "[110/200][1499/4361] Loss_D: 0.00388320 (Loss_D_real: 0.00226829 Loss_D_fake: 0.00161491) Loss_G: 0.33736277 Loss_Enh_Dec: -1.63439584\n",
      "| epoch 110 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  2.39 | ppl    10.96 | acc     0.71 | train_ae_norm     1.00\n",
      "[110/200][1599/4361] Loss_D: 0.00753142 (Loss_D_real: 0.00055704 Loss_D_fake: 0.00697439) Loss_G: 0.48777780 Loss_Enh_Dec: -2.16534495\n",
      "| epoch 110 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  2.39 | ppl    10.96 | acc     0.71 | train_ae_norm     1.00\n",
      "[110/200][1699/4361] Loss_D: 0.01370612 (Loss_D_real: 0.01257406 Loss_D_fake: 0.00113206) Loss_G: 0.45431718 Loss_Enh_Dec: -1.90446556\n",
      "| epoch 110 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  2.37 | ppl    10.73 | acc     0.70 | train_ae_norm     1.00\n",
      "[110/200][1799/4361] Loss_D: 0.00589994 (Loss_D_real: 0.00441010 Loss_D_fake: 0.00148984) Loss_G: 0.38973108 Loss_Enh_Dec: -2.18372869\n",
      "| epoch 110 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  2.33 | ppl    10.27 | acc     0.72 | train_ae_norm     1.00\n",
      "[110/200][1899/4361] Loss_D: 0.02430096 (Loss_D_real: 0.01718853 Loss_D_fake: 0.00711242) Loss_G: 0.39387831 Loss_Enh_Dec: -1.61546028\n",
      "| epoch 110 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.38 | ppl    10.81 | acc     0.73 | train_ae_norm     1.00\n",
      "[110/200][1999/4361] Loss_D: 0.01399567 (Loss_D_real: 0.00638946 Loss_D_fake: 0.00760621) Loss_G: 0.38393268 Loss_Enh_Dec: -2.02457595\n",
      "| epoch 110 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  2.33 | ppl    10.29 | acc     0.73 | train_ae_norm     1.00\n",
      "[110/200][2099/4361] Loss_D: 0.00159610 (Loss_D_real: 0.00032909 Loss_D_fake: 0.00126701) Loss_G: 0.43332177 Loss_Enh_Dec: -1.66187346\n",
      "| epoch 110 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.37 | ppl    10.71 | acc     0.76 | train_ae_norm     1.00\n",
      "[110/200][2199/4361] Loss_D: 0.00339298 (Loss_D_real: 0.00225375 Loss_D_fake: 0.00113922) Loss_G: 0.36359635 Loss_Enh_Dec: -1.69308686\n",
      "| epoch 110 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  2.33 | ppl    10.26 | acc     0.75 | train_ae_norm     1.00\n",
      "[110/200][2299/4361] Loss_D: 0.02027399 (Loss_D_real: 0.01835020 Loss_D_fake: 0.00192380) Loss_G: 0.43168131 Loss_Enh_Dec: -2.33698583\n",
      "| epoch 110 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  2.32 | ppl    10.21 | acc     0.74 | train_ae_norm     1.00\n",
      "[110/200][2399/4361] Loss_D: 0.00267782 (Loss_D_real: 0.00011599 Loss_D_fake: 0.00256183) Loss_G: 0.39534053 Loss_Enh_Dec: -1.99764097\n",
      "| epoch 110 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  2.34 | ppl    10.35 | acc     0.69 | train_ae_norm     1.00\n",
      "[110/200][2499/4361] Loss_D: 0.00313368 (Loss_D_real: 0.00154696 Loss_D_fake: 0.00158672) Loss_G: 0.40799457 Loss_Enh_Dec: -2.27657557\n",
      "| epoch 110 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.08 | loss  2.37 | ppl    10.74 | acc     0.73 | train_ae_norm     1.00\n",
      "[110/200][2599/4361] Loss_D: 0.00463134 (Loss_D_real: 0.00207029 Loss_D_fake: 0.00256106) Loss_G: 0.41240463 Loss_Enh_Dec: -2.13409781\n",
      "| epoch 110 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  2.34 | ppl    10.38 | acc     0.69 | train_ae_norm     1.00\n",
      "[110/200][2699/4361] Loss_D: 0.00420018 (Loss_D_real: 0.00261153 Loss_D_fake: 0.00158865) Loss_G: 0.44649768 Loss_Enh_Dec: -2.05187035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 110 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  2.35 | ppl    10.44 | acc     0.72 | train_ae_norm     1.00\n",
      "[110/200][2799/4361] Loss_D: 0.00470057 (Loss_D_real: 0.00174646 Loss_D_fake: 0.00295412) Loss_G: 0.41233954 Loss_Enh_Dec: -2.22278190\n",
      "| epoch 110 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  2.31 | ppl    10.05 | acc     0.70 | train_ae_norm     1.00\n",
      "[110/200][2899/4361] Loss_D: 0.00117615 (Loss_D_real: 0.00036918 Loss_D_fake: 0.00080697) Loss_G: 0.40676576 Loss_Enh_Dec: -1.41449249\n",
      "| epoch 110 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.72 | loss  2.33 | ppl    10.24 | acc     0.72 | train_ae_norm     1.00\n",
      "[110/200][2999/4361] Loss_D: 0.01809627 (Loss_D_real: 0.01648848 Loss_D_fake: 0.00160780) Loss_G: 0.43827373 Loss_Enh_Dec: -2.26467872\n",
      "| epoch 110 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  2.33 | ppl    10.30 | acc     0.72 | train_ae_norm     1.00\n",
      "[110/200][3099/4361] Loss_D: 0.00233188 (Loss_D_real: 0.00025820 Loss_D_fake: 0.00207367) Loss_G: 0.38384435 Loss_Enh_Dec: -2.35276175\n",
      "| epoch 110 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.34 | ppl    10.42 | acc     0.72 | train_ae_norm     1.00\n",
      "[110/200][3199/4361] Loss_D: 0.00138968 (Loss_D_real: 0.00030039 Loss_D_fake: 0.00108929) Loss_G: 0.36371085 Loss_Enh_Dec: -2.48526168\n",
      "| epoch 110 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  2.37 | ppl    10.70 | acc     0.75 | train_ae_norm     1.00\n",
      "[110/200][3299/4361] Loss_D: 0.02027204 (Loss_D_real: 0.01823547 Loss_D_fake: 0.00203657) Loss_G: 0.48864070 Loss_Enh_Dec: -2.04513383\n",
      "| epoch 110 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.38 | ppl    10.77 | acc     0.73 | train_ae_norm     1.00\n",
      "[110/200][3399/4361] Loss_D: 0.00142842 (Loss_D_real: 0.00078945 Loss_D_fake: 0.00063898) Loss_G: 0.52969533 Loss_Enh_Dec: -2.12299037\n",
      "| epoch 110 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  2.35 | ppl    10.46 | acc     0.72 | train_ae_norm     1.00\n",
      "[110/200][3499/4361] Loss_D: 0.00219530 (Loss_D_real: 0.00053708 Loss_D_fake: 0.00165822) Loss_G: 0.36661163 Loss_Enh_Dec: -1.89890897\n",
      "| epoch 110 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.28 | ppl     9.79 | acc     0.72 | train_ae_norm     1.00\n",
      "[110/200][3599/4361] Loss_D: 0.00136627 (Loss_D_real: 0.00092298 Loss_D_fake: 0.00044329) Loss_G: 0.41706547 Loss_Enh_Dec: -1.90014923\n",
      "| epoch 110 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  2.29 | ppl     9.87 | acc     0.73 | train_ae_norm     1.00\n",
      "[110/200][3699/4361] Loss_D: 0.00075279 (Loss_D_real: 0.00011992 Loss_D_fake: 0.00063288) Loss_G: 0.43983260 Loss_Enh_Dec: -2.29098368\n",
      "| epoch 110 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  2.32 | ppl    10.17 | acc     0.69 | train_ae_norm     1.00\n",
      "[110/200][3799/4361] Loss_D: 0.00096687 (Loss_D_real: 0.00037866 Loss_D_fake: 0.00058821) Loss_G: 0.54980487 Loss_Enh_Dec: -2.26893520\n",
      "| epoch 110 |  3800/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  2.34 | ppl    10.33 | acc     0.77 | train_ae_norm     1.00\n",
      "[110/200][3899/4361] Loss_D: 0.00304534 (Loss_D_real: 0.00115219 Loss_D_fake: 0.00189315) Loss_G: 0.38369304 Loss_Enh_Dec: -2.18292594\n",
      "| epoch 110 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  2.35 | ppl    10.49 | acc     0.70 | train_ae_norm     1.00\n",
      "[110/200][3999/4361] Loss_D: 0.00198110 (Loss_D_real: 0.00012521 Loss_D_fake: 0.00185590) Loss_G: 0.43028507 Loss_Enh_Dec: -2.46920848\n",
      "| epoch 110 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  2.37 | ppl    10.68 | acc     0.70 | train_ae_norm     1.00\n",
      "[110/200][4099/4361] Loss_D: 0.03845015 (Loss_D_real: 0.02021297 Loss_D_fake: 0.01823719) Loss_G: 0.58910960 Loss_Enh_Dec: -2.31634116\n",
      "| epoch 110 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.84 | loss  2.32 | ppl    10.20 | acc     0.71 | train_ae_norm     1.00\n",
      "[110/200][4199/4361] Loss_D: 0.00250059 (Loss_D_real: 0.00008700 Loss_D_fake: 0.00241360) Loss_G: 0.34633848 Loss_Enh_Dec: -1.97543561\n",
      "| epoch 110 |  4200/ 4361 batches | lr 0.000000 | ms/batch 400.80 | loss  2.36 | ppl    10.57 | acc     0.75 | train_ae_norm     1.00\n",
      "[110/200][4299/4361] Loss_D: 0.00158270 (Loss_D_real: 0.00028035 Loss_D_fake: 0.00130235) Loss_G: 0.37049967 Loss_Enh_Dec: -2.89411044\n",
      "| epoch 110 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  2.33 | ppl    10.28 | acc     0.74 | train_ae_norm     1.00\n",
      "| end of epoch 110 | time: 1852.18s | test loss  2.38 | test ppl 10.81 | acc 0.763\n",
      "bleu_self:  [4.10416667e-01 2.96988012e-01 9.21026217e-02 1.59800129e-05\n",
      " 1.00920497e-07]\n",
      "bleu_test:  [7.25000000e-01 1.24736501e-08 3.58244837e-11 2.14954552e-12\n",
      " 9.86712716e-12]\n",
      "bleu_self: [0.41041667,0.29698801,0.09210262,0.00001598,0.00000010]\n",
      "bleu_test: [0.72500000,0.00000001,0.00000000,0.00000000,0.00000000]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 111 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.700\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.505\n",
      "  Test Loss: 4.456\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 111 |     0/ 4361 batches | lr 0.000000 | ms/batch 863.00 | loss  0.02 | ppl     1.02 | acc     0.77 | train_ae_norm     1.00\n",
      "[111/200][99/4361] Loss_D: 0.00474862 (Loss_D_real: 0.00458346 Loss_D_fake: 0.00016516) Loss_G: 0.83635437 Loss_Enh_Dec: -1.76135755\n",
      "| epoch 111 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.34 | ppl    10.36 | acc     0.70 | train_ae_norm     1.00\n",
      "[111/200][199/4361] Loss_D: 0.00490033 (Loss_D_real: 0.00040409 Loss_D_fake: 0.00449624) Loss_G: 0.43387157 Loss_Enh_Dec: -2.24120450\n",
      "| epoch 111 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  2.36 | ppl    10.54 | acc     0.73 | train_ae_norm     1.00\n",
      "[111/200][299/4361] Loss_D: 0.00651881 (Loss_D_real: 0.00499658 Loss_D_fake: 0.00152223) Loss_G: 0.36948431 Loss_Enh_Dec: -1.79729843\n",
      "| epoch 111 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  2.36 | ppl    10.63 | acc     0.66 | train_ae_norm     1.00\n",
      "[111/200][399/4361] Loss_D: 0.00268376 (Loss_D_real: 0.00176219 Loss_D_fake: 0.00092157) Loss_G: 0.39181572 Loss_Enh_Dec: -2.33313131\n",
      "| epoch 111 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.89 | loss  2.27 | ppl     9.71 | acc     0.73 | train_ae_norm     1.00\n",
      "[111/200][499/4361] Loss_D: 0.01154935 (Loss_D_real: 0.00966195 Loss_D_fake: 0.00188740) Loss_G: 0.39533049 Loss_Enh_Dec: -2.40175843\n",
      "| epoch 111 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  2.36 | ppl    10.62 | acc     0.75 | train_ae_norm     1.00\n",
      "[111/200][599/4361] Loss_D: 0.00719975 (Loss_D_real: 0.00316947 Loss_D_fake: 0.00403028) Loss_G: 0.36037672 Loss_Enh_Dec: -2.12160349\n",
      "| epoch 111 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  2.31 | ppl    10.07 | acc     0.71 | train_ae_norm     1.00\n",
      "[111/200][699/4361] Loss_D: 0.07379166 (Loss_D_real: 0.07326050 Loss_D_fake: 0.00053116) Loss_G: 0.65614861 Loss_Enh_Dec: -2.28915381\n",
      "| epoch 111 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  2.37 | ppl    10.68 | acc     0.71 | train_ae_norm     1.00\n",
      "[111/200][799/4361] Loss_D: 0.01200720 (Loss_D_real: 0.00022200 Loss_D_fake: 0.01178520) Loss_G: 0.46853501 Loss_Enh_Dec: -2.43342853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 111 |   800/ 4361 batches | lr 0.000000 | ms/batch 402.31 | loss  2.33 | ppl    10.33 | acc     0.71 | train_ae_norm     1.00\n",
      "[111/200][899/4361] Loss_D: 0.00148005 (Loss_D_real: 0.00030123 Loss_D_fake: 0.00117882) Loss_G: 0.41069242 Loss_Enh_Dec: -2.47907901\n",
      "| epoch 111 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  2.33 | ppl    10.28 | acc     0.75 | train_ae_norm     1.00\n",
      "[111/200][999/4361] Loss_D: 0.00167160 (Loss_D_real: 0.00010997 Loss_D_fake: 0.00156163) Loss_G: 0.41202483 Loss_Enh_Dec: -2.30365062\n",
      "| epoch 111 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  2.32 | ppl    10.19 | acc     0.73 | train_ae_norm     1.00\n",
      "[111/200][1099/4361] Loss_D: 0.00467658 (Loss_D_real: 0.00201658 Loss_D_fake: 0.00265999) Loss_G: 0.39901519 Loss_Enh_Dec: -2.14797950\n",
      "| epoch 111 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  2.31 | ppl    10.09 | acc     0.71 | train_ae_norm     1.00\n",
      "[111/200][1199/4361] Loss_D: 0.00256709 (Loss_D_real: 0.00054541 Loss_D_fake: 0.00202168) Loss_G: 0.41136813 Loss_Enh_Dec: -2.79061580\n",
      "| epoch 111 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  2.32 | ppl    10.15 | acc     0.72 | train_ae_norm     1.00\n",
      "[111/200][1299/4361] Loss_D: 0.00425792 (Loss_D_real: 0.00355225 Loss_D_fake: 0.00070567) Loss_G: 0.38485050 Loss_Enh_Dec: -2.37013459\n",
      "| epoch 111 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  2.35 | ppl    10.46 | acc     0.73 | train_ae_norm     1.00\n",
      "[111/200][1399/4361] Loss_D: 0.00671779 (Loss_D_real: 0.00541769 Loss_D_fake: 0.00130011) Loss_G: 0.41571951 Loss_Enh_Dec: -2.48833919\n",
      "| epoch 111 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  2.32 | ppl    10.21 | acc     0.69 | train_ae_norm     1.00\n",
      "[111/200][1499/4361] Loss_D: 0.00257536 (Loss_D_real: 0.00160263 Loss_D_fake: 0.00097273) Loss_G: 0.41226798 Loss_Enh_Dec: -2.60410190\n",
      "| epoch 111 |  1500/ 4361 batches | lr 0.000000 | ms/batch 400.37 | loss  2.37 | ppl    10.73 | acc     0.71 | train_ae_norm     1.00\n",
      "[111/200][1599/4361] Loss_D: 0.00083167 (Loss_D_real: 0.00016338 Loss_D_fake: 0.00066830) Loss_G: 0.38663694 Loss_Enh_Dec: -2.53291869\n",
      "| epoch 111 |  1600/ 4361 batches | lr 0.000000 | ms/batch 400.31 | loss  2.35 | ppl    10.53 | acc     0.72 | train_ae_norm     1.00\n",
      "[111/200][1699/4361] Loss_D: 0.00167361 (Loss_D_real: 0.00046417 Loss_D_fake: 0.00120945) Loss_G: 0.40723488 Loss_Enh_Dec: -2.46159339\n",
      "| epoch 111 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.25 | loss  2.32 | ppl    10.15 | acc     0.71 | train_ae_norm     1.00\n",
      "[111/200][1799/4361] Loss_D: 0.00220704 (Loss_D_real: 0.00008851 Loss_D_fake: 0.00211853) Loss_G: 0.42847839 Loss_Enh_Dec: -2.30719805\n",
      "| epoch 111 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.21 | loss  2.29 | ppl     9.88 | acc     0.73 | train_ae_norm     1.00\n",
      "[111/200][1899/4361] Loss_D: 0.00092665 (Loss_D_real: 0.00007918 Loss_D_fake: 0.00084747) Loss_G: 0.43470278 Loss_Enh_Dec: -2.62710690\n",
      "| epoch 111 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.33 | loss  2.35 | ppl    10.43 | acc     0.72 | train_ae_norm     1.00\n",
      "[111/200][1999/4361] Loss_D: 0.00397995 (Loss_D_real: 0.00281601 Loss_D_fake: 0.00116394) Loss_G: 0.40206105 Loss_Enh_Dec: -2.44749498\n",
      "| epoch 111 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.86 | loss  2.30 | ppl     9.94 | acc     0.75 | train_ae_norm     1.00\n",
      "[111/200][2099/4361] Loss_D: 0.00117405 (Loss_D_real: 0.00041637 Loss_D_fake: 0.00075768) Loss_G: 0.68135870 Loss_Enh_Dec: -2.10733914\n",
      "| epoch 111 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.57 | loss  2.32 | ppl    10.21 | acc     0.76 | train_ae_norm     1.00\n",
      "[111/200][2199/4361] Loss_D: 0.01850168 (Loss_D_real: 0.01651764 Loss_D_fake: 0.00198404) Loss_G: 0.64669871 Loss_Enh_Dec: -2.41647458\n",
      "| epoch 111 |  2200/ 4361 batches | lr 0.000000 | ms/batch 399.95 | loss  2.29 | ppl     9.89 | acc     0.76 | train_ae_norm     1.00\n",
      "[111/200][2299/4361] Loss_D: 0.00338663 (Loss_D_real: 0.00216433 Loss_D_fake: 0.00122230) Loss_G: 0.37063739 Loss_Enh_Dec: -2.23949575\n",
      "| epoch 111 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.74 | loss  2.29 | ppl     9.88 | acc     0.76 | train_ae_norm     1.00\n",
      "[111/200][2399/4361] Loss_D: 0.00039881 (Loss_D_real: 0.00005414 Loss_D_fake: 0.00034467) Loss_G: 0.38533854 Loss_Enh_Dec: -2.04876232\n",
      "| epoch 111 |  2400/ 4361 batches | lr 0.000000 | ms/batch 400.63 | loss  2.31 | ppl    10.03 | acc     0.71 | train_ae_norm     1.00\n",
      "[111/200][2499/4361] Loss_D: 0.00105034 (Loss_D_real: 0.00029371 Loss_D_fake: 0.00075664) Loss_G: 0.43548271 Loss_Enh_Dec: -2.19375038\n",
      "| epoch 111 |  2500/ 4361 batches | lr 0.000000 | ms/batch 399.88 | loss  2.34 | ppl    10.37 | acc     0.75 | train_ae_norm     1.00\n",
      "[111/200][2599/4361] Loss_D: 0.00211194 (Loss_D_real: 0.00054638 Loss_D_fake: 0.00156555) Loss_G: 0.41159907 Loss_Enh_Dec: -2.28962374\n",
      "| epoch 111 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.05 | loss  2.31 | ppl    10.12 | acc     0.69 | train_ae_norm     1.00\n",
      "[111/200][2699/4361] Loss_D: 0.02454616 (Loss_D_real: 0.00010574 Loss_D_fake: 0.02444042) Loss_G: 0.66759068 Loss_Enh_Dec: -2.29563379\n",
      "| epoch 111 |  2700/ 4361 batches | lr 0.000000 | ms/batch 400.01 | loss  2.31 | ppl    10.10 | acc     0.72 | train_ae_norm     1.00\n",
      "[111/200][2799/4361] Loss_D: 0.00204839 (Loss_D_real: 0.00079413 Loss_D_fake: 0.00125426) Loss_G: 0.36412874 Loss_Enh_Dec: -2.32834625\n",
      "| epoch 111 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.06 | loss  2.28 | ppl     9.79 | acc     0.68 | train_ae_norm     1.00\n",
      "[111/200][2899/4361] Loss_D: 0.00414309 (Loss_D_real: 0.00083390 Loss_D_fake: 0.00330919) Loss_G: 0.36258343 Loss_Enh_Dec: -2.45354652\n",
      "| epoch 111 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  2.28 | ppl     9.78 | acc     0.73 | train_ae_norm     1.00\n",
      "[111/200][2999/4361] Loss_D: 0.00239295 (Loss_D_real: 0.00043974 Loss_D_fake: 0.00195321) Loss_G: 0.39244375 Loss_Enh_Dec: -2.30105138\n",
      "| epoch 111 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.65 | loss  2.31 | ppl    10.04 | acc     0.72 | train_ae_norm     1.00\n",
      "[111/200][3099/4361] Loss_D: 0.00359461 (Loss_D_real: 0.00160015 Loss_D_fake: 0.00199446) Loss_G: 0.44218865 Loss_Enh_Dec: -2.53404689\n",
      "| epoch 111 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  2.32 | ppl    10.14 | acc     0.71 | train_ae_norm     1.00\n",
      "[111/200][3199/4361] Loss_D: 0.00142222 (Loss_D_real: 0.00102577 Loss_D_fake: 0.00039645) Loss_G: 0.46645576 Loss_Enh_Dec: -1.81997073\n",
      "| epoch 111 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  2.32 | ppl    10.22 | acc     0.75 | train_ae_norm     1.00\n",
      "[111/200][3299/4361] Loss_D: 0.01734490 (Loss_D_real: 0.01490816 Loss_D_fake: 0.00243675) Loss_G: 0.41072232 Loss_Enh_Dec: -1.99802971\n",
      "| epoch 111 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  2.34 | ppl    10.37 | acc     0.72 | train_ae_norm     1.00\n",
      "[111/200][3399/4361] Loss_D: 0.00102831 (Loss_D_real: 0.00040610 Loss_D_fake: 0.00062221) Loss_G: 0.42466718 Loss_Enh_Dec: -2.34037757\n",
      "| epoch 111 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.90 | loss  2.33 | ppl    10.25 | acc     0.71 | train_ae_norm     1.00\n",
      "[111/200][3499/4361] Loss_D: 0.00182893 (Loss_D_real: 0.00010594 Loss_D_fake: 0.00172299) Loss_G: 0.41253549 Loss_Enh_Dec: -2.20744061\n",
      "| epoch 111 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.73 | loss  2.29 | ppl     9.86 | acc     0.71 | train_ae_norm     1.00\n",
      "[111/200][3599/4361] Loss_D: 0.00232182 (Loss_D_real: 0.00127129 Loss_D_fake: 0.00105053) Loss_G: 0.46319461 Loss_Enh_Dec: -2.31156516\n",
      "| epoch 111 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  2.32 | ppl    10.13 | acc     0.75 | train_ae_norm     1.00\n",
      "[111/200][3699/4361] Loss_D: 0.00356061 (Loss_D_real: 0.00205792 Loss_D_fake: 0.00150268) Loss_G: 0.48308954 Loss_Enh_Dec: -2.55786657\n",
      "| epoch 111 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.79 | loss  2.36 | ppl    10.56 | acc     0.72 | train_ae_norm     1.00\n",
      "[111/200][3799/4361] Loss_D: 0.00464740 (Loss_D_real: 0.00383597 Loss_D_fake: 0.00081144) Loss_G: 0.46748647 Loss_Enh_Dec: -2.38647628\n",
      "| epoch 111 |  3800/ 4361 batches | lr 0.000000 | ms/batch 400.61 | loss  2.34 | ppl    10.41 | acc     0.76 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111/200][3899/4361] Loss_D: 0.00150301 (Loss_D_real: 0.00016966 Loss_D_fake: 0.00133335) Loss_G: 0.51653647 Loss_Enh_Dec: -1.95350647\n",
      "| epoch 111 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  2.32 | ppl    10.19 | acc     0.70 | train_ae_norm     1.00\n",
      "[111/200][3999/4361] Loss_D: 0.00077799 (Loss_D_real: 0.00004451 Loss_D_fake: 0.00073348) Loss_G: 0.40898535 Loss_Enh_Dec: -1.92406178\n",
      "| epoch 111 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  2.32 | ppl    10.21 | acc     0.73 | train_ae_norm     1.00\n",
      "[111/200][4099/4361] Loss_D: 0.00163398 (Loss_D_real: 0.00043036 Loss_D_fake: 0.00120363) Loss_G: 0.41659591 Loss_Enh_Dec: -2.45647287\n",
      "| epoch 111 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.28 | ppl     9.79 | acc     0.72 | train_ae_norm     1.00\n",
      "[111/200][4199/4361] Loss_D: 0.00354877 (Loss_D_real: 0.00245745 Loss_D_fake: 0.00109132) Loss_G: 0.44667587 Loss_Enh_Dec: -2.14439702\n",
      "| epoch 111 |  4200/ 4361 batches | lr 0.000000 | ms/batch 400.93 | loss  2.33 | ppl    10.31 | acc     0.76 | train_ae_norm     1.00\n",
      "[111/200][4299/4361] Loss_D: 0.00238758 (Loss_D_real: 0.00008866 Loss_D_fake: 0.00229892) Loss_G: 0.46271840 Loss_Enh_Dec: -2.12943053\n",
      "| epoch 111 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  2.30 | ppl     9.95 | acc     0.74 | train_ae_norm     1.00\n",
      "| end of epoch 111 | time: 1850.42s | test loss  2.36 | test ppl 10.57 | acc 0.767\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 112 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.701\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.505\n",
      "  Test Loss: 4.485\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 112 |     0/ 4361 batches | lr 0.000000 | ms/batch 865.39 | loss  0.02 | ppl     1.02 | acc     0.77 | train_ae_norm     1.00\n",
      "[112/200][99/4361] Loss_D: 0.00248614 (Loss_D_real: 0.00078372 Loss_D_fake: 0.00170242) Loss_G: 0.42483100 Loss_Enh_Dec: -1.96358931\n",
      "| epoch 112 |   100/ 4361 batches | lr 0.000000 | ms/batch 400.68 | loss  2.29 | ppl     9.92 | acc     0.71 | train_ae_norm     1.00\n",
      "[112/200][199/4361] Loss_D: 0.00205442 (Loss_D_real: 0.00069733 Loss_D_fake: 0.00135709) Loss_G: 0.42033836 Loss_Enh_Dec: -2.48256373\n",
      "| epoch 112 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  2.32 | ppl    10.21 | acc     0.73 | train_ae_norm     1.00\n",
      "[112/200][299/4361] Loss_D: 0.00272791 (Loss_D_real: 0.00117131 Loss_D_fake: 0.00155660) Loss_G: 0.38802326 Loss_Enh_Dec: -2.64390373\n",
      "| epoch 112 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.32 | ppl    10.19 | acc     0.69 | train_ae_norm     1.00\n",
      "[112/200][399/4361] Loss_D: 0.00327240 (Loss_D_real: 0.00267854 Loss_D_fake: 0.00059386) Loss_G: 0.37119079 Loss_Enh_Dec: -2.42516947\n",
      "| epoch 112 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  2.22 | ppl     9.24 | acc     0.75 | train_ae_norm     1.00\n",
      "[112/200][499/4361] Loss_D: 0.00142933 (Loss_D_real: 0.00030459 Loss_D_fake: 0.00112473) Loss_G: 0.39724094 Loss_Enh_Dec: -2.23778391\n",
      "| epoch 112 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  2.31 | ppl    10.04 | acc     0.76 | train_ae_norm     1.00\n",
      "[112/200][599/4361] Loss_D: 0.00094852 (Loss_D_real: 0.00007462 Loss_D_fake: 0.00087390) Loss_G: 0.43663865 Loss_Enh_Dec: -2.43139815\n",
      "| epoch 112 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  2.27 | ppl     9.68 | acc     0.70 | train_ae_norm     1.00\n",
      "[112/200][699/4361] Loss_D: 0.01702349 (Loss_D_real: 0.01642397 Loss_D_fake: 0.00059952) Loss_G: 0.36317375 Loss_Enh_Dec: -2.13476443\n",
      "| epoch 112 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.78 | loss  2.31 | ppl    10.06 | acc     0.72 | train_ae_norm     1.00\n",
      "[112/200][799/4361] Loss_D: 0.00112318 (Loss_D_real: 0.00031805 Loss_D_fake: 0.00080513) Loss_G: 0.38974592 Loss_Enh_Dec: -1.98364127\n",
      "| epoch 112 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.83 | loss  2.29 | ppl     9.90 | acc     0.74 | train_ae_norm     1.00\n",
      "[112/200][899/4361] Loss_D: 0.00805480 (Loss_D_real: 0.00618187 Loss_D_fake: 0.00187293) Loss_G: 0.35725686 Loss_Enh_Dec: -2.66834068\n",
      "| epoch 112 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  2.31 | ppl    10.06 | acc     0.76 | train_ae_norm     1.00\n",
      "[112/200][999/4361] Loss_D: 0.00701704 (Loss_D_real: 0.00479987 Loss_D_fake: 0.00221717) Loss_G: 0.38059339 Loss_Enh_Dec: -2.16709971\n",
      "| epoch 112 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  2.31 | ppl    10.09 | acc     0.73 | train_ae_norm     1.00\n",
      "[112/200][1099/4361] Loss_D: 0.00218307 (Loss_D_real: 0.00116720 Loss_D_fake: 0.00101587) Loss_G: 0.53754753 Loss_Enh_Dec: -1.69677317\n",
      "| epoch 112 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  2.32 | ppl    10.13 | acc     0.71 | train_ae_norm     1.00\n",
      "[112/200][1199/4361] Loss_D: 0.00181339 (Loss_D_real: 0.00093361 Loss_D_fake: 0.00087977) Loss_G: 0.43196422 Loss_Enh_Dec: -1.86314774\n",
      "| epoch 112 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  2.33 | ppl    10.29 | acc     0.73 | train_ae_norm     1.00\n",
      "[112/200][1299/4361] Loss_D: 0.00125873 (Loss_D_real: 0.00014504 Loss_D_fake: 0.00111369) Loss_G: 0.39712581 Loss_Enh_Dec: -1.62644660\n",
      "| epoch 112 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  2.35 | ppl    10.49 | acc     0.72 | train_ae_norm     1.00\n",
      "[112/200][1399/4361] Loss_D: 0.00192352 (Loss_D_real: 0.00032111 Loss_D_fake: 0.00160241) Loss_G: 0.42012563 Loss_Enh_Dec: -1.77392888\n",
      "| epoch 112 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.89 | loss  2.33 | ppl    10.31 | acc     0.69 | train_ae_norm     1.00\n",
      "[112/200][1499/4361] Loss_D: 0.00197420 (Loss_D_real: 0.00125496 Loss_D_fake: 0.00071924) Loss_G: 0.44698945 Loss_Enh_Dec: -1.83396840\n",
      "| epoch 112 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  2.40 | ppl    10.98 | acc     0.71 | train_ae_norm     1.00\n",
      "[112/200][1599/4361] Loss_D: 0.00128989 (Loss_D_real: 0.00078015 Loss_D_fake: 0.00050974) Loss_G: 0.51026529 Loss_Enh_Dec: -1.36673665\n",
      "| epoch 112 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  2.37 | ppl    10.67 | acc     0.71 | train_ae_norm     1.00\n",
      "[112/200][1699/4361] Loss_D: 0.04030829 (Loss_D_real: 0.04023205 Loss_D_fake: 0.00007624) Loss_G: 0.66363877 Loss_Enh_Dec: -1.40398097\n",
      "| epoch 112 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  2.32 | ppl    10.19 | acc     0.68 | train_ae_norm     1.00\n",
      "[112/200][1799/4361] Loss_D: 0.00627699 (Loss_D_real: 0.00520231 Loss_D_fake: 0.00107468) Loss_G: 0.44064865 Loss_Enh_Dec: -1.92985308\n",
      "| epoch 112 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.32 | ppl    10.13 | acc     0.72 | train_ae_norm     1.00\n",
      "[112/200][1899/4361] Loss_D: 0.00246765 (Loss_D_real: 0.00004779 Loss_D_fake: 0.00241986) Loss_G: 0.40230283 Loss_Enh_Dec: -1.88038599\n",
      "| epoch 112 |  1900/ 4361 batches | lr 0.000000 | ms/batch 402.18 | loss  2.37 | ppl    10.72 | acc     0.73 | train_ae_norm     1.00\n",
      "[112/200][1999/4361] Loss_D: 0.00384698 (Loss_D_real: 0.00244878 Loss_D_fake: 0.00139820) Loss_G: 0.40224028 Loss_Enh_Dec: -2.31828928\n",
      "| epoch 112 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  2.34 | ppl    10.36 | acc     0.73 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[112/200][2099/4361] Loss_D: 0.00976783 (Loss_D_real: 0.00822287 Loss_D_fake: 0.00154496) Loss_G: 0.44004551 Loss_Enh_Dec: -1.59432828\n",
      "| epoch 112 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  2.37 | ppl    10.71 | acc     0.75 | train_ae_norm     1.00\n",
      "[112/200][2199/4361] Loss_D: 0.00157056 (Loss_D_real: 0.00041487 Loss_D_fake: 0.00115568) Loss_G: 0.48662004 Loss_Enh_Dec: -2.15679145\n",
      "| epoch 112 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  2.34 | ppl    10.34 | acc     0.73 | train_ae_norm     1.00\n",
      "[112/200][2299/4361] Loss_D: 0.02945388 (Loss_D_real: 0.02819164 Loss_D_fake: 0.00126224) Loss_G: 0.62805098 Loss_Enh_Dec: -2.01412773\n",
      "| epoch 112 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  2.33 | ppl    10.33 | acc     0.75 | train_ae_norm     1.00\n",
      "[112/200][2399/4361] Loss_D: 0.00260395 (Loss_D_real: 0.00193728 Loss_D_fake: 0.00066667) Loss_G: 0.44455311 Loss_Enh_Dec: -2.06319118\n",
      "| epoch 112 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.34 | ppl    10.34 | acc     0.71 | train_ae_norm     1.00\n",
      "[112/200][2499/4361] Loss_D: 0.00495297 (Loss_D_real: 0.00417457 Loss_D_fake: 0.00077840) Loss_G: 0.42042437 Loss_Enh_Dec: -2.00259161\n",
      "| epoch 112 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.37 | ppl    10.71 | acc     0.74 | train_ae_norm     1.00\n",
      "[112/200][2599/4361] Loss_D: 0.00099029 (Loss_D_real: 0.00050774 Loss_D_fake: 0.00048255) Loss_G: 0.62356377 Loss_Enh_Dec: -1.73393619\n",
      "| epoch 112 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  2.34 | ppl    10.41 | acc     0.71 | train_ae_norm     1.00\n",
      "[112/200][2699/4361] Loss_D: 0.00530994 (Loss_D_real: 0.00507611 Loss_D_fake: 0.00023382) Loss_G: 0.42608243 Loss_Enh_Dec: -1.93010139\n",
      "| epoch 112 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.35 | ppl    10.52 | acc     0.72 | train_ae_norm     1.00\n",
      "[112/200][2799/4361] Loss_D: 0.00175779 (Loss_D_real: 0.00029087 Loss_D_fake: 0.00146691) Loss_G: 0.46008274 Loss_Enh_Dec: -2.28075409\n",
      "| epoch 112 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  2.32 | ppl    10.17 | acc     0.70 | train_ae_norm     1.00\n",
      "[112/200][2899/4361] Loss_D: 0.00037572 (Loss_D_real: 0.00015325 Loss_D_fake: 0.00022247) Loss_G: 0.48575351 Loss_Enh_Dec: -2.41107082\n",
      "| epoch 112 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  2.34 | ppl    10.40 | acc     0.73 | train_ae_norm     1.00\n",
      "[112/200][2999/4361] Loss_D: 0.00167220 (Loss_D_real: 0.00092212 Loss_D_fake: 0.00075008) Loss_G: 0.42527682 Loss_Enh_Dec: -2.66187811\n",
      "| epoch 112 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.74 | loss  2.35 | ppl    10.51 | acc     0.72 | train_ae_norm     1.00\n",
      "[112/200][3099/4361] Loss_D: 0.01576336 (Loss_D_real: 0.00142938 Loss_D_fake: 0.01433398) Loss_G: 0.49212465 Loss_Enh_Dec: -1.84269035\n",
      "| epoch 112 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  2.37 | ppl    10.71 | acc     0.70 | train_ae_norm     1.00\n",
      "[112/200][3199/4361] Loss_D: 0.00311191 (Loss_D_real: 0.00288988 Loss_D_fake: 0.00022204) Loss_G: 0.68444699 Loss_Enh_Dec: -1.99065721\n",
      "| epoch 112 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.39 | ppl    10.97 | acc     0.73 | train_ae_norm     1.00\n",
      "[112/200][3299/4361] Loss_D: 0.00540001 (Loss_D_real: 0.00308797 Loss_D_fake: 0.00231205) Loss_G: 0.35134888 Loss_Enh_Dec: -1.99160409\n",
      "| epoch 112 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  2.40 | ppl    11.07 | acc     0.72 | train_ae_norm     1.00\n",
      "[112/200][3399/4361] Loss_D: 0.00229752 (Loss_D_real: 0.00038003 Loss_D_fake: 0.00191749) Loss_G: 0.34226069 Loss_Enh_Dec: -2.28431463\n",
      "| epoch 112 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  2.38 | ppl    10.85 | acc     0.70 | train_ae_norm     1.00\n",
      "[112/200][3499/4361] Loss_D: 0.00440149 (Loss_D_real: 0.00190377 Loss_D_fake: 0.00249771) Loss_G: 0.42846370 Loss_Enh_Dec: -2.03730321\n",
      "| epoch 112 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  2.33 | ppl    10.24 | acc     0.70 | train_ae_norm     1.00\n",
      "[112/200][3599/4361] Loss_D: 0.00169216 (Loss_D_real: 0.00002216 Loss_D_fake: 0.00167000) Loss_G: 0.35185888 Loss_Enh_Dec: -2.23559570\n",
      "| epoch 112 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  2.33 | ppl    10.26 | acc     0.71 | train_ae_norm     1.00\n",
      "[112/200][3699/4361] Loss_D: 0.00088748 (Loss_D_real: 0.00015186 Loss_D_fake: 0.00073562) Loss_G: 0.43516874 Loss_Enh_Dec: -2.54569554\n",
      "| epoch 112 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  2.36 | ppl    10.58 | acc     0.69 | train_ae_norm     1.00\n",
      "[112/200][3799/4361] Loss_D: 0.00427735 (Loss_D_real: 0.00004022 Loss_D_fake: 0.00423714) Loss_G: 0.39395228 Loss_Enh_Dec: -2.46899986\n",
      "| epoch 112 |  3800/ 4361 batches | lr 0.000000 | ms/batch 402.14 | loss  2.36 | ppl    10.59 | acc     0.76 | train_ae_norm     1.00\n",
      "[112/200][3899/4361] Loss_D: 0.02025507 (Loss_D_real: 0.01756169 Loss_D_fake: 0.00269338) Loss_G: 0.38917395 Loss_Enh_Dec: -2.09325099\n",
      "| epoch 112 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.84 | loss  2.36 | ppl    10.58 | acc     0.72 | train_ae_norm     1.00\n",
      "[112/200][3999/4361] Loss_D: 0.00564268 (Loss_D_real: 0.00321943 Loss_D_fake: 0.00242325) Loss_G: 0.37772128 Loss_Enh_Dec: -2.58201504\n",
      "| epoch 112 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  2.36 | ppl    10.57 | acc     0.70 | train_ae_norm     1.00\n",
      "[112/200][4099/4361] Loss_D: 0.00548544 (Loss_D_real: 0.00006256 Loss_D_fake: 0.00542288) Loss_G: 0.43368512 Loss_Enh_Dec: -2.25876594\n",
      "| epoch 112 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.08 | loss  2.33 | ppl    10.31 | acc     0.72 | train_ae_norm     1.00\n",
      "[112/200][4199/4361] Loss_D: 0.00184112 (Loss_D_real: 0.00012994 Loss_D_fake: 0.00171118) Loss_G: 0.37847212 Loss_Enh_Dec: -2.39664769\n",
      "| epoch 112 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  2.35 | ppl    10.49 | acc     0.76 | train_ae_norm     1.00\n",
      "[112/200][4299/4361] Loss_D: 0.00179583 (Loss_D_real: 0.00023571 Loss_D_fake: 0.00156012) Loss_G: 0.41727838 Loss_Enh_Dec: -2.27295089\n",
      "| epoch 112 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  2.32 | ppl    10.13 | acc     0.74 | train_ae_norm     1.00\n",
      "| end of epoch 112 | time: 1852.59s | test loss  2.40 | test ppl 11.08 | acc 0.767\n",
      "bleu_self:  [4.11149483e-01 1.14051713e-01 9.70801232e-07 2.96246301e-09\n",
      " 9.57632943e-11]\n",
      "bleu_test:  [9.09469697e-01 2.57325697e-01 1.84085082e-06 5.15739798e-09\n",
      " 1.58538977e-10]\n",
      "bleu_self: [0.41114948,0.11405171,0.00000097,0.00000000,0.00000000]\n",
      "bleu_test: [0.90946970,0.25732570,0.00000184,0.00000001,0.00000000]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 113 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:57.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.698\n",
      "  Training epcoh took: 0:01:27\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.495\n",
      "  Test Loss: 4.543\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 113 |     0/ 4361 batches | lr 0.000000 | ms/batch 868.95 | loss  0.02 | ppl     1.02 | acc     0.76 | train_ae_norm     1.00\n",
      "[113/200][99/4361] Loss_D: 0.00270673 (Loss_D_real: 0.00071718 Loss_D_fake: 0.00198955) Loss_G: 0.36597869 Loss_Enh_Dec: -2.22900438\n",
      "| epoch 113 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.33 | loss  2.34 | ppl    10.37 | acc     0.68 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[113/200][199/4361] Loss_D: 0.00269865 (Loss_D_real: 0.00049215 Loss_D_fake: 0.00220650) Loss_G: 0.53255051 Loss_Enh_Dec: -1.92767799\n",
      "| epoch 113 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  2.35 | ppl    10.51 | acc     0.74 | train_ae_norm     1.00\n",
      "[113/200][299/4361] Loss_D: 0.00682903 (Loss_D_real: 0.00639779 Loss_D_fake: 0.00043123) Loss_G: 0.59810901 Loss_Enh_Dec: -2.18943691\n",
      "| epoch 113 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  2.37 | ppl    10.72 | acc     0.66 | train_ae_norm     1.00\n",
      "[113/200][399/4361] Loss_D: 0.01075337 (Loss_D_real: 0.00952671 Loss_D_fake: 0.00122665) Loss_G: 0.37275195 Loss_Enh_Dec: -1.99749684\n",
      "| epoch 113 |   400/ 4361 batches | lr 0.000000 | ms/batch 402.04 | loss  2.29 | ppl     9.85 | acc     0.72 | train_ae_norm     1.00\n",
      "[113/200][499/4361] Loss_D: 0.00290534 (Loss_D_real: 0.00159725 Loss_D_fake: 0.00130808) Loss_G: 0.38240337 Loss_Enh_Dec: -2.38920951\n",
      "| epoch 113 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  2.35 | ppl    10.48 | acc     0.74 | train_ae_norm     1.00\n",
      "[113/200][599/4361] Loss_D: 0.00609055 (Loss_D_real: 0.00232065 Loss_D_fake: 0.00376990) Loss_G: 0.39848480 Loss_Enh_Dec: -2.42709923\n",
      "| epoch 113 |   600/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  2.29 | ppl     9.86 | acc     0.70 | train_ae_norm     1.00\n",
      "[113/200][699/4361] Loss_D: 0.00096612 (Loss_D_real: 0.00014320 Loss_D_fake: 0.00082291) Loss_G: 0.40049192 Loss_Enh_Dec: -2.80843592\n",
      "| epoch 113 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.94 | loss  2.33 | ppl    10.26 | acc     0.71 | train_ae_norm     1.00\n",
      "[113/200][799/4361] Loss_D: 0.00119928 (Loss_D_real: 0.00004395 Loss_D_fake: 0.00115533) Loss_G: 0.39993843 Loss_Enh_Dec: -2.13962150\n",
      "| epoch 113 |   800/ 4361 batches | lr 0.000000 | ms/batch 402.27 | loss  2.30 | ppl     9.93 | acc     0.72 | train_ae_norm     1.00\n",
      "[113/200][899/4361] Loss_D: 0.00160624 (Loss_D_real: 0.00028004 Loss_D_fake: 0.00132620) Loss_G: 0.45028821 Loss_Enh_Dec: -2.30474544\n",
      "| epoch 113 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  2.31 | ppl    10.06 | acc     0.75 | train_ae_norm     1.00\n",
      "[113/200][999/4361] Loss_D: 0.00141616 (Loss_D_real: 0.00028269 Loss_D_fake: 0.00113347) Loss_G: 0.40184689 Loss_Enh_Dec: -1.92977905\n",
      "| epoch 113 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  2.29 | ppl     9.85 | acc     0.72 | train_ae_norm     1.00\n",
      "[113/200][1099/4361] Loss_D: 0.01044621 (Loss_D_real: 0.00008731 Loss_D_fake: 0.01035890) Loss_G: 0.45163402 Loss_Enh_Dec: -2.22204876\n",
      "| epoch 113 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  2.29 | ppl     9.91 | acc     0.72 | train_ae_norm     1.00\n",
      "[113/200][1199/4361] Loss_D: 0.00187093 (Loss_D_real: 0.00004331 Loss_D_fake: 0.00182762) Loss_G: 0.40543434 Loss_Enh_Dec: -1.73630524\n",
      "| epoch 113 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.29 | ppl     9.87 | acc     0.77 | train_ae_norm     1.00\n",
      "[113/200][1299/4361] Loss_D: 0.26461071 (Loss_D_real: 0.00005045 Loss_D_fake: 0.26456025) Loss_G: 0.73856413 Loss_Enh_Dec: -2.07808375\n",
      "| epoch 113 |  1300/ 4361 batches | lr 0.000000 | ms/batch 402.16 | loss  2.32 | ppl    10.18 | acc     0.71 | train_ae_norm     1.00\n",
      "[113/200][1399/4361] Loss_D: 0.00390916 (Loss_D_real: 0.00025276 Loss_D_fake: 0.00365641) Loss_G: 0.42260075 Loss_Enh_Dec: -2.02088261\n",
      "| epoch 113 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  2.30 | ppl    10.02 | acc     0.70 | train_ae_norm     1.00\n",
      "[113/200][1499/4361] Loss_D: 0.00580807 (Loss_D_real: 0.00529826 Loss_D_fake: 0.00050981) Loss_G: 0.56839609 Loss_Enh_Dec: -2.42751527\n",
      "| epoch 113 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  2.35 | ppl    10.51 | acc     0.70 | train_ae_norm     1.00\n",
      "[113/200][1599/4361] Loss_D: 0.00141934 (Loss_D_real: 0.00002084 Loss_D_fake: 0.00139850) Loss_G: 0.38229054 Loss_Enh_Dec: -2.48385215\n",
      "| epoch 113 |  1600/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  2.34 | ppl    10.41 | acc     0.73 | train_ae_norm     1.00\n",
      "[113/200][1699/4361] Loss_D: 0.01907339 (Loss_D_real: 0.00635882 Loss_D_fake: 0.01271457) Loss_G: 0.39621016 Loss_Enh_Dec: -1.82558179\n",
      "| epoch 113 |  1700/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  2.31 | ppl    10.03 | acc     0.72 | train_ae_norm     1.00\n",
      "[113/200][1799/4361] Loss_D: 0.00301634 (Loss_D_real: 0.00037918 Loss_D_fake: 0.00263715) Loss_G: 0.35253569 Loss_Enh_Dec: -1.73611283\n",
      "| epoch 113 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  2.31 | ppl    10.08 | acc     0.71 | train_ae_norm     1.00\n",
      "[113/200][1899/4361] Loss_D: 0.00381063 (Loss_D_real: 0.00267789 Loss_D_fake: 0.00113273) Loss_G: 0.40677950 Loss_Enh_Dec: -1.79996228\n",
      "| epoch 113 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  2.37 | ppl    10.72 | acc     0.73 | train_ae_norm     1.00\n",
      "[113/200][1999/4361] Loss_D: 0.00541579 (Loss_D_real: 0.00109106 Loss_D_fake: 0.00432473) Loss_G: 0.36361071 Loss_Enh_Dec: -2.37010407\n",
      "| epoch 113 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.33 | ppl    10.32 | acc     0.77 | train_ae_norm     1.00\n",
      "[113/200][2099/4361] Loss_D: 0.00256433 (Loss_D_real: 0.00008992 Loss_D_fake: 0.00247441) Loss_G: 0.54017133 Loss_Enh_Dec: -2.16870189\n",
      "| epoch 113 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.38 | ppl    10.82 | acc     0.75 | train_ae_norm     1.00\n",
      "[113/200][2199/4361] Loss_D: 0.01147837 (Loss_D_real: 0.01050171 Loss_D_fake: 0.00097666) Loss_G: 0.38564906 Loss_Enh_Dec: -1.61847615\n",
      "| epoch 113 |  2200/ 4361 batches | lr 0.000000 | ms/batch 402.35 | loss  2.35 | ppl    10.52 | acc     0.71 | train_ae_norm     1.00\n",
      "[113/200][2299/4361] Loss_D: 0.00701665 (Loss_D_real: 0.00038986 Loss_D_fake: 0.00662679) Loss_G: 0.37688237 Loss_Enh_Dec: -2.16126108\n",
      "| epoch 113 |  2300/ 4361 batches | lr 0.000000 | ms/batch 402.32 | loss  2.34 | ppl    10.34 | acc     0.74 | train_ae_norm     1.00\n",
      "[113/200][2399/4361] Loss_D: 0.00677488 (Loss_D_real: 0.00455608 Loss_D_fake: 0.00221880) Loss_G: 0.43403980 Loss_Enh_Dec: -1.70097852\n",
      "| epoch 113 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.36 | ppl    10.61 | acc     0.69 | train_ae_norm     1.00\n",
      "[113/200][2499/4361] Loss_D: 0.00220172 (Loss_D_real: 0.00150485 Loss_D_fake: 0.00069687) Loss_G: 0.45502186 Loss_Enh_Dec: -2.21249318\n",
      "| epoch 113 |  2500/ 4361 batches | lr 0.000000 | ms/batch 402.06 | loss  2.39 | ppl    10.97 | acc     0.75 | train_ae_norm     1.00\n",
      "[113/200][2599/4361] Loss_D: 0.12989275 (Loss_D_real: 0.12895674 Loss_D_fake: 0.00093602) Loss_G: 0.41455078 Loss_Enh_Dec: -1.69682777\n",
      "| epoch 113 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.35 | ppl    10.52 | acc     0.69 | train_ae_norm     1.00\n",
      "[113/200][2699/4361] Loss_D: 0.02640460 (Loss_D_real: 0.02502649 Loss_D_fake: 0.00137811) Loss_G: 0.35824236 Loss_Enh_Dec: -2.41735315\n",
      "| epoch 113 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  2.39 | ppl    10.88 | acc     0.70 | train_ae_norm     1.00\n",
      "[113/200][2799/4361] Loss_D: 0.00917288 (Loss_D_real: 0.00166018 Loss_D_fake: 0.00751270) Loss_G: 0.37253064 Loss_Enh_Dec: -2.54344416\n",
      "| epoch 113 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  2.36 | ppl    10.59 | acc     0.71 | train_ae_norm     1.00\n",
      "[113/200][2899/4361] Loss_D: 0.00159003 (Loss_D_real: 0.00032044 Loss_D_fake: 0.00126959) Loss_G: 0.50178164 Loss_Enh_Dec: -2.24012232\n",
      "| epoch 113 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  2.38 | ppl    10.76 | acc     0.71 | train_ae_norm     1.00\n",
      "[113/200][2999/4361] Loss_D: 0.00164099 (Loss_D_real: 0.00007332 Loss_D_fake: 0.00156767) Loss_G: 0.43209368 Loss_Enh_Dec: -2.78208518\n",
      "| epoch 113 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  2.40 | ppl    11.05 | acc     0.71 | train_ae_norm     1.00\n",
      "[113/200][3099/4361] Loss_D: 0.03455725 (Loss_D_real: 0.03404197 Loss_D_fake: 0.00051528) Loss_G: 0.68113267 Loss_Enh_Dec: -2.41048789\n",
      "| epoch 113 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.40 | ppl    10.98 | acc     0.72 | train_ae_norm     1.00\n",
      "[113/200][3199/4361] Loss_D: 0.06636831 (Loss_D_real: 0.06443017 Loss_D_fake: 0.00193814) Loss_G: 0.46450463 Loss_Enh_Dec: -2.61656809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 113 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  2.45 | ppl    11.60 | acc     0.74 | train_ae_norm     1.00\n",
      "[113/200][3299/4361] Loss_D: 0.00422459 (Loss_D_real: 0.00051651 Loss_D_fake: 0.00370808) Loss_G: 0.42006031 Loss_Enh_Dec: -2.54760480\n",
      "| epoch 113 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  2.45 | ppl    11.63 | acc     0.68 | train_ae_norm     1.00\n",
      "[113/200][3399/4361] Loss_D: 0.00693321 (Loss_D_real: 0.00084357 Loss_D_fake: 0.00608964) Loss_G: 0.34486988 Loss_Enh_Dec: -2.72795224\n",
      "| epoch 113 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  2.43 | ppl    11.39 | acc     0.70 | train_ae_norm     1.00\n",
      "[113/200][3499/4361] Loss_D: 0.00678823 (Loss_D_real: 0.00313772 Loss_D_fake: 0.00365051) Loss_G: 0.36104706 Loss_Enh_Dec: -2.36450124\n",
      "| epoch 113 |  3500/ 4361 batches | lr 0.000000 | ms/batch 402.21 | loss  2.37 | ppl    10.74 | acc     0.70 | train_ae_norm     1.00\n",
      "[113/200][3599/4361] Loss_D: 0.00222693 (Loss_D_real: 0.00045504 Loss_D_fake: 0.00177189) Loss_G: 0.38528812 Loss_Enh_Dec: -2.31427765\n",
      "| epoch 113 |  3600/ 4361 batches | lr 0.000000 | ms/batch 402.16 | loss  2.38 | ppl    10.84 | acc     0.72 | train_ae_norm     1.00\n",
      "[113/200][3699/4361] Loss_D: 0.00273499 (Loss_D_real: 0.00186346 Loss_D_fake: 0.00087153) Loss_G: 0.40850553 Loss_Enh_Dec: -2.30626225\n",
      "| epoch 113 |  3700/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  2.42 | ppl    11.28 | acc     0.66 | train_ae_norm     1.00\n",
      "[113/200][3799/4361] Loss_D: 0.00213056 (Loss_D_real: 0.00027278 Loss_D_fake: 0.00185778) Loss_G: 0.38638124 Loss_Enh_Dec: -2.22607398\n",
      "| epoch 113 |  3800/ 4361 batches | lr 0.000000 | ms/batch 402.18 | loss  2.47 | ppl    11.78 | acc     0.73 | train_ae_norm     1.00\n",
      "[113/200][3899/4361] Loss_D: 0.00228444 (Loss_D_real: 0.00002507 Loss_D_fake: 0.00225936) Loss_G: 0.36251235 Loss_Enh_Dec: -2.43559313\n",
      "| epoch 113 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  2.46 | ppl    11.69 | acc     0.67 | train_ae_norm     1.00\n",
      "[113/200][3999/4361] Loss_D: 0.01043915 (Loss_D_real: 0.00161945 Loss_D_fake: 0.00881969) Loss_G: 0.43029895 Loss_Enh_Dec: -1.98630655\n",
      "| epoch 113 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.43 | ppl    11.34 | acc     0.69 | train_ae_norm     1.00\n",
      "[113/200][4099/4361] Loss_D: 0.01591058 (Loss_D_real: 0.01384073 Loss_D_fake: 0.00206985) Loss_G: 0.37441903 Loss_Enh_Dec: -2.28925967\n",
      "| epoch 113 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  2.39 | ppl    10.95 | acc     0.71 | train_ae_norm     1.00\n",
      "[113/200][4199/4361] Loss_D: 0.01745758 (Loss_D_real: 0.00215221 Loss_D_fake: 0.01530537) Loss_G: 0.50239772 Loss_Enh_Dec: -1.81213892\n",
      "| epoch 113 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  2.42 | ppl    11.30 | acc     0.74 | train_ae_norm     1.00\n",
      "[113/200][4299/4361] Loss_D: 0.00416208 (Loss_D_real: 0.00031509 Loss_D_fake: 0.00384699) Loss_G: 0.37847775 Loss_Enh_Dec: -1.77718818\n",
      "| epoch 113 |  4300/ 4361 batches | lr 0.000000 | ms/batch 402.09 | loss  2.40 | ppl    11.02 | acc     0.74 | train_ae_norm     1.00\n",
      "| end of epoch 113 | time: 1854.78s | test loss  2.40 | test ppl 11.06 | acc 0.761\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 114 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:57.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.698\n",
      "  Training epcoh took: 0:01:27\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.502\n",
      "  Test Loss: 4.628\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 114 |     0/ 4361 batches | lr 0.000000 | ms/batch 869.80 | loss  0.02 | ppl     1.02 | acc     0.73 | train_ae_norm     1.00\n",
      "[114/200][99/4361] Loss_D: 0.00316355 (Loss_D_real: 0.00204835 Loss_D_fake: 0.00111521) Loss_G: 0.48803741 Loss_Enh_Dec: -1.76536262\n",
      "| epoch 114 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  2.42 | ppl    11.20 | acc     0.71 | train_ae_norm     1.00\n",
      "[114/200][199/4361] Loss_D: 0.00286020 (Loss_D_real: 0.00007554 Loss_D_fake: 0.00278466) Loss_G: 0.41497660 Loss_Enh_Dec: -1.74973810\n",
      "| epoch 114 |   200/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  2.44 | ppl    11.44 | acc     0.70 | train_ae_norm     1.00\n",
      "[114/200][299/4361] Loss_D: 0.00173981 (Loss_D_real: 0.00004341 Loss_D_fake: 0.00169641) Loss_G: 0.43557951 Loss_Enh_Dec: -2.25104499\n",
      "| epoch 114 |   300/ 4361 batches | lr 0.000000 | ms/batch 402.03 | loss  2.47 | ppl    11.84 | acc     0.67 | train_ae_norm     1.00\n",
      "[114/200][399/4361] Loss_D: 0.16315204 (Loss_D_real: 0.16079628 Loss_D_fake: 0.00235575) Loss_G: 0.67266077 Loss_Enh_Dec: -1.65231550\n",
      "| epoch 114 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  2.38 | ppl    10.77 | acc     0.70 | train_ae_norm     1.00\n",
      "[114/200][499/4361] Loss_D: 0.12319909 (Loss_D_real: 0.00056661 Loss_D_fake: 0.12263247) Loss_G: 0.56163877 Loss_Enh_Dec: -1.92391741\n",
      "| epoch 114 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  2.51 | ppl    12.28 | acc     0.73 | train_ae_norm     1.00\n",
      "[114/200][599/4361] Loss_D: 0.00528409 (Loss_D_real: 0.00155533 Loss_D_fake: 0.00372875) Loss_G: 0.38008538 Loss_Enh_Dec: -2.07886004\n",
      "| epoch 114 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  2.42 | ppl    11.27 | acc     0.67 | train_ae_norm     1.00\n",
      "[114/200][699/4361] Loss_D: 0.00660323 (Loss_D_real: 0.00511877 Loss_D_fake: 0.00148446) Loss_G: 0.49462816 Loss_Enh_Dec: -2.16366434\n",
      "| epoch 114 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  2.45 | ppl    11.57 | acc     0.71 | train_ae_norm     1.00\n",
      "[114/200][799/4361] Loss_D: 0.00383223 (Loss_D_real: 0.00003919 Loss_D_fake: 0.00379304) Loss_G: 0.43930075 Loss_Enh_Dec: -2.28715491\n",
      "| epoch 114 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  2.44 | ppl    11.49 | acc     0.68 | train_ae_norm     1.00\n",
      "[114/200][899/4361] Loss_D: 0.00139968 (Loss_D_real: 0.00045796 Loss_D_fake: 0.00094172) Loss_G: 0.38163307 Loss_Enh_Dec: -2.39877868\n",
      "| epoch 114 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  2.44 | ppl    11.49 | acc     0.74 | train_ae_norm     1.00\n",
      "[114/200][999/4361] Loss_D: 0.00363244 (Loss_D_real: 0.00201170 Loss_D_fake: 0.00162074) Loss_G: 0.38939667 Loss_Enh_Dec: -2.32612729\n",
      "| epoch 114 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  2.45 | ppl    11.59 | acc     0.69 | train_ae_norm     1.00\n",
      "[114/200][1099/4361] Loss_D: 0.00380737 (Loss_D_real: 0.00295760 Loss_D_fake: 0.00084977) Loss_G: 0.40534630 Loss_Enh_Dec: -2.30352139\n",
      "| epoch 114 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  2.42 | ppl    11.22 | acc     0.69 | train_ae_norm     1.00\n",
      "[114/200][1199/4361] Loss_D: 0.00284436 (Loss_D_real: 0.00038099 Loss_D_fake: 0.00246336) Loss_G: 0.44588754 Loss_Enh_Dec: -2.23295975\n",
      "| epoch 114 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  2.43 | ppl    11.35 | acc     0.70 | train_ae_norm     1.00\n",
      "[114/200][1299/4361] Loss_D: 0.00243786 (Loss_D_real: 0.00033359 Loss_D_fake: 0.00210428) Loss_G: 0.38031742 Loss_Enh_Dec: -2.42312598\n",
      "| epoch 114 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  2.47 | ppl    11.79 | acc     0.69 | train_ae_norm     1.00\n",
      "[114/200][1399/4361] Loss_D: 0.00576347 (Loss_D_real: 0.00332331 Loss_D_fake: 0.00244016) Loss_G: 0.38424441 Loss_Enh_Dec: -2.60236239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 114 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.84 | loss  2.47 | ppl    11.79 | acc     0.68 | train_ae_norm     1.00\n",
      "[114/200][1499/4361] Loss_D: 0.00235209 (Loss_D_real: 0.00033259 Loss_D_fake: 0.00201950) Loss_G: 0.43641922 Loss_Enh_Dec: -2.40079951\n",
      "| epoch 114 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  2.50 | ppl    12.14 | acc     0.69 | train_ae_norm     1.00\n",
      "[114/200][1599/4361] Loss_D: 0.00237015 (Loss_D_real: 0.00009686 Loss_D_fake: 0.00227329) Loss_G: 0.41005918 Loss_Enh_Dec: -2.36847162\n",
      "| epoch 114 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  2.49 | ppl    12.10 | acc     0.68 | train_ae_norm     1.00\n",
      "[114/200][1699/4361] Loss_D: 0.00246083 (Loss_D_real: 0.00113411 Loss_D_fake: 0.00132672) Loss_G: 0.44866291 Loss_Enh_Dec: -2.66747832\n",
      "| epoch 114 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.81 | loss  2.47 | ppl    11.84 | acc     0.68 | train_ae_norm     1.00\n",
      "[114/200][1799/4361] Loss_D: 0.00469098 (Loss_D_real: 0.00221375 Loss_D_fake: 0.00247723) Loss_G: 0.50607842 Loss_Enh_Dec: -2.64627671\n",
      "| epoch 114 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  2.44 | ppl    11.50 | acc     0.69 | train_ae_norm     1.00\n",
      "[114/200][1899/4361] Loss_D: 0.02346108 (Loss_D_real: 0.01319504 Loss_D_fake: 0.01026604) Loss_G: 0.37613159 Loss_Enh_Dec: -2.57983088\n",
      "| epoch 114 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  2.49 | ppl    12.01 | acc     0.71 | train_ae_norm     1.00\n",
      "[114/200][1999/4361] Loss_D: 0.00146030 (Loss_D_real: 0.00042763 Loss_D_fake: 0.00103267) Loss_G: 0.60546088 Loss_Enh_Dec: -2.13472509\n",
      "| epoch 114 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.43 | ppl    11.38 | acc     0.72 | train_ae_norm     1.00\n",
      "[114/200][2099/4361] Loss_D: 0.01148058 (Loss_D_real: 0.00016014 Loss_D_fake: 0.01132044) Loss_G: 0.39716455 Loss_Enh_Dec: -2.55776334\n",
      "| epoch 114 |  2100/ 4361 batches | lr 0.000000 | ms/batch 402.55 | loss  2.47 | ppl    11.87 | acc     0.72 | train_ae_norm     1.00\n",
      "[114/200][2199/4361] Loss_D: 0.00278040 (Loss_D_real: 0.00125508 Loss_D_fake: 0.00152532) Loss_G: 0.41680121 Loss_Enh_Dec: -2.34281421\n",
      "| epoch 114 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.28 | loss  2.46 | ppl    11.70 | acc     0.70 | train_ae_norm     1.00\n",
      "[114/200][2499/4361] Loss_D: 0.00367771 (Loss_D_real: 0.00195917 Loss_D_fake: 0.00171855) Loss_G: 0.43973342 Loss_Enh_Dec: -1.47958744\n",
      "| epoch 114 |  2500/ 4361 batches | lr 0.000000 | ms/batch 399.86 | loss  2.54 | ppl    12.64 | acc     0.70 | train_ae_norm     1.00\n",
      "[114/200][2599/4361] Loss_D: 0.01398198 (Loss_D_real: 0.01000514 Loss_D_fake: 0.00397684) Loss_G: 0.39778414 Loss_Enh_Dec: -1.94860518\n",
      "| epoch 114 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.11 | loss  2.50 | ppl    12.20 | acc     0.67 | train_ae_norm     1.00\n",
      "[114/200][2699/4361] Loss_D: 0.00798873 (Loss_D_real: 0.00619259 Loss_D_fake: 0.00179614) Loss_G: 0.43881580 Loss_Enh_Dec: -2.29471183\n",
      "| epoch 114 |  2700/ 4361 batches | lr 0.000000 | ms/batch 400.38 | loss  2.49 | ppl    12.09 | acc     0.68 | train_ae_norm     1.00\n",
      "[114/200][2799/4361] Loss_D: 0.00853138 (Loss_D_real: 0.00642440 Loss_D_fake: 0.00210698) Loss_G: 0.37480736 Loss_Enh_Dec: -2.14762187\n",
      "| epoch 114 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.05 | loss  2.51 | ppl    12.28 | acc     0.66 | train_ae_norm     1.00\n",
      "[114/200][2899/4361] Loss_D: 0.00391072 (Loss_D_real: 0.00192375 Loss_D_fake: 0.00198696) Loss_G: 0.36230120 Loss_Enh_Dec: -1.91995430\n",
      "| epoch 114 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.63 | loss  2.50 | ppl    12.16 | acc     0.68 | train_ae_norm     1.00\n",
      "[114/200][2999/4361] Loss_D: 0.00569836 (Loss_D_real: 0.00184877 Loss_D_fake: 0.00384958) Loss_G: 0.41501841 Loss_Enh_Dec: -2.06741762\n",
      "| epoch 114 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.79 | loss  2.51 | ppl    12.26 | acc     0.70 | train_ae_norm     1.00\n",
      "[114/200][3099/4361] Loss_D: 0.20765674 (Loss_D_real: 0.20652544 Loss_D_fake: 0.00113129) Loss_G: 0.42263290 Loss_Enh_Dec: -1.89877021\n",
      "| epoch 114 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  2.52 | ppl    12.44 | acc     0.66 | train_ae_norm     1.00\n",
      "[114/200][3199/4361] Loss_D: 0.01000892 (Loss_D_real: 0.00796141 Loss_D_fake: 0.00204751) Loss_G: 0.52308768 Loss_Enh_Dec: -2.42660642\n",
      "| epoch 114 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  2.56 | ppl    12.96 | acc     0.69 | train_ae_norm     1.00\n",
      "[114/200][3299/4361] Loss_D: 0.00379679 (Loss_D_real: 0.00010326 Loss_D_fake: 0.00369353) Loss_G: 0.44939169 Loss_Enh_Dec: -2.60860682\n",
      "| epoch 114 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  2.59 | ppl    13.29 | acc     0.70 | train_ae_norm     1.00\n",
      "[114/200][3399/4361] Loss_D: 0.00155518 (Loss_D_real: 0.00063576 Loss_D_fake: 0.00091942) Loss_G: 0.41572103 Loss_Enh_Dec: -2.48809290\n",
      "| epoch 114 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.45 | loss  2.57 | ppl    13.03 | acc     0.67 | train_ae_norm     1.00\n",
      "[114/200][3499/4361] Loss_D: 0.00189950 (Loss_D_real: 0.00028226 Loss_D_fake: 0.00161724) Loss_G: 0.40128765 Loss_Enh_Dec: -2.23841763\n",
      "| epoch 114 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  2.47 | ppl    11.84 | acc     0.68 | train_ae_norm     1.00\n",
      "[114/200][3599/4361] Loss_D: 0.00478741 (Loss_D_real: 0.00138594 Loss_D_fake: 0.00340147) Loss_G: 0.42571002 Loss_Enh_Dec: -2.18306041\n",
      "| epoch 114 |  3600/ 4361 batches | lr 0.000000 | ms/batch 400.59 | loss  2.53 | ppl    12.52 | acc     0.71 | train_ae_norm     1.00\n",
      "[114/200][3699/4361] Loss_D: 0.00211362 (Loss_D_real: 0.00069586 Loss_D_fake: 0.00141776) Loss_G: 0.40748572 Loss_Enh_Dec: -2.48765779\n",
      "| epoch 114 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.47 | loss  2.53 | ppl    12.54 | acc     0.66 | train_ae_norm     1.00\n",
      "[114/200][3799/4361] Loss_D: 0.00211613 (Loss_D_real: 0.00056288 Loss_D_fake: 0.00155325) Loss_G: 0.40695101 Loss_Enh_Dec: -2.33257747\n",
      "| epoch 114 |  3800/ 4361 batches | lr 0.000000 | ms/batch 400.84 | loss  2.53 | ppl    12.52 | acc     0.72 | train_ae_norm     1.00\n",
      "[114/200][3899/4361] Loss_D: 0.00170558 (Loss_D_real: 0.00034237 Loss_D_fake: 0.00136321) Loss_G: 0.36677599 Loss_Enh_Dec: -2.31944394\n",
      "| epoch 114 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  2.52 | ppl    12.38 | acc     0.67 | train_ae_norm     1.00\n",
      "[114/200][3999/4361] Loss_D: 0.00844170 (Loss_D_real: 0.00367541 Loss_D_fake: 0.00476628) Loss_G: 0.41675463 Loss_Enh_Dec: -1.79540122\n",
      "| epoch 114 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  2.50 | ppl    12.15 | acc     0.69 | train_ae_norm     1.00\n",
      "[114/200][4099/4361] Loss_D: 0.00442503 (Loss_D_real: 0.00019163 Loss_D_fake: 0.00423339) Loss_G: 0.36081526 Loss_Enh_Dec: -1.86273253\n",
      "| epoch 114 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  2.46 | ppl    11.67 | acc     0.70 | train_ae_norm     1.00\n",
      "[114/200][4199/4361] Loss_D: 0.00267085 (Loss_D_real: 0.00090093 Loss_D_fake: 0.00176991) Loss_G: 0.38811520 Loss_Enh_Dec: -2.26660132\n",
      "| epoch 114 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  2.50 | ppl    12.21 | acc     0.71 | train_ae_norm     1.00\n",
      "[114/200][4299/4361] Loss_D: 0.00229569 (Loss_D_real: 0.00027831 Loss_D_fake: 0.00201738) Loss_G: 0.43189082 Loss_Enh_Dec: -2.22894597\n",
      "| epoch 114 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  2.48 | ppl    11.95 | acc     0.69 | train_ae_norm     1.00\n",
      "| end of epoch 114 | time: 1852.28s | test loss  2.47 | test ppl 11.85 | acc 0.753\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 115 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.698\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.487\n",
      "  Test Loss: 4.632\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 115 |     0/ 4361 batches | lr 0.000000 | ms/batch 866.77 | loss  0.02 | ppl     1.02 | acc     0.72 | train_ae_norm     1.00\n",
      "[115/200][99/4361] Loss_D: 0.00820733 (Loss_D_real: 0.00487988 Loss_D_fake: 0.00332744) Loss_G: 0.35289308 Loss_Enh_Dec: -2.30223656\n",
      "| epoch 115 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.24 | loss  2.52 | ppl    12.38 | acc     0.67 | train_ae_norm     1.00\n",
      "[115/200][199/4361] Loss_D: 0.00293608 (Loss_D_real: 0.00024019 Loss_D_fake: 0.00269589) Loss_G: 0.38135841 Loss_Enh_Dec: -2.64916420\n",
      "| epoch 115 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.53 | ppl    12.52 | acc     0.70 | train_ae_norm     1.00\n",
      "[115/200][299/4361] Loss_D: 0.00514105 (Loss_D_real: 0.00045764 Loss_D_fake: 0.00468341) Loss_G: 0.38575277 Loss_Enh_Dec: -2.44305873\n",
      "| epoch 115 |   300/ 4361 batches | lr 0.000000 | ms/batch 402.04 | loss  2.52 | ppl    12.48 | acc     0.62 | train_ae_norm     1.00\n",
      "[115/200][399/4361] Loss_D: 0.00834804 (Loss_D_real: 0.00350090 Loss_D_fake: 0.00484714) Loss_G: 0.43312317 Loss_Enh_Dec: -2.34371328\n",
      "| epoch 115 |   400/ 4361 batches | lr 0.000000 | ms/batch 402.24 | loss  2.46 | ppl    11.73 | acc     0.72 | train_ae_norm     1.00\n",
      "[115/200][499/4361] Loss_D: 0.00708337 (Loss_D_real: 0.00469711 Loss_D_fake: 0.00238626) Loss_G: 0.42733938 Loss_Enh_Dec: -2.31074381\n",
      "| epoch 115 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  2.54 | ppl    12.70 | acc     0.71 | train_ae_norm     1.00\n",
      "[115/200][599/4361] Loss_D: 0.00426440 (Loss_D_real: 0.00233017 Loss_D_fake: 0.00193423) Loss_G: 0.36451203 Loss_Enh_Dec: -1.96199918\n",
      "| epoch 115 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  2.49 | ppl    12.05 | acc     0.66 | train_ae_norm     1.00\n",
      "[115/200][699/4361] Loss_D: 0.00750551 (Loss_D_real: 0.00001438 Loss_D_fake: 0.00749113) Loss_G: 0.42760611 Loss_Enh_Dec: -2.19803691\n",
      "| epoch 115 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  2.55 | ppl    12.81 | acc     0.71 | train_ae_norm     1.00\n",
      "[115/200][799/4361] Loss_D: 0.00631223 (Loss_D_real: 0.00254371 Loss_D_fake: 0.00376852) Loss_G: 0.34008572 Loss_Enh_Dec: -2.20728421\n",
      "| epoch 115 |   800/ 4361 batches | lr 0.000000 | ms/batch 402.32 | loss  2.54 | ppl    12.68 | acc     0.68 | train_ae_norm     1.00\n",
      "[115/200][899/4361] Loss_D: 0.00476540 (Loss_D_real: 0.00130254 Loss_D_fake: 0.00346286) Loss_G: 0.73162645 Loss_Enh_Dec: -1.45779216\n",
      "| epoch 115 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  2.55 | ppl    12.79 | acc     0.70 | train_ae_norm     1.00\n",
      "[115/200][999/4361] Loss_D: 0.00153315 (Loss_D_real: 0.00036179 Loss_D_fake: 0.00117136) Loss_G: 0.42369404 Loss_Enh_Dec: -2.14977026\n",
      "| epoch 115 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.55 | ppl    12.84 | acc     0.67 | train_ae_norm     1.00\n",
      "[115/200][1099/4361] Loss_D: 0.01173984 (Loss_D_real: 0.00116015 Loss_D_fake: 0.01057969) Loss_G: 0.45350179 Loss_Enh_Dec: -1.64670646\n",
      "| epoch 115 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.79 | loss  2.52 | ppl    12.44 | acc     0.68 | train_ae_norm     1.00\n",
      "[115/200][1199/4361] Loss_D: 0.00987352 (Loss_D_real: 0.00774372 Loss_D_fake: 0.00212980) Loss_G: 0.40064707 Loss_Enh_Dec: -1.93182838\n",
      "| epoch 115 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  2.51 | ppl    12.36 | acc     0.69 | train_ae_norm     1.00\n",
      "[115/200][1299/4361] Loss_D: 0.01695556 (Loss_D_real: 0.00448570 Loss_D_fake: 0.01246986) Loss_G: 0.29117304 Loss_Enh_Dec: -2.06278443\n",
      "| epoch 115 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  2.56 | ppl    12.87 | acc     0.68 | train_ae_norm     1.00\n",
      "[115/200][1399/4361] Loss_D: 0.01259116 (Loss_D_real: 0.00103561 Loss_D_fake: 0.01155555) Loss_G: 0.32088718 Loss_Enh_Dec: -1.77536130\n",
      "| epoch 115 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  2.55 | ppl    12.78 | acc     0.63 | train_ae_norm     1.00\n",
      "[115/200][1499/4361] Loss_D: 0.00747967 (Loss_D_real: 0.00120742 Loss_D_fake: 0.00627225) Loss_G: 0.25478280 Loss_Enh_Dec: -1.87526631\n",
      "| epoch 115 |  1500/ 4361 batches | lr 0.000000 | ms/batch 402.55 | loss  2.59 | ppl    13.35 | acc     0.68 | train_ae_norm     1.00\n",
      "[115/200][1599/4361] Loss_D: 0.00952961 (Loss_D_real: 0.00109014 Loss_D_fake: 0.00843946) Loss_G: 0.25699255 Loss_Enh_Dec: -1.82481277\n",
      "| epoch 115 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  2.55 | ppl    12.80 | acc     0.69 | train_ae_norm     1.00\n",
      "[115/200][1699/4361] Loss_D: 0.02339363 (Loss_D_real: 0.01726146 Loss_D_fake: 0.00613216) Loss_G: 0.25582576 Loss_Enh_Dec: -1.73812771\n",
      "| epoch 115 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  2.52 | ppl    12.37 | acc     0.67 | train_ae_norm     1.00\n",
      "[115/200][1799/4361] Loss_D: 0.01428544 (Loss_D_real: 0.00683179 Loss_D_fake: 0.00745365) Loss_G: 0.25704905 Loss_Enh_Dec: -1.96664751\n",
      "| epoch 115 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  2.49 | ppl    12.04 | acc     0.68 | train_ae_norm     1.00\n",
      "[115/200][1899/4361] Loss_D: 0.01058707 (Loss_D_real: 0.00567265 Loss_D_fake: 0.00491442) Loss_G: 0.26823968 Loss_Enh_Dec: -1.68948400\n",
      "| epoch 115 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.56 | ppl    12.93 | acc     0.70 | train_ae_norm     1.00\n",
      "[115/200][1999/4361] Loss_D: 0.00567302 (Loss_D_real: 0.00114149 Loss_D_fake: 0.00453153) Loss_G: 0.28544816 Loss_Enh_Dec: -1.90419960\n",
      "| epoch 115 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.49 | ppl    12.06 | acc     0.70 | train_ae_norm     1.00\n",
      "[115/200][2099/4361] Loss_D: 0.01319241 (Loss_D_real: 0.00894182 Loss_D_fake: 0.00425060) Loss_G: 0.27633968 Loss_Enh_Dec: -2.38887691\n",
      "| epoch 115 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  2.54 | ppl    12.63 | acc     0.69 | train_ae_norm     1.00\n",
      "[115/200][2199/4361] Loss_D: 0.00608696 (Loss_D_real: 0.00221615 Loss_D_fake: 0.00387081) Loss_G: 0.28837010 Loss_Enh_Dec: -1.94450605\n",
      "| epoch 115 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  2.50 | ppl    12.16 | acc     0.70 | train_ae_norm     1.00\n",
      "[115/200][2499/4361] Loss_D: 0.01178371 (Loss_D_real: 0.00769791 Loss_D_fake: 0.00408580) Loss_G: 0.27767438 Loss_Enh_Dec: -1.62892044\n",
      "| epoch 115 |  2500/ 4361 batches | lr 0.000000 | ms/batch 400.28 | loss  2.54 | ppl    12.68 | acc     0.71 | train_ae_norm     1.00\n",
      "[115/200][2599/4361] Loss_D: 0.00334263 (Loss_D_real: 0.00097389 Loss_D_fake: 0.00236874) Loss_G: 0.30870852 Loss_Enh_Dec: -1.71043479\n",
      "| epoch 115 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.02 | loss  2.51 | ppl    12.25 | acc     0.68 | train_ae_norm     1.00\n",
      "[115/200][2699/4361] Loss_D: 0.00545868 (Loss_D_real: 0.00196287 Loss_D_fake: 0.00349582) Loss_G: 0.28448200 Loss_Enh_Dec: -1.87448406\n",
      "| epoch 115 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  2.50 | ppl    12.22 | acc     0.68 | train_ae_norm     1.00\n",
      "[115/200][2799/4361] Loss_D: 0.00602299 (Loss_D_real: 0.00154206 Loss_D_fake: 0.00448093) Loss_G: 0.28615886 Loss_Enh_Dec: -1.93384135\n",
      "| epoch 115 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  2.45 | ppl    11.63 | acc     0.66 | train_ae_norm     1.00\n",
      "[115/200][2899/4361] Loss_D: 0.00364734 (Loss_D_real: 0.00163483 Loss_D_fake: 0.00201250) Loss_G: 0.33460656 Loss_Enh_Dec: -1.44307601\n",
      "| epoch 115 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.77 | loss  2.50 | ppl    12.13 | acc     0.68 | train_ae_norm     1.00\n",
      "[115/200][2999/4361] Loss_D: 0.00555991 (Loss_D_real: 0.00291543 Loss_D_fake: 0.00264448) Loss_G: 0.30234918 Loss_Enh_Dec: -1.67169189\n",
      "| epoch 115 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  2.49 | ppl    12.01 | acc     0.68 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115/200][3099/4361] Loss_D: 0.03899097 (Loss_D_real: 0.03639429 Loss_D_fake: 0.00259668) Loss_G: 0.30028149 Loss_Enh_Dec: -2.68060875\n",
      "| epoch 115 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  2.49 | ppl    12.05 | acc     0.68 | train_ae_norm     1.00\n",
      "[115/200][3199/4361] Loss_D: 0.00338249 (Loss_D_real: 0.00016290 Loss_D_fake: 0.00321959) Loss_G: 0.29402193 Loss_Enh_Dec: -2.06865668\n",
      "| epoch 115 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  2.50 | ppl    12.23 | acc     0.72 | train_ae_norm     1.00\n",
      "[115/200][3299/4361] Loss_D: 0.00304446 (Loss_D_real: 0.00094316 Loss_D_fake: 0.00210131) Loss_G: 0.33383045 Loss_Enh_Dec: -2.64394116\n",
      "| epoch 115 |  3300/ 4361 batches | lr 0.000000 | ms/batch 400.88 | loss  2.51 | ppl    12.31 | acc     0.69 | train_ae_norm     1.00\n",
      "[115/200][3399/4361] Loss_D: 0.00326662 (Loss_D_real: 0.00035662 Loss_D_fake: 0.00291000) Loss_G: 0.31019560 Loss_Enh_Dec: -2.25464368\n",
      "| epoch 115 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.67 | loss  2.49 | ppl    12.09 | acc     0.68 | train_ae_norm     1.00\n",
      "[115/200][3499/4361] Loss_D: 0.00587174 (Loss_D_real: 0.00338324 Loss_D_fake: 0.00248850) Loss_G: 0.29517028 Loss_Enh_Dec: -2.19435620\n",
      "| epoch 115 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  2.44 | ppl    11.52 | acc     0.70 | train_ae_norm     1.00\n",
      "[115/200][3599/4361] Loss_D: 0.00414500 (Loss_D_real: 0.00102065 Loss_D_fake: 0.00312435) Loss_G: 0.30118433 Loss_Enh_Dec: -2.30467796\n",
      "| epoch 115 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  2.46 | ppl    11.76 | acc     0.72 | train_ae_norm     1.00\n",
      "[115/200][3699/4361] Loss_D: 0.00523276 (Loss_D_real: 0.00142632 Loss_D_fake: 0.00380644) Loss_G: 0.28968322 Loss_Enh_Dec: -2.13381958\n",
      "| epoch 115 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  2.49 | ppl    12.02 | acc     0.68 | train_ae_norm     1.00\n",
      "[115/200][3799/4361] Loss_D: 0.00671010 (Loss_D_real: 0.00122794 Loss_D_fake: 0.00548216) Loss_G: 0.27227148 Loss_Enh_Dec: -3.06844521\n",
      "| epoch 115 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.48 | ppl    11.94 | acc     0.74 | train_ae_norm     1.00\n",
      "[115/200][3899/4361] Loss_D: 0.04040479 (Loss_D_real: 0.03206450 Loss_D_fake: 0.00834029) Loss_G: 0.24530979 Loss_Enh_Dec: -2.56873989\n",
      "| epoch 115 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  2.50 | ppl    12.16 | acc     0.67 | train_ae_norm     1.00\n",
      "[115/200][3999/4361] Loss_D: 0.00519032 (Loss_D_real: 0.00083772 Loss_D_fake: 0.00435261) Loss_G: 0.26977286 Loss_Enh_Dec: -3.02450538\n",
      "| epoch 115 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  2.48 | ppl    11.90 | acc     0.70 | train_ae_norm     1.00\n",
      "[115/200][4099/4361] Loss_D: 0.00621620 (Loss_D_real: 0.00032587 Loss_D_fake: 0.00589034) Loss_G: 0.26113996 Loss_Enh_Dec: -2.87476420\n",
      "| epoch 115 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  2.45 | ppl    11.62 | acc     0.71 | train_ae_norm     1.00\n",
      "[115/200][4199/4361] Loss_D: 0.00607086 (Loss_D_real: 0.00067398 Loss_D_fake: 0.00539688) Loss_G: 0.29579839 Loss_Enh_Dec: -2.89363670\n",
      "| epoch 115 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  2.51 | ppl    12.25 | acc     0.72 | train_ae_norm     1.00\n",
      "[115/200][4299/4361] Loss_D: 0.00269881 (Loss_D_real: 0.00015938 Loss_D_fake: 0.00253943) Loss_G: 0.37803668 Loss_Enh_Dec: -2.60228729\n",
      "| epoch 115 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  2.48 | ppl    11.96 | acc     0.72 | train_ae_norm     1.00\n",
      "| end of epoch 115 | time: 1852.93s | test loss  2.45 | test ppl 11.54 | acc 0.754\n",
      "bleu_self:  [4.20848421e-01 1.03345244e-01 8.19481001e-07 2.10664527e-08\n",
      " 2.74360455e-08]\n",
      "bleu_test:  [7.83035714e-01 2.47841289e-01 1.82672190e-06 4.25001406e-08\n",
      " 5.48266605e-08]\n",
      "bleu_self: [0.42084842,0.10334524,0.00000082,0.00000002,0.00000003]\n",
      "bleu_test: [0.78303571,0.24784129,0.00000183,0.00000004,0.00000005]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 116 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.698\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.487\n",
      "  Test Loss: 4.646\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 116 |     0/ 4361 batches | lr 0.000000 | ms/batch 866.35 | loss  0.02 | ppl     1.02 | acc     0.74 | train_ae_norm     1.00\n",
      "[116/200][299/4361] Loss_D: 0.00498112 (Loss_D_real: 0.00344735 Loss_D_fake: 0.00153378) Loss_G: 0.43070012 Loss_Enh_Dec: -2.68455482\n",
      "| epoch 116 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  2.49 | ppl    12.01 | acc     0.67 | train_ae_norm     1.00\n",
      "[116/200][399/4361] Loss_D: 0.01166810 (Loss_D_real: 0.00128155 Loss_D_fake: 0.01038656) Loss_G: 0.62258279 Loss_Enh_Dec: -2.58868480\n",
      "| epoch 116 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  2.42 | ppl    11.30 | acc     0.73 | train_ae_norm     1.00\n",
      "[116/200][499/4361] Loss_D: 0.00592468 (Loss_D_real: 0.00304431 Loss_D_fake: 0.00288037) Loss_G: 0.35460183 Loss_Enh_Dec: -2.41449809\n",
      "| epoch 116 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  2.46 | ppl    11.70 | acc     0.73 | train_ae_norm     1.00\n",
      "[116/200][599/4361] Loss_D: 0.00714357 (Loss_D_real: 0.00347501 Loss_D_fake: 0.00366856) Loss_G: 0.35672662 Loss_Enh_Dec: -2.42464447\n",
      "| epoch 116 |   600/ 4361 batches | lr 0.000000 | ms/batch 402.39 | loss  2.42 | ppl    11.23 | acc     0.69 | train_ae_norm     1.00\n",
      "[116/200][699/4361] Loss_D: 0.00450013 (Loss_D_real: 0.00102537 Loss_D_fake: 0.00347476) Loss_G: 0.32311210 Loss_Enh_Dec: -2.70024252\n",
      "| epoch 116 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  2.45 | ppl    11.62 | acc     0.71 | train_ae_norm     1.00\n",
      "[116/200][799/4361] Loss_D: 0.01051529 (Loss_D_real: 0.00881609 Loss_D_fake: 0.00169920) Loss_G: 0.35163906 Loss_Enh_Dec: -2.76482010\n",
      "| epoch 116 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.78 | loss  2.43 | ppl    11.40 | acc     0.69 | train_ae_norm     1.00\n",
      "[116/200][899/4361] Loss_D: 0.00234081 (Loss_D_real: 0.00057450 Loss_D_fake: 0.00176632) Loss_G: 0.36030623 Loss_Enh_Dec: -3.04170656\n",
      "| epoch 116 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  2.44 | ppl    11.42 | acc     0.73 | train_ae_norm     1.00\n",
      "[116/200][999/4361] Loss_D: 0.07118336 (Loss_D_real: 0.06946524 Loss_D_fake: 0.00171812) Loss_G: 0.39569187 Loss_Enh_Dec: -2.77725863\n",
      "| epoch 116 |  1000/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  2.41 | ppl    11.14 | acc     0.69 | train_ae_norm     1.00\n",
      "[116/200][1099/4361] Loss_D: 0.00505555 (Loss_D_real: 0.00225297 Loss_D_fake: 0.00280258) Loss_G: 0.38830972 Loss_Enh_Dec: -2.72574878\n",
      "| epoch 116 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.40 | ppl    11.04 | acc     0.66 | train_ae_norm     1.00\n",
      "[116/200][1199/4361] Loss_D: 0.01571347 (Loss_D_real: 0.01283854 Loss_D_fake: 0.00287493) Loss_G: 0.39989698 Loss_Enh_Dec: -2.60726070\n",
      "| epoch 116 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.77 | loss  2.42 | ppl    11.29 | acc     0.73 | train_ae_norm     1.00\n",
      "[116/200][1299/4361] Loss_D: 0.00819675 (Loss_D_real: 0.00650817 Loss_D_fake: 0.00168859) Loss_G: 0.38436729 Loss_Enh_Dec: -2.52106333\n",
      "| epoch 116 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.57 | loss  2.45 | ppl    11.53 | acc     0.73 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[116/200][1399/4361] Loss_D: 0.02259871 (Loss_D_real: 0.01841294 Loss_D_fake: 0.00418577) Loss_G: 0.47468731 Loss_Enh_Dec: -2.58623123\n",
      "| epoch 116 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.43 | ppl    11.41 | acc     0.65 | train_ae_norm     1.00\n",
      "[116/200][1499/4361] Loss_D: 0.00886580 (Loss_D_real: 0.00536797 Loss_D_fake: 0.00349783) Loss_G: 0.32018051 Loss_Enh_Dec: -2.36084914\n",
      "| epoch 116 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  2.48 | ppl    11.99 | acc     0.68 | train_ae_norm     1.00\n",
      "[116/200][1599/4361] Loss_D: 0.00889422 (Loss_D_real: 0.00014974 Loss_D_fake: 0.00874448) Loss_G: 0.35569543 Loss_Enh_Dec: -2.34372878\n",
      "| epoch 116 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.91 | loss  2.45 | ppl    11.61 | acc     0.70 | train_ae_norm     1.00\n",
      "[116/200][1699/4361] Loss_D: 0.00479756 (Loss_D_real: 0.00119786 Loss_D_fake: 0.00359970) Loss_G: 0.33385429 Loss_Enh_Dec: -2.23283815\n",
      "| epoch 116 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  2.43 | ppl    11.33 | acc     0.68 | train_ae_norm     1.00\n",
      "[116/200][1799/4361] Loss_D: 0.00579876 (Loss_D_real: 0.00420321 Loss_D_fake: 0.00159555) Loss_G: 0.40884972 Loss_Enh_Dec: -2.26253629\n",
      "| epoch 116 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  2.42 | ppl    11.25 | acc     0.70 | train_ae_norm     1.00\n",
      "[116/200][1899/4361] Loss_D: 0.00976915 (Loss_D_real: 0.00259908 Loss_D_fake: 0.00717007) Loss_G: 0.38178155 Loss_Enh_Dec: -2.04954386\n",
      "| epoch 116 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.45 | ppl    11.63 | acc     0.69 | train_ae_norm     1.00\n",
      "[116/200][1999/4361] Loss_D: 0.02072249 (Loss_D_real: 0.00689885 Loss_D_fake: 0.01382363) Loss_G: 0.33572778 Loss_Enh_Dec: -2.00203180\n",
      "| epoch 116 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.43 | ppl    11.39 | acc     0.72 | train_ae_norm     1.00\n",
      "[116/200][2099/4361] Loss_D: 0.01117397 (Loss_D_real: 0.00680115 Loss_D_fake: 0.00437282) Loss_G: 0.36256814 Loss_Enh_Dec: -2.66889262\n",
      "| epoch 116 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  2.45 | ppl    11.59 | acc     0.73 | train_ae_norm     1.00\n",
      "[116/200][2199/4361] Loss_D: 0.03262620 (Loss_D_real: 0.02878503 Loss_D_fake: 0.00384118) Loss_G: 0.37146166 Loss_Enh_Dec: -2.23525238\n",
      "| epoch 116 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  2.43 | ppl    11.39 | acc     0.70 | train_ae_norm     1.00\n",
      "[116/200][2299/4361] Loss_D: 0.00409184 (Loss_D_real: 0.00146534 Loss_D_fake: 0.00262650) Loss_G: 0.35257328 Loss_Enh_Dec: -2.58565784\n",
      "| epoch 116 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.48 | ppl    11.91 | acc     0.72 | train_ae_norm     1.00\n",
      "[116/200][2399/4361] Loss_D: 0.01133412 (Loss_D_real: 0.00755319 Loss_D_fake: 0.00378093) Loss_G: 0.40908310 Loss_Enh_Dec: -2.54868627\n",
      "| epoch 116 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.43 | ppl    11.40 | acc     0.66 | train_ae_norm     1.00\n",
      "[116/200][2499/4361] Loss_D: 0.08893929 (Loss_D_real: 0.08660299 Loss_D_fake: 0.00233631) Loss_G: 0.33527258 Loss_Enh_Dec: -2.57026744\n",
      "| epoch 116 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  2.46 | ppl    11.70 | acc     0.74 | train_ae_norm     1.00\n",
      "[116/200][2599/4361] Loss_D: 0.07168667 (Loss_D_real: 0.06734782 Loss_D_fake: 0.00433885) Loss_G: 0.32024309 Loss_Enh_Dec: -2.47122502\n",
      "| epoch 116 |  2600/ 4361 batches | lr 0.000000 | ms/batch 402.27 | loss  2.42 | ppl    11.24 | acc     0.70 | train_ae_norm     1.00\n",
      "[116/200][2899/4361] Loss_D: 0.00331327 (Loss_D_real: 0.00045929 Loss_D_fake: 0.00285398) Loss_G: 0.33435658 Loss_Enh_Dec: -2.79953742\n",
      "| epoch 116 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.78 | loss  2.43 | ppl    11.37 | acc     0.70 | train_ae_norm     1.00\n",
      "[116/200][2999/4361] Loss_D: 0.00566790 (Loss_D_real: 0.00267432 Loss_D_fake: 0.00299358) Loss_G: 0.32665181 Loss_Enh_Dec: -2.78983974\n",
      "| epoch 116 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.91 | loss  2.45 | ppl    11.56 | acc     0.68 | train_ae_norm     1.00\n",
      "[116/200][3099/4361] Loss_D: 0.02784341 (Loss_D_real: 0.00140112 Loss_D_fake: 0.02644229) Loss_G: 0.29886219 Loss_Enh_Dec: -2.97465372\n",
      "| epoch 116 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  2.45 | ppl    11.56 | acc     0.69 | train_ae_norm     1.00\n",
      "[116/200][3199/4361] Loss_D: 0.07184619 (Loss_D_real: 0.06853115 Loss_D_fake: 0.00331504) Loss_G: 0.34569329 Loss_Enh_Dec: -2.46089172\n",
      "| epoch 116 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.48 | ppl    11.99 | acc     0.73 | train_ae_norm     1.00\n",
      "[116/200][3299/4361] Loss_D: 0.00437646 (Loss_D_real: 0.00125533 Loss_D_fake: 0.00312113) Loss_G: 0.34513730 Loss_Enh_Dec: -2.57774043\n",
      "| epoch 116 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  2.47 | ppl    11.84 | acc     0.68 | train_ae_norm     1.00\n",
      "[116/200][3399/4361] Loss_D: 0.00640276 (Loss_D_real: 0.00190367 Loss_D_fake: 0.00449908) Loss_G: 0.30291149 Loss_Enh_Dec: -2.39499497\n",
      "| epoch 116 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  2.44 | ppl    11.46 | acc     0.70 | train_ae_norm     1.00\n",
      "[116/200][3499/4361] Loss_D: 0.00577257 (Loss_D_real: 0.00111912 Loss_D_fake: 0.00465345) Loss_G: 0.33493328 Loss_Enh_Dec: -2.63706183\n",
      "| epoch 116 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.55 | loss  2.39 | ppl    10.87 | acc     0.70 | train_ae_norm     1.00\n",
      "[116/200][3599/4361] Loss_D: 0.02196389 (Loss_D_real: 0.00191022 Loss_D_fake: 0.02005367) Loss_G: 0.44801378 Loss_Enh_Dec: -2.52527308\n",
      "| epoch 116 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  2.39 | ppl    10.88 | acc     0.70 | train_ae_norm     1.00\n",
      "[116/200][3699/4361] Loss_D: 0.01341864 (Loss_D_real: 0.01177877 Loss_D_fake: 0.00163987) Loss_G: 0.45555016 Loss_Enh_Dec: -1.88613021\n",
      "| epoch 116 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.81 | loss  2.42 | ppl    11.22 | acc     0.70 | train_ae_norm     1.00\n",
      "[116/200][3799/4361] Loss_D: 0.00523896 (Loss_D_real: 0.00016463 Loss_D_fake: 0.00507433) Loss_G: 0.34072247 Loss_Enh_Dec: -2.52891588\n",
      "| epoch 116 |  3800/ 4361 batches | lr 0.000000 | ms/batch 400.59 | loss  2.41 | ppl    11.16 | acc     0.76 | train_ae_norm     1.00\n",
      "[116/200][3899/4361] Loss_D: 0.01174339 (Loss_D_real: 0.00500818 Loss_D_fake: 0.00673522) Loss_G: 0.31367183 Loss_Enh_Dec: -2.41073275\n",
      "| epoch 116 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  2.43 | ppl    11.32 | acc     0.66 | train_ae_norm     1.00\n",
      "[116/200][3999/4361] Loss_D: 0.01139582 (Loss_D_real: 0.00385805 Loss_D_fake: 0.00753777) Loss_G: 0.43102455 Loss_Enh_Dec: -2.52450728\n",
      "| epoch 116 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.41 | ppl    11.09 | acc     0.69 | train_ae_norm     1.00\n",
      "[116/200][4099/4361] Loss_D: 0.00538852 (Loss_D_real: 0.00106402 Loss_D_fake: 0.00432450) Loss_G: 0.39131287 Loss_Enh_Dec: -2.46703529\n",
      "| epoch 116 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  2.39 | ppl    10.86 | acc     0.71 | train_ae_norm     1.00\n",
      "[116/200][4199/4361] Loss_D: 0.00733838 (Loss_D_real: 0.00222518 Loss_D_fake: 0.00511320) Loss_G: 0.28650904 Loss_Enh_Dec: -2.11013842\n",
      "| epoch 116 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.42 | ppl    11.25 | acc     0.73 | train_ae_norm     1.00\n",
      "[116/200][4299/4361] Loss_D: 0.01530451 (Loss_D_real: 0.01185006 Loss_D_fake: 0.00345446) Loss_G: 0.31580648 Loss_Enh_Dec: -2.50909591\n",
      "| epoch 116 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  2.38 | ppl    10.81 | acc     0.74 | train_ae_norm     1.00\n",
      "| end of epoch 116 | time: 1852.54s | test loss  2.40 | test ppl 10.98 | acc 0.765\n",
      "bleu_self:  [2.58030074e-01 7.46538206e-02 7.72541535e-07 2.91594326e-09\n",
      " 5.07209940e-09]\n",
      "bleu_test:  [6.60962301e-01 1.17096730e-08 3.46340138e-11 1.06529820e-10\n",
      " 4.38203664e-10]\n",
      "bleu_self: [0.25803007,0.07465382,0.00000077,0.00000000,0.00000001]\n",
      "bleu_test: [0.66096230,0.00000001,0.00000000,0.00000000,0.00000000]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 117 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.699\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.497\n",
      "  Test Loss: 4.337\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 117 |     0/ 4361 batches | lr 0.000000 | ms/batch 866.21 | loss  0.02 | ppl     1.02 | acc     0.75 | train_ae_norm     1.00\n",
      "[117/200][99/4361] Loss_D: 0.02180668 (Loss_D_real: 0.00878560 Loss_D_fake: 0.01302108) Loss_G: 0.38059643 Loss_Enh_Dec: -2.48176169\n",
      "| epoch 117 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  2.39 | ppl    10.89 | acc     0.69 | train_ae_norm     1.00\n",
      "[117/200][199/4361] Loss_D: 0.22819108 (Loss_D_real: 0.22627383 Loss_D_fake: 0.00191724) Loss_G: 0.47951785 Loss_Enh_Dec: -2.14372015\n",
      "| epoch 117 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.42 | ppl    11.30 | acc     0.73 | train_ae_norm     1.00\n",
      "[117/200][299/4361] Loss_D: 0.00384415 (Loss_D_real: 0.00126178 Loss_D_fake: 0.00258237) Loss_G: 0.36613050 Loss_Enh_Dec: -2.14895606\n",
      "| epoch 117 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  2.44 | ppl    11.46 | acc     0.66 | train_ae_norm     1.00\n",
      "[117/200][399/4361] Loss_D: 0.01370955 (Loss_D_real: 0.00909727 Loss_D_fake: 0.00461228) Loss_G: 0.37193084 Loss_Enh_Dec: -1.94208610\n",
      "| epoch 117 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.35 | ppl    10.49 | acc     0.69 | train_ae_norm     1.00\n",
      "[117/200][499/4361] Loss_D: 0.00489885 (Loss_D_real: 0.00177151 Loss_D_fake: 0.00312733) Loss_G: 0.36049458 Loss_Enh_Dec: -2.38049054\n",
      "| epoch 117 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  2.43 | ppl    11.31 | acc     0.73 | train_ae_norm     1.00\n",
      "[117/200][599/4361] Loss_D: 0.00711216 (Loss_D_real: 0.00068588 Loss_D_fake: 0.00642628) Loss_G: 0.31358823 Loss_Enh_Dec: -2.61077476\n",
      "| epoch 117 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.37 | ppl    10.70 | acc     0.68 | train_ae_norm     1.00\n",
      "[117/200][699/4361] Loss_D: 0.01514610 (Loss_D_real: 0.01341708 Loss_D_fake: 0.00172902) Loss_G: 0.37687674 Loss_Enh_Dec: -2.01497579\n",
      "| epoch 117 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.39 | ppl    10.90 | acc     0.69 | train_ae_norm     1.00\n",
      "[117/200][799/4361] Loss_D: 0.01271440 (Loss_D_real: 0.00930646 Loss_D_fake: 0.00340795) Loss_G: 0.33490524 Loss_Enh_Dec: -2.39143634\n",
      "| epoch 117 |   800/ 4361 batches | lr 0.000000 | ms/batch 402.17 | loss  2.38 | ppl    10.85 | acc     0.70 | train_ae_norm     1.00\n",
      "[117/200][899/4361] Loss_D: 0.06472192 (Loss_D_real: 0.00584912 Loss_D_fake: 0.05887280) Loss_G: 0.50894332 Loss_Enh_Dec: -2.49673343\n",
      "| epoch 117 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  2.40 | ppl    10.99 | acc     0.74 | train_ae_norm     1.00\n",
      "[117/200][999/4361] Loss_D: 0.03392050 (Loss_D_real: 0.02403804 Loss_D_fake: 0.00988246) Loss_G: 0.54918474 Loss_Enh_Dec: -2.50512242\n",
      "| epoch 117 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  2.37 | ppl    10.73 | acc     0.71 | train_ae_norm     1.00\n",
      "[117/200][1099/4361] Loss_D: 0.00997052 (Loss_D_real: 0.00104823 Loss_D_fake: 0.00892229) Loss_G: 0.33279687 Loss_Enh_Dec: -2.38428879\n",
      "| epoch 117 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  2.36 | ppl    10.55 | acc     0.70 | train_ae_norm     1.00\n",
      "[117/200][1199/4361] Loss_D: 0.00293301 (Loss_D_real: 0.00025551 Loss_D_fake: 0.00267749) Loss_G: 0.32465589 Loss_Enh_Dec: -2.35010839\n",
      "| epoch 117 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  2.40 | ppl    10.99 | acc     0.76 | train_ae_norm     1.00\n",
      "[117/200][1299/4361] Loss_D: 0.00638578 (Loss_D_real: 0.00078809 Loss_D_fake: 0.00559770) Loss_G: 0.33048701 Loss_Enh_Dec: -2.38421607\n",
      "| epoch 117 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  2.39 | ppl    10.91 | acc     0.71 | train_ae_norm     1.00\n",
      "[117/200][1399/4361] Loss_D: 0.03335112 (Loss_D_real: 0.03205375 Loss_D_fake: 0.00129737) Loss_G: 0.31261253 Loss_Enh_Dec: -2.72088075\n",
      "| epoch 117 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.90 | loss  2.40 | ppl    11.02 | acc     0.67 | train_ae_norm     1.00\n",
      "[117/200][1499/4361] Loss_D: 0.06820072 (Loss_D_real: 0.06680691 Loss_D_fake: 0.00139381) Loss_G: 0.37767896 Loss_Enh_Dec: -2.52818108\n",
      "| epoch 117 |  1500/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  2.45 | ppl    11.64 | acc     0.69 | train_ae_norm     1.00\n",
      "[117/200][1599/4361] Loss_D: 0.02163685 (Loss_D_real: 0.02036789 Loss_D_fake: 0.00126896) Loss_G: 0.49102435 Loss_Enh_Dec: -2.67030191\n",
      "| epoch 117 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.45 | ppl    11.62 | acc     0.69 | train_ae_norm     1.00\n",
      "[117/200][1699/4361] Loss_D: 0.00535510 (Loss_D_real: 0.00338519 Loss_D_fake: 0.00196990) Loss_G: 0.39954472 Loss_Enh_Dec: -2.81033444\n",
      "| epoch 117 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  2.37 | ppl    10.73 | acc     0.67 | train_ae_norm     1.00\n",
      "[117/200][1799/4361] Loss_D: 0.01044992 (Loss_D_real: 0.00745908 Loss_D_fake: 0.00299084) Loss_G: 0.58054155 Loss_Enh_Dec: -2.57045412\n",
      "| epoch 117 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  2.36 | ppl    10.63 | acc     0.70 | train_ae_norm     1.00\n",
      "[117/200][1899/4361] Loss_D: 0.04649337 (Loss_D_real: 0.03142294 Loss_D_fake: 0.01507043) Loss_G: 0.52841151 Loss_Enh_Dec: -2.59415412\n",
      "| epoch 117 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  2.43 | ppl    11.36 | acc     0.71 | train_ae_norm     1.00\n",
      "[117/200][1999/4361] Loss_D: 0.01388561 (Loss_D_real: 0.01075557 Loss_D_fake: 0.00313005) Loss_G: 0.31910741 Loss_Enh_Dec: -2.60656548\n",
      "| epoch 117 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.43 | loss  2.37 | ppl    10.70 | acc     0.74 | train_ae_norm     1.00\n",
      "[117/200][2099/4361] Loss_D: 0.01636950 (Loss_D_real: 0.01128175 Loss_D_fake: 0.00508776) Loss_G: 0.32647344 Loss_Enh_Dec: -2.63465023\n",
      "| epoch 117 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  2.39 | ppl    10.86 | acc     0.78 | train_ae_norm     1.00\n",
      "[117/200][2199/4361] Loss_D: 0.00459503 (Loss_D_real: 0.00013480 Loss_D_fake: 0.00446023) Loss_G: 0.34646231 Loss_Enh_Dec: -2.70603323\n",
      "| epoch 117 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  2.37 | ppl    10.67 | acc     0.72 | train_ae_norm     1.00\n",
      "[117/200][2299/4361] Loss_D: 0.00595332 (Loss_D_real: 0.00131494 Loss_D_fake: 0.00463838) Loss_G: 0.36483762 Loss_Enh_Dec: -2.82556891\n",
      "| epoch 117 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.08 | loss  2.37 | ppl    10.68 | acc     0.71 | train_ae_norm     1.00\n",
      "[117/200][2399/4361] Loss_D: 0.00275348 (Loss_D_real: 0.00191105 Loss_D_fake: 0.00084243) Loss_G: 0.40385628 Loss_Enh_Dec: -2.66118741\n",
      "| epoch 117 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.40 | ppl    11.00 | acc     0.69 | train_ae_norm     1.00\n",
      "[117/200][2499/4361] Loss_D: 0.01508487 (Loss_D_real: 0.00452139 Loss_D_fake: 0.01056348) Loss_G: 0.32916686 Loss_Enh_Dec: -2.56313252\n",
      "| epoch 117 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  2.45 | ppl    11.57 | acc     0.72 | train_ae_norm     1.00\n",
      "[117/200][2599/4361] Loss_D: 0.00403373 (Loss_D_real: 0.00068306 Loss_D_fake: 0.00335067) Loss_G: 0.39530870 Loss_Enh_Dec: -2.47982287\n",
      "| epoch 117 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  2.40 | ppl    11.00 | acc     0.67 | train_ae_norm     1.00\n",
      "[117/200][2699/4361] Loss_D: 0.00214734 (Loss_D_real: 0.00145198 Loss_D_fake: 0.00069537) Loss_G: 0.61212415 Loss_Enh_Dec: -2.43915677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 117 |  2700/ 4361 batches | lr 0.000000 | ms/batch 400.53 | loss  2.41 | ppl    11.13 | acc     0.70 | train_ae_norm     1.00\n",
      "[117/200][2799/4361] Loss_D: 0.00706445 (Loss_D_real: 0.00235079 Loss_D_fake: 0.00471366) Loss_G: 0.30727804 Loss_Enh_Dec: -2.64344597\n",
      "| epoch 117 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.74 | loss  2.37 | ppl    10.66 | acc     0.69 | train_ae_norm     1.00\n",
      "[117/200][2899/4361] Loss_D: 0.03635345 (Loss_D_real: 0.03270496 Loss_D_fake: 0.00364850) Loss_G: 0.34716636 Loss_Enh_Dec: -2.44827533\n",
      "| epoch 117 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  2.41 | ppl    11.17 | acc     0.71 | train_ae_norm     1.00\n",
      "[117/200][2999/4361] Loss_D: 0.01550237 (Loss_D_real: 0.01071817 Loss_D_fake: 0.00478419) Loss_G: 0.35868043 Loss_Enh_Dec: -2.90693879\n",
      "| epoch 117 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  2.42 | ppl    11.22 | acc     0.71 | train_ae_norm     1.00\n",
      "[117/200][3099/4361] Loss_D: 0.01219727 (Loss_D_real: 0.00878482 Loss_D_fake: 0.00341245) Loss_G: 0.59873694 Loss_Enh_Dec: -2.37816858\n",
      "| epoch 117 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  2.43 | ppl    11.41 | acc     0.70 | train_ae_norm     1.00\n",
      "[117/200][3199/4361] Loss_D: 0.01267400 (Loss_D_real: 0.00618845 Loss_D_fake: 0.00648555) Loss_G: 0.36563298 Loss_Enh_Dec: -2.35741782\n",
      "| epoch 117 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  2.45 | ppl    11.59 | acc     0.70 | train_ae_norm     1.00\n",
      "[117/200][3299/4361] Loss_D: 0.01341667 (Loss_D_real: 0.00615417 Loss_D_fake: 0.00726251) Loss_G: 0.35139188 Loss_Enh_Dec: -2.29509044\n",
      "| epoch 117 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  2.46 | ppl    11.72 | acc     0.69 | train_ae_norm     1.00\n",
      "[117/200][3399/4361] Loss_D: 0.00213055 (Loss_D_real: 0.00018678 Loss_D_fake: 0.00194377) Loss_G: 0.32499942 Loss_Enh_Dec: -2.76341987\n",
      "| epoch 117 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  2.46 | ppl    11.69 | acc     0.67 | train_ae_norm     1.00\n",
      "[117/200][3499/4361] Loss_D: 0.04650843 (Loss_D_real: 0.04002236 Loss_D_fake: 0.00648607) Loss_G: 0.61336929 Loss_Enh_Dec: -2.16533399\n",
      "| epoch 117 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  2.39 | ppl    10.92 | acc     0.68 | train_ae_norm     1.00\n",
      "[117/200][3599/4361] Loss_D: 0.00849622 (Loss_D_real: 0.00186555 Loss_D_fake: 0.00663067) Loss_G: 0.38525763 Loss_Enh_Dec: -2.38838363\n",
      "| epoch 117 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.91 | loss  2.42 | ppl    11.28 | acc     0.71 | train_ae_norm     1.00\n",
      "[117/200][3699/4361] Loss_D: 0.05677215 (Loss_D_real: 0.05268890 Loss_D_fake: 0.00408326) Loss_G: 0.34225294 Loss_Enh_Dec: -2.56530595\n",
      "| epoch 117 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  2.42 | ppl    11.24 | acc     0.69 | train_ae_norm     1.00\n",
      "[117/200][3799/4361] Loss_D: 0.00268402 (Loss_D_real: 0.00019690 Loss_D_fake: 0.00248713) Loss_G: 0.33426103 Loss_Enh_Dec: -2.85839438\n",
      "| epoch 117 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  2.43 | ppl    11.35 | acc     0.76 | train_ae_norm     1.00\n",
      "[117/200][3899/4361] Loss_D: 0.00463730 (Loss_D_real: 0.00015948 Loss_D_fake: 0.00447782) Loss_G: 0.35591361 Loss_Enh_Dec: -2.82796073\n",
      "| epoch 117 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.43 | ppl    11.37 | acc     0.67 | train_ae_norm     1.00\n",
      "[117/200][3999/4361] Loss_D: 0.00954326 (Loss_D_real: 0.00628140 Loss_D_fake: 0.00326186) Loss_G: 0.33730197 Loss_Enh_Dec: -2.72844553\n",
      "| epoch 117 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  2.44 | ppl    11.46 | acc     0.70 | train_ae_norm     1.00\n",
      "[117/200][4099/4361] Loss_D: 0.01049761 (Loss_D_real: 0.00994210 Loss_D_fake: 0.00055551) Loss_G: 0.54691935 Loss_Enh_Dec: -3.18699622\n",
      "| epoch 117 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  2.41 | ppl    11.14 | acc     0.69 | train_ae_norm     1.00\n",
      "[117/200][4199/4361] Loss_D: 0.01079580 (Loss_D_real: 0.00245563 Loss_D_fake: 0.00834017) Loss_G: 0.37881818 Loss_Enh_Dec: -2.59510398\n",
      "| epoch 117 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  2.45 | ppl    11.62 | acc     0.70 | train_ae_norm     1.00\n",
      "[117/200][4299/4361] Loss_D: 0.00781587 (Loss_D_real: 0.00124375 Loss_D_fake: 0.00657212) Loss_G: 0.33203286 Loss_Enh_Dec: -2.93477321\n",
      "| epoch 117 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  2.43 | ppl    11.34 | acc     0.70 | train_ae_norm     1.00\n",
      "| end of epoch 117 | time: 1852.42s | test loss  2.44 | test ppl 11.45 | acc 0.758\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 118 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.702\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.505\n",
      "  Test Loss: 4.331\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 118 |     0/ 4361 batches | lr 0.000000 | ms/batch 861.77 | loss  0.02 | ppl     1.02 | acc     0.74 | train_ae_norm     1.00\n",
      "[118/200][99/4361] Loss_D: 0.01657984 (Loss_D_real: 0.01059189 Loss_D_fake: 0.00598795) Loss_G: 0.33734807 Loss_Enh_Dec: -2.84091258\n",
      "| epoch 118 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  2.42 | ppl    11.20 | acc     0.68 | train_ae_norm     1.00\n",
      "[118/200][199/4361] Loss_D: 0.01350027 (Loss_D_real: 0.00074402 Loss_D_fake: 0.01275625) Loss_G: 0.33568439 Loss_Enh_Dec: -3.04069686\n",
      "| epoch 118 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  2.45 | ppl    11.58 | acc     0.72 | train_ae_norm     1.00\n",
      "[118/200][299/4361] Loss_D: 0.01576851 (Loss_D_real: 0.01134472 Loss_D_fake: 0.00442379) Loss_G: 0.33879441 Loss_Enh_Dec: -3.06962705\n",
      "| epoch 118 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.65 | loss  2.45 | ppl    11.62 | acc     0.66 | train_ae_norm     1.00\n",
      "[118/200][399/4361] Loss_D: 0.02253674 (Loss_D_real: 0.00014534 Loss_D_fake: 0.02239140) Loss_G: 0.36004281 Loss_Enh_Dec: -2.87286997\n",
      "| epoch 118 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  2.36 | ppl    10.64 | acc     0.71 | train_ae_norm     1.00\n",
      "[118/200][499/4361] Loss_D: 0.12560949 (Loss_D_real: 0.12020017 Loss_D_fake: 0.00540932) Loss_G: 0.31401405 Loss_Enh_Dec: -2.84383845\n",
      "| epoch 118 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.44 | ppl    11.45 | acc     0.71 | train_ae_norm     1.00\n",
      "[118/200][599/4361] Loss_D: 0.00319991 (Loss_D_real: 0.00017043 Loss_D_fake: 0.00302949) Loss_G: 0.33816841 Loss_Enh_Dec: -3.10267854\n",
      "| epoch 118 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.38 | ppl    10.78 | acc     0.67 | train_ae_norm     1.00\n",
      "[118/200][699/4361] Loss_D: 0.18389888 (Loss_D_real: 0.16505080 Loss_D_fake: 0.01884808) Loss_G: 0.33408338 Loss_Enh_Dec: -2.87079287\n",
      "| epoch 118 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.42 | ppl    11.27 | acc     0.71 | train_ae_norm     1.00\n",
      "[118/200][799/4361] Loss_D: 0.01150399 (Loss_D_real: 0.00425458 Loss_D_fake: 0.00724942) Loss_G: 0.39163491 Loss_Enh_Dec: -2.62642264\n",
      "| epoch 118 |   800/ 4361 batches | lr 0.000000 | ms/batch 402.24 | loss  2.39 | ppl    10.89 | acc     0.68 | train_ae_norm     1.00\n",
      "[118/200][899/4361] Loss_D: 0.00338733 (Loss_D_real: 0.00222757 Loss_D_fake: 0.00115976) Loss_G: 0.37530518 Loss_Enh_Dec: -2.68922591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 118 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  2.40 | ppl    11.08 | acc     0.71 | train_ae_norm     1.00\n",
      "[118/200][999/4361] Loss_D: 0.00834324 (Loss_D_real: 0.00635187 Loss_D_fake: 0.00199138) Loss_G: 0.37927589 Loss_Enh_Dec: -2.47224021\n",
      "| epoch 118 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  2.38 | ppl    10.81 | acc     0.71 | train_ae_norm     1.00\n",
      "[118/200][1099/4361] Loss_D: 0.02066432 (Loss_D_real: 0.01757117 Loss_D_fake: 0.00309315) Loss_G: 0.39551783 Loss_Enh_Dec: -2.17799592\n",
      "| epoch 118 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  2.37 | ppl    10.65 | acc     0.72 | train_ae_norm     1.00\n",
      "[118/200][1199/4361] Loss_D: 0.23863348 (Loss_D_real: 0.00871333 Loss_D_fake: 0.22992015) Loss_G: 0.54725164 Loss_Enh_Dec: -2.45020652\n",
      "| epoch 118 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  2.39 | ppl    10.88 | acc     0.74 | train_ae_norm     1.00\n",
      "[118/200][1299/4361] Loss_D: 0.00636858 (Loss_D_real: 0.00196353 Loss_D_fake: 0.00440505) Loss_G: 0.38809413 Loss_Enh_Dec: -2.28010631\n",
      "| epoch 118 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.40 | ppl    11.06 | acc     0.68 | train_ae_norm     1.00\n",
      "[118/200][1399/4361] Loss_D: 0.00192841 (Loss_D_real: 0.00180451 Loss_D_fake: 0.00012389) Loss_G: 0.58033770 Loss_Enh_Dec: -2.26871777\n",
      "| epoch 118 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.38 | ppl    10.80 | acc     0.69 | train_ae_norm     1.00\n",
      "[118/200][1499/4361] Loss_D: 0.02360377 (Loss_D_real: 0.01334535 Loss_D_fake: 0.01025843) Loss_G: 0.39811417 Loss_Enh_Dec: -2.10894823\n",
      "| epoch 118 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  2.43 | ppl    11.36 | acc     0.70 | train_ae_norm     1.00\n",
      "[118/200][1599/4361] Loss_D: 0.05116377 (Loss_D_real: 0.00042088 Loss_D_fake: 0.05074289) Loss_G: 0.47509837 Loss_Enh_Dec: -2.28207946\n",
      "| epoch 118 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  2.41 | ppl    11.12 | acc     0.71 | train_ae_norm     1.00\n",
      "[118/200][1699/4361] Loss_D: 0.01952581 (Loss_D_real: 0.01027374 Loss_D_fake: 0.00925207) Loss_G: 0.34680590 Loss_Enh_Dec: -2.12536263\n",
      "| epoch 118 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  2.39 | ppl    10.91 | acc     0.69 | train_ae_norm     1.00\n",
      "[118/200][1999/4361] Loss_D: 0.01519955 (Loss_D_real: 0.01229233 Loss_D_fake: 0.00290721) Loss_G: 0.38136777 Loss_Enh_Dec: -2.42807364\n",
      "| epoch 118 |  2000/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  2.37 | ppl    10.74 | acc     0.72 | train_ae_norm     1.00\n",
      "[118/200][2099/4361] Loss_D: 0.00706708 (Loss_D_real: 0.00103557 Loss_D_fake: 0.00603151) Loss_G: 0.30995855 Loss_Enh_Dec: -2.43687797\n",
      "| epoch 118 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.39 | ppl    10.95 | acc     0.75 | train_ae_norm     1.00\n",
      "[118/200][2199/4361] Loss_D: 0.01208193 (Loss_D_real: 0.00404229 Loss_D_fake: 0.00803964) Loss_G: 0.34013915 Loss_Enh_Dec: -2.19998312\n",
      "| epoch 118 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  2.36 | ppl    10.64 | acc     0.72 | train_ae_norm     1.00\n",
      "[118/200][2299/4361] Loss_D: 0.00475825 (Loss_D_real: 0.00076479 Loss_D_fake: 0.00399346) Loss_G: 0.30342075 Loss_Enh_Dec: -1.96068501\n",
      "| epoch 118 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.35 | ppl    10.53 | acc     0.72 | train_ae_norm     1.00\n",
      "[118/200][2399/4361] Loss_D: 0.00697910 (Loss_D_real: 0.00033166 Loss_D_fake: 0.00664744) Loss_G: 0.37357622 Loss_Enh_Dec: -2.60831809\n",
      "| epoch 118 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.06 | loss  2.37 | ppl    10.66 | acc     0.65 | train_ae_norm     1.00\n",
      "[118/200][2499/4361] Loss_D: 0.03288935 (Loss_D_real: 0.02898341 Loss_D_fake: 0.00390594) Loss_G: 0.35629019 Loss_Enh_Dec: -2.04008937\n",
      "| epoch 118 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.41 | ppl    11.11 | acc     0.72 | train_ae_norm     1.00\n",
      "[118/200][2599/4361] Loss_D: 0.00943909 (Loss_D_real: 0.00171352 Loss_D_fake: 0.00772557) Loss_G: 0.35396677 Loss_Enh_Dec: -2.73311496\n",
      "| epoch 118 |  2600/ 4361 batches | lr 0.000000 | ms/batch 402.01 | loss  2.39 | ppl    10.88 | acc     0.68 | train_ae_norm     1.00\n",
      "[118/200][2699/4361] Loss_D: 0.02912230 (Loss_D_real: 0.02626208 Loss_D_fake: 0.00286022) Loss_G: 0.35986304 Loss_Enh_Dec: -2.39025879\n",
      "| epoch 118 |  2700/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  2.37 | ppl    10.67 | acc     0.70 | train_ae_norm     1.00\n",
      "[118/200][2799/4361] Loss_D: 0.00893340 (Loss_D_real: 0.00356147 Loss_D_fake: 0.00537193) Loss_G: 0.34716853 Loss_Enh_Dec: -2.41011405\n",
      "| epoch 118 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  2.35 | ppl    10.50 | acc     0.68 | train_ae_norm     1.00\n",
      "[118/200][2899/4361] Loss_D: 0.02236384 (Loss_D_real: 0.01601975 Loss_D_fake: 0.00634409) Loss_G: 0.37119189 Loss_Enh_Dec: -1.85352695\n",
      "| epoch 118 |  2900/ 4361 batches | lr 0.000000 | ms/batch 402.28 | loss  2.37 | ppl    10.67 | acc     0.72 | train_ae_norm     1.00\n",
      "[118/200][2999/4361] Loss_D: 0.00824365 (Loss_D_real: 0.00082406 Loss_D_fake: 0.00741959) Loss_G: 0.46202031 Loss_Enh_Dec: -1.93478394\n",
      "| epoch 118 |  3000/ 4361 batches | lr 0.000000 | ms/batch 421.03 | loss  2.37 | ppl    10.69 | acc     0.72 | train_ae_norm     1.00\n",
      "[118/200][3099/4361] Loss_D: 0.01165784 (Loss_D_real: 0.00725593 Loss_D_fake: 0.00440191) Loss_G: 0.38110715 Loss_Enh_Dec: -2.03039908\n",
      "| epoch 118 |  3100/ 4361 batches | lr 0.000000 | ms/batch 407.82 | loss  2.40 | ppl    10.99 | acc     0.72 | train_ae_norm     1.00\n",
      "[118/200][3199/4361] Loss_D: 0.00425641 (Loss_D_real: 0.00146920 Loss_D_fake: 0.00278721) Loss_G: 0.38666713 Loss_Enh_Dec: -2.25962734\n",
      "| epoch 118 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  2.42 | ppl    11.19 | acc     0.73 | train_ae_norm     1.00\n",
      "[118/200][3299/4361] Loss_D: 0.00249124 (Loss_D_real: 0.00050717 Loss_D_fake: 0.00198407) Loss_G: 0.44197106 Loss_Enh_Dec: -2.40941310\n",
      "| epoch 118 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  2.40 | ppl    11.07 | acc     0.68 | train_ae_norm     1.00\n",
      "[118/200][3399/4361] Loss_D: 0.00226463 (Loss_D_real: 0.00047060 Loss_D_fake: 0.00179403) Loss_G: 0.42500630 Loss_Enh_Dec: -1.81784272\n",
      "| epoch 118 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  2.41 | ppl    11.13 | acc     0.68 | train_ae_norm     1.00\n",
      "[118/200][3499/4361] Loss_D: 0.00618804 (Loss_D_real: 0.00189932 Loss_D_fake: 0.00428873) Loss_G: 0.35373092 Loss_Enh_Dec: -1.96278191\n",
      "| epoch 118 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  2.33 | ppl    10.29 | acc     0.69 | train_ae_norm     1.00\n",
      "[118/200][3599/4361] Loss_D: 0.00744664 (Loss_D_real: 0.00004074 Loss_D_fake: 0.00740591) Loss_G: 0.31248435 Loss_Enh_Dec: -2.07174540\n",
      "| epoch 118 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  2.37 | ppl    10.70 | acc     0.71 | train_ae_norm     1.00\n",
      "[118/200][3699/4361] Loss_D: 0.00713403 (Loss_D_real: 0.00106027 Loss_D_fake: 0.00607376) Loss_G: 0.37550017 Loss_Enh_Dec: -2.31857061\n",
      "| epoch 118 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  2.36 | ppl    10.60 | acc     0.68 | train_ae_norm     1.00\n",
      "[118/200][3799/4361] Loss_D: 0.00459196 (Loss_D_real: 0.00042086 Loss_D_fake: 0.00417110) Loss_G: 0.35659286 Loss_Enh_Dec: -1.90417945\n",
      "| epoch 118 |  3800/ 4361 batches | lr 0.000000 | ms/batch 400.65 | loss  2.36 | ppl    10.59 | acc     0.75 | train_ae_norm     1.00\n",
      "[118/200][3899/4361] Loss_D: 0.00342955 (Loss_D_real: 0.00101644 Loss_D_fake: 0.00241311) Loss_G: 0.37858933 Loss_Enh_Dec: -2.27042603\n",
      "| epoch 118 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  2.37 | ppl    10.73 | acc     0.71 | train_ae_norm     1.00\n",
      "[118/200][3999/4361] Loss_D: 0.02767246 (Loss_D_real: 0.02298428 Loss_D_fake: 0.00468818) Loss_G: 0.35481736 Loss_Enh_Dec: -2.48995280\n",
      "| epoch 118 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  2.36 | ppl    10.55 | acc     0.71 | train_ae_norm     1.00\n",
      "[118/200][4099/4361] Loss_D: 0.00600943 (Loss_D_real: 0.00203675 Loss_D_fake: 0.00397268) Loss_G: 0.37608296 Loss_Enh_Dec: -2.37643099\n",
      "| epoch 118 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  2.34 | ppl    10.36 | acc     0.73 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[118/200][4199/4361] Loss_D: 0.00497432 (Loss_D_real: 0.00034121 Loss_D_fake: 0.00463311) Loss_G: 0.39351621 Loss_Enh_Dec: -2.51665878\n",
      "| epoch 118 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  2.37 | ppl    10.71 | acc     0.73 | train_ae_norm     1.00\n",
      "[118/200][4299/4361] Loss_D: 0.00533830 (Loss_D_real: 0.00230689 Loss_D_fake: 0.00303140) Loss_G: 0.32739410 Loss_Enh_Dec: -2.68937182\n",
      "| epoch 118 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.39 | loss  2.34 | ppl    10.41 | acc     0.72 | train_ae_norm     1.00\n",
      "| end of epoch 118 | time: 1854.57s | test loss  2.36 | test ppl 10.61 | acc 0.767\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 119 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:03.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:18.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.696\n",
      "  Average training loss discriminator: 0.700\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.495\n",
      "  Test Loss: 4.474\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 119 |     0/ 4361 batches | lr 0.000000 | ms/batch 860.23 | loss  0.02 | ppl     1.02 | acc     0.76 | train_ae_norm     1.00\n",
      "[119/200][99/4361] Loss_D: 0.01744993 (Loss_D_real: 0.00216485 Loss_D_fake: 0.01528508) Loss_G: 0.34050873 Loss_Enh_Dec: -2.19282341\n",
      "| epoch 119 |   100/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.33 | ppl    10.29 | acc     0.67 | train_ae_norm     1.00\n",
      "[119/200][199/4361] Loss_D: 0.02611139 (Loss_D_real: 0.02353145 Loss_D_fake: 0.00257994) Loss_G: 0.31293294 Loss_Enh_Dec: -2.90110087\n",
      "| epoch 119 |   200/ 4361 batches | lr 0.000000 | ms/batch 400.71 | loss  2.36 | ppl    10.57 | acc     0.71 | train_ae_norm     1.00\n",
      "[119/200][299/4361] Loss_D: 0.00586317 (Loss_D_real: 0.00034173 Loss_D_fake: 0.00552144) Loss_G: 0.37082884 Loss_Enh_Dec: -2.89838719\n",
      "| epoch 119 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.90 | loss  2.37 | ppl    10.75 | acc     0.67 | train_ae_norm     1.00\n",
      "[119/200][399/4361] Loss_D: 0.00225198 (Loss_D_real: 0.00017542 Loss_D_fake: 0.00207656) Loss_G: 0.38740432 Loss_Enh_Dec: -2.22454762\n",
      "| epoch 119 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  2.27 | ppl     9.71 | acc     0.73 | train_ae_norm     1.00\n",
      "[119/200][499/4361] Loss_D: 0.08630876 (Loss_D_real: 0.01534245 Loss_D_fake: 0.07096631) Loss_G: 0.70381659 Loss_Enh_Dec: -2.05993533\n",
      "| epoch 119 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.89 | loss  2.34 | ppl    10.36 | acc     0.73 | train_ae_norm     1.00\n",
      "[119/200][599/4361] Loss_D: 0.00411853 (Loss_D_real: 0.00212850 Loss_D_fake: 0.00199003) Loss_G: 0.38585010 Loss_Enh_Dec: -2.17815638\n",
      "| epoch 119 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.30 | ppl     9.93 | acc     0.70 | train_ae_norm     1.00\n",
      "[119/200][699/4361] Loss_D: 0.10326663 (Loss_D_real: 0.03805028 Loss_D_fake: 0.06521636) Loss_G: 0.46369645 Loss_Enh_Dec: -2.51081920\n",
      "| epoch 119 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.51 | loss  2.34 | ppl    10.35 | acc     0.71 | train_ae_norm     1.00\n",
      "[119/200][799/4361] Loss_D: 0.00706749 (Loss_D_real: 0.00205302 Loss_D_fake: 0.00501447) Loss_G: 0.37159401 Loss_Enh_Dec: -2.22748160\n",
      "| epoch 119 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  2.31 | ppl    10.11 | acc     0.72 | train_ae_norm     1.00\n",
      "[119/200][899/4361] Loss_D: 0.00783610 (Loss_D_real: 0.00548731 Loss_D_fake: 0.00234878) Loss_G: 0.32858649 Loss_Enh_Dec: -2.53135276\n",
      "| epoch 119 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  2.32 | ppl    10.19 | acc     0.75 | train_ae_norm     1.00\n",
      "[119/200][999/4361] Loss_D: 0.00698134 (Loss_D_real: 0.00262128 Loss_D_fake: 0.00436006) Loss_G: 0.34188634 Loss_Enh_Dec: -2.04854274\n",
      "| epoch 119 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  2.33 | ppl    10.26 | acc     0.73 | train_ae_norm     1.00\n",
      "[119/200][1099/4361] Loss_D: 0.00498885 (Loss_D_real: 0.00236626 Loss_D_fake: 0.00262259) Loss_G: 0.42348757 Loss_Enh_Dec: -2.37107253\n",
      "| epoch 119 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  2.31 | ppl    10.11 | acc     0.68 | train_ae_norm     1.00\n",
      "[119/200][1199/4361] Loss_D: 0.00621906 (Loss_D_real: 0.00052770 Loss_D_fake: 0.00569137) Loss_G: 0.31818748 Loss_Enh_Dec: -2.24182010\n",
      "| epoch 119 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  2.33 | ppl    10.32 | acc     0.74 | train_ae_norm     1.00\n",
      "[119/200][1299/4361] Loss_D: 0.05766727 (Loss_D_real: 0.05605164 Loss_D_fake: 0.00161563) Loss_G: 0.40108377 Loss_Enh_Dec: -2.44377303\n",
      "| epoch 119 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  2.38 | ppl    10.77 | acc     0.74 | train_ae_norm     1.00\n",
      "[119/200][1399/4361] Loss_D: 0.00291078 (Loss_D_real: 0.00014007 Loss_D_fake: 0.00277072) Loss_G: 0.39697972 Loss_Enh_Dec: -2.23414683\n",
      "| epoch 119 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.67 | loss  2.39 | ppl    10.95 | acc     0.68 | train_ae_norm     1.00\n",
      "[119/200][1499/4361] Loss_D: 0.02787780 (Loss_D_real: 0.01927889 Loss_D_fake: 0.00859891) Loss_G: 0.39762086 Loss_Enh_Dec: -2.22724009\n",
      "| epoch 119 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.43 | ppl    11.36 | acc     0.69 | train_ae_norm     1.00\n",
      "[119/200][1599/4361] Loss_D: 0.00295044 (Loss_D_real: 0.00094390 Loss_D_fake: 0.00200654) Loss_G: 0.41493455 Loss_Enh_Dec: -2.19021726\n",
      "| epoch 119 |  1600/ 4361 batches | lr 0.000000 | ms/batch 402.27 | loss  2.43 | ppl    11.35 | acc     0.68 | train_ae_norm     1.00\n",
      "[119/200][1699/4361] Loss_D: 0.00689870 (Loss_D_real: 0.00513256 Loss_D_fake: 0.00176613) Loss_G: 0.36791039 Loss_Enh_Dec: -2.06524515\n",
      "| epoch 119 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  2.39 | ppl    10.86 | acc     0.69 | train_ae_norm     1.00\n",
      "[119/200][1799/4361] Loss_D: 0.00252792 (Loss_D_real: 0.00016686 Loss_D_fake: 0.00236106) Loss_G: 0.37888819 Loss_Enh_Dec: -1.86694169\n",
      "| epoch 119 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  2.36 | ppl    10.62 | acc     0.68 | train_ae_norm     1.00\n",
      "[119/200][1899/4361] Loss_D: 0.02539905 (Loss_D_real: 0.00180481 Loss_D_fake: 0.02359424) Loss_G: 0.42246532 Loss_Enh_Dec: -1.56144428\n",
      "| epoch 119 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  2.40 | ppl    11.06 | acc     0.71 | train_ae_norm     1.00\n",
      "[119/200][1999/4361] Loss_D: 0.01486802 (Loss_D_real: 0.00967143 Loss_D_fake: 0.00519659) Loss_G: 0.42860147 Loss_Enh_Dec: -2.05731845\n",
      "| epoch 119 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.35 | ppl    10.51 | acc     0.73 | train_ae_norm     1.00\n",
      "[119/200][2099/4361] Loss_D: 0.02013784 (Loss_D_real: 0.00502260 Loss_D_fake: 0.01511524) Loss_G: 0.36777970 Loss_Enh_Dec: -2.15166640\n",
      "| epoch 119 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  2.38 | ppl    10.81 | acc     0.75 | train_ae_norm     1.00\n",
      "[119/200][2199/4361] Loss_D: 0.00362334 (Loss_D_real: 0.00117938 Loss_D_fake: 0.00244396) Loss_G: 0.38533765 Loss_Enh_Dec: -2.18290615\n",
      "| epoch 119 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  2.35 | ppl    10.49 | acc     0.73 | train_ae_norm     1.00\n",
      "[119/200][2299/4361] Loss_D: 0.03617009 (Loss_D_real: 0.03391508 Loss_D_fake: 0.00225500) Loss_G: 0.37810868 Loss_Enh_Dec: -2.26649141\n",
      "| epoch 119 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  2.36 | ppl    10.55 | acc     0.74 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[119/200][2399/4361] Loss_D: 0.00486411 (Loss_D_real: 0.00203953 Loss_D_fake: 0.00282458) Loss_G: 0.35355550 Loss_Enh_Dec: -2.07973957\n",
      "| epoch 119 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  2.34 | ppl    10.34 | acc     0.69 | train_ae_norm     1.00\n",
      "[119/200][2499/4361] Loss_D: 0.02835406 (Loss_D_real: 0.02333953 Loss_D_fake: 0.00501453) Loss_G: 0.34620282 Loss_Enh_Dec: -2.45301986\n",
      "| epoch 119 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  2.38 | ppl    10.79 | acc     0.72 | train_ae_norm     1.00\n",
      "[119/200][2599/4361] Loss_D: 0.00831577 (Loss_D_real: 0.00424967 Loss_D_fake: 0.00406610) Loss_G: 0.30766019 Loss_Enh_Dec: -2.42614245\n",
      "| epoch 119 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.36 | ppl    10.61 | acc     0.68 | train_ae_norm     1.00\n",
      "[119/200][2699/4361] Loss_D: 0.00380989 (Loss_D_real: 0.00004689 Loss_D_fake: 0.00376300) Loss_G: 0.35138413 Loss_Enh_Dec: -1.88013732\n",
      "| epoch 119 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  2.36 | ppl    10.61 | acc     0.70 | train_ae_norm     1.00\n",
      "[119/200][2799/4361] Loss_D: 0.01414315 (Loss_D_real: 0.00715674 Loss_D_fake: 0.00698641) Loss_G: 0.35957801 Loss_Enh_Dec: -2.40368247\n",
      "| epoch 119 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.78 | loss  2.33 | ppl    10.31 | acc     0.67 | train_ae_norm     1.00\n",
      "[119/200][2899/4361] Loss_D: 0.00199078 (Loss_D_real: 0.00028699 Loss_D_fake: 0.00170380) Loss_G: 0.50601763 Loss_Enh_Dec: -1.90493548\n",
      "| epoch 119 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.60 | loss  2.36 | ppl    10.55 | acc     0.71 | train_ae_norm     1.00\n",
      "[119/200][2999/4361] Loss_D: 0.00398068 (Loss_D_real: 0.00139731 Loss_D_fake: 0.00258338) Loss_G: 0.38019210 Loss_Enh_Dec: -2.10471511\n",
      "| epoch 119 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  2.35 | ppl    10.50 | acc     0.70 | train_ae_norm     1.00\n",
      "[119/200][3099/4361] Loss_D: 0.00268769 (Loss_D_real: 0.00031354 Loss_D_fake: 0.00237415) Loss_G: 0.35575414 Loss_Enh_Dec: -2.13568783\n",
      "| epoch 119 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  2.37 | ppl    10.72 | acc     0.70 | train_ae_norm     1.00\n",
      "[119/200][3199/4361] Loss_D: 0.11892924 (Loss_D_real: 0.10934803 Loss_D_fake: 0.00958120) Loss_G: 0.41399416 Loss_Enh_Dec: -2.00475049\n",
      "| epoch 119 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.92 | loss  2.38 | ppl    10.81 | acc     0.73 | train_ae_norm     1.00\n",
      "[119/200][3299/4361] Loss_D: 0.41720828 (Loss_D_real: 0.00539924 Loss_D_fake: 0.41180906) Loss_G: 0.50647777 Loss_Enh_Dec: -2.31604576\n",
      "| epoch 119 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  2.40 | ppl    10.99 | acc     0.68 | train_ae_norm     1.00\n",
      "[119/200][3399/4361] Loss_D: 0.11912101 (Loss_D_real: 0.11591550 Loss_D_fake: 0.00320551) Loss_G: 0.40527686 Loss_Enh_Dec: -2.18703270\n",
      "| epoch 119 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  2.37 | ppl    10.73 | acc     0.71 | train_ae_norm     1.00\n",
      "[119/200][3499/4361] Loss_D: 0.00583913 (Loss_D_real: 0.00089470 Loss_D_fake: 0.00494443) Loss_G: 0.40038282 Loss_Enh_Dec: -2.07675362\n",
      "| epoch 119 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  2.31 | ppl    10.11 | acc     0.70 | train_ae_norm     1.00\n",
      "[119/200][3599/4361] Loss_D: 0.02622718 (Loss_D_real: 0.01929359 Loss_D_fake: 0.00693360) Loss_G: 0.38132569 Loss_Enh_Dec: -2.11676693\n",
      "| epoch 119 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.34 | ppl    10.34 | acc     0.74 | train_ae_norm     1.00\n",
      "[119/200][3699/4361] Loss_D: 0.00306367 (Loss_D_real: 0.00078201 Loss_D_fake: 0.00228167) Loss_G: 0.33983913 Loss_Enh_Dec: -1.93722141\n",
      "| epoch 119 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.35 | ppl    10.51 | acc     0.68 | train_ae_norm     1.00\n",
      "[119/200][3799/4361] Loss_D: 0.02761147 (Loss_D_real: 0.02193244 Loss_D_fake: 0.00567903) Loss_G: 0.37405893 Loss_Enh_Dec: -1.60381162\n",
      "| epoch 119 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  2.36 | ppl    10.64 | acc     0.77 | train_ae_norm     1.00\n",
      "[119/200][3899/4361] Loss_D: 0.01195182 (Loss_D_real: 0.00266179 Loss_D_fake: 0.00929003) Loss_G: 0.41490746 Loss_Enh_Dec: -2.12250757\n",
      "| epoch 119 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  2.36 | ppl    10.57 | acc     0.70 | train_ae_norm     1.00\n",
      "[119/200][3999/4361] Loss_D: 0.00507568 (Loss_D_real: 0.00245431 Loss_D_fake: 0.00262137) Loss_G: 0.38005409 Loss_Enh_Dec: -1.84985983\n",
      "| epoch 119 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.85 | loss  2.36 | ppl    10.56 | acc     0.72 | train_ae_norm     1.00\n",
      "[119/200][4099/4361] Loss_D: 0.01839539 (Loss_D_real: 0.01302468 Loss_D_fake: 0.00537070) Loss_G: 0.36243939 Loss_Enh_Dec: -1.86583352\n",
      "| epoch 119 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  2.33 | ppl    10.31 | acc     0.70 | train_ae_norm     1.00\n",
      "[119/200][4199/4361] Loss_D: 0.00547466 (Loss_D_real: 0.00296120 Loss_D_fake: 0.00251346) Loss_G: 0.37999696 Loss_Enh_Dec: -1.75584924\n",
      "| epoch 119 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  2.38 | ppl    10.82 | acc     0.72 | train_ae_norm     1.00\n",
      "[119/200][4299/4361] Loss_D: 0.02573545 (Loss_D_real: 0.01861310 Loss_D_fake: 0.00712235) Loss_G: 0.35293844 Loss_Enh_Dec: -1.95168674\n",
      "| epoch 119 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  2.35 | ppl    10.49 | acc     0.72 | train_ae_norm     1.00\n",
      "| end of epoch 119 | time: 1851.30s | test loss  2.42 | test ppl 11.28 | acc 0.760\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 120 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.698\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.492\n",
      "  Test Loss: 4.516\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 120 |     0/ 4361 batches | lr 0.000000 | ms/batch 866.65 | loss  0.02 | ppl     1.02 | acc     0.77 | train_ae_norm     1.00\n",
      "[120/200][99/4361] Loss_D: 0.00572107 (Loss_D_real: 0.00033774 Loss_D_fake: 0.00538333) Loss_G: 0.31859812 Loss_Enh_Dec: -1.89297569\n",
      "| epoch 120 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  2.37 | ppl    10.70 | acc     0.69 | train_ae_norm     1.00\n",
      "[120/200][199/4361] Loss_D: 0.14064252 (Loss_D_real: 0.13884498 Loss_D_fake: 0.00179754) Loss_G: 0.45236126 Loss_Enh_Dec: -2.15864921\n",
      "| epoch 120 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.99 | loss  2.40 | ppl    11.03 | acc     0.72 | train_ae_norm     1.00\n",
      "[120/200][299/4361] Loss_D: 0.00388385 (Loss_D_real: 0.00014714 Loss_D_fake: 0.00373671) Loss_G: 0.38835171 Loss_Enh_Dec: -2.05485415\n",
      "| epoch 120 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.43 | ppl    11.36 | acc     0.63 | train_ae_norm     1.00\n",
      "[120/200][399/4361] Loss_D: 0.02755080 (Loss_D_real: 0.00043737 Loss_D_fake: 0.02711344) Loss_G: 0.43547425 Loss_Enh_Dec: -2.12223577\n",
      "| epoch 120 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.35 | ppl    10.51 | acc     0.71 | train_ae_norm     1.00\n",
      "[120/200][499/4361] Loss_D: 0.00759557 (Loss_D_real: 0.00404425 Loss_D_fake: 0.00355132) Loss_G: 0.33583733 Loss_Enh_Dec: -2.79974437\n",
      "| epoch 120 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  2.42 | ppl    11.26 | acc     0.72 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120/200][599/4361] Loss_D: 0.00502030 (Loss_D_real: 0.00115614 Loss_D_fake: 0.00386416) Loss_G: 0.35184336 Loss_Enh_Dec: -2.70245361\n",
      "| epoch 120 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.88 | loss  2.38 | ppl    10.76 | acc     0.66 | train_ae_norm     1.00\n",
      "[120/200][699/4361] Loss_D: 0.00274830 (Loss_D_real: 0.00081846 Loss_D_fake: 0.00192985) Loss_G: 0.43404523 Loss_Enh_Dec: -2.39775538\n",
      "| epoch 120 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  2.41 | ppl    11.09 | acc     0.72 | train_ae_norm     1.00\n",
      "[120/200][799/4361] Loss_D: 0.00431623 (Loss_D_real: 0.00019128 Loss_D_fake: 0.00412495) Loss_G: 0.41859689 Loss_Enh_Dec: -2.27343559\n",
      "| epoch 120 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  2.38 | ppl    10.85 | acc     0.70 | train_ae_norm     1.00\n",
      "[120/200][899/4361] Loss_D: 0.03578070 (Loss_D_real: 0.03091903 Loss_D_fake: 0.00486167) Loss_G: 0.34584960 Loss_Enh_Dec: -2.54521799\n",
      "| epoch 120 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.40 | ppl    11.02 | acc     0.73 | train_ae_norm     1.00\n",
      "[120/200][999/4361] Loss_D: 0.00388874 (Loss_D_real: 0.00227034 Loss_D_fake: 0.00161840) Loss_G: 0.38032231 Loss_Enh_Dec: -2.14196277\n",
      "| epoch 120 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  2.38 | ppl    10.80 | acc     0.68 | train_ae_norm     1.00\n",
      "[120/200][1099/4361] Loss_D: 0.00192106 (Loss_D_real: 0.00008995 Loss_D_fake: 0.00183112) Loss_G: 0.37831888 Loss_Enh_Dec: -2.52222610\n",
      "| epoch 120 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  2.37 | ppl    10.66 | acc     0.65 | train_ae_norm     1.00\n",
      "[120/200][1199/4361] Loss_D: 0.00896300 (Loss_D_real: 0.00209565 Loss_D_fake: 0.00686735) Loss_G: 0.34166452 Loss_Enh_Dec: -2.35070777\n",
      "| epoch 120 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  2.37 | ppl    10.69 | acc     0.72 | train_ae_norm     1.00\n",
      "[120/200][1299/4361] Loss_D: 0.01670078 (Loss_D_real: 0.01514255 Loss_D_fake: 0.00155822) Loss_G: 0.42684743 Loss_Enh_Dec: -2.66485500\n",
      "| epoch 120 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  2.40 | ppl    11.07 | acc     0.72 | train_ae_norm     1.00\n",
      "[120/200][1399/4361] Loss_D: 0.00205323 (Loss_D_real: 0.00030642 Loss_D_fake: 0.00174681) Loss_G: 0.38573712 Loss_Enh_Dec: -2.64131427\n",
      "| epoch 120 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.38 | ppl    10.76 | acc     0.66 | train_ae_norm     1.00\n",
      "[120/200][1499/4361] Loss_D: 0.11097332 (Loss_D_real: 0.00192482 Loss_D_fake: 0.10904851) Loss_G: 0.49987373 Loss_Enh_Dec: -2.09953380\n",
      "| epoch 120 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  2.43 | ppl    11.38 | acc     0.68 | train_ae_norm     1.00\n",
      "[120/200][1599/4361] Loss_D: 0.01483280 (Loss_D_real: 0.01113572 Loss_D_fake: 0.00369708) Loss_G: 0.36724624 Loss_Enh_Dec: -2.86521816\n",
      "| epoch 120 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  2.43 | ppl    11.34 | acc     0.69 | train_ae_norm     1.00\n",
      "[120/200][1699/4361] Loss_D: 0.01046662 (Loss_D_real: 0.00828708 Loss_D_fake: 0.00217954) Loss_G: 0.44933447 Loss_Enh_Dec: -2.38222718\n",
      "| epoch 120 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.90 | loss  2.40 | ppl    11.05 | acc     0.67 | train_ae_norm     1.00\n",
      "[120/200][2099/4361] Loss_D: 0.02276484 (Loss_D_real: 0.01817685 Loss_D_fake: 0.00458800) Loss_G: 0.34842479 Loss_Enh_Dec: -2.89715815\n",
      "| epoch 120 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  2.44 | ppl    11.49 | acc     0.74 | train_ae_norm     1.00\n",
      "[120/200][2199/4361] Loss_D: 0.03449527 (Loss_D_real: 0.00024896 Loss_D_fake: 0.03424631) Loss_G: 0.37978962 Loss_Enh_Dec: -2.86981487\n",
      "| epoch 120 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  2.40 | ppl    11.01 | acc     0.73 | train_ae_norm     1.00\n",
      "[120/200][2299/4361] Loss_D: 0.00656228 (Loss_D_real: 0.00149971 Loss_D_fake: 0.00506257) Loss_G: 0.39319512 Loss_Enh_Dec: -2.64863515\n",
      "| epoch 120 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  2.39 | ppl    10.87 | acc     0.76 | train_ae_norm     1.00\n",
      "[120/200][2399/4361] Loss_D: 0.00213158 (Loss_D_real: 0.00023338 Loss_D_fake: 0.00189821) Loss_G: 0.39488769 Loss_Enh_Dec: -2.30962682\n",
      "| epoch 120 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.41 | ppl    11.10 | acc     0.66 | train_ae_norm     1.00\n",
      "[120/200][2499/4361] Loss_D: 0.00499885 (Loss_D_real: 0.00271437 Loss_D_fake: 0.00228447) Loss_G: 0.36106178 Loss_Enh_Dec: -2.75095344\n",
      "| epoch 120 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  2.42 | ppl    11.26 | acc     0.69 | train_ae_norm     1.00\n",
      "[120/200][2599/4361] Loss_D: 0.00559497 (Loss_D_real: 0.00230592 Loss_D_fake: 0.00328904) Loss_G: 0.37771463 Loss_Enh_Dec: -2.89640355\n",
      "| epoch 120 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  2.39 | ppl    10.93 | acc     0.70 | train_ae_norm     1.00\n",
      "[120/200][2699/4361] Loss_D: 0.02396873 (Loss_D_real: 0.00065624 Loss_D_fake: 0.02331249) Loss_G: 0.39762220 Loss_Enh_Dec: -2.44278550\n",
      "| epoch 120 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.38 | ppl    10.83 | acc     0.69 | train_ae_norm     1.00\n",
      "[120/200][2799/4361] Loss_D: 0.07075121 (Loss_D_real: 0.06477621 Loss_D_fake: 0.00597500) Loss_G: 0.35306832 Loss_Enh_Dec: -2.76710558\n",
      "| epoch 120 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  2.35 | ppl    10.48 | acc     0.67 | train_ae_norm     1.00\n",
      "[120/200][2899/4361] Loss_D: 0.01719731 (Loss_D_real: 0.00209945 Loss_D_fake: 0.01509786) Loss_G: 0.38214961 Loss_Enh_Dec: -2.57759762\n",
      "| epoch 120 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  2.37 | ppl    10.70 | acc     0.72 | train_ae_norm     1.00\n",
      "[120/200][2999/4361] Loss_D: 0.00224348 (Loss_D_real: 0.00070894 Loss_D_fake: 0.00153454) Loss_G: 0.35179883 Loss_Enh_Dec: -2.34010291\n",
      "| epoch 120 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  2.38 | ppl    10.82 | acc     0.71 | train_ae_norm     1.00\n",
      "[120/200][3099/4361] Loss_D: 0.21215110 (Loss_D_real: 0.00024046 Loss_D_fake: 0.21191064) Loss_G: 0.43019992 Loss_Enh_Dec: -2.18127894\n",
      "| epoch 120 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.65 | loss  2.39 | ppl    10.87 | acc     0.70 | train_ae_norm     1.00\n",
      "[120/200][3199/4361] Loss_D: 0.02456031 (Loss_D_real: 0.01977950 Loss_D_fake: 0.00478081) Loss_G: 0.38800305 Loss_Enh_Dec: -2.06084824\n",
      "| epoch 120 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  2.41 | ppl    11.13 | acc     0.73 | train_ae_norm     1.00\n",
      "[120/200][3299/4361] Loss_D: 0.00309251 (Loss_D_real: 0.00130247 Loss_D_fake: 0.00179004) Loss_G: 0.44199458 Loss_Enh_Dec: -1.84263599\n",
      "| epoch 120 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  2.41 | ppl    11.10 | acc     0.71 | train_ae_norm     1.00\n",
      "[120/200][3399/4361] Loss_D: 0.01484399 (Loss_D_real: 0.00025841 Loss_D_fake: 0.01458558) Loss_G: 0.34970492 Loss_Enh_Dec: -2.21664715\n",
      "| epoch 120 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  2.39 | ppl    10.87 | acc     0.69 | train_ae_norm     1.00\n",
      "[120/200][3499/4361] Loss_D: 0.00663169 (Loss_D_real: 0.00588341 Loss_D_fake: 0.00074828) Loss_G: 0.56988329 Loss_Enh_Dec: -1.80027771\n",
      "| epoch 120 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  2.33 | ppl    10.26 | acc     0.69 | train_ae_norm     1.00\n",
      "[120/200][3599/4361] Loss_D: 0.00292217 (Loss_D_real: 0.00037923 Loss_D_fake: 0.00254294) Loss_G: 0.34085739 Loss_Enh_Dec: -1.83633327\n",
      "| epoch 120 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  2.35 | ppl    10.44 | acc     0.71 | train_ae_norm     1.00\n",
      "[120/200][3699/4361] Loss_D: 0.00723055 (Loss_D_real: 0.00284526 Loss_D_fake: 0.00438529) Loss_G: 0.39573774 Loss_Enh_Dec: -1.90373611\n",
      "| epoch 120 |  3700/ 4361 batches | lr 0.000000 | ms/batch 402.09 | loss  2.37 | ppl    10.72 | acc     0.68 | train_ae_norm     1.00\n",
      "[120/200][3799/4361] Loss_D: 0.00126204 (Loss_D_real: 0.00059620 Loss_D_fake: 0.00066584) Loss_G: 0.40319473 Loss_Enh_Dec: -1.66855645\n",
      "| epoch 120 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  2.37 | ppl    10.72 | acc     0.77 | train_ae_norm     1.00\n",
      "[120/200][3899/4361] Loss_D: 0.00265547 (Loss_D_real: 0.00157165 Loss_D_fake: 0.00108382) Loss_G: 0.40882036 Loss_Enh_Dec: -1.56538713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 120 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  2.37 | ppl    10.70 | acc     0.68 | train_ae_norm     1.00\n",
      "[120/200][3999/4361] Loss_D: 0.01426957 (Loss_D_real: 0.00136090 Loss_D_fake: 0.01290867) Loss_G: 0.39129016 Loss_Enh_Dec: -1.76249468\n",
      "| epoch 120 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  2.36 | ppl    10.57 | acc     0.73 | train_ae_norm     1.00\n",
      "[120/200][4099/4361] Loss_D: 0.01350223 (Loss_D_real: 0.00967436 Loss_D_fake: 0.00382787) Loss_G: 0.41171393 Loss_Enh_Dec: -1.93458235\n",
      "| epoch 120 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  2.30 | ppl     9.97 | acc     0.73 | train_ae_norm     1.00\n",
      "[120/200][4199/4361] Loss_D: 0.00220867 (Loss_D_real: 0.00014626 Loss_D_fake: 0.00206240) Loss_G: 0.35905427 Loss_Enh_Dec: -1.58429801\n",
      "| epoch 120 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  2.36 | ppl    10.62 | acc     0.72 | train_ae_norm     1.00\n",
      "[120/200][4299/4361] Loss_D: 0.01456578 (Loss_D_real: 0.00082314 Loss_D_fake: 0.01374264) Loss_G: 0.37857950 Loss_Enh_Dec: -1.34947741\n",
      "| epoch 120 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  2.33 | ppl    10.29 | acc     0.71 | train_ae_norm     1.00\n",
      "| end of epoch 120 | time: 1852.84s | test loss  2.43 | test ppl 11.40 | acc 0.760\n",
      "bleu_self:  [1.30814887e-01 2.55385561e-09 5.36156950e-11 2.72291261e-10\n",
      " 8.38222920e-10]\n",
      "bleu_test:  [8.62499999e-01 1.07057751e-01 1.33126356e-05 4.03684858e-06\n",
      " 2.09864861e-06]\n",
      "bleu_self: [0.13081489,0.00000000,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.86250000,0.10705775,0.00001331,0.00000404,0.00000210]\n",
      "New saving model: epoch 120.\n",
      "Saving models to ./results/yahoo_merge_assigned_results\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 121 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.696\n",
      "  Average training loss discriminator: 0.700\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.487\n",
      "  Test Loss: 4.520\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 121 |     0/ 4361 batches | lr 0.000000 | ms/batch 861.67 | loss  0.02 | ppl     1.02 | acc     0.75 | train_ae_norm     1.00\n",
      "[121/200][99/4361] Loss_D: 0.00629527 (Loss_D_real: 0.00034089 Loss_D_fake: 0.00595437) Loss_G: 0.38381132 Loss_Enh_Dec: -2.20959306\n",
      "| epoch 121 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  2.36 | ppl    10.54 | acc     0.68 | train_ae_norm     1.00\n",
      "[121/200][199/4361] Loss_D: 0.00308960 (Loss_D_real: 0.00055986 Loss_D_fake: 0.00252974) Loss_G: 0.36047173 Loss_Enh_Dec: -1.85119915\n",
      "| epoch 121 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  2.41 | ppl    11.10 | acc     0.70 | train_ae_norm     1.00\n",
      "[121/200][299/4361] Loss_D: 0.00499426 (Loss_D_real: 0.00021574 Loss_D_fake: 0.00477853) Loss_G: 0.34853959 Loss_Enh_Dec: -2.29438949\n",
      "| epoch 121 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  2.48 | ppl    11.96 | acc     0.64 | train_ae_norm     1.00\n",
      "[121/200][399/4361] Loss_D: 0.00200239 (Loss_D_real: 0.00025985 Loss_D_fake: 0.00174254) Loss_G: 0.41074690 Loss_Enh_Dec: -2.09455657\n",
      "| epoch 121 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  2.33 | ppl    10.32 | acc     0.72 | train_ae_norm     1.00\n",
      "[121/200][499/4361] Loss_D: 0.00847101 (Loss_D_real: 0.00528746 Loss_D_fake: 0.00318355) Loss_G: 0.38492793 Loss_Enh_Dec: -1.73819089\n",
      "| epoch 121 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  2.40 | ppl    10.99 | acc     0.72 | train_ae_norm     1.00\n",
      "[121/200][599/4361] Loss_D: 0.00191137 (Loss_D_real: 0.00005984 Loss_D_fake: 0.00185152) Loss_G: 0.33831072 Loss_Enh_Dec: -2.10289812\n",
      "| epoch 121 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.30 | ppl     9.97 | acc     0.67 | train_ae_norm     1.00\n",
      "[121/200][699/4361] Loss_D: 0.00320246 (Loss_D_real: 0.00031208 Loss_D_fake: 0.00289038) Loss_G: 0.36192244 Loss_Enh_Dec: -1.19938350\n",
      "| epoch 121 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  2.33 | ppl    10.33 | acc     0.72 | train_ae_norm     1.00\n",
      "[121/200][799/4361] Loss_D: 0.01169037 (Loss_D_real: 0.00143478 Loss_D_fake: 0.01025559) Loss_G: 0.37294587 Loss_Enh_Dec: -1.61363602\n",
      "| epoch 121 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  2.31 | ppl    10.12 | acc     0.68 | train_ae_norm     1.00\n",
      "[121/200][899/4361] Loss_D: 0.00846794 (Loss_D_real: 0.00159479 Loss_D_fake: 0.00687315) Loss_G: 0.35177028 Loss_Enh_Dec: -1.98164833\n",
      "| epoch 121 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.32 | ppl    10.20 | acc     0.72 | train_ae_norm     1.00\n",
      "[121/200][999/4361] Loss_D: 0.00289633 (Loss_D_real: 0.00008137 Loss_D_fake: 0.00281496) Loss_G: 0.37522584 Loss_Enh_Dec: -1.66079032\n",
      "| epoch 121 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  2.31 | ppl    10.11 | acc     0.74 | train_ae_norm     1.00\n",
      "[121/200][1099/4361] Loss_D: 0.00717942 (Loss_D_real: 0.00043226 Loss_D_fake: 0.00674716) Loss_G: 0.33438012 Loss_Enh_Dec: -1.76907313\n",
      "| epoch 121 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  2.29 | ppl     9.84 | acc     0.72 | train_ae_norm     1.00\n",
      "[121/200][1199/4361] Loss_D: 0.00312752 (Loss_D_real: 0.00007285 Loss_D_fake: 0.00305467) Loss_G: 0.35460538 Loss_Enh_Dec: -2.20880127\n",
      "| epoch 121 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  2.30 | ppl     9.96 | acc     0.78 | train_ae_norm     1.00\n",
      "[121/200][1299/4361] Loss_D: 0.00358857 (Loss_D_real: 0.00108879 Loss_D_fake: 0.00249978) Loss_G: 0.40812364 Loss_Enh_Dec: -1.29495728\n",
      "| epoch 121 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.85 | loss  2.32 | ppl    10.18 | acc     0.70 | train_ae_norm     1.00\n",
      "[121/200][1399/4361] Loss_D: 0.01895684 (Loss_D_real: 0.01291474 Loss_D_fake: 0.00604210) Loss_G: 0.38093835 Loss_Enh_Dec: -1.79515266\n",
      "| epoch 121 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.56 | loss  2.31 | ppl    10.10 | acc     0.70 | train_ae_norm     1.00\n",
      "[121/200][1499/4361] Loss_D: 0.06848501 (Loss_D_real: 0.06718412 Loss_D_fake: 0.00130088) Loss_G: 0.43859044 Loss_Enh_Dec: -1.73398650\n",
      "| epoch 121 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.36 | ppl    10.57 | acc     0.70 | train_ae_norm     1.00\n",
      "[121/200][1599/4361] Loss_D: 0.00165212 (Loss_D_real: 0.00019029 Loss_D_fake: 0.00146183) Loss_G: 0.40142664 Loss_Enh_Dec: -1.61183202\n",
      "| epoch 121 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  2.35 | ppl    10.47 | acc     0.72 | train_ae_norm     1.00\n",
      "[121/200][1699/4361] Loss_D: 0.00456564 (Loss_D_real: 0.00437246 Loss_D_fake: 0.00019318) Loss_G: 0.52790779 Loss_Enh_Dec: -1.80055463\n",
      "| epoch 121 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.32 | ppl    10.16 | acc     0.68 | train_ae_norm     1.00\n",
      "[121/200][1799/4361] Loss_D: 0.02952841 (Loss_D_real: 0.02627081 Loss_D_fake: 0.00325760) Loss_G: 0.37843034 Loss_Enh_Dec: -1.72521746\n",
      "| epoch 121 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  2.34 | ppl    10.42 | acc     0.72 | train_ae_norm     1.00\n",
      "[121/200][1899/4361] Loss_D: 0.00420952 (Loss_D_real: 0.00329717 Loss_D_fake: 0.00091235) Loss_G: 0.46976265 Loss_Enh_Dec: -1.87905920\n",
      "| epoch 121 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  2.36 | ppl    10.56 | acc     0.73 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121/200][1999/4361] Loss_D: 0.00660890 (Loss_D_real: 0.00547323 Loss_D_fake: 0.00113567) Loss_G: 0.37217721 Loss_Enh_Dec: -2.26199126\n",
      "| epoch 121 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  2.30 | ppl     9.96 | acc     0.76 | train_ae_norm     1.00\n",
      "[121/200][2099/4361] Loss_D: 0.00045021 (Loss_D_real: 0.00013989 Loss_D_fake: 0.00031031) Loss_G: 0.61161590 Loss_Enh_Dec: -1.72129822\n",
      "| epoch 121 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  2.30 | ppl     9.93 | acc     0.78 | train_ae_norm     1.00\n",
      "[121/200][2199/4361] Loss_D: 0.00183161 (Loss_D_real: 0.00026679 Loss_D_fake: 0.00156482) Loss_G: 0.44964454 Loss_Enh_Dec: -1.58667684\n",
      "| epoch 121 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.27 | ppl     9.68 | acc     0.71 | train_ae_norm     1.00\n",
      "[121/200][2299/4361] Loss_D: 0.00117718 (Loss_D_real: 0.00037064 Loss_D_fake: 0.00080654) Loss_G: 0.40727788 Loss_Enh_Dec: -1.76345766\n",
      "| epoch 121 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.28 | ppl     9.75 | acc     0.75 | train_ae_norm     1.00\n",
      "[121/200][2399/4361] Loss_D: 0.00194228 (Loss_D_real: 0.00059397 Loss_D_fake: 0.00134831) Loss_G: 0.41469952 Loss_Enh_Dec: -1.81935012\n",
      "| epoch 121 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  2.27 | ppl     9.66 | acc     0.68 | train_ae_norm     1.00\n",
      "[121/200][2499/4361] Loss_D: 0.00352540 (Loss_D_real: 0.00036274 Loss_D_fake: 0.00316266) Loss_G: 0.41532570 Loss_Enh_Dec: -1.93077147\n",
      "| epoch 121 |  2500/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  2.30 | ppl     9.94 | acc     0.74 | train_ae_norm     1.00\n",
      "[121/200][2599/4361] Loss_D: 0.00288900 (Loss_D_real: 0.00049567 Loss_D_fake: 0.00239333) Loss_G: 0.54473257 Loss_Enh_Dec: -1.84081209\n",
      "| epoch 121 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  2.29 | ppl     9.90 | acc     0.69 | train_ae_norm     1.00\n",
      "[121/200][2699/4361] Loss_D: 0.00254639 (Loss_D_real: 0.00039080 Loss_D_fake: 0.00215559) Loss_G: 0.43300030 Loss_Enh_Dec: -2.14319491\n",
      "| epoch 121 |  2700/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  2.31 | ppl    10.10 | acc     0.69 | train_ae_norm     1.00\n",
      "[121/200][2799/4361] Loss_D: 0.01201341 (Loss_D_real: 0.00998409 Loss_D_fake: 0.00202931) Loss_G: 0.36180636 Loss_Enh_Dec: -2.43722534\n",
      "| epoch 121 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.29 | ppl     9.85 | acc     0.66 | train_ae_norm     1.00\n",
      "[121/200][2899/4361] Loss_D: 0.00364283 (Loss_D_real: 0.00285984 Loss_D_fake: 0.00078299) Loss_G: 0.50601387 Loss_Enh_Dec: -1.86295688\n",
      "| epoch 121 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.41 | loss  2.28 | ppl     9.74 | acc     0.72 | train_ae_norm     1.00\n",
      "[121/200][2999/4361] Loss_D: 0.01328309 (Loss_D_real: 0.00149750 Loss_D_fake: 0.01178559) Loss_G: 0.46177450 Loss_Enh_Dec: -1.64259422\n",
      "| epoch 121 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  2.27 | ppl     9.68 | acc     0.71 | train_ae_norm     1.00\n",
      "[121/200][3099/4361] Loss_D: 0.00957781 (Loss_D_real: 0.00809338 Loss_D_fake: 0.00148443) Loss_G: 0.51224244 Loss_Enh_Dec: -2.02759242\n",
      "| epoch 121 |  3100/ 4361 batches | lr 0.000000 | ms/batch 402.36 | loss  2.29 | ppl     9.87 | acc     0.72 | train_ae_norm     1.00\n",
      "[121/200][3199/4361] Loss_D: 0.00514995 (Loss_D_real: 0.00136080 Loss_D_fake: 0.00378915) Loss_G: 0.41646051 Loss_Enh_Dec: -2.30561137\n",
      "| epoch 121 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  2.29 | ppl     9.89 | acc     0.75 | train_ae_norm     1.00\n",
      "[121/200][3299/4361] Loss_D: 0.00480548 (Loss_D_real: 0.00159875 Loss_D_fake: 0.00320672) Loss_G: 0.46290046 Loss_Enh_Dec: -1.96410525\n",
      "| epoch 121 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  2.31 | ppl    10.05 | acc     0.72 | train_ae_norm     1.00\n",
      "[121/200][3399/4361] Loss_D: 0.00143481 (Loss_D_real: 0.00089478 Loss_D_fake: 0.00054003) Loss_G: 0.52536386 Loss_Enh_Dec: -2.04322290\n",
      "| epoch 121 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.08 | loss  2.27 | ppl     9.72 | acc     0.74 | train_ae_norm     1.00\n",
      "[121/200][3499/4361] Loss_D: 0.01001043 (Loss_D_real: 0.00852521 Loss_D_fake: 0.00148523) Loss_G: 0.41732582 Loss_Enh_Dec: -2.30806994\n",
      "| epoch 121 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  2.21 | ppl     9.08 | acc     0.73 | train_ae_norm     1.00\n",
      "[121/200][3599/4361] Loss_D: 0.03038559 (Loss_D_real: 0.02525005 Loss_D_fake: 0.00513554) Loss_G: 0.45100927 Loss_Enh_Dec: -1.78684616\n",
      "| epoch 121 |  3600/ 4361 batches | lr 0.000000 | ms/batch 400.83 | loss  2.25 | ppl     9.48 | acc     0.73 | train_ae_norm     1.00\n",
      "[121/200][3699/4361] Loss_D: 0.00511904 (Loss_D_real: 0.00101422 Loss_D_fake: 0.00410481) Loss_G: 0.42361161 Loss_Enh_Dec: -1.97317684\n",
      "| epoch 121 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  2.25 | ppl     9.50 | acc     0.67 | train_ae_norm     1.00\n",
      "[121/200][3799/4361] Loss_D: 0.00816516 (Loss_D_real: 0.00260129 Loss_D_fake: 0.00556387) Loss_G: 0.35354611 Loss_Enh_Dec: -2.50530767\n",
      "| epoch 121 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  2.27 | ppl     9.69 | acc     0.76 | train_ae_norm     1.00\n",
      "[121/200][3899/4361] Loss_D: 0.00574629 (Loss_D_real: 0.00412415 Loss_D_fake: 0.00162215) Loss_G: 0.38193932 Loss_Enh_Dec: -2.20439672\n",
      "| epoch 121 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.29 | ppl     9.89 | acc     0.71 | train_ae_norm     1.00\n",
      "[121/200][3999/4361] Loss_D: 0.00424184 (Loss_D_real: 0.00147924 Loss_D_fake: 0.00276260) Loss_G: 0.38793972 Loss_Enh_Dec: -1.82173049\n",
      "| epoch 121 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.28 | ppl     9.81 | acc     0.74 | train_ae_norm     1.00\n",
      "[121/200][4099/4361] Loss_D: 0.00383970 (Loss_D_real: 0.00189149 Loss_D_fake: 0.00194821) Loss_G: 0.48707104 Loss_Enh_Dec: -1.92380106\n",
      "| epoch 121 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.92 | loss  2.22 | ppl     9.24 | acc     0.71 | train_ae_norm     1.00\n",
      "[121/200][4199/4361] Loss_D: 0.02934802 (Loss_D_real: 0.00243417 Loss_D_fake: 0.02691386) Loss_G: 0.61749476 Loss_Enh_Dec: -1.73758531\n",
      "| epoch 121 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.05 | loss  2.29 | ppl     9.89 | acc     0.70 | train_ae_norm     1.00\n",
      "[121/200][4299/4361] Loss_D: 0.00124385 (Loss_D_real: 0.00024890 Loss_D_fake: 0.00099495) Loss_G: 0.40967304 Loss_Enh_Dec: -1.86388433\n",
      "| epoch 121 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.39 | ppl    10.90 | acc     0.65 | train_ae_norm     1.00\n",
      "| end of epoch 121 | time: 1851.96s | test loss  3.73 | test ppl 41.51 | acc 0.558\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 122 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.696\n",
      "  Average training loss discriminator: 0.701\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.472\n",
      "  Test Loss: 4.625\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 122 |     0/ 4361 batches | lr 0.000000 | ms/batch 864.72 | loss  0.03 | ppl     1.03 | acc     0.61 | train_ae_norm     1.00\n",
      "[122/200][99/4361] Loss_D: 0.02677694 (Loss_D_real: 0.02643274 Loss_D_fake: 0.00034421) Loss_G: 0.45590112 Loss_Enh_Dec: -2.12452745\n",
      "| epoch 122 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  2.60 | ppl    13.43 | acc     0.69 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[122/200][199/4361] Loss_D: 0.00519025 (Loss_D_real: 0.00112408 Loss_D_fake: 0.00406617) Loss_G: 0.37780261 Loss_Enh_Dec: -2.41061091\n",
      "| epoch 122 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  2.34 | ppl    10.43 | acc     0.74 | train_ae_norm     1.00\n",
      "[122/200][299/4361] Loss_D: 0.00334872 (Loss_D_real: 0.00146823 Loss_D_fake: 0.00188049) Loss_G: 0.38439760 Loss_Enh_Dec: -2.45025992\n",
      "| epoch 122 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  2.29 | ppl     9.83 | acc     0.67 | train_ae_norm     1.00\n",
      "[122/200][399/4361] Loss_D: 0.00130878 (Loss_D_real: 0.00023031 Loss_D_fake: 0.00107848) Loss_G: 0.45791212 Loss_Enh_Dec: -2.23855019\n",
      "| epoch 122 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  2.21 | ppl     9.08 | acc     0.74 | train_ae_norm     1.00\n",
      "[122/200][499/4361] Loss_D: 0.00490393 (Loss_D_real: 0.00229995 Loss_D_fake: 0.00260397) Loss_G: 0.38808450 Loss_Enh_Dec: -2.44466591\n",
      "| epoch 122 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  2.28 | ppl     9.82 | acc     0.73 | train_ae_norm     1.00\n",
      "[122/200][599/4361] Loss_D: 0.00327803 (Loss_D_real: 0.00017409 Loss_D_fake: 0.00310394) Loss_G: 0.36957890 Loss_Enh_Dec: -2.21714473\n",
      "| epoch 122 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.79 | loss  2.24 | ppl     9.39 | acc     0.68 | train_ae_norm     1.00\n",
      "[122/200][699/4361] Loss_D: 0.00157584 (Loss_D_real: 0.00069741 Loss_D_fake: 0.00087843) Loss_G: 0.41872177 Loss_Enh_Dec: -2.39939189\n",
      "| epoch 122 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  2.30 | ppl     9.95 | acc     0.70 | train_ae_norm     1.00\n",
      "[122/200][799/4361] Loss_D: 0.00403823 (Loss_D_real: 0.00047984 Loss_D_fake: 0.00355839) Loss_G: 0.35530964 Loss_Enh_Dec: -2.40851092\n",
      "| epoch 122 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.27 | ppl     9.68 | acc     0.73 | train_ae_norm     1.00\n",
      "[122/200][899/4361] Loss_D: 0.00312953 (Loss_D_real: 0.00190269 Loss_D_fake: 0.00122684) Loss_G: 0.32720593 Loss_Enh_Dec: -2.49523401\n",
      "| epoch 122 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.28 | ppl     9.82 | acc     0.74 | train_ae_norm     1.00\n",
      "[122/200][999/4361] Loss_D: 0.00171994 (Loss_D_real: 0.00012997 Loss_D_fake: 0.00158997) Loss_G: 0.38379484 Loss_Enh_Dec: -2.05291224\n",
      "| epoch 122 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.27 | ppl     9.66 | acc     0.73 | train_ae_norm     1.00\n",
      "[122/200][1099/4361] Loss_D: 0.00128444 (Loss_D_real: 0.00020660 Loss_D_fake: 0.00107784) Loss_G: 0.39351007 Loss_Enh_Dec: -2.28527641\n",
      "| epoch 122 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.26 | ppl     9.60 | acc     0.73 | train_ae_norm     1.00\n",
      "[122/200][1199/4361] Loss_D: 0.00241025 (Loss_D_real: 0.00014547 Loss_D_fake: 0.00226479) Loss_G: 0.35120773 Loss_Enh_Dec: -2.41609263\n",
      "| epoch 122 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  2.26 | ppl     9.61 | acc     0.76 | train_ae_norm     1.00\n",
      "[122/200][1299/4361] Loss_D: 0.00117205 (Loss_D_real: 0.00033336 Loss_D_fake: 0.00083869) Loss_G: 0.42213890 Loss_Enh_Dec: -2.33364844\n",
      "| epoch 122 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  2.32 | ppl    10.14 | acc     0.72 | train_ae_norm     1.00\n",
      "[122/200][1399/4361] Loss_D: 0.00127498 (Loss_D_real: 0.00020689 Loss_D_fake: 0.00106809) Loss_G: 0.37644863 Loss_Enh_Dec: -2.07490468\n",
      "| epoch 122 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  2.29 | ppl     9.85 | acc     0.65 | train_ae_norm     1.00\n",
      "[122/200][1499/4361] Loss_D: 0.00174066 (Loss_D_real: 0.00047446 Loss_D_fake: 0.00126620) Loss_G: 0.44112086 Loss_Enh_Dec: -1.57842016\n",
      "| epoch 122 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  2.34 | ppl    10.40 | acc     0.72 | train_ae_norm     1.00\n",
      "[122/200][1799/4361] Loss_D: 0.04736230 (Loss_D_real: 0.04666929 Loss_D_fake: 0.00069302) Loss_G: 0.61569160 Loss_Enh_Dec: -1.81984866\n",
      "| epoch 122 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  2.25 | ppl     9.53 | acc     0.72 | train_ae_norm     1.00\n",
      "[122/200][1899/4361] Loss_D: 0.02490850 (Loss_D_real: 0.01831157 Loss_D_fake: 0.00659693) Loss_G: 0.51870900 Loss_Enh_Dec: -1.89540863\n",
      "| epoch 122 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.31 | ppl    10.05 | acc     0.73 | train_ae_norm     1.00\n",
      "[122/200][1999/4361] Loss_D: 0.00212194 (Loss_D_real: 0.00028250 Loss_D_fake: 0.00183944) Loss_G: 0.41303894 Loss_Enh_Dec: -2.14489555\n",
      "| epoch 122 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.83 | loss  2.24 | ppl     9.39 | acc     0.77 | train_ae_norm     1.00\n",
      "[122/200][2099/4361] Loss_D: 0.00310008 (Loss_D_real: 0.00158266 Loss_D_fake: 0.00151741) Loss_G: 0.40149054 Loss_Enh_Dec: -2.22325492\n",
      "| epoch 122 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  2.27 | ppl     9.65 | acc     0.76 | train_ae_norm     1.00\n",
      "[122/200][2199/4361] Loss_D: 0.00658981 (Loss_D_real: 0.00102824 Loss_D_fake: 0.00556157) Loss_G: 0.40899482 Loss_Enh_Dec: -2.67531085\n",
      "| epoch 122 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.91 | loss  2.23 | ppl     9.28 | acc     0.73 | train_ae_norm     1.00\n",
      "[122/200][2299/4361] Loss_D: 0.11911877 (Loss_D_real: 0.11728602 Loss_D_fake: 0.00183275) Loss_G: 0.38000426 Loss_Enh_Dec: -2.24123573\n",
      "| epoch 122 |  2300/ 4361 batches | lr 0.000000 | ms/batch 402.00 | loss  2.25 | ppl     9.48 | acc     0.74 | train_ae_norm     1.00\n",
      "[122/200][2399/4361] Loss_D: 0.00193329 (Loss_D_real: 0.00119533 Loss_D_fake: 0.00073796) Loss_G: 0.43527499 Loss_Enh_Dec: -2.65304327\n",
      "| epoch 122 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  2.23 | ppl     9.27 | acc     0.68 | train_ae_norm     1.00\n",
      "[122/200][2499/4361] Loss_D: 0.00129888 (Loss_D_real: 0.00003881 Loss_D_fake: 0.00126006) Loss_G: 0.42138892 Loss_Enh_Dec: -2.53914714\n",
      "| epoch 122 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  2.30 | ppl    10.01 | acc     0.72 | train_ae_norm     1.00\n",
      "[122/200][2599/4361] Loss_D: 0.00316450 (Loss_D_real: 0.00254018 Loss_D_fake: 0.00062431) Loss_G: 0.50433230 Loss_Enh_Dec: -2.65892076\n",
      "| epoch 122 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  2.26 | ppl     9.63 | acc     0.68 | train_ae_norm     1.00\n",
      "[122/200][2699/4361] Loss_D: 0.06410781 (Loss_D_real: 0.06078768 Loss_D_fake: 0.00332013) Loss_G: 0.39321432 Loss_Enh_Dec: -2.83682036\n",
      "| epoch 122 |  2700/ 4361 batches | lr 0.000000 | ms/batch 400.54 | loss  2.25 | ppl     9.52 | acc     0.70 | train_ae_norm     1.00\n",
      "[122/200][2799/4361] Loss_D: 0.01519281 (Loss_D_real: 0.01077017 Loss_D_fake: 0.00442263) Loss_G: 0.42643785 Loss_Enh_Dec: -2.66563797\n",
      "| epoch 122 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  2.22 | ppl     9.22 | acc     0.69 | train_ae_norm     1.00\n",
      "[122/200][2899/4361] Loss_D: 0.00078535 (Loss_D_real: 0.00012659 Loss_D_fake: 0.00065876) Loss_G: 0.49727488 Loss_Enh_Dec: -2.53703070\n",
      "| epoch 122 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  2.26 | ppl     9.57 | acc     0.74 | train_ae_norm     1.00\n",
      "[122/200][2999/4361] Loss_D: 0.05233908 (Loss_D_real: 0.00282075 Loss_D_fake: 0.04951834) Loss_G: 0.41247582 Loss_Enh_Dec: -2.68737555\n",
      "| epoch 122 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  2.25 | ppl     9.45 | acc     0.74 | train_ae_norm     1.00\n",
      "[122/200][3099/4361] Loss_D: 0.00278912 (Loss_D_real: 0.00016225 Loss_D_fake: 0.00262687) Loss_G: 0.38604245 Loss_Enh_Dec: -2.68456078\n",
      "| epoch 122 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  2.26 | ppl     9.60 | acc     0.71 | train_ae_norm     1.00\n",
      "[122/200][3199/4361] Loss_D: 0.00154050 (Loss_D_real: 0.00032276 Loss_D_fake: 0.00121774) Loss_G: 0.47754952 Loss_Enh_Dec: -2.58360076\n",
      "| epoch 122 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  2.27 | ppl     9.66 | acc     0.75 | train_ae_norm     1.00\n",
      "[122/200][3299/4361] Loss_D: 0.02156934 (Loss_D_real: 0.01822456 Loss_D_fake: 0.00334478) Loss_G: 0.41367483 Loss_Enh_Dec: -2.42436194\n",
      "| epoch 122 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  2.27 | ppl     9.70 | acc     0.72 | train_ae_norm     1.00\n",
      "[122/200][3399/4361] Loss_D: 0.00257135 (Loss_D_real: 0.00147960 Loss_D_fake: 0.00109174) Loss_G: 0.39650160 Loss_Enh_Dec: -2.58634758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 122 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.35 | loss  2.25 | ppl     9.49 | acc     0.71 | train_ae_norm     1.00\n",
      "[122/200][3499/4361] Loss_D: 0.00237216 (Loss_D_real: 0.00109235 Loss_D_fake: 0.00127981) Loss_G: 0.58327389 Loss_Enh_Dec: -2.17915511\n",
      "| epoch 122 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.64 | loss  2.17 | ppl     8.80 | acc     0.72 | train_ae_norm     1.00\n",
      "[122/200][3599/4361] Loss_D: 0.00631679 (Loss_D_real: 0.00122023 Loss_D_fake: 0.00509656) Loss_G: 0.40321040 Loss_Enh_Dec: -2.20701957\n",
      "| epoch 122 |  3600/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  2.22 | ppl     9.25 | acc     0.75 | train_ae_norm     1.00\n",
      "[122/200][3699/4361] Loss_D: 0.00336712 (Loss_D_real: 0.00190576 Loss_D_fake: 0.00146137) Loss_G: 0.36614624 Loss_Enh_Dec: -2.29902411\n",
      "| epoch 122 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  2.23 | ppl     9.26 | acc     0.71 | train_ae_norm     1.00\n",
      "[122/200][3799/4361] Loss_D: 0.00325600 (Loss_D_real: 0.00162771 Loss_D_fake: 0.00162829) Loss_G: 0.34740731 Loss_Enh_Dec: -2.42600894\n",
      "| epoch 122 |  3800/ 4361 batches | lr 0.000000 | ms/batch 400.63 | loss  2.26 | ppl     9.56 | acc     0.79 | train_ae_norm     1.00\n",
      "[122/200][3899/4361] Loss_D: 0.00752106 (Loss_D_real: 0.00137436 Loss_D_fake: 0.00614670) Loss_G: 0.42024365 Loss_Enh_Dec: -2.45156765\n",
      "| epoch 122 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  2.25 | ppl     9.49 | acc     0.72 | train_ae_norm     1.00\n",
      "[122/200][3999/4361] Loss_D: 0.00681635 (Loss_D_real: 0.00047571 Loss_D_fake: 0.00634065) Loss_G: 0.43867168 Loss_Enh_Dec: -2.45750117\n",
      "| epoch 122 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  2.24 | ppl     9.39 | acc     0.73 | train_ae_norm     1.00\n",
      "[122/200][4099/4361] Loss_D: 0.01933137 (Loss_D_real: 0.01005050 Loss_D_fake: 0.00928087) Loss_G: 0.46616039 Loss_Enh_Dec: -2.40863800\n",
      "| epoch 122 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  2.19 | ppl     8.93 | acc     0.74 | train_ae_norm     1.00\n",
      "[122/200][4199/4361] Loss_D: 0.00565169 (Loss_D_real: 0.00469077 Loss_D_fake: 0.00096091) Loss_G: 0.43743640 Loss_Enh_Dec: -2.68179917\n",
      "| epoch 122 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  2.26 | ppl     9.58 | acc     0.76 | train_ae_norm     1.00\n",
      "[122/200][4299/4361] Loss_D: 0.00410065 (Loss_D_real: 0.00053952 Loss_D_fake: 0.00356113) Loss_G: 0.32178208 Loss_Enh_Dec: -2.63425517\n",
      "| epoch 122 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.63 | loss  2.25 | ppl     9.45 | acc     0.71 | train_ae_norm     1.00\n",
      "| end of epoch 122 | time: 1851.89s | test loss  2.34 | test ppl 10.41 | acc 0.769\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 123 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.696\n",
      "  Average training loss discriminator: 0.702\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.482\n",
      "  Test Loss: 4.576\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 123 |     0/ 4361 batches | lr 0.000000 | ms/batch 862.56 | loss  0.02 | ppl     1.02 | acc     0.76 | train_ae_norm     1.00\n",
      "[123/200][99/4361] Loss_D: 0.00315245 (Loss_D_real: 0.00129866 Loss_D_fake: 0.00185379) Loss_G: 0.44079429 Loss_Enh_Dec: -2.60540223\n",
      "| epoch 123 |   100/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  2.26 | ppl     9.57 | acc     0.68 | train_ae_norm     1.00\n",
      "[123/200][199/4361] Loss_D: 0.00202913 (Loss_D_real: 0.00122880 Loss_D_fake: 0.00080033) Loss_G: 0.34048411 Loss_Enh_Dec: -1.52056146\n",
      "| epoch 123 |   200/ 4361 batches | lr 0.000000 | ms/batch 400.75 | loss  2.29 | ppl     9.92 | acc     0.74 | train_ae_norm     1.00\n",
      "[123/200][299/4361] Loss_D: 0.00710485 (Loss_D_real: 0.00090386 Loss_D_fake: 0.00620099) Loss_G: 0.35223541 Loss_Enh_Dec: -1.69497132\n",
      "| epoch 123 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.50 | loss  2.31 | ppl    10.12 | acc     0.68 | train_ae_norm     1.00\n",
      "[123/200][399/4361] Loss_D: 0.00453681 (Loss_D_real: 0.00123195 Loss_D_fake: 0.00330486) Loss_G: 0.45080814 Loss_Enh_Dec: -1.69339907\n",
      "| epoch 123 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  2.19 | ppl     8.93 | acc     0.73 | train_ae_norm     1.00\n",
      "[123/200][499/4361] Loss_D: 0.00617991 (Loss_D_real: 0.00064849 Loss_D_fake: 0.00553141) Loss_G: 0.48588824 Loss_Enh_Dec: -1.79492497\n",
      "| epoch 123 |   500/ 4361 batches | lr 0.000000 | ms/batch 399.82 | loss  2.25 | ppl     9.49 | acc     0.77 | train_ae_norm     1.00\n",
      "[123/200][599/4361] Loss_D: 0.01273366 (Loss_D_real: 0.00058149 Loss_D_fake: 0.01215217) Loss_G: 0.27079654 Loss_Enh_Dec: -1.52264559\n",
      "| epoch 123 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.25 | loss  2.21 | ppl     9.15 | acc     0.71 | train_ae_norm     1.00\n",
      "[123/200][699/4361] Loss_D: 0.01002561 (Loss_D_real: 0.00300527 Loss_D_fake: 0.00702035) Loss_G: 0.26043984 Loss_Enh_Dec: -1.89227104\n",
      "| epoch 123 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.83 | loss  2.28 | ppl     9.77 | acc     0.71 | train_ae_norm     1.00\n",
      "[123/200][799/4361] Loss_D: 0.00644564 (Loss_D_real: 0.00048970 Loss_D_fake: 0.00595594) Loss_G: 0.26002884 Loss_Enh_Dec: -2.42677975\n",
      "| epoch 123 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.15 | loss  2.24 | ppl     9.43 | acc     0.72 | train_ae_norm     1.00\n",
      "[123/200][899/4361] Loss_D: 0.01312211 (Loss_D_real: 0.00645622 Loss_D_fake: 0.00666588) Loss_G: 0.26010165 Loss_Enh_Dec: -2.46911430\n",
      "| epoch 123 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.72 | loss  2.26 | ppl     9.54 | acc     0.75 | train_ae_norm     1.00\n",
      "[123/200][999/4361] Loss_D: 0.00443461 (Loss_D_real: 0.00083364 Loss_D_fake: 0.00360097) Loss_G: 0.28404462 Loss_Enh_Dec: -2.05684924\n",
      "| epoch 123 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  2.26 | ppl     9.61 | acc     0.71 | train_ae_norm     1.00\n",
      "[123/200][1099/4361] Loss_D: 0.01729821 (Loss_D_real: 0.01182887 Loss_D_fake: 0.00546934) Loss_G: 0.27836052 Loss_Enh_Dec: -2.16456580\n",
      "| epoch 123 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.10 | loss  2.26 | ppl     9.57 | acc     0.69 | train_ae_norm     1.00\n",
      "[123/200][1199/4361] Loss_D: 0.02351752 (Loss_D_real: 0.01760273 Loss_D_fake: 0.00591479) Loss_G: 0.27177551 Loss_Enh_Dec: -2.14694262\n",
      "| epoch 123 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.72 | loss  2.28 | ppl     9.76 | acc     0.76 | train_ae_norm     1.00\n",
      "[123/200][1299/4361] Loss_D: 0.00873122 (Loss_D_real: 0.00323365 Loss_D_fake: 0.00549757) Loss_G: 0.27425691 Loss_Enh_Dec: -2.45472217\n",
      "| epoch 123 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.55 | loss  2.35 | ppl    10.45 | acc     0.70 | train_ae_norm     1.00\n",
      "[123/200][1399/4361] Loss_D: 0.04188482 (Loss_D_real: 0.03724191 Loss_D_fake: 0.00464292) Loss_G: 0.27612665 Loss_Enh_Dec: -2.46743155\n",
      "| epoch 123 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.54 | loss  2.33 | ppl    10.28 | acc     0.66 | train_ae_norm     1.00\n",
      "[123/200][1499/4361] Loss_D: 0.00487199 (Loss_D_real: 0.00134596 Loss_D_fake: 0.00352603) Loss_G: 0.28406104 Loss_Enh_Dec: -2.03842664\n",
      "| epoch 123 |  1500/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  2.54 | ppl    12.63 | acc     0.66 | train_ae_norm     1.00\n",
      "[123/200][1599/4361] Loss_D: 0.00585888 (Loss_D_real: 0.00080170 Loss_D_fake: 0.00505717) Loss_G: 0.28209648 Loss_Enh_Dec: -2.41392112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 123 |  1600/ 4361 batches | lr 0.000000 | ms/batch 400.93 | loss  2.43 | ppl    11.38 | acc     0.71 | train_ae_norm     1.00\n",
      "[123/200][1699/4361] Loss_D: 0.01176746 (Loss_D_real: 0.00622808 Loss_D_fake: 0.00553939) Loss_G: 0.28583688 Loss_Enh_Dec: -2.48653913\n",
      "| epoch 123 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.91 | loss  2.40 | ppl    11.01 | acc     0.69 | train_ae_norm     1.00\n",
      "[123/200][1799/4361] Loss_D: 0.00324546 (Loss_D_real: 0.00053513 Loss_D_fake: 0.00271034) Loss_G: 0.31505638 Loss_Enh_Dec: -2.31437945\n",
      "| epoch 123 |  1800/ 4361 batches | lr 0.000000 | ms/batch 399.57 | loss  2.34 | ppl    10.40 | acc     0.69 | train_ae_norm     1.00\n",
      "[123/200][1899/4361] Loss_D: 0.02766219 (Loss_D_real: 0.02404102 Loss_D_fake: 0.00362117) Loss_G: 0.32123598 Loss_Enh_Dec: -2.54307818\n",
      "| epoch 123 |  1900/ 4361 batches | lr 0.000000 | ms/batch 399.56 | loss  2.39 | ppl    10.86 | acc     0.72 | train_ae_norm     1.00\n",
      "[123/200][1999/4361] Loss_D: 0.00944081 (Loss_D_real: 0.00657090 Loss_D_fake: 0.00286991) Loss_G: 0.28412262 Loss_Enh_Dec: -1.87140787\n",
      "| epoch 123 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.68 | loss  2.35 | ppl    10.52 | acc     0.74 | train_ae_norm     1.00\n",
      "[123/200][2099/4361] Loss_D: 0.02008889 (Loss_D_real: 0.01676543 Loss_D_fake: 0.00332346) Loss_G: 0.29711807 Loss_Enh_Dec: -2.25133967\n",
      "| epoch 123 |  2100/ 4361 batches | lr 0.000000 | ms/batch 399.75 | loss  2.38 | ppl    10.75 | acc     0.72 | train_ae_norm     1.00\n",
      "[123/200][2199/4361] Loss_D: 0.00575950 (Loss_D_real: 0.00039724 Loss_D_fake: 0.00536226) Loss_G: 0.29993531 Loss_Enh_Dec: -2.17728281\n",
      "| epoch 123 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.25 | loss  2.34 | ppl    10.35 | acc     0.74 | train_ae_norm     1.00\n",
      "[123/200][2299/4361] Loss_D: 0.00368227 (Loss_D_real: 0.00012964 Loss_D_fake: 0.00355263) Loss_G: 0.31621677 Loss_Enh_Dec: -2.26266932\n",
      "| epoch 123 |  2300/ 4361 batches | lr 0.000000 | ms/batch 399.06 | loss  2.33 | ppl    10.23 | acc     0.74 | train_ae_norm     1.00\n",
      "[123/200][2399/4361] Loss_D: 0.00898006 (Loss_D_real: 0.00390842 Loss_D_fake: 0.00507165) Loss_G: 0.34422463 Loss_Enh_Dec: -2.42224360\n",
      "| epoch 123 |  2400/ 4361 batches | lr 0.000000 | ms/batch 399.17 | loss  2.31 | ppl    10.12 | acc     0.70 | train_ae_norm     1.00\n",
      "[123/200][2499/4361] Loss_D: 0.00377827 (Loss_D_real: 0.00012958 Loss_D_fake: 0.00364869) Loss_G: 0.31625172 Loss_Enh_Dec: -2.04435539\n",
      "| epoch 123 |  2500/ 4361 batches | lr 0.000000 | ms/batch 399.97 | loss  2.32 | ppl    10.20 | acc     0.73 | train_ae_norm     1.00\n",
      "[123/200][2599/4361] Loss_D: 0.03653225 (Loss_D_real: 0.03283127 Loss_D_fake: 0.00370099) Loss_G: 0.29851061 Loss_Enh_Dec: -2.41104484\n",
      "| epoch 123 |  2600/ 4361 batches | lr 0.000000 | ms/batch 399.88 | loss  2.28 | ppl     9.79 | acc     0.71 | train_ae_norm     1.00\n",
      "[123/200][2699/4361] Loss_D: 0.00232956 (Loss_D_real: 0.00026932 Loss_D_fake: 0.00206023) Loss_G: 0.33327970 Loss_Enh_Dec: -2.22856975\n",
      "| epoch 123 |  2700/ 4361 batches | lr 0.000000 | ms/batch 399.63 | loss  2.27 | ppl     9.64 | acc     0.72 | train_ae_norm     1.00\n",
      "[123/200][2799/4361] Loss_D: 0.01855602 (Loss_D_real: 0.01200819 Loss_D_fake: 0.00654783) Loss_G: 0.37411201 Loss_Enh_Dec: -2.59828305\n",
      "| epoch 123 |  2800/ 4361 batches | lr 0.000000 | ms/batch 399.52 | loss  2.22 | ppl     9.23 | acc     0.71 | train_ae_norm     1.00\n",
      "[123/200][2899/4361] Loss_D: 0.00294689 (Loss_D_real: 0.00004684 Loss_D_fake: 0.00290004) Loss_G: 0.32556579 Loss_Enh_Dec: -2.63591433\n",
      "| epoch 123 |  2900/ 4361 batches | lr 0.000000 | ms/batch 399.72 | loss  2.26 | ppl     9.58 | acc     0.72 | train_ae_norm     1.00\n",
      "[123/200][2999/4361] Loss_D: 0.00381530 (Loss_D_real: 0.00009151 Loss_D_fake: 0.00372379) Loss_G: 0.36923397 Loss_Enh_Dec: -2.58696818\n",
      "| epoch 123 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.25 | ppl     9.45 | acc     0.74 | train_ae_norm     1.00\n",
      "[123/200][3099/4361] Loss_D: 0.01246100 (Loss_D_real: 0.00906832 Loss_D_fake: 0.00339268) Loss_G: 0.31822163 Loss_Enh_Dec: -2.47191000\n",
      "| epoch 123 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.25 | loss  2.26 | ppl     9.55 | acc     0.71 | train_ae_norm     1.00\n",
      "[123/200][3199/4361] Loss_D: 0.00391046 (Loss_D_real: 0.00050286 Loss_D_fake: 0.00340759) Loss_G: 0.32127216 Loss_Enh_Dec: -2.83737946\n",
      "| epoch 123 |  3200/ 4361 batches | lr 0.000000 | ms/batch 399.53 | loss  2.29 | ppl     9.87 | acc     0.75 | train_ae_norm     1.00\n",
      "[123/200][3299/4361] Loss_D: 0.00511213 (Loss_D_real: 0.00295460 Loss_D_fake: 0.00215752) Loss_G: 0.33215177 Loss_Enh_Dec: -1.78633237\n",
      "| epoch 123 |  3300/ 4361 batches | lr 0.000000 | ms/batch 399.57 | loss  2.31 | ppl    10.04 | acc     0.71 | train_ae_norm     1.00\n",
      "[123/200][3399/4361] Loss_D: 0.00824677 (Loss_D_real: 0.00058644 Loss_D_fake: 0.00766032) Loss_G: 0.37443265 Loss_Enh_Dec: -2.48588872\n",
      "| epoch 123 |  3400/ 4361 batches | lr 0.000000 | ms/batch 399.54 | loss  2.30 | ppl    10.02 | acc     0.70 | train_ae_norm     1.00\n",
      "[123/200][3499/4361] Loss_D: 0.00094664 (Loss_D_real: 0.00015872 Loss_D_fake: 0.00078792) Loss_G: 0.41965437 Loss_Enh_Dec: -2.23306131\n",
      "| epoch 123 |  3500/ 4361 batches | lr 0.000000 | ms/batch 399.71 | loss  2.21 | ppl     9.14 | acc     0.73 | train_ae_norm     1.00\n",
      "[123/200][3599/4361] Loss_D: 0.01646540 (Loss_D_real: 0.01386163 Loss_D_fake: 0.00260377) Loss_G: 0.33737409 Loss_Enh_Dec: -2.38421988\n",
      "| epoch 123 |  3600/ 4361 batches | lr 0.000000 | ms/batch 399.53 | loss  2.23 | ppl     9.32 | acc     0.74 | train_ae_norm     1.00\n",
      "[123/200][3699/4361] Loss_D: 0.02882606 (Loss_D_real: 0.02694014 Loss_D_fake: 0.00188591) Loss_G: 0.34238729 Loss_Enh_Dec: -2.69194245\n",
      "| epoch 123 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.12 | loss  2.25 | ppl     9.52 | acc     0.70 | train_ae_norm     1.00\n",
      "[123/200][3799/4361] Loss_D: 0.00366051 (Loss_D_real: 0.00094922 Loss_D_fake: 0.00271130) Loss_G: 0.37795407 Loss_Enh_Dec: -2.67335582\n",
      "| epoch 123 |  3800/ 4361 batches | lr 0.000000 | ms/batch 400.79 | loss  2.25 | ppl     9.53 | acc     0.77 | train_ae_norm     1.00\n",
      "[123/200][3899/4361] Loss_D: 0.01190884 (Loss_D_real: 0.00714205 Loss_D_fake: 0.00476679) Loss_G: 0.29429540 Loss_Enh_Dec: -2.68085837\n",
      "| epoch 123 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.41 | loss  2.26 | ppl     9.60 | acc     0.68 | train_ae_norm     1.00\n",
      "[123/200][3999/4361] Loss_D: 0.00408684 (Loss_D_real: 0.00187754 Loss_D_fake: 0.00220930) Loss_G: 0.31522939 Loss_Enh_Dec: -2.38406944\n",
      "| epoch 123 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.69 | loss  2.24 | ppl     9.44 | acc     0.71 | train_ae_norm     1.00\n",
      "[123/200][4099/4361] Loss_D: 0.00382717 (Loss_D_real: 0.00006241 Loss_D_fake: 0.00376476) Loss_G: 0.30098376 Loss_Enh_Dec: -2.53680682\n",
      "| epoch 123 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.76 | loss  2.22 | ppl     9.19 | acc     0.73 | train_ae_norm     1.00\n",
      "[123/200][4199/4361] Loss_D: 0.03439062 (Loss_D_real: 0.03094063 Loss_D_fake: 0.00344999) Loss_G: 0.30794856 Loss_Enh_Dec: -2.55149341\n",
      "| epoch 123 |  4200/ 4361 batches | lr 0.000000 | ms/batch 400.08 | loss  2.27 | ppl     9.69 | acc     0.76 | train_ae_norm     1.00\n",
      "[123/200][4299/4361] Loss_D: 0.09451748 (Loss_D_real: 0.00442656 Loss_D_fake: 0.09009092) Loss_G: 0.49163148 Loss_Enh_Dec: -2.71507168\n",
      "| epoch 123 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.57 | loss  2.27 | ppl     9.72 | acc     0.71 | train_ae_norm     1.00\n",
      "| end of epoch 123 | time: 1847.31s | test loss  2.34 | test ppl 10.37 | acc 0.772\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 124 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.696\n",
      "  Average training loss discriminator: 0.701\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.472\n",
      "  Test Loss: 4.564\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 124 |     0/ 4361 batches | lr 0.000000 | ms/batch 863.53 | loss  0.02 | ppl     1.02 | acc     0.74 | train_ae_norm     1.00\n",
      "[124/200][99/4361] Loss_D: 0.00625678 (Loss_D_real: 0.00246043 Loss_D_fake: 0.00379636) Loss_G: 0.30552813 Loss_Enh_Dec: -2.50589561\n",
      "| epoch 124 |   100/ 4361 batches | lr 0.000000 | ms/batch 400.86 | loss  2.27 | ppl     9.66 | acc     0.70 | train_ae_norm     1.00\n",
      "[124/200][199/4361] Loss_D: 0.00405290 (Loss_D_real: 0.00220808 Loss_D_fake: 0.00184482) Loss_G: 0.30626401 Loss_Enh_Dec: -2.61095214\n",
      "| epoch 124 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  2.29 | ppl     9.88 | acc     0.74 | train_ae_norm     1.00\n",
      "[124/200][299/4361] Loss_D: 0.00449919 (Loss_D_real: 0.00229261 Loss_D_fake: 0.00220659) Loss_G: 0.33831722 Loss_Enh_Dec: -2.37556696\n",
      "| epoch 124 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  2.29 | ppl     9.85 | acc     0.68 | train_ae_norm     1.00\n",
      "[124/200][399/4361] Loss_D: 0.00321984 (Loss_D_real: 0.00057066 Loss_D_fake: 0.00264918) Loss_G: 0.32030565 Loss_Enh_Dec: -2.37749267\n",
      "| epoch 124 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  2.20 | ppl     9.02 | acc     0.74 | train_ae_norm     1.00\n",
      "[124/200][499/4361] Loss_D: 0.00308012 (Loss_D_real: 0.00000620 Loss_D_fake: 0.00307392) Loss_G: 0.34355438 Loss_Enh_Dec: -2.59433103\n",
      "| epoch 124 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.27 | ppl     9.64 | acc     0.76 | train_ae_norm     1.00\n",
      "[124/200][599/4361] Loss_D: 0.01731797 (Loss_D_real: 0.01378379 Loss_D_fake: 0.00353418) Loss_G: 0.42199692 Loss_Enh_Dec: -2.72808599\n",
      "| epoch 124 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.23 | ppl     9.33 | acc     0.71 | train_ae_norm     1.00\n",
      "[124/200][699/4361] Loss_D: 0.00318517 (Loss_D_real: 0.00149450 Loss_D_fake: 0.00169067) Loss_G: 0.40689728 Loss_Enh_Dec: -2.75353789\n",
      "| epoch 124 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  2.25 | ppl     9.48 | acc     0.72 | train_ae_norm     1.00\n",
      "[124/200][799/4361] Loss_D: 0.00775745 (Loss_D_real: 0.00586980 Loss_D_fake: 0.00188765) Loss_G: 0.39101914 Loss_Enh_Dec: -2.62372208\n",
      "| epoch 124 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  2.24 | ppl     9.40 | acc     0.71 | train_ae_norm     1.00\n",
      "[124/200][899/4361] Loss_D: 0.02677801 (Loss_D_real: 0.02308951 Loss_D_fake: 0.00368850) Loss_G: 0.38181052 Loss_Enh_Dec: -2.27988696\n",
      "| epoch 124 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.29 | loss  2.25 | ppl     9.51 | acc     0.76 | train_ae_norm     1.00\n",
      "[124/200][999/4361] Loss_D: 0.00196732 (Loss_D_real: 0.00088895 Loss_D_fake: 0.00107837) Loss_G: 0.42498627 Loss_Enh_Dec: -2.19099498\n",
      "| epoch 124 |  1000/ 4361 batches | lr 0.000000 | ms/batch 400.51 | loss  2.25 | ppl     9.46 | acc     0.72 | train_ae_norm     1.00\n",
      "[124/200][1099/4361] Loss_D: 0.00160694 (Loss_D_real: 0.00054983 Loss_D_fake: 0.00105711) Loss_G: 0.38238198 Loss_Enh_Dec: -2.53361106\n",
      "| epoch 124 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  2.23 | ppl     9.25 | acc     0.73 | train_ae_norm     1.00\n",
      "[124/200][1199/4361] Loss_D: 0.02198698 (Loss_D_real: 0.01988532 Loss_D_fake: 0.00210166) Loss_G: 0.40280619 Loss_Enh_Dec: -2.56346583\n",
      "| epoch 124 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  2.24 | ppl     9.40 | acc     0.75 | train_ae_norm     1.00\n",
      "[124/200][1299/4361] Loss_D: 0.00704271 (Loss_D_real: 0.00429075 Loss_D_fake: 0.00275196) Loss_G: 0.38567188 Loss_Enh_Dec: -2.57621574\n",
      "| epoch 124 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.84 | loss  2.28 | ppl     9.74 | acc     0.73 | train_ae_norm     1.00\n",
      "[124/200][1399/4361] Loss_D: 0.00434992 (Loss_D_real: 0.00015928 Loss_D_fake: 0.00419064) Loss_G: 0.40561867 Loss_Enh_Dec: -2.66142631\n",
      "| epoch 124 |  1400/ 4361 batches | lr 0.000000 | ms/batch 400.57 | loss  2.28 | ppl     9.79 | acc     0.68 | train_ae_norm     1.00\n",
      "[124/200][1699/4361] Loss_D: 0.00264554 (Loss_D_real: 0.00032483 Loss_D_fake: 0.00232071) Loss_G: 0.39184666 Loss_Enh_Dec: -2.58425355\n",
      "| epoch 124 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  2.25 | ppl     9.45 | acc     0.70 | train_ae_norm     1.00\n",
      "[124/200][1799/4361] Loss_D: 0.01347727 (Loss_D_real: 0.01336592 Loss_D_fake: 0.00011135) Loss_G: 0.55862159 Loss_Enh_Dec: -2.09865117\n",
      "| epoch 124 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  2.25 | ppl     9.51 | acc     0.71 | train_ae_norm     1.00\n",
      "[124/200][1899/4361] Loss_D: 0.00634504 (Loss_D_real: 0.00013363 Loss_D_fake: 0.00621141) Loss_G: 0.40160370 Loss_Enh_Dec: -1.91572881\n",
      "| epoch 124 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.22 | loss  2.30 | ppl     9.97 | acc     0.73 | train_ae_norm     1.00\n",
      "[124/200][1999/4361] Loss_D: 0.00491673 (Loss_D_real: 0.00248584 Loss_D_fake: 0.00243089) Loss_G: 0.35160980 Loss_Enh_Dec: -1.79395509\n",
      "| epoch 124 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.78 | loss  2.23 | ppl     9.33 | acc     0.75 | train_ae_norm     1.00\n",
      "[124/200][2099/4361] Loss_D: 0.00563620 (Loss_D_real: 0.00211488 Loss_D_fake: 0.00352132) Loss_G: 0.35100070 Loss_Enh_Dec: -1.65693367\n",
      "| epoch 124 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  2.27 | ppl     9.66 | acc     0.76 | train_ae_norm     1.00\n",
      "[124/200][2199/4361] Loss_D: 0.00591410 (Loss_D_real: 0.00459828 Loss_D_fake: 0.00131582) Loss_G: 0.51123857 Loss_Enh_Dec: -1.21405292\n",
      "| epoch 124 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.42 | loss  2.24 | ppl     9.41 | acc     0.76 | train_ae_norm     1.00\n",
      "[124/200][2299/4361] Loss_D: 0.01137521 (Loss_D_real: 0.00704388 Loss_D_fake: 0.00433133) Loss_G: 0.36202717 Loss_Enh_Dec: -1.75678277\n",
      "| epoch 124 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.37 | loss  2.25 | ppl     9.44 | acc     0.76 | train_ae_norm     1.00\n",
      "[124/200][2399/4361] Loss_D: 0.00311393 (Loss_D_real: 0.00088176 Loss_D_fake: 0.00223217) Loss_G: 0.39404961 Loss_Enh_Dec: -1.80016136\n",
      "| epoch 124 |  2400/ 4361 batches | lr 0.000000 | ms/batch 400.69 | loss  2.24 | ppl     9.37 | acc     0.68 | train_ae_norm     1.00\n",
      "[124/200][2499/4361] Loss_D: 0.00219380 (Loss_D_real: 0.00019394 Loss_D_fake: 0.00199986) Loss_G: 0.36198586 Loss_Enh_Dec: -1.93026245\n",
      "| epoch 124 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  2.27 | ppl     9.71 | acc     0.73 | train_ae_norm     1.00\n",
      "[124/200][2599/4361] Loss_D: 0.00498447 (Loss_D_real: 0.00393889 Loss_D_fake: 0.00104558) Loss_G: 0.49467525 Loss_Enh_Dec: -2.07528567\n",
      "| epoch 124 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  2.26 | ppl     9.54 | acc     0.70 | train_ae_norm     1.00\n",
      "[124/200][2699/4361] Loss_D: 0.01262182 (Loss_D_real: 0.01122199 Loss_D_fake: 0.00139983) Loss_G: 0.43867642 Loss_Enh_Dec: -1.75751460\n",
      "| epoch 124 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  2.27 | ppl     9.69 | acc     0.69 | train_ae_norm     1.00\n",
      "[124/200][2799/4361] Loss_D: 0.00190083 (Loss_D_real: 0.00065628 Loss_D_fake: 0.00124455) Loss_G: 0.43716335 Loss_Enh_Dec: -2.07321215\n",
      "| epoch 124 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  2.24 | ppl     9.41 | acc     0.71 | train_ae_norm     1.00\n",
      "[124/200][2899/4361] Loss_D: 0.00602404 (Loss_D_real: 0.00012459 Loss_D_fake: 0.00589944) Loss_G: 0.35758951 Loss_Enh_Dec: -2.31736422\n",
      "| epoch 124 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.71 | loss  2.23 | ppl     9.27 | acc     0.74 | train_ae_norm     1.00\n",
      "[124/200][2999/4361] Loss_D: 0.00361285 (Loss_D_real: 0.00023630 Loss_D_fake: 0.00337655) Loss_G: 0.36552858 Loss_Enh_Dec: -2.18561745\n",
      "| epoch 124 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  2.26 | ppl     9.62 | acc     0.74 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[124/200][3099/4361] Loss_D: 0.00557393 (Loss_D_real: 0.00476644 Loss_D_fake: 0.00080748) Loss_G: 0.39838922 Loss_Enh_Dec: -2.12489700\n",
      "| epoch 124 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  2.29 | ppl     9.85 | acc     0.72 | train_ae_norm     1.00\n",
      "[124/200][3199/4361] Loss_D: 0.00191922 (Loss_D_real: 0.00010250 Loss_D_fake: 0.00181673) Loss_G: 0.51831228 Loss_Enh_Dec: -2.06025910\n",
      "| epoch 124 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.26 | loss  2.27 | ppl     9.71 | acc     0.75 | train_ae_norm     1.00\n",
      "[124/200][3299/4361] Loss_D: 0.00228878 (Loss_D_real: 0.00092370 Loss_D_fake: 0.00136507) Loss_G: 0.39418879 Loss_Enh_Dec: -1.96015775\n",
      "| epoch 124 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  2.26 | ppl     9.62 | acc     0.73 | train_ae_norm     1.00\n",
      "[124/200][3399/4361] Loss_D: 0.00314978 (Loss_D_real: 0.00236324 Loss_D_fake: 0.00078654) Loss_G: 0.44913360 Loss_Enh_Dec: -1.84850299\n",
      "| epoch 124 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  2.27 | ppl     9.71 | acc     0.71 | train_ae_norm     1.00\n",
      "[124/200][3499/4361] Loss_D: 0.00412399 (Loss_D_real: 0.00135625 Loss_D_fake: 0.00276773) Loss_G: 0.38097706 Loss_Enh_Dec: -1.84358716\n",
      "| epoch 124 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  2.24 | ppl     9.43 | acc     0.71 | train_ae_norm     1.00\n",
      "[124/200][3599/4361] Loss_D: 0.00318230 (Loss_D_real: 0.00126125 Loss_D_fake: 0.00192105) Loss_G: 0.49978670 Loss_Enh_Dec: -1.66351545\n",
      "| epoch 124 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  2.32 | ppl    10.13 | acc     0.72 | train_ae_norm     1.00\n",
      "[124/200][3699/4361] Loss_D: 0.00490184 (Loss_D_real: 0.00175932 Loss_D_fake: 0.00314252) Loss_G: 0.40403968 Loss_Enh_Dec: -2.12523818\n",
      "| epoch 124 |  3700/ 4361 batches | lr 0.000000 | ms/batch 402.46 | loss  2.31 | ppl    10.04 | acc     0.70 | train_ae_norm     1.00\n",
      "[124/200][3799/4361] Loss_D: 0.00212298 (Loss_D_real: 0.00069211 Loss_D_fake: 0.00143087) Loss_G: 0.37052390 Loss_Enh_Dec: -1.83701169\n",
      "| epoch 124 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  2.29 | ppl     9.85 | acc     0.77 | train_ae_norm     1.00\n",
      "[124/200][3899/4361] Loss_D: 0.00202698 (Loss_D_real: 0.00046133 Loss_D_fake: 0.00156566) Loss_G: 0.41349229 Loss_Enh_Dec: -2.03483200\n",
      "| epoch 124 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  2.28 | ppl     9.80 | acc     0.70 | train_ae_norm     1.00\n",
      "[124/200][3999/4361] Loss_D: 0.00169561 (Loss_D_real: 0.00044557 Loss_D_fake: 0.00125004) Loss_G: 0.42083207 Loss_Enh_Dec: -2.42128587\n",
      "| epoch 124 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  2.27 | ppl     9.70 | acc     0.74 | train_ae_norm     1.00\n",
      "[124/200][4099/4361] Loss_D: 0.00320703 (Loss_D_real: 0.00066649 Loss_D_fake: 0.00254054) Loss_G: 0.44111776 Loss_Enh_Dec: -2.03609061\n",
      "| epoch 124 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  2.25 | ppl     9.45 | acc     0.72 | train_ae_norm     1.00\n",
      "[124/200][4199/4361] Loss_D: 0.00322622 (Loss_D_real: 0.00013755 Loss_D_fake: 0.00308867) Loss_G: 0.47008815 Loss_Enh_Dec: -1.96171534\n",
      "| epoch 124 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  2.28 | ppl     9.76 | acc     0.76 | train_ae_norm     1.00\n",
      "[124/200][4299/4361] Loss_D: 0.00434677 (Loss_D_real: 0.00080073 Loss_D_fake: 0.00354605) Loss_G: 0.36533400 Loss_Enh_Dec: -1.72164750\n",
      "| epoch 124 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  2.27 | ppl     9.64 | acc     0.72 | train_ae_norm     1.00\n",
      "| end of epoch 124 | time: 1851.66s | test loss  2.36 | test ppl 10.56 | acc 0.772\n",
      "bleu_self:  [3.78201426e-01 2.27441435e-01 2.29985823e-06 2.80529150e-07\n",
      " 2.05327823e-07]\n",
      "bleu_test:  [7.92708333e-01 3.91742771e-01 3.53999508e-06 5.42511656e-07\n",
      " 3.99764844e-07]\n",
      "bleu_self: [0.37820143,0.22744143,0.00000230,0.00000028,0.00000021]\n",
      "bleu_test: [0.79270833,0.39174277,0.00000354,0.00000054,0.00000040]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 125 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.697\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.480\n",
      "  Test Loss: 4.641\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 125 |     0/ 4361 batches | lr 0.000000 | ms/batch 863.25 | loss  0.02 | ppl     1.02 | acc     0.74 | train_ae_norm     1.00\n",
      "[125/200][99/4361] Loss_D: 0.00224203 (Loss_D_real: 0.00025487 Loss_D_fake: 0.00198716) Loss_G: 0.39072791 Loss_Enh_Dec: -2.36621451\n",
      "| epoch 125 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  2.25 | ppl     9.50 | acc     0.70 | train_ae_norm     1.00\n",
      "[125/200][199/4361] Loss_D: 0.07083087 (Loss_D_real: 0.06880275 Loss_D_fake: 0.00202811) Loss_G: 0.39447147 Loss_Enh_Dec: -2.35128856\n",
      "| epoch 125 |   200/ 4361 batches | lr 0.000000 | ms/batch 402.28 | loss  2.27 | ppl     9.66 | acc     0.74 | train_ae_norm     1.00\n",
      "[125/200][299/4361] Loss_D: 0.00777396 (Loss_D_real: 0.00667379 Loss_D_fake: 0.00110016) Loss_G: 0.49996549 Loss_Enh_Dec: -2.52773023\n",
      "| epoch 125 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  2.28 | ppl     9.79 | acc     0.67 | train_ae_norm     1.00\n",
      "[125/200][399/4361] Loss_D: 0.00960909 (Loss_D_real: 0.00735169 Loss_D_fake: 0.00225741) Loss_G: 0.40146747 Loss_Enh_Dec: -2.33451200\n",
      "| epoch 125 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  2.23 | ppl     9.31 | acc     0.73 | train_ae_norm     1.00\n",
      "[125/200][499/4361] Loss_D: 0.00453272 (Loss_D_real: 0.00087223 Loss_D_fake: 0.00366049) Loss_G: 0.36509350 Loss_Enh_Dec: -1.85254824\n",
      "| epoch 125 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.30 | ppl     9.97 | acc     0.74 | train_ae_norm     1.00\n",
      "[125/200][599/4361] Loss_D: 0.00262504 (Loss_D_real: 0.00028646 Loss_D_fake: 0.00233858) Loss_G: 0.36194625 Loss_Enh_Dec: -2.07466197\n",
      "| epoch 125 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  2.25 | ppl     9.53 | acc     0.71 | train_ae_norm     1.00\n",
      "[125/200][699/4361] Loss_D: 0.00215882 (Loss_D_real: 0.00084503 Loss_D_fake: 0.00131379) Loss_G: 0.39243001 Loss_Enh_Dec: -2.30089927\n",
      "| epoch 125 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  2.31 | ppl    10.09 | acc     0.73 | train_ae_norm     1.00\n",
      "[125/200][799/4361] Loss_D: 0.00928331 (Loss_D_real: 0.00682458 Loss_D_fake: 0.00245873) Loss_G: 0.35479698 Loss_Enh_Dec: -2.79601932\n",
      "| epoch 125 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  2.32 | ppl    10.18 | acc     0.69 | train_ae_norm     1.00\n",
      "[125/200][899/4361] Loss_D: 0.01917348 (Loss_D_real: 0.00725067 Loss_D_fake: 0.01192281) Loss_G: 0.47127676 Loss_Enh_Dec: -2.68939209\n",
      "| epoch 125 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  2.32 | ppl    10.20 | acc     0.73 | train_ae_norm     1.00\n",
      "[125/200][999/4361] Loss_D: 0.00290454 (Loss_D_real: 0.00044374 Loss_D_fake: 0.00246080) Loss_G: 0.37948570 Loss_Enh_Dec: -2.49469352\n",
      "| epoch 125 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  2.26 | ppl     9.62 | acc     0.75 | train_ae_norm     1.00\n",
      "[125/200][1099/4361] Loss_D: 0.00425261 (Loss_D_real: 0.00269133 Loss_D_fake: 0.00156128) Loss_G: 0.34158400 Loss_Enh_Dec: -2.00549197\n",
      "| epoch 125 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.26 | ppl     9.62 | acc     0.70 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[125/200][1199/4361] Loss_D: 0.00268770 (Loss_D_real: 0.00022677 Loss_D_fake: 0.00246093) Loss_G: 0.33572385 Loss_Enh_Dec: -2.56734967\n",
      "| epoch 125 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.73 | loss  2.28 | ppl     9.78 | acc     0.76 | train_ae_norm     1.00\n",
      "[125/200][1299/4361] Loss_D: 0.00397958 (Loss_D_real: 0.00191351 Loss_D_fake: 0.00206608) Loss_G: 0.37147853 Loss_Enh_Dec: -2.44011664\n",
      "| epoch 125 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  2.31 | ppl    10.08 | acc     0.72 | train_ae_norm     1.00\n",
      "[125/200][1399/4361] Loss_D: 0.01287963 (Loss_D_real: 0.01031091 Loss_D_fake: 0.00256872) Loss_G: 0.38013825 Loss_Enh_Dec: -2.49835801\n",
      "| epoch 125 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  2.30 | ppl     9.94 | acc     0.67 | train_ae_norm     1.00\n",
      "[125/200][1499/4361] Loss_D: 0.00500027 (Loss_D_real: 0.00177189 Loss_D_fake: 0.00322839) Loss_G: 0.50575864 Loss_Enh_Dec: -2.52412558\n",
      "| epoch 125 |  1500/ 4361 batches | lr 0.000000 | ms/batch 402.07 | loss  2.34 | ppl    10.42 | acc     0.70 | train_ae_norm     1.00\n",
      "[125/200][1599/4361] Loss_D: 0.00356305 (Loss_D_real: 0.00306109 Loss_D_fake: 0.00050196) Loss_G: 0.50231630 Loss_Enh_Dec: -2.13862205\n",
      "| epoch 125 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  2.32 | ppl    10.20 | acc     0.73 | train_ae_norm     1.00\n",
      "[125/200][1699/4361] Loss_D: 0.00434147 (Loss_D_real: 0.00276845 Loss_D_fake: 0.00157303) Loss_G: 0.37735108 Loss_Enh_Dec: -2.01263475\n",
      "| epoch 125 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.50 | loss  2.28 | ppl     9.78 | acc     0.72 | train_ae_norm     1.00\n",
      "[125/200][1999/4361] Loss_D: 0.00606096 (Loss_D_real: 0.00225042 Loss_D_fake: 0.00381054) Loss_G: 0.39859197 Loss_Enh_Dec: -0.95050013\n",
      "| epoch 125 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.91 | loss  2.27 | ppl     9.64 | acc     0.76 | train_ae_norm     1.00\n",
      "[125/200][2099/4361] Loss_D: 0.01354507 (Loss_D_real: 0.00197281 Loss_D_fake: 0.01157226) Loss_G: 0.38151607 Loss_Enh_Dec: -1.68261755\n",
      "| epoch 125 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.88 | loss  2.29 | ppl     9.91 | acc     0.75 | train_ae_norm     1.00\n",
      "[125/200][2199/4361] Loss_D: 0.04404901 (Loss_D_real: 0.03966347 Loss_D_fake: 0.00438554) Loss_G: 0.54002184 Loss_Enh_Dec: -2.14241910\n",
      "| epoch 125 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  2.28 | ppl     9.73 | acc     0.74 | train_ae_norm     1.00\n",
      "[125/200][2299/4361] Loss_D: 0.00346754 (Loss_D_real: 0.00164808 Loss_D_fake: 0.00181946) Loss_G: 0.38074911 Loss_Enh_Dec: -1.28720224\n",
      "| epoch 125 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  2.26 | ppl     9.58 | acc     0.74 | train_ae_norm     1.00\n",
      "[125/200][2399/4361] Loss_D: 0.00408781 (Loss_D_real: 0.00001702 Loss_D_fake: 0.00407080) Loss_G: 0.37142596 Loss_Enh_Dec: -2.49046850\n",
      "| epoch 125 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  2.25 | ppl     9.53 | acc     0.68 | train_ae_norm     1.00\n",
      "[125/200][2499/4361] Loss_D: 0.00526227 (Loss_D_real: 0.00421048 Loss_D_fake: 0.00105179) Loss_G: 0.36853093 Loss_Enh_Dec: -2.13740683\n",
      "| epoch 125 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  2.30 | ppl    10.01 | acc     0.71 | train_ae_norm     1.00\n",
      "[125/200][2599/4361] Loss_D: 0.00755258 (Loss_D_real: 0.00283196 Loss_D_fake: 0.00472063) Loss_G: 0.40271577 Loss_Enh_Dec: -1.89370048\n",
      "| epoch 125 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  2.27 | ppl     9.70 | acc     0.67 | train_ae_norm     1.00\n",
      "[125/200][2699/4361] Loss_D: 0.14169347 (Loss_D_real: 0.04385769 Loss_D_fake: 0.09783579) Loss_G: 0.44330344 Loss_Enh_Dec: -2.36279893\n",
      "| epoch 125 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.28 | ppl     9.75 | acc     0.72 | train_ae_norm     1.00\n",
      "[125/200][2799/4361] Loss_D: 0.00702190 (Loss_D_real: 0.00509171 Loss_D_fake: 0.00193019) Loss_G: 0.34780309 Loss_Enh_Dec: -2.26665616\n",
      "| epoch 125 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.91 | loss  2.24 | ppl     9.37 | acc     0.68 | train_ae_norm     1.00\n",
      "[125/200][2899/4361] Loss_D: 0.00789136 (Loss_D_real: 0.00498417 Loss_D_fake: 0.00290718) Loss_G: 0.39913908 Loss_Enh_Dec: -2.13814998\n",
      "| epoch 125 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  2.27 | ppl     9.69 | acc     0.74 | train_ae_norm     1.00\n",
      "[125/200][2999/4361] Loss_D: 0.00481013 (Loss_D_real: 0.00324772 Loss_D_fake: 0.00156241) Loss_G: 0.36271825 Loss_Enh_Dec: -2.14844489\n",
      "| epoch 125 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.63 | loss  2.27 | ppl     9.72 | acc     0.73 | train_ae_norm     1.00\n",
      "[125/200][3099/4361] Loss_D: 0.00326579 (Loss_D_real: 0.00172299 Loss_D_fake: 0.00154280) Loss_G: 0.36955020 Loss_Enh_Dec: -1.94254196\n",
      "| epoch 125 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  2.27 | ppl     9.63 | acc     0.69 | train_ae_norm     1.00\n",
      "[125/200][3199/4361] Loss_D: 0.00148587 (Loss_D_real: 0.00045630 Loss_D_fake: 0.00102958) Loss_G: 0.42347962 Loss_Enh_Dec: -1.65386236\n",
      "| epoch 125 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.28 | ppl     9.74 | acc     0.75 | train_ae_norm     1.00\n",
      "[125/200][3299/4361] Loss_D: 0.00174492 (Loss_D_real: 0.00011567 Loss_D_fake: 0.00162924) Loss_G: 0.38363835 Loss_Enh_Dec: -1.78047872\n",
      "| epoch 125 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  2.30 | ppl     9.96 | acc     0.71 | train_ae_norm     1.00\n",
      "[125/200][3399/4361] Loss_D: 0.00504974 (Loss_D_real: 0.00271294 Loss_D_fake: 0.00233680) Loss_G: 0.35759974 Loss_Enh_Dec: -1.88906062\n",
      "| epoch 125 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  2.28 | ppl     9.81 | acc     0.71 | train_ae_norm     1.00\n",
      "[125/200][3499/4361] Loss_D: 0.01656776 (Loss_D_real: 0.00867112 Loss_D_fake: 0.00789664) Loss_G: 0.39845735 Loss_Enh_Dec: -1.91115272\n",
      "| epoch 125 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.21 | ppl     9.13 | acc     0.71 | train_ae_norm     1.00\n",
      "[125/200][3599/4361] Loss_D: 0.00105177 (Loss_D_real: 0.00070104 Loss_D_fake: 0.00035073) Loss_G: 0.47330865 Loss_Enh_Dec: -1.82783127\n",
      "| epoch 125 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  2.23 | ppl     9.28 | acc     0.72 | train_ae_norm     1.00\n",
      "[125/200][3699/4361] Loss_D: 0.04794422 (Loss_D_real: 0.04476335 Loss_D_fake: 0.00318086) Loss_G: 0.39575100 Loss_Enh_Dec: -2.11656380\n",
      "| epoch 125 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.83 | loss  2.25 | ppl     9.45 | acc     0.70 | train_ae_norm     1.00\n",
      "[125/200][3799/4361] Loss_D: 0.00243369 (Loss_D_real: 0.00066963 Loss_D_fake: 0.00176406) Loss_G: 0.38688439 Loss_Enh_Dec: -2.36561346\n",
      "| epoch 125 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  2.24 | ppl     9.36 | acc     0.77 | train_ae_norm     1.00\n",
      "[125/200][3899/4361] Loss_D: 0.00551985 (Loss_D_real: 0.00031145 Loss_D_fake: 0.00520841) Loss_G: 0.38215491 Loss_Enh_Dec: -2.26860166\n",
      "| epoch 125 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  2.27 | ppl     9.69 | acc     0.71 | train_ae_norm     1.00\n",
      "[125/200][3999/4361] Loss_D: 0.00549067 (Loss_D_real: 0.00190301 Loss_D_fake: 0.00358766) Loss_G: 0.50922489 Loss_Enh_Dec: -2.29737091\n",
      "| epoch 125 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.42 | loss  2.25 | ppl     9.48 | acc     0.71 | train_ae_norm     1.00\n",
      "[125/200][4299/4361] Loss_D: 0.00371885 (Loss_D_real: 0.00160578 Loss_D_fake: 0.00211307) Loss_G: 0.44806084 Loss_Enh_Dec: -1.75725842\n",
      "| epoch 125 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  2.22 | ppl     9.25 | acc     0.74 | train_ae_norm     1.00\n",
      "| end of epoch 125 | time: 1852.11s | test loss  2.31 | test ppl 10.07 | acc 0.776\n",
      "bleu_self:  [2.34084925e-01 4.72688161e-02 4.56251643e-07 1.58682723e-09\n",
      " 3.19185103e-10]\n",
      "bleu_test:  [7.09226190e-01 1.01542662e-08 1.53022855e-10 7.98911062e-10\n",
      " 2.38076600e-09]\n",
      "bleu_self: [0.23408492,0.04726882,0.00000046,0.00000000,0.00000000]\n",
      "bleu_test: [0.70922619,0.00000001,0.00000000,0.00000000,0.00000000]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 126 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.698\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.480\n",
      "  Test Loss: 4.589\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 126 |     0/ 4361 batches | lr 0.000000 | ms/batch 865.90 | loss  0.02 | ppl     1.02 | acc     0.78 | train_ae_norm     1.00\n",
      "[126/200][99/4361] Loss_D: 0.00274758 (Loss_D_real: 0.00054230 Loss_D_fake: 0.00220528) Loss_G: 0.43343353 Loss_Enh_Dec: -2.35833335\n",
      "| epoch 126 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  2.24 | ppl     9.40 | acc     0.68 | train_ae_norm     1.00\n",
      "[126/200][199/4361] Loss_D: 0.04562492 (Loss_D_real: 0.04549065 Loss_D_fake: 0.00013427) Loss_G: 0.58871990 Loss_Enh_Dec: -2.05496812\n",
      "| epoch 126 |   200/ 4361 batches | lr 0.000000 | ms/batch 402.14 | loss  2.28 | ppl     9.79 | acc     0.74 | train_ae_norm     1.00\n",
      "[126/200][299/4361] Loss_D: 0.00216805 (Loss_D_real: 0.00080569 Loss_D_fake: 0.00136236) Loss_G: 0.39658198 Loss_Enh_Dec: -2.03674340\n",
      "| epoch 126 |   300/ 4361 batches | lr 0.000000 | ms/batch 402.13 | loss  2.25 | ppl     9.51 | acc     0.69 | train_ae_norm     1.00\n",
      "[126/200][399/4361] Loss_D: 0.00268819 (Loss_D_real: 0.00172375 Loss_D_fake: 0.00096444) Loss_G: 0.53123742 Loss_Enh_Dec: -1.81471562\n",
      "| epoch 126 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  2.18 | ppl     8.85 | acc     0.73 | train_ae_norm     1.00\n",
      "[126/200][499/4361] Loss_D: 0.00093816 (Loss_D_real: 0.00029017 Loss_D_fake: 0.00064799) Loss_G: 0.39759281 Loss_Enh_Dec: -1.99028289\n",
      "| epoch 126 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.81 | loss  2.25 | ppl     9.50 | acc     0.74 | train_ae_norm     1.00\n",
      "[126/200][599/4361] Loss_D: 0.00399186 (Loss_D_real: 0.00005163 Loss_D_fake: 0.00394023) Loss_G: 0.41592398 Loss_Enh_Dec: -2.20406604\n",
      "| epoch 126 |   600/ 4361 batches | lr 0.000000 | ms/batch 402.10 | loss  2.19 | ppl     8.92 | acc     0.71 | train_ae_norm     1.00\n",
      "[126/200][699/4361] Loss_D: 0.00332194 (Loss_D_real: 0.00048861 Loss_D_fake: 0.00283333) Loss_G: 0.43709141 Loss_Enh_Dec: -2.06922078\n",
      "| epoch 126 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.20 | ppl     9.02 | acc     0.73 | train_ae_norm     1.00\n",
      "[126/200][799/4361] Loss_D: 0.00562263 (Loss_D_real: 0.00052689 Loss_D_fake: 0.00509574) Loss_G: 0.45881328 Loss_Enh_Dec: -2.13865781\n",
      "| epoch 126 |   800/ 4361 batches | lr 0.000000 | ms/batch 402.08 | loss  2.19 | ppl     8.93 | acc     0.74 | train_ae_norm     1.00\n",
      "[126/200][899/4361] Loss_D: 0.00316136 (Loss_D_real: 0.00173725 Loss_D_fake: 0.00142412) Loss_G: 0.40402293 Loss_Enh_Dec: -2.24017501\n",
      "| epoch 126 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.72 | loss  2.19 | ppl     8.94 | acc     0.76 | train_ae_norm     1.00\n",
      "[126/200][999/4361] Loss_D: 0.00142524 (Loss_D_real: 0.00097436 Loss_D_fake: 0.00045088) Loss_G: 0.45985088 Loss_Enh_Dec: -2.17969584\n",
      "| epoch 126 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  2.19 | ppl     8.93 | acc     0.72 | train_ae_norm     1.00\n",
      "[126/200][1099/4361] Loss_D: 0.01192852 (Loss_D_real: 0.00401545 Loss_D_fake: 0.00791307) Loss_G: 0.43523961 Loss_Enh_Dec: -2.50240684\n",
      "| epoch 126 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  2.18 | ppl     8.85 | acc     0.73 | train_ae_norm     1.00\n",
      "[126/200][1199/4361] Loss_D: 0.00098225 (Loss_D_real: 0.00076958 Loss_D_fake: 0.00021267) Loss_G: 0.46893254 Loss_Enh_Dec: -2.25746465\n",
      "| epoch 126 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  2.19 | ppl     8.94 | acc     0.76 | train_ae_norm     1.00\n",
      "[126/200][1299/4361] Loss_D: 0.00217454 (Loss_D_real: 0.00121200 Loss_D_fake: 0.00096254) Loss_G: 0.44160852 Loss_Enh_Dec: -2.43151069\n",
      "| epoch 126 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  2.21 | ppl     9.13 | acc     0.76 | train_ae_norm     1.00\n",
      "[126/200][1399/4361] Loss_D: 0.00468782 (Loss_D_real: 0.00329344 Loss_D_fake: 0.00139438) Loss_G: 0.44404441 Loss_Enh_Dec: -2.29493999\n",
      "| epoch 126 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.05 | loss  2.21 | ppl     9.09 | acc     0.71 | train_ae_norm     1.00\n",
      "[126/200][1499/4361] Loss_D: 0.00074167 (Loss_D_real: 0.00021848 Loss_D_fake: 0.00052318) Loss_G: 0.69176841 Loss_Enh_Dec: -2.22219515\n",
      "| epoch 126 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  2.26 | ppl     9.57 | acc     0.72 | train_ae_norm     1.00\n",
      "[126/200][1599/4361] Loss_D: 0.00387786 (Loss_D_real: 0.00150984 Loss_D_fake: 0.00236802) Loss_G: 0.39496037 Loss_Enh_Dec: -2.45937037\n",
      "| epoch 126 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.24 | ppl     9.41 | acc     0.74 | train_ae_norm     1.00\n",
      "[126/200][1699/4361] Loss_D: 0.00312548 (Loss_D_real: 0.00174700 Loss_D_fake: 0.00137848) Loss_G: 0.39553508 Loss_Enh_Dec: -2.14740729\n",
      "| epoch 126 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.20 | ppl     9.00 | acc     0.73 | train_ae_norm     1.00\n",
      "[126/200][1799/4361] Loss_D: 0.00444651 (Loss_D_real: 0.00242617 Loss_D_fake: 0.00202034) Loss_G: 0.41526863 Loss_Enh_Dec: -1.81712723\n",
      "| epoch 126 |  1800/ 4361 batches | lr 0.000000 | ms/batch 402.12 | loss  2.20 | ppl     9.01 | acc     0.72 | train_ae_norm     1.00\n",
      "[126/200][2099/4361] Loss_D: 0.00419456 (Loss_D_real: 0.00173423 Loss_D_fake: 0.00246033) Loss_G: 0.42364618 Loss_Enh_Dec: -2.69633079\n",
      "| epoch 126 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  2.24 | ppl     9.40 | acc     0.75 | train_ae_norm     1.00\n",
      "[126/200][2199/4361] Loss_D: 0.00485958 (Loss_D_real: 0.00380283 Loss_D_fake: 0.00105675) Loss_G: 0.46811318 Loss_Enh_Dec: -2.08476830\n",
      "| epoch 126 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  2.23 | ppl     9.32 | acc     0.74 | train_ae_norm     1.00\n",
      "[126/200][2299/4361] Loss_D: 0.04685049 (Loss_D_real: 0.04625916 Loss_D_fake: 0.00059133) Loss_G: 0.84369099 Loss_Enh_Dec: -2.27824140\n",
      "| epoch 126 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.64 | loss  2.25 | ppl     9.46 | acc     0.73 | train_ae_norm     1.00\n",
      "[126/200][2399/4361] Loss_D: 0.00288978 (Loss_D_real: 0.00199622 Loss_D_fake: 0.00089357) Loss_G: 0.44400978 Loss_Enh_Dec: -2.25980139\n",
      "| epoch 126 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  2.24 | ppl     9.39 | acc     0.68 | train_ae_norm     1.00\n",
      "[126/200][2499/4361] Loss_D: 0.00208916 (Loss_D_real: 0.00042784 Loss_D_fake: 0.00166132) Loss_G: 0.43526611 Loss_Enh_Dec: -2.43858457\n",
      "| epoch 126 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.99 | loss  2.33 | ppl    10.27 | acc     0.74 | train_ae_norm     1.00\n",
      "[126/200][2599/4361] Loss_D: 0.00296616 (Loss_D_real: 0.00117482 Loss_D_fake: 0.00179135) Loss_G: 0.46505609 Loss_Enh_Dec: -2.08103013\n",
      "| epoch 126 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  2.29 | ppl     9.89 | acc     0.68 | train_ae_norm     1.00\n",
      "[126/200][2699/4361] Loss_D: 0.00173621 (Loss_D_real: 0.00100625 Loss_D_fake: 0.00072996) Loss_G: 0.42612416 Loss_Enh_Dec: -2.47271585\n",
      "| epoch 126 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  2.26 | ppl     9.56 | acc     0.72 | train_ae_norm     1.00\n",
      "[126/200][2799/4361] Loss_D: 0.00258555 (Loss_D_real: 0.00134691 Loss_D_fake: 0.00123864) Loss_G: 0.42113215 Loss_Enh_Dec: -2.32387590\n",
      "| epoch 126 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  2.22 | ppl     9.20 | acc     0.73 | train_ae_norm     1.00\n",
      "[126/200][2899/4361] Loss_D: 0.00509760 (Loss_D_real: 0.00009198 Loss_D_fake: 0.00500562) Loss_G: 0.37859741 Loss_Enh_Dec: -2.16993093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 126 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.39 | loss  2.27 | ppl     9.65 | acc     0.74 | train_ae_norm     1.00\n",
      "[126/200][2999/4361] Loss_D: 0.00254147 (Loss_D_real: 0.00115740 Loss_D_fake: 0.00138407) Loss_G: 0.44246131 Loss_Enh_Dec: -1.91836393\n",
      "| epoch 126 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.75 | loss  2.27 | ppl     9.73 | acc     0.73 | train_ae_norm     1.00\n",
      "[126/200][3099/4361] Loss_D: 0.00082182 (Loss_D_real: 0.00014170 Loss_D_fake: 0.00068013) Loss_G: 0.40276098 Loss_Enh_Dec: -2.10287738\n",
      "| epoch 126 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.27 | ppl     9.69 | acc     0.72 | train_ae_norm     1.00\n",
      "[126/200][3199/4361] Loss_D: 0.00116202 (Loss_D_real: 0.00038598 Loss_D_fake: 0.00077604) Loss_G: 0.40946442 Loss_Enh_Dec: -2.27199221\n",
      "| epoch 126 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.03 | loss  2.27 | ppl     9.63 | acc     0.75 | train_ae_norm     1.00\n",
      "[126/200][3299/4361] Loss_D: 0.00382220 (Loss_D_real: 0.00312475 Loss_D_fake: 0.00069744) Loss_G: 0.45758867 Loss_Enh_Dec: -2.16739821\n",
      "| epoch 126 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  2.27 | ppl     9.68 | acc     0.72 | train_ae_norm     1.00\n",
      "[126/200][3399/4361] Loss_D: 0.00171342 (Loss_D_real: 0.00034285 Loss_D_fake: 0.00137057) Loss_G: 0.40030441 Loss_Enh_Dec: -2.18346381\n",
      "| epoch 126 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.11 | loss  2.29 | ppl     9.91 | acc     0.69 | train_ae_norm     1.00\n",
      "[126/200][3499/4361] Loss_D: 0.00079656 (Loss_D_real: 0.00014805 Loss_D_fake: 0.00064851) Loss_G: 0.42786342 Loss_Enh_Dec: -2.60327578\n",
      "| epoch 126 |  3500/ 4361 batches | lr 0.000000 | ms/batch 400.48 | loss  2.28 | ppl     9.78 | acc     0.72 | train_ae_norm     1.00\n",
      "[126/200][3599/4361] Loss_D: 0.00625139 (Loss_D_real: 0.00552360 Loss_D_fake: 0.00072779) Loss_G: 0.47532687 Loss_Enh_Dec: -2.49450660\n",
      "| epoch 126 |  3600/ 4361 batches | lr 0.000000 | ms/batch 400.93 | loss  2.27 | ppl     9.71 | acc     0.75 | train_ae_norm     1.00\n",
      "[126/200][3699/4361] Loss_D: 0.00124359 (Loss_D_real: 0.00004050 Loss_D_fake: 0.00120309) Loss_G: 0.41644597 Loss_Enh_Dec: -2.25031543\n",
      "| epoch 126 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.84 | loss  2.25 | ppl     9.48 | acc     0.71 | train_ae_norm     1.00\n",
      "[126/200][3799/4361] Loss_D: 0.00221247 (Loss_D_real: 0.00074661 Loss_D_fake: 0.00146585) Loss_G: 0.41643006 Loss_Enh_Dec: -2.22027946\n",
      "| epoch 126 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  2.25 | ppl     9.46 | acc     0.77 | train_ae_norm     1.00\n",
      "[126/200][3899/4361] Loss_D: 0.00092431 (Loss_D_real: 0.00043186 Loss_D_fake: 0.00049246) Loss_G: 0.52195495 Loss_Enh_Dec: -2.32916427\n",
      "| epoch 126 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  2.36 | ppl    10.57 | acc     0.70 | train_ae_norm     1.00\n",
      "[126/200][3999/4361] Loss_D: 0.00117045 (Loss_D_real: 0.00059658 Loss_D_fake: 0.00057387) Loss_G: 0.40386978 Loss_Enh_Dec: -2.26695299\n",
      "| epoch 126 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.60 | ppl    13.44 | acc     0.71 | train_ae_norm     1.00\n",
      "[126/200][4099/4361] Loss_D: 0.00393818 (Loss_D_real: 0.00092244 Loss_D_fake: 0.00301574) Loss_G: 0.39165142 Loss_Enh_Dec: -2.05257869\n",
      "| epoch 126 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.65 | loss  2.41 | ppl    11.15 | acc     0.70 | train_ae_norm     1.00\n",
      "| end of epoch 126 | time: 1852.40s | test loss  2.28 | test ppl  9.79 | acc 0.777\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 127 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.699\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.475\n",
      "  Test Loss: 4.513\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 127 |     0/ 4361 batches | lr 0.000000 | ms/batch 863.61 | loss  0.02 | ppl     1.02 | acc     0.77 | train_ae_norm     1.00\n",
      "[127/200][99/4361] Loss_D: 0.00150270 (Loss_D_real: 0.00039182 Loss_D_fake: 0.00111088) Loss_G: 0.39205751 Loss_Enh_Dec: -1.45364749\n",
      "| epoch 127 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  2.20 | ppl     9.02 | acc     0.70 | train_ae_norm     1.00\n",
      "[127/200][199/4361] Loss_D: 0.00421266 (Loss_D_real: 0.00340711 Loss_D_fake: 0.00080555) Loss_G: 0.80679321 Loss_Enh_Dec: -1.37100220\n",
      "| epoch 127 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  2.23 | ppl     9.33 | acc     0.75 | train_ae_norm     1.00\n",
      "[127/200][299/4361] Loss_D: 0.00365568 (Loss_D_real: 0.00293759 Loss_D_fake: 0.00071808) Loss_G: 0.47764817 Loss_Enh_Dec: -1.96254766\n",
      "| epoch 127 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.25 | ppl     9.53 | acc     0.69 | train_ae_norm     1.00\n",
      "[127/200][399/4361] Loss_D: 0.00351901 (Loss_D_real: 0.00030648 Loss_D_fake: 0.00321253) Loss_G: 0.38027659 Loss_Enh_Dec: -2.13848877\n",
      "| epoch 127 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.56 | loss  2.19 | ppl     8.91 | acc     0.72 | train_ae_norm     1.00\n",
      "[127/200][499/4361] Loss_D: 0.01023482 (Loss_D_real: 0.00534900 Loss_D_fake: 0.00488583) Loss_G: 0.41788217 Loss_Enh_Dec: -2.24481177\n",
      "| epoch 127 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  2.25 | ppl     9.44 | acc     0.74 | train_ae_norm     1.00\n",
      "[127/200][599/4361] Loss_D: 0.01248684 (Loss_D_real: 0.00345883 Loss_D_fake: 0.00902801) Loss_G: 0.42692715 Loss_Enh_Dec: -2.27194333\n",
      "| epoch 127 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.17 | ppl     8.75 | acc     0.71 | train_ae_norm     1.00\n",
      "[127/200][699/4361] Loss_D: 0.00211111 (Loss_D_real: 0.00026507 Loss_D_fake: 0.00184604) Loss_G: 0.46670181 Loss_Enh_Dec: -1.88420093\n",
      "| epoch 127 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.29 | loss  2.21 | ppl     9.15 | acc     0.74 | train_ae_norm     1.00\n",
      "[127/200][799/4361] Loss_D: 0.00297529 (Loss_D_real: 0.00099860 Loss_D_fake: 0.00197669) Loss_G: 0.40362135 Loss_Enh_Dec: -2.33890367\n",
      "| epoch 127 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.64 | loss  2.20 | ppl     9.00 | acc     0.73 | train_ae_norm     1.00\n",
      "[127/200][899/4361] Loss_D: 0.00086336 (Loss_D_real: 0.00052876 Loss_D_fake: 0.00033460) Loss_G: 0.52806228 Loss_Enh_Dec: -2.38135433\n",
      "| epoch 127 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.54 | loss  2.22 | ppl     9.20 | acc     0.77 | train_ae_norm     1.00\n",
      "[127/200][999/4361] Loss_D: 0.00202721 (Loss_D_real: 0.00036656 Loss_D_fake: 0.00166064) Loss_G: 0.43196711 Loss_Enh_Dec: -2.37647247\n",
      "| epoch 127 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  2.20 | ppl     9.01 | acc     0.73 | train_ae_norm     1.00\n",
      "[127/200][1099/4361] Loss_D: 0.00315675 (Loss_D_real: 0.00021813 Loss_D_fake: 0.00293863) Loss_G: 0.36958227 Loss_Enh_Dec: -1.77346420\n",
      "| epoch 127 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  2.19 | ppl     8.90 | acc     0.71 | train_ae_norm     1.00\n",
      "[127/200][1199/4361] Loss_D: 0.00236888 (Loss_D_real: 0.00087354 Loss_D_fake: 0.00149534) Loss_G: 0.40642872 Loss_Enh_Dec: -2.60356975\n",
      "| epoch 127 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.86 | loss  2.19 | ppl     8.90 | acc     0.77 | train_ae_norm     1.00\n",
      "[127/200][1299/4361] Loss_D: 0.00439689 (Loss_D_real: 0.00394036 Loss_D_fake: 0.00045653) Loss_G: 0.54393476 Loss_Enh_Dec: -2.12525773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 127 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  2.23 | ppl     9.26 | acc     0.72 | train_ae_norm     1.00\n",
      "[127/200][1399/4361] Loss_D: 0.00172042 (Loss_D_real: 0.00035158 Loss_D_fake: 0.00136884) Loss_G: 0.38489199 Loss_Enh_Dec: -2.23068666\n",
      "| epoch 127 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  2.19 | ppl     8.92 | acc     0.72 | train_ae_norm     1.00\n",
      "[127/200][1499/4361] Loss_D: 0.00459782 (Loss_D_real: 0.00338017 Loss_D_fake: 0.00121765) Loss_G: 0.38271165 Loss_Enh_Dec: -2.45503592\n",
      "| epoch 127 |  1500/ 4361 batches | lr 0.000000 | ms/batch 400.88 | loss  2.26 | ppl     9.62 | acc     0.73 | train_ae_norm     1.00\n",
      "[127/200][1599/4361] Loss_D: 0.00124117 (Loss_D_real: 0.00025925 Loss_D_fake: 0.00098192) Loss_G: 0.43564969 Loss_Enh_Dec: -2.23235893\n",
      "| epoch 127 |  1600/ 4361 batches | lr 0.000000 | ms/batch 400.27 | loss  2.22 | ppl     9.16 | acc     0.73 | train_ae_norm     1.00\n",
      "[127/200][1699/4361] Loss_D: 0.00346506 (Loss_D_real: 0.00102529 Loss_D_fake: 0.00243977) Loss_G: 0.44511887 Loss_Enh_Dec: -2.33298731\n",
      "| epoch 127 |  1700/ 4361 batches | lr 0.000000 | ms/batch 400.56 | loss  2.18 | ppl     8.81 | acc     0.69 | train_ae_norm     1.00\n",
      "[127/200][1799/4361] Loss_D: 0.00269267 (Loss_D_real: 0.00178357 Loss_D_fake: 0.00090911) Loss_G: 0.47820482 Loss_Enh_Dec: -2.25992274\n",
      "| epoch 127 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  2.20 | ppl     9.00 | acc     0.74 | train_ae_norm     1.00\n",
      "[127/200][1899/4361] Loss_D: 0.00716185 (Loss_D_real: 0.00669594 Loss_D_fake: 0.00046591) Loss_G: 0.64032477 Loss_Enh_Dec: -2.36385942\n",
      "| epoch 127 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.90 | loss  2.23 | ppl     9.27 | acc     0.74 | train_ae_norm     1.00\n",
      "[127/200][2199/4361] Loss_D: 0.00046846 (Loss_D_real: 0.00009819 Loss_D_fake: 0.00037027) Loss_G: 0.49702102 Loss_Enh_Dec: -2.29082179\n",
      "| epoch 127 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  2.25 | ppl     9.45 | acc     0.72 | train_ae_norm     1.00\n",
      "[127/200][2299/4361] Loss_D: 0.01297400 (Loss_D_real: 0.01090657 Loss_D_fake: 0.00206743) Loss_G: 0.40394995 Loss_Enh_Dec: -2.58373618\n",
      "| epoch 127 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.25 | ppl     9.51 | acc     0.73 | train_ae_norm     1.00\n",
      "[127/200][2399/4361] Loss_D: 0.00702369 (Loss_D_real: 0.00501231 Loss_D_fake: 0.00201139) Loss_G: 0.46346876 Loss_Enh_Dec: -2.15195584\n",
      "| epoch 127 |  2400/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  2.23 | ppl     9.33 | acc     0.68 | train_ae_norm     1.00\n",
      "[127/200][2499/4361] Loss_D: 0.00081914 (Loss_D_real: 0.00024109 Loss_D_fake: 0.00057805) Loss_G: 0.46570849 Loss_Enh_Dec: -2.48228526\n",
      "| epoch 127 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  2.24 | ppl     9.39 | acc     0.72 | train_ae_norm     1.00\n",
      "[127/200][2599/4361] Loss_D: 0.00196545 (Loss_D_real: 0.00093631 Loss_D_fake: 0.00102915) Loss_G: 0.41999266 Loss_Enh_Dec: -2.28695273\n",
      "| epoch 127 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.51 | loss  2.22 | ppl     9.22 | acc     0.70 | train_ae_norm     1.00\n",
      "[127/200][2699/4361] Loss_D: 0.00248731 (Loss_D_real: 0.00188051 Loss_D_fake: 0.00060680) Loss_G: 0.40017691 Loss_Enh_Dec: -2.56853771\n",
      "| epoch 127 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  2.19 | ppl     8.92 | acc     0.72 | train_ae_norm     1.00\n",
      "[127/200][2799/4361] Loss_D: 0.02315809 (Loss_D_real: 0.02222055 Loss_D_fake: 0.00093754) Loss_G: 0.39378819 Loss_Enh_Dec: -2.54043937\n",
      "| epoch 127 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.65 | loss  2.15 | ppl     8.59 | acc     0.73 | train_ae_norm     1.00\n",
      "[127/200][2899/4361] Loss_D: 0.00303628 (Loss_D_real: 0.00236106 Loss_D_fake: 0.00067522) Loss_G: 0.43774691 Loss_Enh_Dec: -2.46659851\n",
      "| epoch 127 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.69 | loss  2.17 | ppl     8.74 | acc     0.75 | train_ae_norm     1.00\n",
      "[127/200][2999/4361] Loss_D: 0.00137108 (Loss_D_real: 0.00045576 Loss_D_fake: 0.00091532) Loss_G: 0.55748886 Loss_Enh_Dec: -2.58859873\n",
      "| epoch 127 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  2.21 | ppl     9.16 | acc     0.71 | train_ae_norm     1.00\n",
      "[127/200][3099/4361] Loss_D: 0.00110512 (Loss_D_real: 0.00040694 Loss_D_fake: 0.00069818) Loss_G: 0.65956855 Loss_Enh_Dec: -2.50596309\n",
      "| epoch 127 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  2.22 | ppl     9.20 | acc     0.74 | train_ae_norm     1.00\n",
      "[127/200][3199/4361] Loss_D: 0.00466128 (Loss_D_real: 0.00307726 Loss_D_fake: 0.00158402) Loss_G: 0.51371855 Loss_Enh_Dec: -2.34125638\n",
      "| epoch 127 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  2.21 | ppl     9.14 | acc     0.75 | train_ae_norm     1.00\n",
      "[127/200][3299/4361] Loss_D: 0.00209111 (Loss_D_real: 0.00118790 Loss_D_fake: 0.00090321) Loss_G: 0.49283496 Loss_Enh_Dec: -2.44097948\n",
      "| epoch 127 |  3300/ 4361 batches | lr 0.000000 | ms/batch 400.60 | loss  2.21 | ppl     9.10 | acc     0.73 | train_ae_norm     1.00\n",
      "[127/200][3399/4361] Loss_D: 0.00473221 (Loss_D_real: 0.00268918 Loss_D_fake: 0.00204303) Loss_G: 0.44328982 Loss_Enh_Dec: -2.23190188\n",
      "| epoch 127 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.57 | loss  2.23 | ppl     9.26 | acc     0.71 | train_ae_norm     1.00\n",
      "[127/200][3499/4361] Loss_D: 0.02003877 (Loss_D_real: 0.01989401 Loss_D_fake: 0.00014477) Loss_G: 0.51426601 Loss_Enh_Dec: -2.20541644\n",
      "| epoch 127 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.94 | loss  2.15 | ppl     8.60 | acc     0.73 | train_ae_norm     1.00\n",
      "[127/200][3599/4361] Loss_D: 0.00996653 (Loss_D_real: 0.00926371 Loss_D_fake: 0.00070281) Loss_G: 0.73180658 Loss_Enh_Dec: -2.00012279\n",
      "| epoch 127 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  2.17 | ppl     8.76 | acc     0.74 | train_ae_norm     1.00\n",
      "[127/200][3699/4361] Loss_D: 0.00438333 (Loss_D_real: 0.00348511 Loss_D_fake: 0.00089822) Loss_G: 0.47383076 Loss_Enh_Dec: -2.33127499\n",
      "| epoch 127 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.17 | ppl     8.73 | acc     0.74 | train_ae_norm     1.00\n",
      "[127/200][3799/4361] Loss_D: 0.00169620 (Loss_D_real: 0.00131780 Loss_D_fake: 0.00037841) Loss_G: 0.42275944 Loss_Enh_Dec: -1.94012070\n",
      "| epoch 127 |  3800/ 4361 batches | lr 0.000000 | ms/batch 400.47 | loss  2.16 | ppl     8.71 | acc     0.78 | train_ae_norm     1.00\n",
      "[127/200][3899/4361] Loss_D: 0.00329162 (Loss_D_real: 0.00065090 Loss_D_fake: 0.00264072) Loss_G: 0.46913868 Loss_Enh_Dec: -1.89108849\n",
      "| epoch 127 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.04 | loss  2.17 | ppl     8.72 | acc     0.73 | train_ae_norm     1.00\n",
      "[127/200][3999/4361] Loss_D: 0.00268455 (Loss_D_real: 0.00026827 Loss_D_fake: 0.00241628) Loss_G: 0.38143536 Loss_Enh_Dec: -1.74166811\n",
      "| epoch 127 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  2.16 | ppl     8.71 | acc     0.75 | train_ae_norm     1.00\n",
      "[127/200][4099/4361] Loss_D: 0.00821253 (Loss_D_real: 0.00570561 Loss_D_fake: 0.00250692) Loss_G: 0.37896690 Loss_Enh_Dec: -1.98890769\n",
      "| epoch 127 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  2.12 | ppl     8.31 | acc     0.76 | train_ae_norm     1.00\n",
      "[127/200][4199/4361] Loss_D: 0.01971913 (Loss_D_real: 0.01721320 Loss_D_fake: 0.00250592) Loss_G: 0.43396378 Loss_Enh_Dec: -2.16363001\n",
      "| epoch 127 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  2.18 | ppl     8.89 | acc     0.76 | train_ae_norm     1.00\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.696\n",
      "  Average training loss discriminator: 0.698\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.487\n",
      "  Test Loss: 4.593\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 128 |     0/ 4361 batches | lr 0.000000 | ms/batch 864.40 | loss  0.02 | ppl     1.02 | acc     0.77 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128/200][99/4361] Loss_D: 0.00541895 (Loss_D_real: 0.00036525 Loss_D_fake: 0.00505370) Loss_G: 0.74300855 Loss_Enh_Dec: -2.25772667\n",
      "| epoch 128 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.15 | ppl     8.58 | acc     0.72 | train_ae_norm     1.00\n",
      "[128/200][199/4361] Loss_D: 0.00672172 (Loss_D_real: 0.00620294 Loss_D_fake: 0.00051878) Loss_G: 0.46993238 Loss_Enh_Dec: -1.47687411\n",
      "| epoch 128 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  2.18 | ppl     8.86 | acc     0.75 | train_ae_norm     1.00\n",
      "[128/200][299/4361] Loss_D: 0.00329110 (Loss_D_real: 0.00221271 Loss_D_fake: 0.00107839) Loss_G: 0.46369371 Loss_Enh_Dec: -2.53351355\n",
      "| epoch 128 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.18 | ppl     8.83 | acc     0.69 | train_ae_norm     1.00\n",
      "[128/200][399/4361] Loss_D: 0.00107170 (Loss_D_real: 0.00043562 Loss_D_fake: 0.00063608) Loss_G: 0.43421790 Loss_Enh_Dec: -2.24746013\n",
      "| epoch 128 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.46 | loss  2.12 | ppl     8.32 | acc     0.74 | train_ae_norm     1.00\n",
      "[128/200][499/4361] Loss_D: 0.00193414 (Loss_D_real: 0.00131896 Loss_D_fake: 0.00061518) Loss_G: 0.46390960 Loss_Enh_Dec: -2.02860641\n",
      "| epoch 128 |   500/ 4361 batches | lr 0.000000 | ms/batch 400.95 | loss  2.18 | ppl     8.84 | acc     0.75 | train_ae_norm     1.00\n",
      "[128/200][599/4361] Loss_D: 0.01295833 (Loss_D_real: 0.00023621 Loss_D_fake: 0.01272212) Loss_G: 0.48063573 Loss_Enh_Dec: -1.78130305\n",
      "| epoch 128 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.30 | loss  2.16 | ppl     8.71 | acc     0.68 | train_ae_norm     1.00\n",
      "[128/200][699/4361] Loss_D: 0.00096265 (Loss_D_real: 0.00009777 Loss_D_fake: 0.00086488) Loss_G: 0.43482947 Loss_Enh_Dec: -1.78470445\n",
      "| epoch 128 |   700/ 4361 batches | lr 0.000000 | ms/batch 400.22 | loss  2.19 | ppl     8.94 | acc     0.74 | train_ae_norm     1.00\n",
      "[128/200][799/4361] Loss_D: 0.00302007 (Loss_D_real: 0.00095922 Loss_D_fake: 0.00206085) Loss_G: 0.35560799 Loss_Enh_Dec: -2.65357184\n",
      "| epoch 128 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  2.18 | ppl     8.85 | acc     0.73 | train_ae_norm     1.00\n",
      "[128/200][899/4361] Loss_D: 0.00122713 (Loss_D_real: 0.00028841 Loss_D_fake: 0.00093872) Loss_G: 0.45597431 Loss_Enh_Dec: -2.46508479\n",
      "| epoch 128 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  2.19 | ppl     8.95 | acc     0.73 | train_ae_norm     1.00\n",
      "[128/200][999/4361] Loss_D: 0.00092937 (Loss_D_real: 0.00014608 Loss_D_fake: 0.00078329) Loss_G: 0.41981822 Loss_Enh_Dec: -2.75589132\n",
      "| epoch 128 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  2.16 | ppl     8.66 | acc     0.73 | train_ae_norm     1.00\n",
      "[128/200][1099/4361] Loss_D: 0.00640464 (Loss_D_real: 0.00557038 Loss_D_fake: 0.00083426) Loss_G: 0.38676620 Loss_Enh_Dec: -2.76001716\n",
      "| epoch 128 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.86 | loss  2.14 | ppl     8.47 | acc     0.71 | train_ae_norm     1.00\n",
      "[128/200][1199/4361] Loss_D: 0.06147140 (Loss_D_real: 0.06097087 Loss_D_fake: 0.00050053) Loss_G: 0.78219980 Loss_Enh_Dec: -2.51558065\n",
      "| epoch 128 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  2.15 | ppl     8.57 | acc     0.78 | train_ae_norm     1.00\n",
      "[128/200][1299/4361] Loss_D: 0.00358593 (Loss_D_real: 0.00038369 Loss_D_fake: 0.00320225) Loss_G: 0.41012689 Loss_Enh_Dec: -2.73751283\n",
      "| epoch 128 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.66 | loss  2.19 | ppl     8.94 | acc     0.73 | train_ae_norm     1.00\n",
      "[128/200][1399/4361] Loss_D: 0.00167047 (Loss_D_real: 0.00038821 Loss_D_fake: 0.00128226) Loss_G: 0.40671730 Loss_Enh_Dec: -2.48217511\n",
      "| epoch 128 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  2.19 | ppl     8.93 | acc     0.70 | train_ae_norm     1.00\n",
      "[128/200][1499/4361] Loss_D: 0.00085596 (Loss_D_real: 0.00057285 Loss_D_fake: 0.00028311) Loss_G: 0.46381614 Loss_Enh_Dec: -2.55069041\n",
      "| epoch 128 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  2.24 | ppl     9.41 | acc     0.70 | train_ae_norm     1.00\n",
      "[128/200][1599/4361] Loss_D: 0.00127025 (Loss_D_real: 0.00065352 Loss_D_fake: 0.00061673) Loss_G: 0.45597291 Loss_Enh_Dec: -2.79884410\n",
      "| epoch 128 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  2.21 | ppl     9.13 | acc     0.74 | train_ae_norm     1.00\n",
      "[128/200][1699/4361] Loss_D: 0.00202490 (Loss_D_real: 0.00021179 Loss_D_fake: 0.00181310) Loss_G: 0.40153351 Loss_Enh_Dec: -2.60262370\n",
      "| epoch 128 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  2.16 | ppl     8.69 | acc     0.71 | train_ae_norm     1.00\n",
      "[128/200][1799/4361] Loss_D: 0.00133719 (Loss_D_real: 0.00016533 Loss_D_fake: 0.00117186) Loss_G: 0.44744354 Loss_Enh_Dec: -2.72948980\n",
      "| epoch 128 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  2.17 | ppl     8.80 | acc     0.75 | train_ae_norm     1.00\n",
      "[128/200][1899/4361] Loss_D: 0.00186760 (Loss_D_real: 0.00116555 Loss_D_fake: 0.00070205) Loss_G: 0.91418803 Loss_Enh_Dec: -2.52050686\n",
      "| epoch 128 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  2.23 | ppl     9.30 | acc     0.72 | train_ae_norm     1.00\n",
      "[128/200][1999/4361] Loss_D: 0.00271603 (Loss_D_real: 0.00229787 Loss_D_fake: 0.00041816) Loss_G: 0.45422503 Loss_Enh_Dec: -2.33556628\n",
      "| epoch 128 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  2.17 | ppl     8.74 | acc     0.77 | train_ae_norm     1.00\n",
      "[128/200][2299/4361] Loss_D: 0.05396647 (Loss_D_real: 0.05207143 Loss_D_fake: 0.00189504) Loss_G: 0.46102768 Loss_Enh_Dec: -2.35529780\n",
      "| epoch 128 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  2.15 | ppl     8.60 | acc     0.77 | train_ae_norm     1.00\n",
      "[128/200][2399/4361] Loss_D: 0.00050429 (Loss_D_real: 0.00013322 Loss_D_fake: 0.00037106) Loss_G: 0.50940186 Loss_Enh_Dec: -2.68250418\n",
      "| epoch 128 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  2.16 | ppl     8.70 | acc     0.72 | train_ae_norm     1.00\n",
      "[128/200][2499/4361] Loss_D: 0.01022476 (Loss_D_real: 0.00814079 Loss_D_fake: 0.00208398) Loss_G: 0.38208798 Loss_Enh_Dec: -2.64599586\n",
      "| epoch 128 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  2.36 | ppl    10.57 | acc     0.68 | train_ae_norm     1.00\n",
      "[128/200][2599/4361] Loss_D: 0.00104566 (Loss_D_real: 0.00023344 Loss_D_fake: 0.00081222) Loss_G: 0.43403935 Loss_Enh_Dec: -2.09409308\n",
      "| epoch 128 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.30 | ppl    10.01 | acc     0.70 | train_ae_norm     1.00\n",
      "[128/200][2699/4361] Loss_D: 0.00354313 (Loss_D_real: 0.00112834 Loss_D_fake: 0.00241479) Loss_G: 0.71204585 Loss_Enh_Dec: -2.23210382\n",
      "| epoch 128 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  2.21 | ppl     9.09 | acc     0.73 | train_ae_norm     1.00\n",
      "[128/200][2799/4361] Loss_D: 0.00273789 (Loss_D_real: 0.00056297 Loss_D_fake: 0.00217492) Loss_G: 0.46867037 Loss_Enh_Dec: -2.13416362\n",
      "| epoch 128 |  2800/ 4361 batches | lr 0.000000 | ms/batch 400.92 | loss  2.49 | ppl    12.08 | acc     0.54 | train_ae_norm     1.00\n",
      "[128/200][2899/4361] Loss_D: 0.00242222 (Loss_D_real: 0.00155518 Loss_D_fake: 0.00086704) Loss_G: 0.45055065 Loss_Enh_Dec: -2.60127020\n",
      "| epoch 128 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.26 | ppl     9.57 | acc     0.74 | train_ae_norm     1.00\n",
      "[128/200][2999/4361] Loss_D: 0.00034459 (Loss_D_real: 0.00019419 Loss_D_fake: 0.00015040) Loss_G: 0.45319209 Loss_Enh_Dec: -2.21975470\n",
      "| epoch 128 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.18 | ppl     8.86 | acc     0.72 | train_ae_norm     1.00\n",
      "[128/200][3099/4361] Loss_D: 0.00127063 (Loss_D_real: 0.00048895 Loss_D_fake: 0.00078169) Loss_G: 0.46440768 Loss_Enh_Dec: -2.04647136\n",
      "| epoch 128 |  3100/ 4361 batches | lr 0.000000 | ms/batch 402.07 | loss  2.18 | ppl     8.82 | acc     0.74 | train_ae_norm     1.00\n",
      "[128/200][3199/4361] Loss_D: 0.00526992 (Loss_D_real: 0.00410444 Loss_D_fake: 0.00116548) Loss_G: 0.51805717 Loss_Enh_Dec: -2.22328162\n",
      "| epoch 128 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  2.19 | ppl     8.89 | acc     0.76 | train_ae_norm     1.00\n",
      "[128/200][3299/4361] Loss_D: 0.00201675 (Loss_D_real: 0.00040198 Loss_D_fake: 0.00161477) Loss_G: 0.45443621 Loss_Enh_Dec: -2.35277867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 128 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.08 | loss  2.19 | ppl     8.96 | acc     0.74 | train_ae_norm     1.00\n",
      "[128/200][3399/4361] Loss_D: 0.00434334 (Loss_D_real: 0.00165461 Loss_D_fake: 0.00268873) Loss_G: 0.39646339 Loss_Enh_Dec: -2.44580817\n",
      "| epoch 128 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.79 | loss  2.19 | ppl     8.96 | acc     0.72 | train_ae_norm     1.00\n",
      "[128/200][3499/4361] Loss_D: 0.00124870 (Loss_D_real: 0.00044747 Loss_D_fake: 0.00080123) Loss_G: 0.46803814 Loss_Enh_Dec: -2.06790280\n",
      "| epoch 128 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  2.14 | ppl     8.50 | acc     0.72 | train_ae_norm     1.00\n",
      "[128/200][3599/4361] Loss_D: 0.00479614 (Loss_D_real: 0.00043347 Loss_D_fake: 0.00436267) Loss_G: 0.37846261 Loss_Enh_Dec: -2.01093030\n",
      "| epoch 128 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  2.19 | ppl     8.97 | acc     0.75 | train_ae_norm     1.00\n",
      "[128/200][3699/4361] Loss_D: 0.00545905 (Loss_D_real: 0.00019864 Loss_D_fake: 0.00526041) Loss_G: 0.41987363 Loss_Enh_Dec: -2.25362849\n",
      "| epoch 128 |  3700/ 4361 batches | lr 0.000000 | ms/batch 402.05 | loss  2.19 | ppl     8.95 | acc     0.70 | train_ae_norm     1.00\n",
      "[128/200][3799/4361] Loss_D: 0.00206136 (Loss_D_real: 0.00063106 Loss_D_fake: 0.00143029) Loss_G: 0.41880581 Loss_Enh_Dec: -2.38647127\n",
      "| epoch 128 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  2.20 | ppl     9.04 | acc     0.78 | train_ae_norm     1.00\n",
      "[128/200][3899/4361] Loss_D: 0.00542217 (Loss_D_real: 0.00047808 Loss_D_fake: 0.00494409) Loss_G: 0.40550706 Loss_Enh_Dec: -2.49086308\n",
      "| epoch 128 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.09 | loss  2.22 | ppl     9.19 | acc     0.73 | train_ae_norm     1.00\n",
      "[128/200][3999/4361] Loss_D: 0.00131914 (Loss_D_real: 0.00045368 Loss_D_fake: 0.00086546) Loss_G: 0.44944867 Loss_Enh_Dec: -2.63935256\n",
      "| epoch 128 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  2.20 | ppl     8.98 | acc     0.74 | train_ae_norm     1.00\n",
      "[128/200][4099/4361] Loss_D: 0.00119638 (Loss_D_real: 0.00035538 Loss_D_fake: 0.00084101) Loss_G: 0.34800860 Loss_Enh_Dec: -2.48295784\n",
      "| epoch 128 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  2.18 | ppl     8.83 | acc     0.75 | train_ae_norm     1.00\n",
      "[128/200][4199/4361] Loss_D: 0.00185987 (Loss_D_real: 0.00044676 Loss_D_fake: 0.00141310) Loss_G: 0.40804091 Loss_Enh_Dec: -2.41596365\n",
      "| epoch 128 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.84 | loss  2.20 | ppl     9.04 | acc     0.76 | train_ae_norm     1.00\n",
      "[128/200][4299/4361] Loss_D: 0.00101836 (Loss_D_real: 0.00030588 Loss_D_fake: 0.00071249) Loss_G: 0.43463030 Loss_Enh_Dec: -2.36097479\n",
      "| epoch 128 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.18 | ppl     8.85 | acc     0.74 | train_ae_norm     1.00\n",
      "| end of epoch 128 | time: 1851.93s | test loss  2.30 | test ppl 10.00 | acc 0.776\n",
      "bleu_self:  [3.67155722e-01 1.38608652e-01 1.27418719e-06 4.37233299e-09\n",
      " 5.11730682e-09]\n",
      "bleu_test:  [7.96875000e-01 1.04166675e-01 8.46481380e-07 2.70104821e-09\n",
      " 4.44035940e-10]\n",
      "bleu_self: [0.36715572,0.13860865,0.00000127,0.00000000,0.00000001]\n",
      "bleu_test: [0.79687500,0.10416668,0.00000085,0.00000000,0.00000000]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 129 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.697\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.490\n",
      "  Test Loss: 4.595\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 129 |     0/ 4361 batches | lr 0.000000 | ms/batch 863.93 | loss  0.02 | ppl     1.02 | acc     0.77 | train_ae_norm     1.00\n",
      "[129/200][99/4361] Loss_D: 0.00191354 (Loss_D_real: 0.00133523 Loss_D_fake: 0.00057831) Loss_G: 0.46201840 Loss_Enh_Dec: -2.53962684\n",
      "| epoch 129 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  2.20 | ppl     8.99 | acc     0.67 | train_ae_norm     1.00\n",
      "[129/200][199/4361] Loss_D: 0.00137502 (Loss_D_real: 0.00055692 Loss_D_fake: 0.00081810) Loss_G: 0.38747680 Loss_Enh_Dec: -2.50435495\n",
      "| epoch 129 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  2.23 | ppl     9.27 | acc     0.74 | train_ae_norm     1.00\n",
      "[129/200][299/4361] Loss_D: 0.00152057 (Loss_D_real: 0.00102650 Loss_D_fake: 0.00049407) Loss_G: 0.44865251 Loss_Enh_Dec: -2.85852742\n",
      "| epoch 129 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.07 | loss  2.24 | ppl     9.41 | acc     0.69 | train_ae_norm     1.00\n",
      "[129/200][399/4361] Loss_D: 0.00296023 (Loss_D_real: 0.00020233 Loss_D_fake: 0.00275790) Loss_G: 0.41989303 Loss_Enh_Dec: -2.79304314\n",
      "| epoch 129 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  2.15 | ppl     8.62 | acc     0.72 | train_ae_norm     1.00\n",
      "[129/200][499/4361] Loss_D: 0.00171701 (Loss_D_real: 0.00007791 Loss_D_fake: 0.00163910) Loss_G: 0.40270707 Loss_Enh_Dec: -2.51639843\n",
      "| epoch 129 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  2.24 | ppl     9.41 | acc     0.76 | train_ae_norm     1.00\n",
      "[129/200][599/4361] Loss_D: 0.02803505 (Loss_D_real: 0.02298748 Loss_D_fake: 0.00504757) Loss_G: 0.38522545 Loss_Enh_Dec: -2.69234967\n",
      "| epoch 129 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.18 | ppl     8.87 | acc     0.71 | train_ae_norm     1.00\n",
      "[129/200][699/4361] Loss_D: 0.00179010 (Loss_D_real: 0.00060173 Loss_D_fake: 0.00118837) Loss_G: 0.47356310 Loss_Enh_Dec: -2.52984071\n",
      "| epoch 129 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  2.20 | ppl     9.00 | acc     0.73 | train_ae_norm     1.00\n",
      "[129/200][799/4361] Loss_D: 0.00789139 (Loss_D_real: 0.00466438 Loss_D_fake: 0.00322701) Loss_G: 0.43659836 Loss_Enh_Dec: -2.54537082\n",
      "| epoch 129 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.71 | loss  2.18 | ppl     8.84 | acc     0.73 | train_ae_norm     1.00\n",
      "[129/200][899/4361] Loss_D: 0.00061602 (Loss_D_real: 0.00029273 Loss_D_fake: 0.00032328) Loss_G: 0.46394834 Loss_Enh_Dec: -2.47742057\n",
      "| epoch 129 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  2.20 | ppl     9.06 | acc     0.74 | train_ae_norm     1.00\n",
      "[129/200][999/4361] Loss_D: 0.00095059 (Loss_D_real: 0.00014872 Loss_D_fake: 0.00080187) Loss_G: 0.40628415 Loss_Enh_Dec: -2.39365315\n",
      "| epoch 129 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  2.20 | ppl     9.05 | acc     0.72 | train_ae_norm     1.00\n",
      "[129/200][1099/4361] Loss_D: 0.06653468 (Loss_D_real: 0.06447276 Loss_D_fake: 0.00206191) Loss_G: 0.41117078 Loss_Enh_Dec: -2.38912940\n",
      "| epoch 129 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.35 | loss  2.19 | ppl     8.98 | acc     0.70 | train_ae_norm     1.00\n",
      "[129/200][1199/4361] Loss_D: 0.00145407 (Loss_D_real: 0.00044425 Loss_D_fake: 0.00100982) Loss_G: 0.42739478 Loss_Enh_Dec: -1.71730828\n",
      "| epoch 129 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  2.22 | ppl     9.19 | acc     0.76 | train_ae_norm     1.00\n",
      "[129/200][1299/4361] Loss_D: 0.00104323 (Loss_D_real: 0.00042263 Loss_D_fake: 0.00062060) Loss_G: 0.49686313 Loss_Enh_Dec: -2.64265895\n",
      "| epoch 129 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  2.29 | ppl     9.92 | acc     0.74 | train_ae_norm     1.00\n",
      "[129/200][1399/4361] Loss_D: 0.00040490 (Loss_D_real: 0.00013096 Loss_D_fake: 0.00027393) Loss_G: 0.40937111 Loss_Enh_Dec: -2.15962982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 129 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.53 | loss  2.20 | ppl     9.01 | acc     0.68 | train_ae_norm     1.00\n",
      "[129/200][1499/4361] Loss_D: 0.00266448 (Loss_D_real: 0.00069454 Loss_D_fake: 0.00196994) Loss_G: 0.42610261 Loss_Enh_Dec: -2.32381821\n",
      "| epoch 129 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.27 | ppl     9.66 | acc     0.72 | train_ae_norm     1.00\n",
      "[129/200][1599/4361] Loss_D: 0.00221421 (Loss_D_real: 0.00072867 Loss_D_fake: 0.00148553) Loss_G: 0.43264005 Loss_Enh_Dec: -2.58060980\n",
      "| epoch 129 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  2.23 | ppl     9.34 | acc     0.70 | train_ae_norm     1.00\n",
      "[129/200][1699/4361] Loss_D: 0.00607176 (Loss_D_real: 0.00294603 Loss_D_fake: 0.00312573) Loss_G: 0.37674615 Loss_Enh_Dec: -2.65412903\n",
      "| epoch 129 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  2.18 | ppl     8.83 | acc     0.73 | train_ae_norm     1.00\n",
      "[129/200][1799/4361] Loss_D: 0.00304466 (Loss_D_real: 0.00016785 Loss_D_fake: 0.00287681) Loss_G: 0.43184158 Loss_Enh_Dec: -2.66654253\n",
      "| epoch 129 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  2.15 | ppl     8.61 | acc     0.75 | train_ae_norm     1.00\n",
      "[129/200][1899/4361] Loss_D: 0.00848243 (Loss_D_real: 0.00612616 Loss_D_fake: 0.00235627) Loss_G: 0.34551048 Loss_Enh_Dec: -2.52548909\n",
      "| epoch 129 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  2.22 | ppl     9.23 | acc     0.74 | train_ae_norm     1.00\n",
      "[129/200][1999/4361] Loss_D: 0.00061971 (Loss_D_real: 0.00005491 Loss_D_fake: 0.00056480) Loss_G: 0.45830917 Loss_Enh_Dec: -2.24392772\n",
      "| epoch 129 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.47 | loss  2.17 | ppl     8.75 | acc     0.75 | train_ae_norm     1.00\n",
      "[129/200][2099/4361] Loss_D: 0.00171722 (Loss_D_real: 0.00044192 Loss_D_fake: 0.00127530) Loss_G: 0.39254609 Loss_Enh_Dec: -2.30302215\n",
      "| epoch 129 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.65 | loss  2.21 | ppl     9.09 | acc     0.76 | train_ae_norm     1.00\n",
      "[129/200][2199/4361] Loss_D: 0.00830066 (Loss_D_real: 0.00097686 Loss_D_fake: 0.00732380) Loss_G: 0.35880554 Loss_Enh_Dec: -2.15030980\n",
      "| epoch 129 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  2.19 | ppl     8.94 | acc     0.73 | train_ae_norm     1.00\n",
      "[129/200][2299/4361] Loss_D: 0.00120768 (Loss_D_real: 0.00048668 Loss_D_fake: 0.00072100) Loss_G: 0.45691273 Loss_Enh_Dec: -2.11304212\n",
      "| epoch 129 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.15 | loss  2.18 | ppl     8.82 | acc     0.77 | train_ae_norm     1.00\n",
      "[129/200][2399/4361] Loss_D: 0.00442030 (Loss_D_real: 0.00210026 Loss_D_fake: 0.00232004) Loss_G: 0.36127034 Loss_Enh_Dec: -2.51893735\n",
      "| epoch 129 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  2.17 | ppl     8.73 | acc     0.70 | train_ae_norm     1.00\n",
      "[129/200][2499/4361] Loss_D: 0.00456267 (Loss_D_real: 0.00008017 Loss_D_fake: 0.00448249) Loss_G: 0.40845796 Loss_Enh_Dec: -2.41526771\n",
      "| epoch 129 |  2500/ 4361 batches | lr 0.000000 | ms/batch 400.91 | loss  2.21 | ppl     9.14 | acc     0.75 | train_ae_norm     1.00\n",
      "[129/200][2599/4361] Loss_D: 0.00279553 (Loss_D_real: 0.00047800 Loss_D_fake: 0.00231753) Loss_G: 0.43395457 Loss_Enh_Dec: -1.98848116\n",
      "| epoch 129 |  2600/ 4361 batches | lr 0.000000 | ms/batch 400.18 | loss  2.19 | ppl     8.95 | acc     0.71 | train_ae_norm     1.00\n",
      "[129/200][2699/4361] Loss_D: 0.00614543 (Loss_D_real: 0.00486552 Loss_D_fake: 0.00127990) Loss_G: 0.41427729 Loss_Enh_Dec: -1.95552528\n",
      "| epoch 129 |  2700/ 4361 batches | lr 0.000000 | ms/batch 399.85 | loss  2.21 | ppl     9.09 | acc     0.71 | train_ae_norm     1.00\n",
      "[129/200][2799/4361] Loss_D: 0.00570642 (Loss_D_real: 0.00520209 Loss_D_fake: 0.00050433) Loss_G: 0.35547692 Loss_Enh_Dec: -1.63672376\n",
      "| epoch 129 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  2.17 | ppl     8.75 | acc     0.70 | train_ae_norm     1.00\n",
      "[129/200][2899/4361] Loss_D: 0.00138391 (Loss_D_real: 0.00047745 Loss_D_fake: 0.00090646) Loss_G: 0.44603071 Loss_Enh_Dec: -2.08684421\n",
      "| epoch 129 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.19 | ppl     8.92 | acc     0.73 | train_ae_norm     1.00\n",
      "[129/200][2999/4361] Loss_D: 0.02471705 (Loss_D_real: 0.00544022 Loss_D_fake: 0.01927683) Loss_G: 0.25261545 Loss_Enh_Dec: -2.21662951\n",
      "| epoch 129 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.56 | loss  2.23 | ppl     9.33 | acc     0.73 | train_ae_norm     1.00\n",
      "[129/200][3099/4361] Loss_D: 0.01133219 (Loss_D_real: 0.00050909 Loss_D_fake: 0.01082310) Loss_G: 0.23479632 Loss_Enh_Dec: -2.05938792\n",
      "| epoch 129 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.92 | loss  2.24 | ppl     9.43 | acc     0.71 | train_ae_norm     1.00\n",
      "[129/200][3199/4361] Loss_D: 0.00425675 (Loss_D_real: 0.00089329 Loss_D_fake: 0.00336345) Loss_G: 0.29289180 Loss_Enh_Dec: -1.69219673\n",
      "| epoch 129 |  3200/ 4361 batches | lr 0.000000 | ms/batch 400.10 | loss  2.27 | ppl     9.68 | acc     0.75 | train_ae_norm     1.00\n",
      "[129/200][3299/4361] Loss_D: 0.00500903 (Loss_D_real: 0.00191026 Loss_D_fake: 0.00309877) Loss_G: 0.30995998 Loss_Enh_Dec: -1.42198670\n",
      "| epoch 129 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.27 | ppl     9.70 | acc     0.72 | train_ae_norm     1.00\n",
      "[129/200][3399/4361] Loss_D: 0.00589276 (Loss_D_real: 0.00016852 Loss_D_fake: 0.00572424) Loss_G: 0.27298334 Loss_Enh_Dec: -1.65112209\n",
      "| epoch 129 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  2.26 | ppl     9.60 | acc     0.72 | train_ae_norm     1.00\n",
      "[129/200][3499/4361] Loss_D: 0.00500422 (Loss_D_real: 0.00001624 Loss_D_fake: 0.00498798) Loss_G: 0.26935586 Loss_Enh_Dec: -1.61508524\n",
      "| epoch 129 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.22 | ppl     9.22 | acc     0.71 | train_ae_norm     1.00\n",
      "[129/200][3599/4361] Loss_D: 0.00603663 (Loss_D_real: 0.00054212 Loss_D_fake: 0.00549451) Loss_G: 0.27611789 Loss_Enh_Dec: -1.85789490\n",
      "| epoch 129 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.59 | loss  2.24 | ppl     9.40 | acc     0.75 | train_ae_norm     1.00\n",
      "[129/200][3699/4361] Loss_D: 0.00391639 (Loss_D_real: 0.00044932 Loss_D_fake: 0.00346708) Loss_G: 0.29056463 Loss_Enh_Dec: -1.97942245\n",
      "| epoch 129 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.23 | ppl     9.31 | acc     0.68 | train_ae_norm     1.00\n",
      "[129/200][3799/4361] Loss_D: 0.02013812 (Loss_D_real: 0.01669776 Loss_D_fake: 0.00344037) Loss_G: 0.28701276 Loss_Enh_Dec: -1.79463696\n",
      "| epoch 129 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  2.25 | ppl     9.51 | acc     0.76 | train_ae_norm     1.00\n",
      "[129/200][3899/4361] Loss_D: 0.00440458 (Loss_D_real: 0.00063168 Loss_D_fake: 0.00377290) Loss_G: 0.28222176 Loss_Enh_Dec: -1.49069321\n",
      "| epoch 129 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  2.26 | ppl     9.62 | acc     0.71 | train_ae_norm     1.00\n",
      "[129/200][3999/4361] Loss_D: 0.00538298 (Loss_D_real: 0.00087419 Loss_D_fake: 0.00450879) Loss_G: 0.27967805 Loss_Enh_Dec: -1.72010732\n",
      "| epoch 129 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.27 | ppl     9.67 | acc     0.73 | train_ae_norm     1.00\n",
      "[129/200][4099/4361] Loss_D: 0.00598198 (Loss_D_real: 0.00264725 Loss_D_fake: 0.00333473) Loss_G: 0.28888738 Loss_Enh_Dec: -1.85228848\n",
      "| epoch 129 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.21 | ppl     9.10 | acc     0.72 | train_ae_norm     1.00\n",
      "[129/200][4199/4361] Loss_D: 0.00383045 (Loss_D_real: 0.00136659 Loss_D_fake: 0.00246386) Loss_G: 0.28929296 Loss_Enh_Dec: -1.87827456\n",
      "| epoch 129 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  2.25 | ppl     9.52 | acc     0.73 | train_ae_norm     1.00\n",
      "[129/200][4299/4361] Loss_D: 0.00541673 (Loss_D_real: 0.00135131 Loss_D_fake: 0.00406542) Loss_G: 0.28246042 Loss_Enh_Dec: -2.32348704\n",
      "| epoch 129 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.83 | loss  2.23 | ppl     9.28 | acc     0.74 | train_ae_norm     1.00\n",
      "| end of epoch 129 | time: 1851.35s | test loss  2.39 | test ppl 10.94 | acc 0.767\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 130 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.698\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.480\n",
      "  Test Loss: 4.690\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 130 |     0/ 4361 batches | lr 0.000000 | ms/batch 862.19 | loss  0.02 | ppl     1.02 | acc     0.75 | train_ae_norm     1.00\n",
      "[130/200][99/4361] Loss_D: 0.00495433 (Loss_D_real: 0.00053268 Loss_D_fake: 0.00442165) Loss_G: 0.28742540 Loss_Enh_Dec: -2.15354753\n",
      "| epoch 130 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  2.31 | ppl    10.09 | acc     0.66 | train_ae_norm     1.00\n",
      "[130/200][199/4361] Loss_D: 0.00502606 (Loss_D_real: 0.00175202 Loss_D_fake: 0.00327404) Loss_G: 0.29889008 Loss_Enh_Dec: -2.39703488\n",
      "| epoch 130 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  2.31 | ppl    10.11 | acc     0.74 | train_ae_norm     1.00\n",
      "[130/200][299/4361] Loss_D: 0.00239477 (Loss_D_real: 0.00009284 Loss_D_fake: 0.00230192) Loss_G: 0.30628133 Loss_Enh_Dec: -2.04164290\n",
      "| epoch 130 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.33 | ppl    10.23 | acc     0.64 | train_ae_norm     1.00\n",
      "[130/200][399/4361] Loss_D: 0.01232469 (Loss_D_real: 0.00910812 Loss_D_fake: 0.00321657) Loss_G: 0.29658762 Loss_Enh_Dec: -1.92619956\n",
      "| epoch 130 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  2.23 | ppl     9.28 | acc     0.70 | train_ae_norm     1.00\n",
      "[130/200][499/4361] Loss_D: 0.00348383 (Loss_D_real: 0.00020899 Loss_D_fake: 0.00327484) Loss_G: 0.29577279 Loss_Enh_Dec: -2.68808341\n",
      "| epoch 130 |   500/ 4361 batches | lr 0.000000 | ms/batch 402.16 | loss  2.28 | ppl     9.81 | acc     0.74 | train_ae_norm     1.00\n",
      "[130/200][599/4361] Loss_D: 0.00382700 (Loss_D_real: 0.00138299 Loss_D_fake: 0.00244400) Loss_G: 0.29637879 Loss_Enh_Dec: -2.39833832\n",
      "| epoch 130 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  2.26 | ppl     9.56 | acc     0.69 | train_ae_norm     1.00\n",
      "[130/200][699/4361] Loss_D: 0.00223288 (Loss_D_real: 0.00020415 Loss_D_fake: 0.00202873) Loss_G: 0.31307733 Loss_Enh_Dec: -2.56221271\n",
      "| epoch 130 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  2.24 | ppl     9.44 | acc     0.72 | train_ae_norm     1.00\n",
      "[130/200][799/4361] Loss_D: 0.00420507 (Loss_D_real: 0.00138368 Loss_D_fake: 0.00282139) Loss_G: 0.30481791 Loss_Enh_Dec: -2.46196580\n",
      "| epoch 130 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  2.20 | ppl     8.98 | acc     0.70 | train_ae_norm     1.00\n",
      "[130/200][899/4361] Loss_D: 0.01600355 (Loss_D_real: 0.01306072 Loss_D_fake: 0.00294282) Loss_G: 0.31649858 Loss_Enh_Dec: -2.53210902\n",
      "| epoch 130 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  2.20 | ppl     9.01 | acc     0.74 | train_ae_norm     1.00\n",
      "[130/200][999/4361] Loss_D: 0.02305783 (Loss_D_real: 0.01940961 Loss_D_fake: 0.00364822) Loss_G: 0.29555637 Loss_Enh_Dec: -2.80868196\n",
      "| epoch 130 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.21 | ppl     9.09 | acc     0.70 | train_ae_norm     1.00\n",
      "[130/200][1099/4361] Loss_D: 0.00285372 (Loss_D_real: 0.00018948 Loss_D_fake: 0.00266424) Loss_G: 0.29620978 Loss_Enh_Dec: -2.82285881\n",
      "| epoch 130 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  2.23 | ppl     9.30 | acc     0.73 | train_ae_norm     1.00\n",
      "[130/200][1199/4361] Loss_D: 0.00265540 (Loss_D_real: 0.00048801 Loss_D_fake: 0.00216739) Loss_G: 0.32676879 Loss_Enh_Dec: -2.69835877\n",
      "| epoch 130 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.58 | loss  2.18 | ppl     8.84 | acc     0.76 | train_ae_norm     1.00\n",
      "[130/200][1299/4361] Loss_D: 0.00520663 (Loss_D_real: 0.00328166 Loss_D_fake: 0.00192498) Loss_G: 0.33462593 Loss_Enh_Dec: -2.63119650\n",
      "| epoch 130 |  1300/ 4361 batches | lr 0.000000 | ms/batch 402.28 | loss  2.22 | ppl     9.20 | acc     0.74 | train_ae_norm     1.00\n",
      "[130/200][1399/4361] Loss_D: 0.01158461 (Loss_D_real: 0.00753791 Loss_D_fake: 0.00404670) Loss_G: 0.30180961 Loss_Enh_Dec: -2.42836666\n",
      "| epoch 130 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  2.21 | ppl     9.15 | acc     0.71 | train_ae_norm     1.00\n",
      "[130/200][1499/4361] Loss_D: 0.00204580 (Loss_D_real: 0.00029678 Loss_D_fake: 0.00174901) Loss_G: 0.34042004 Loss_Enh_Dec: -2.72752380\n",
      "| epoch 130 |  1500/ 4361 batches | lr 0.000000 | ms/batch 400.89 | loss  2.26 | ppl     9.61 | acc     0.70 | train_ae_norm     1.00\n",
      "[130/200][1599/4361] Loss_D: 0.00175644 (Loss_D_real: 0.00031800 Loss_D_fake: 0.00143844) Loss_G: 0.36425340 Loss_Enh_Dec: -2.64325118\n",
      "| epoch 130 |  1600/ 4361 batches | lr 0.000000 | ms/batch 402.06 | loss  2.26 | ppl     9.54 | acc     0.73 | train_ae_norm     1.00\n",
      "[130/200][1699/4361] Loss_D: 0.00349160 (Loss_D_real: 0.00006555 Loss_D_fake: 0.00342604) Loss_G: 0.47138807 Loss_Enh_Dec: -2.52247357\n",
      "| epoch 130 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.34 | loss  2.21 | ppl     9.09 | acc     0.74 | train_ae_norm     1.00\n",
      "[130/200][1799/4361] Loss_D: 0.05356711 (Loss_D_real: 0.05136389 Loss_D_fake: 0.00220322) Loss_G: 0.32477117 Loss_Enh_Dec: -2.88891792\n",
      "| epoch 130 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  2.20 | ppl     9.03 | acc     0.71 | train_ae_norm     1.00\n",
      "[130/200][1899/4361] Loss_D: 0.00183753 (Loss_D_real: 0.00001169 Loss_D_fake: 0.00182584) Loss_G: 0.32636762 Loss_Enh_Dec: -2.83536983\n",
      "| epoch 130 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.25 | ppl     9.44 | acc     0.73 | train_ae_norm     1.00\n",
      "[130/200][1999/4361] Loss_D: 0.00829666 (Loss_D_real: 0.00583307 Loss_D_fake: 0.00246358) Loss_G: 0.31492680 Loss_Enh_Dec: -2.80114484\n",
      "| epoch 130 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.21 | ppl     9.14 | acc     0.75 | train_ae_norm     1.00\n",
      "[130/200][2099/4361] Loss_D: 0.00228469 (Loss_D_real: 0.00009908 Loss_D_fake: 0.00218561) Loss_G: 0.32087922 Loss_Enh_Dec: -3.18415642\n",
      "| epoch 130 |  2100/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  2.23 | ppl     9.29 | acc     0.75 | train_ae_norm     1.00\n",
      "[130/200][2199/4361] Loss_D: 0.00254129 (Loss_D_real: 0.00002931 Loss_D_fake: 0.00251198) Loss_G: 0.32072866 Loss_Enh_Dec: -2.86628485\n",
      "| epoch 130 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  2.21 | ppl     9.08 | acc     0.75 | train_ae_norm     1.00\n",
      "[130/200][2299/4361] Loss_D: 0.00276841 (Loss_D_real: 0.00038749 Loss_D_fake: 0.00238092) Loss_G: 0.32903057 Loss_Enh_Dec: -2.83698821\n",
      "| epoch 130 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.14 | loss  2.18 | ppl     8.87 | acc     0.76 | train_ae_norm     1.00\n",
      "[130/200][2399/4361] Loss_D: 0.00235464 (Loss_D_real: 0.00072617 Loss_D_fake: 0.00162847) Loss_G: 0.42539850 Loss_Enh_Dec: -2.40742660\n",
      "| epoch 130 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.18 | ppl     8.84 | acc     0.69 | train_ae_norm     1.00\n",
      "[130/200][2699/4361] Loss_D: 0.00258919 (Loss_D_real: 0.00174710 Loss_D_fake: 0.00084209) Loss_G: 0.52953571 Loss_Enh_Dec: -2.57695580\n",
      "| epoch 130 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  2.19 | ppl     8.96 | acc     0.72 | train_ae_norm     1.00\n",
      "[130/200][2799/4361] Loss_D: 0.00122510 (Loss_D_real: 0.00041566 Loss_D_fake: 0.00080943) Loss_G: 0.59517992 Loss_Enh_Dec: -2.45430970\n",
      "| epoch 130 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.49 | loss  2.18 | ppl     8.82 | acc     0.71 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[130/200][2899/4361] Loss_D: 0.01137129 (Loss_D_real: 0.00946638 Loss_D_fake: 0.00190491) Loss_G: 0.40877029 Loss_Enh_Dec: -2.74082637\n",
      "| epoch 130 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  2.20 | ppl     9.04 | acc     0.73 | train_ae_norm     1.00\n",
      "[130/200][2999/4361] Loss_D: 0.02080284 (Loss_D_real: 0.01838173 Loss_D_fake: 0.00242112) Loss_G: 0.40492830 Loss_Enh_Dec: -3.04315710\n",
      "| epoch 130 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.29 | loss  2.20 | ppl     9.01 | acc     0.70 | train_ae_norm     1.00\n",
      "[130/200][3099/4361] Loss_D: 0.01508431 (Loss_D_real: 0.01489416 Loss_D_fake: 0.00019015) Loss_G: 0.57999951 Loss_Enh_Dec: -2.68488431\n",
      "| epoch 130 |  3100/ 4361 batches | lr 0.000000 | ms/batch 400.92 | loss  2.19 | ppl     8.95 | acc     0.74 | train_ae_norm     1.00\n",
      "[130/200][3199/4361] Loss_D: 0.00121137 (Loss_D_real: 0.00052170 Loss_D_fake: 0.00068967) Loss_G: 0.43771297 Loss_Enh_Dec: -2.86856580\n",
      "| epoch 130 |  3200/ 4361 batches | lr 0.000000 | ms/batch 402.13 | loss  2.20 | ppl     9.06 | acc     0.76 | train_ae_norm     1.00\n",
      "[130/200][3299/4361] Loss_D: 0.00116800 (Loss_D_real: 0.00003886 Loss_D_fake: 0.00112914) Loss_G: 0.36111078 Loss_Enh_Dec: -2.90829062\n",
      "| epoch 130 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  2.23 | ppl     9.33 | acc     0.73 | train_ae_norm     1.00\n",
      "[130/200][3399/4361] Loss_D: 0.01292731 (Loss_D_real: 0.01231530 Loss_D_fake: 0.00061201) Loss_G: 0.51148146 Loss_Enh_Dec: -2.77564740\n",
      "| epoch 130 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.99 | loss  2.22 | ppl     9.19 | acc     0.74 | train_ae_norm     1.00\n",
      "[130/200][3499/4361] Loss_D: 0.02257076 (Loss_D_real: 0.02196355 Loss_D_fake: 0.00060721) Loss_G: 0.42573556 Loss_Enh_Dec: -3.18808818\n",
      "| epoch 130 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.50 | loss  2.16 | ppl     8.70 | acc     0.72 | train_ae_norm     1.00\n",
      "[130/200][3599/4361] Loss_D: 0.01824720 (Loss_D_real: 0.01784480 Loss_D_fake: 0.00040239) Loss_G: 0.42502356 Loss_Enh_Dec: -2.64857554\n",
      "| epoch 130 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  2.20 | ppl     8.99 | acc     0.72 | train_ae_norm     1.00\n",
      "[130/200][3699/4361] Loss_D: 0.02654744 (Loss_D_real: 0.00069435 Loss_D_fake: 0.02585309) Loss_G: 0.43064299 Loss_Enh_Dec: -2.82168746\n",
      "| epoch 130 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.90 | loss  2.21 | ppl     9.12 | acc     0.68 | train_ae_norm     1.00\n",
      "[130/200][3799/4361] Loss_D: 0.03533483 (Loss_D_real: 0.03457982 Loss_D_fake: 0.00075501) Loss_G: 0.62655586 Loss_Enh_Dec: -2.34899449\n",
      "| epoch 130 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  2.24 | ppl     9.41 | acc     0.76 | train_ae_norm     1.00\n",
      "[130/200][3899/4361] Loss_D: 0.00368489 (Loss_D_real: 0.00187188 Loss_D_fake: 0.00181301) Loss_G: 0.40166292 Loss_Enh_Dec: -2.57504940\n",
      "| epoch 130 |  3900/ 4361 batches | lr 0.000000 | ms/batch 402.20 | loss  2.23 | ppl     9.28 | acc     0.70 | train_ae_norm     1.00\n",
      "[130/200][3999/4361] Loss_D: 0.00483635 (Loss_D_real: 0.00111571 Loss_D_fake: 0.00372064) Loss_G: 0.39580670 Loss_Enh_Dec: -2.58498907\n",
      "| epoch 130 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  2.19 | ppl     8.96 | acc     0.71 | train_ae_norm     1.00\n",
      "[130/200][4099/4361] Loss_D: 0.09607396 (Loss_D_real: 0.01609474 Loss_D_fake: 0.07997921) Loss_G: 0.71531713 Loss_Enh_Dec: -2.51350403\n",
      "| epoch 130 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.18 | ppl     8.82 | acc     0.72 | train_ae_norm     1.00\n",
      "[130/200][4199/4361] Loss_D: 0.00569288 (Loss_D_real: 0.00108203 Loss_D_fake: 0.00461085) Loss_G: 0.37426487 Loss_Enh_Dec: -2.75587845\n",
      "| epoch 130 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  2.24 | ppl     9.43 | acc     0.75 | train_ae_norm     1.00\n",
      "[130/200][4299/4361] Loss_D: 0.00207039 (Loss_D_real: 0.00129782 Loss_D_fake: 0.00077257) Loss_G: 0.36199796 Loss_Enh_Dec: -2.85520911\n",
      "| epoch 130 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.91 | loss  2.23 | ppl     9.25 | acc     0.74 | train_ae_norm     1.00\n",
      "| end of epoch 130 | time: 1853.18s | test loss  2.35 | test ppl 10.48 | acc 0.774\n",
      "bleu_self:  [3.05488318e-01 6.90189561e-09 2.14202012e-11 1.30840919e-12\n",
      " 3.46391043e-12]\n",
      "bleu_test:  [8.52480158e-01 1.01545255e-01 8.62817109e-07 2.70512655e-09\n",
      " 9.76323188e-11]\n",
      "bleu_self: [0.30548832,0.00000001,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.85248016,0.10154526,0.00000086,0.00000000,0.00000000]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 131 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.696\n",
      "  Average training loss discriminator: 0.699\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.477\n",
      "  Test Loss: 4.551\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 131 |     0/ 4361 batches | lr 0.000000 | ms/batch 863.92 | loss  0.02 | ppl     1.02 | acc     0.76 | train_ae_norm     1.00\n",
      "[131/200][99/4361] Loss_D: 0.00292036 (Loss_D_real: 0.00112736 Loss_D_fake: 0.00179300) Loss_G: 0.39084530 Loss_Enh_Dec: -2.89657903\n",
      "| epoch 131 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.33 | loss  2.22 | ppl     9.18 | acc     0.68 | train_ae_norm     1.00\n",
      "[131/200][199/4361] Loss_D: 0.00125189 (Loss_D_real: 0.00021127 Loss_D_fake: 0.00104062) Loss_G: 0.35919064 Loss_Enh_Dec: -2.87968135\n",
      "| epoch 131 |   200/ 4361 batches | lr 0.000000 | ms/batch 402.31 | loss  2.22 | ppl     9.24 | acc     0.74 | train_ae_norm     1.00\n",
      "[131/200][299/4361] Loss_D: 0.00187644 (Loss_D_real: 0.00014895 Loss_D_fake: 0.00172748) Loss_G: 0.40204677 Loss_Enh_Dec: -2.75635290\n",
      "| epoch 131 |   300/ 4361 batches | lr 0.000000 | ms/batch 402.07 | loss  2.22 | ppl     9.23 | acc     0.69 | train_ae_norm     1.00\n",
      "[131/200][399/4361] Loss_D: 0.00439681 (Loss_D_real: 0.00187826 Loss_D_fake: 0.00251854) Loss_G: 0.37276897 Loss_Enh_Dec: -2.94174862\n",
      "| epoch 131 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  2.13 | ppl     8.38 | acc     0.74 | train_ae_norm     1.00\n",
      "[131/200][499/4361] Loss_D: 0.03148627 (Loss_D_real: 0.00256731 Loss_D_fake: 0.02891896) Loss_G: 0.57935691 Loss_Enh_Dec: -3.05520630\n",
      "| epoch 131 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  2.20 | ppl     9.01 | acc     0.78 | train_ae_norm     1.00\n",
      "[131/200][599/4361] Loss_D: 0.01791932 (Loss_D_real: 0.00056936 Loss_D_fake: 0.01734995) Loss_G: 0.36348668 Loss_Enh_Dec: -3.04118323\n",
      "| epoch 131 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  2.16 | ppl     8.71 | acc     0.68 | train_ae_norm     1.00\n",
      "[131/200][699/4361] Loss_D: 0.02805646 (Loss_D_real: 0.02492555 Loss_D_fake: 0.00313091) Loss_G: 0.37470850 Loss_Enh_Dec: -2.98733902\n",
      "| epoch 131 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  2.21 | ppl     9.09 | acc     0.73 | train_ae_norm     1.00\n",
      "[131/200][799/4361] Loss_D: 0.00233056 (Loss_D_real: 0.00018619 Loss_D_fake: 0.00214437) Loss_G: 0.36789912 Loss_Enh_Dec: -2.96053743\n",
      "| epoch 131 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.23 | ppl     9.32 | acc     0.71 | train_ae_norm     1.00\n",
      "[131/200][899/4361] Loss_D: 0.00103605 (Loss_D_real: 0.00024672 Loss_D_fake: 0.00078933) Loss_G: 0.47369489 Loss_Enh_Dec: -3.03364992\n",
      "| epoch 131 |   900/ 4361 batches | lr 0.000000 | ms/batch 402.21 | loss  2.36 | ppl    10.57 | acc     0.75 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[131/200][999/4361] Loss_D: 0.00617059 (Loss_D_real: 0.00605194 Loss_D_fake: 0.00011865) Loss_G: 0.66004813 Loss_Enh_Dec: -2.72441411\n",
      "| epoch 131 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  2.19 | ppl     8.97 | acc     0.74 | train_ae_norm     1.00\n",
      "[131/200][1099/4361] Loss_D: 0.00529500 (Loss_D_real: 0.00440124 Loss_D_fake: 0.00089376) Loss_G: 0.45314366 Loss_Enh_Dec: -2.56233478\n",
      "| epoch 131 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.89 | loss  2.19 | ppl     8.89 | acc     0.71 | train_ae_norm     1.00\n",
      "[131/200][1199/4361] Loss_D: 0.00156725 (Loss_D_real: 0.00049696 Loss_D_fake: 0.00107029) Loss_G: 0.40638217 Loss_Enh_Dec: -2.47098899\n",
      "| epoch 131 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.74 | loss  2.19 | ppl     8.93 | acc     0.77 | train_ae_norm     1.00\n",
      "[131/200][1299/4361] Loss_D: 0.00356518 (Loss_D_real: 0.00107836 Loss_D_fake: 0.00248681) Loss_G: 0.41632327 Loss_Enh_Dec: -2.28129864\n",
      "| epoch 131 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.22 | ppl     9.24 | acc     0.75 | train_ae_norm     1.00\n",
      "[131/200][1399/4361] Loss_D: 0.08562192 (Loss_D_real: 0.03562469 Loss_D_fake: 0.04999723) Loss_G: 0.78874797 Loss_Enh_Dec: -2.57118082\n",
      "| epoch 131 |  1400/ 4361 batches | lr 0.000000 | ms/batch 402.35 | loss  2.23 | ppl     9.34 | acc     0.71 | train_ae_norm     1.00\n",
      "[131/200][1499/4361] Loss_D: 0.00316225 (Loss_D_real: 0.00007688 Loss_D_fake: 0.00308538) Loss_G: 0.40779018 Loss_Enh_Dec: -2.35554099\n",
      "| epoch 131 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  2.25 | ppl     9.53 | acc     0.74 | train_ae_norm     1.00\n",
      "[131/200][1599/4361] Loss_D: 0.00138752 (Loss_D_real: 0.00059265 Loss_D_fake: 0.00079486) Loss_G: 0.42285299 Loss_Enh_Dec: -2.47657156\n",
      "| epoch 131 |  1600/ 4361 batches | lr 0.000000 | ms/batch 402.13 | loss  2.23 | ppl     9.28 | acc     0.72 | train_ae_norm     1.00\n",
      "[131/200][1699/4361] Loss_D: 0.02441463 (Loss_D_real: 0.02367314 Loss_D_fake: 0.00074149) Loss_G: 0.39532393 Loss_Enh_Dec: -2.19775367\n",
      "| epoch 131 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  2.19 | ppl     8.96 | acc     0.73 | train_ae_norm     1.00\n",
      "[131/200][1799/4361] Loss_D: 0.00365666 (Loss_D_real: 0.00168093 Loss_D_fake: 0.00197573) Loss_G: 0.39324355 Loss_Enh_Dec: -2.70332503\n",
      "| epoch 131 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.18 | ppl     8.84 | acc     0.74 | train_ae_norm     1.00\n",
      "[131/200][1899/4361] Loss_D: 0.00223475 (Loss_D_real: 0.00124098 Loss_D_fake: 0.00099377) Loss_G: 0.39875886 Loss_Enh_Dec: -2.58894348\n",
      "| epoch 131 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  2.25 | ppl     9.52 | acc     0.73 | train_ae_norm     1.00\n",
      "[131/200][1999/4361] Loss_D: 0.00408598 (Loss_D_real: 0.00007690 Loss_D_fake: 0.00400908) Loss_G: 0.41810328 Loss_Enh_Dec: -2.65457416\n",
      "| epoch 131 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  2.20 | ppl     8.99 | acc     0.74 | train_ae_norm     1.00\n",
      "[131/200][2099/4361] Loss_D: 0.00140688 (Loss_D_real: 0.00011917 Loss_D_fake: 0.00128771) Loss_G: 0.45162353 Loss_Enh_Dec: -2.44274974\n",
      "| epoch 131 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.21 | ppl     9.08 | acc     0.77 | train_ae_norm     1.00\n",
      "[131/200][2199/4361] Loss_D: 0.00112486 (Loss_D_real: 0.00010099 Loss_D_fake: 0.00102386) Loss_G: 0.40010643 Loss_Enh_Dec: -2.67066932\n",
      "| epoch 131 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  2.20 | ppl     9.00 | acc     0.75 | train_ae_norm     1.00\n",
      "[131/200][2299/4361] Loss_D: 0.00227907 (Loss_D_real: 0.00021195 Loss_D_fake: 0.00206711) Loss_G: 0.41001579 Loss_Enh_Dec: -2.62024498\n",
      "| epoch 131 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.00 | loss  2.20 | ppl     9.06 | acc     0.77 | train_ae_norm     1.00\n",
      "[131/200][2399/4361] Loss_D: 0.08054397 (Loss_D_real: 0.07847676 Loss_D_fake: 0.00206721) Loss_G: 0.43564716 Loss_Enh_Dec: -2.62007713\n",
      "| epoch 131 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  2.20 | ppl     9.01 | acc     0.69 | train_ae_norm     1.00\n",
      "[131/200][2499/4361] Loss_D: 0.00065219 (Loss_D_real: 0.00018781 Loss_D_fake: 0.00046438) Loss_G: 0.55569839 Loss_Enh_Dec: -2.21736193\n",
      "| epoch 131 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  2.29 | ppl     9.85 | acc     0.74 | train_ae_norm     1.00\n",
      "[131/200][2599/4361] Loss_D: 0.01155407 (Loss_D_real: 0.01051232 Loss_D_fake: 0.00104175) Loss_G: 0.42467219 Loss_Enh_Dec: -2.32879543\n",
      "| epoch 131 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  2.26 | ppl     9.61 | acc     0.69 | train_ae_norm     1.00\n",
      "[131/200][2699/4361] Loss_D: 0.00233539 (Loss_D_real: 0.00029779 Loss_D_fake: 0.00203760) Loss_G: 0.38199806 Loss_Enh_Dec: -2.30557680\n",
      "| epoch 131 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  2.23 | ppl     9.31 | acc     0.72 | train_ae_norm     1.00\n",
      "[131/200][2799/4361] Loss_D: 0.00327326 (Loss_D_real: 0.00014188 Loss_D_fake: 0.00313137) Loss_G: 0.39075682 Loss_Enh_Dec: -1.83127630\n",
      "| epoch 131 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  2.20 | ppl     9.06 | acc     0.70 | train_ae_norm     1.00\n",
      "[131/200][2899/4361] Loss_D: 0.00426391 (Loss_D_real: 0.00248131 Loss_D_fake: 0.00178260) Loss_G: 0.38391382 Loss_Enh_Dec: -2.46332550\n",
      "| epoch 131 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  2.22 | ppl     9.20 | acc     0.73 | train_ae_norm     1.00\n",
      "[131/200][2999/4361] Loss_D: 0.00133697 (Loss_D_real: 0.00042386 Loss_D_fake: 0.00091312) Loss_G: 0.39539313 Loss_Enh_Dec: -2.32033801\n",
      "| epoch 131 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  2.23 | ppl     9.32 | acc     0.73 | train_ae_norm     1.00\n",
      "[131/200][3099/4361] Loss_D: 0.00209079 (Loss_D_real: 0.00018617 Loss_D_fake: 0.00190461) Loss_G: 0.37862191 Loss_Enh_Dec: -2.17614245\n",
      "| epoch 131 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  2.23 | ppl     9.27 | acc     0.71 | train_ae_norm     1.00\n",
      "[131/200][3199/4361] Loss_D: 0.00117232 (Loss_D_real: 0.00003188 Loss_D_fake: 0.00114044) Loss_G: 0.40400991 Loss_Enh_Dec: -2.43185973\n",
      "| epoch 131 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.27 | ppl     9.64 | acc     0.72 | train_ae_norm     1.00\n",
      "[131/200][3299/4361] Loss_D: 0.00074629 (Loss_D_real: 0.00033581 Loss_D_fake: 0.00041048) Loss_G: 0.49582061 Loss_Enh_Dec: -2.42975688\n",
      "| epoch 131 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  2.27 | ppl     9.72 | acc     0.73 | train_ae_norm     1.00\n",
      "[131/200][3399/4361] Loss_D: 0.00189178 (Loss_D_real: 0.00007282 Loss_D_fake: 0.00181896) Loss_G: 0.38759175 Loss_Enh_Dec: -2.51304460\n",
      "| epoch 131 |  3400/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  2.26 | ppl     9.56 | acc     0.70 | train_ae_norm     1.00\n",
      "[131/200][3499/4361] Loss_D: 0.00266526 (Loss_D_real: 0.00153218 Loss_D_fake: 0.00113308) Loss_G: 0.41675052 Loss_Enh_Dec: -2.89338589\n",
      "| epoch 131 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  2.23 | ppl     9.34 | acc     0.70 | train_ae_norm     1.00\n",
      "[131/200][3599/4361] Loss_D: 0.00166425 (Loss_D_real: 0.00036018 Loss_D_fake: 0.00130407) Loss_G: 0.54926634 Loss_Enh_Dec: -2.20803237\n",
      "| epoch 131 |  3600/ 4361 batches | lr 0.000000 | ms/batch 400.89 | loss  2.26 | ppl     9.59 | acc     0.72 | train_ae_norm     1.00\n",
      "[131/200][3699/4361] Loss_D: 0.00754326 (Loss_D_real: 0.00329406 Loss_D_fake: 0.00424920) Loss_G: 0.38518405 Loss_Enh_Dec: -2.30386710\n",
      "| epoch 131 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  2.27 | ppl     9.72 | acc     0.71 | train_ae_norm     1.00\n",
      "[131/200][3799/4361] Loss_D: 0.00185698 (Loss_D_real: 0.00063141 Loss_D_fake: 0.00122557) Loss_G: 0.44748479 Loss_Enh_Dec: -2.53177333\n",
      "| epoch 131 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.24 | ppl     9.43 | acc     0.78 | train_ae_norm     1.00\n",
      "[131/200][3899/4361] Loss_D: 0.00161808 (Loss_D_real: 0.00072097 Loss_D_fake: 0.00089711) Loss_G: 0.49519125 Loss_Enh_Dec: -2.57742143\n",
      "| epoch 131 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  2.39 | ppl    10.90 | acc     0.68 | train_ae_norm     1.00\n",
      "[131/200][3999/4361] Loss_D: 0.00295390 (Loss_D_real: 0.00100907 Loss_D_fake: 0.00194482) Loss_G: 0.45678806 Loss_Enh_Dec: -1.90874064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 131 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  2.29 | ppl     9.85 | acc     0.75 | train_ae_norm     1.00\n",
      "[131/200][4099/4361] Loss_D: 0.00236913 (Loss_D_real: 0.00085590 Loss_D_fake: 0.00151324) Loss_G: 0.41745597 Loss_Enh_Dec: -2.19749308\n",
      "| epoch 131 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.19 | loss  2.21 | ppl     9.12 | acc     0.72 | train_ae_norm     1.00\n",
      "[131/200][4199/4361] Loss_D: 0.00168713 (Loss_D_real: 0.00049357 Loss_D_fake: 0.00119356) Loss_G: 0.45820308 Loss_Enh_Dec: -1.97350538\n",
      "| epoch 131 |  4200/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  2.24 | ppl     9.35 | acc     0.74 | train_ae_norm     1.00\n",
      "[131/200][4299/4361] Loss_D: 0.00263180 (Loss_D_real: 0.00047253 Loss_D_fake: 0.00215927) Loss_G: 0.40728280 Loss_Enh_Dec: -2.22147107\n",
      "| epoch 131 |  4300/ 4361 batches | lr 0.000000 | ms/batch 402.07 | loss  2.19 | ppl     8.97 | acc     0.74 | train_ae_norm     1.00\n",
      "| end of epoch 131 | time: 1852.99s | test loss  2.35 | test ppl 10.51 | acc 0.771\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 132 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:57.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.698\n",
      "  Training epcoh took: 0:01:27\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.482\n",
      "  Test Loss: 4.668\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 132 |     0/ 4361 batches | lr 0.000000 | ms/batch 869.06 | loss  0.02 | ppl     1.02 | acc     0.76 | train_ae_norm     1.00\n",
      "[132/200][99/4361] Loss_D: 0.02088399 (Loss_D_real: 0.01993888 Loss_D_fake: 0.00094510) Loss_G: 0.60288686 Loss_Enh_Dec: -1.41391623\n",
      "| epoch 132 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.96 | loss  2.22 | ppl     9.21 | acc     0.70 | train_ae_norm     1.00\n",
      "[132/200][199/4361] Loss_D: 0.01591813 (Loss_D_real: 0.01064828 Loss_D_fake: 0.00526986) Loss_G: 0.49272057 Loss_Enh_Dec: -1.77719772\n",
      "| epoch 132 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  2.24 | ppl     9.40 | acc     0.72 | train_ae_norm     1.00\n",
      "[132/200][299/4361] Loss_D: 0.00149379 (Loss_D_real: 0.00052869 Loss_D_fake: 0.00096510) Loss_G: 0.37327859 Loss_Enh_Dec: -1.77814281\n",
      "| epoch 132 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  2.27 | ppl     9.72 | acc     0.66 | train_ae_norm     1.00\n",
      "[132/200][399/4361] Loss_D: 0.00359643 (Loss_D_real: 0.00139535 Loss_D_fake: 0.00220108) Loss_G: 0.45349923 Loss_Enh_Dec: -2.02291942\n",
      "| epoch 132 |   400/ 4361 batches | lr 0.000000 | ms/batch 402.14 | loss  2.18 | ppl     8.82 | acc     0.73 | train_ae_norm     1.00\n",
      "[132/200][499/4361] Loss_D: 0.00215630 (Loss_D_real: 0.00129358 Loss_D_fake: 0.00086272) Loss_G: 0.42596656 Loss_Enh_Dec: -1.75649679\n",
      "| epoch 132 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  2.22 | ppl     9.18 | acc     0.75 | train_ae_norm     1.00\n",
      "[132/200][599/4361] Loss_D: 0.00265040 (Loss_D_real: 0.00139159 Loss_D_fake: 0.00125881) Loss_G: 0.40058246 Loss_Enh_Dec: -1.89723289\n",
      "| epoch 132 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.02 | loss  2.18 | ppl     8.82 | acc     0.70 | train_ae_norm     1.00\n",
      "[132/200][699/4361] Loss_D: 0.00314086 (Loss_D_real: 0.00073663 Loss_D_fake: 0.00240423) Loss_G: 0.44231734 Loss_Enh_Dec: -2.42130780\n",
      "| epoch 132 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  2.24 | ppl     9.38 | acc     0.73 | train_ae_norm     1.00\n",
      "[132/200][799/4361] Loss_D: 0.00216664 (Loss_D_real: 0.00077423 Loss_D_fake: 0.00139241) Loss_G: 0.43138894 Loss_Enh_Dec: -2.18660092\n",
      "| epoch 132 |   800/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  2.21 | ppl     9.10 | acc     0.72 | train_ae_norm     1.00\n",
      "[132/200][899/4361] Loss_D: 0.04690830 (Loss_D_real: 0.00466882 Loss_D_fake: 0.04223948) Loss_G: 0.62544173 Loss_Enh_Dec: -2.57716274\n",
      "| epoch 132 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  2.22 | ppl     9.17 | acc     0.75 | train_ae_norm     1.00\n",
      "[132/200][999/4361] Loss_D: 0.06448193 (Loss_D_real: 0.00321447 Loss_D_fake: 0.06126746) Loss_G: 0.62257195 Loss_Enh_Dec: -2.22351742\n",
      "| epoch 132 |  1000/ 4361 batches | lr 0.000000 | ms/batch 400.87 | loss  2.20 | ppl     9.06 | acc     0.75 | train_ae_norm     1.00\n",
      "[132/200][1099/4361] Loss_D: 0.00659415 (Loss_D_real: 0.00541938 Loss_D_fake: 0.00117477) Loss_G: 0.55216950 Loss_Enh_Dec: -1.91867030\n",
      "| epoch 132 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.75 | loss  2.20 | ppl     9.06 | acc     0.69 | train_ae_norm     1.00\n",
      "[132/200][1199/4361] Loss_D: 0.00136355 (Loss_D_real: 0.00050229 Loss_D_fake: 0.00086127) Loss_G: 0.41214353 Loss_Enh_Dec: -2.57937479\n",
      "| epoch 132 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  2.21 | ppl     9.12 | acc     0.76 | train_ae_norm     1.00\n",
      "[132/200][1299/4361] Loss_D: 0.00161933 (Loss_D_real: 0.00118987 Loss_D_fake: 0.00042946) Loss_G: 0.58251971 Loss_Enh_Dec: -1.92421043\n",
      "| epoch 132 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.84 | loss  2.24 | ppl     9.38 | acc     0.73 | train_ae_norm     1.00\n",
      "[132/200][1399/4361] Loss_D: 0.00107498 (Loss_D_real: 0.00030137 Loss_D_fake: 0.00077361) Loss_G: 0.39310095 Loss_Enh_Dec: -2.12783194\n",
      "| epoch 132 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  2.24 | ppl     9.37 | acc     0.72 | train_ae_norm     1.00\n",
      "[132/200][1499/4361] Loss_D: 0.00304488 (Loss_D_real: 0.00236052 Loss_D_fake: 0.00068437) Loss_G: 0.42403626 Loss_Enh_Dec: -2.00262523\n",
      "| epoch 132 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.54 | loss  2.29 | ppl     9.87 | acc     0.71 | train_ae_norm     1.00\n",
      "[132/200][1599/4361] Loss_D: 0.02992461 (Loss_D_real: 0.02722663 Loss_D_fake: 0.00269798) Loss_G: 0.38578156 Loss_Enh_Dec: -2.62609267\n",
      "| epoch 132 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  2.30 | ppl     9.98 | acc     0.70 | train_ae_norm     1.00\n",
      "[132/200][1699/4361] Loss_D: 0.01630614 (Loss_D_real: 0.00049133 Loss_D_fake: 0.01581481) Loss_G: 0.39635941 Loss_Enh_Dec: -2.40922523\n",
      "| epoch 132 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  2.27 | ppl     9.65 | acc     0.69 | train_ae_norm     1.00\n",
      "[132/200][1799/4361] Loss_D: 0.00075560 (Loss_D_real: 0.00033203 Loss_D_fake: 0.00042357) Loss_G: 0.47278166 Loss_Enh_Dec: -2.60114431\n",
      "| epoch 132 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  2.22 | ppl     9.18 | acc     0.72 | train_ae_norm     1.00\n",
      "[132/200][1899/4361] Loss_D: 0.00355296 (Loss_D_real: 0.00195398 Loss_D_fake: 0.00159898) Loss_G: 0.51235861 Loss_Enh_Dec: -2.36832929\n",
      "| epoch 132 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  2.26 | ppl     9.61 | acc     0.73 | train_ae_norm     1.00\n",
      "[132/200][1999/4361] Loss_D: 0.00390796 (Loss_D_real: 0.00321280 Loss_D_fake: 0.00069516) Loss_G: 0.55587637 Loss_Enh_Dec: -1.86373794\n",
      "| epoch 132 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  2.21 | ppl     9.08 | acc     0.75 | train_ae_norm     1.00\n",
      "[132/200][2099/4361] Loss_D: 0.00135374 (Loss_D_real: 0.00026619 Loss_D_fake: 0.00108755) Loss_G: 0.47918063 Loss_Enh_Dec: -2.24136472\n",
      "| epoch 132 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  2.26 | ppl     9.55 | acc     0.72 | train_ae_norm     1.00\n",
      "[132/200][2199/4361] Loss_D: 0.00278767 (Loss_D_real: 0.00033850 Loss_D_fake: 0.00244917) Loss_G: 0.39735019 Loss_Enh_Dec: -2.07314467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 132 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  2.36 | ppl    10.61 | acc     0.71 | train_ae_norm     1.00\n",
      "[132/200][2299/4361] Loss_D: 0.00812195 (Loss_D_real: 0.00616074 Loss_D_fake: 0.00196122) Loss_G: 0.38158700 Loss_Enh_Dec: -1.74609268\n",
      "| epoch 132 |  2300/ 4361 batches | lr 0.000000 | ms/batch 402.36 | loss  2.38 | ppl    10.84 | acc     0.72 | train_ae_norm     1.00\n",
      "[132/200][2399/4361] Loss_D: 0.00314400 (Loss_D_real: 0.00169556 Loss_D_fake: 0.00144844) Loss_G: 0.43500826 Loss_Enh_Dec: -1.91641939\n",
      "| epoch 132 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  2.31 | ppl    10.08 | acc     0.68 | train_ae_norm     1.00\n",
      "[132/200][2499/4361] Loss_D: 0.00282806 (Loss_D_real: 0.00069611 Loss_D_fake: 0.00213194) Loss_G: 0.52217239 Loss_Enh_Dec: -2.37964129\n",
      "| epoch 132 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  2.25 | ppl     9.53 | acc     0.72 | train_ae_norm     1.00\n",
      "[132/200][2599/4361] Loss_D: 0.00314177 (Loss_D_real: 0.00100186 Loss_D_fake: 0.00213991) Loss_G: 0.42513713 Loss_Enh_Dec: -2.47680259\n",
      "| epoch 132 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.76 | loss  2.22 | ppl     9.18 | acc     0.70 | train_ae_norm     1.00\n",
      "[132/200][2699/4361] Loss_D: 0.00257119 (Loss_D_real: 0.00044604 Loss_D_fake: 0.00212515) Loss_G: 0.54996139 Loss_Enh_Dec: -1.78717935\n",
      "| epoch 132 |  2700/ 4361 batches | lr 0.000000 | ms/batch 402.00 | loss  2.20 | ppl     9.05 | acc     0.72 | train_ae_norm     1.00\n",
      "[132/200][2799/4361] Loss_D: 0.00098748 (Loss_D_real: 0.00025918 Loss_D_fake: 0.00072830) Loss_G: 0.40612718 Loss_Enh_Dec: -1.70187414\n",
      "| epoch 132 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.25 | loss  2.18 | ppl     8.83 | acc     0.73 | train_ae_norm     1.00\n",
      "[132/200][2899/4361] Loss_D: 0.02140578 (Loss_D_real: 0.00369789 Loss_D_fake: 0.01770789) Loss_G: 0.52076429 Loss_Enh_Dec: -1.52452981\n",
      "| epoch 132 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.64 | loss  2.19 | ppl     8.93 | acc     0.71 | train_ae_norm     1.00\n",
      "[132/200][2999/4361] Loss_D: 0.00905988 (Loss_D_real: 0.00836223 Loss_D_fake: 0.00069766) Loss_G: 0.45461103 Loss_Enh_Dec: -1.64709413\n",
      "| epoch 132 |  3000/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  2.21 | ppl     9.14 | acc     0.74 | train_ae_norm     1.00\n",
      "[132/200][3099/4361] Loss_D: 0.02196847 (Loss_D_real: 0.02153275 Loss_D_fake: 0.00043573) Loss_G: 0.61372197 Loss_Enh_Dec: -1.82443893\n",
      "| epoch 132 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.21 | ppl     9.14 | acc     0.73 | train_ae_norm     1.00\n",
      "[132/200][3199/4361] Loss_D: 0.00393959 (Loss_D_real: 0.00047576 Loss_D_fake: 0.00346382) Loss_G: 0.44598103 Loss_Enh_Dec: -1.98087394\n",
      "| epoch 132 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  2.24 | ppl     9.35 | acc     0.72 | train_ae_norm     1.00\n",
      "[132/200][3299/4361] Loss_D: 0.00447225 (Loss_D_real: 0.00048035 Loss_D_fake: 0.00399190) Loss_G: 0.43754005 Loss_Enh_Dec: -1.94548476\n",
      "| epoch 132 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.23 | ppl     9.26 | acc     0.73 | train_ae_norm     1.00\n",
      "[132/200][3399/4361] Loss_D: 0.00256208 (Loss_D_real: 0.00212399 Loss_D_fake: 0.00043808) Loss_G: 0.41494521 Loss_Enh_Dec: -2.35321021\n",
      "| epoch 132 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.23 | ppl     9.31 | acc     0.74 | train_ae_norm     1.00\n",
      "[132/200][3499/4361] Loss_D: 0.00173484 (Loss_D_real: 0.00045682 Loss_D_fake: 0.00127802) Loss_G: 0.48877484 Loss_Enh_Dec: -2.50163627\n",
      "| epoch 132 |  3500/ 4361 batches | lr 0.000000 | ms/batch 402.69 | loss  2.18 | ppl     8.87 | acc     0.70 | train_ae_norm     1.00\n",
      "[132/200][3799/4361] Loss_D: 0.00233501 (Loss_D_real: 0.00103623 Loss_D_fake: 0.00129878) Loss_G: 0.45368138 Loss_Enh_Dec: -2.54218960\n",
      "| epoch 132 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  2.18 | ppl     8.88 | acc     0.78 | train_ae_norm     1.00\n",
      "[132/200][3899/4361] Loss_D: 0.00238838 (Loss_D_real: 0.00026669 Loss_D_fake: 0.00212169) Loss_G: 0.44744119 Loss_Enh_Dec: -2.02956200\n",
      "| epoch 132 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.45 | loss  2.19 | ppl     8.94 | acc     0.69 | train_ae_norm     1.00\n",
      "[132/200][3999/4361] Loss_D: 0.00317428 (Loss_D_real: 0.00046891 Loss_D_fake: 0.00270537) Loss_G: 0.43670923 Loss_Enh_Dec: -2.51943946\n",
      "| epoch 132 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.20 | ppl     8.98 | acc     0.73 | train_ae_norm     1.00\n",
      "[132/200][4099/4361] Loss_D: 0.00325014 (Loss_D_real: 0.00148001 Loss_D_fake: 0.00177013) Loss_G: 0.59829402 Loss_Enh_Dec: -1.95183837\n",
      "| epoch 132 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  2.15 | ppl     8.58 | acc     0.74 | train_ae_norm     1.00\n",
      "[132/200][4199/4361] Loss_D: 0.00123164 (Loss_D_real: 0.00028800 Loss_D_fake: 0.00094363) Loss_G: 0.44159269 Loss_Enh_Dec: -1.99698961\n",
      "| epoch 132 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  2.21 | ppl     9.08 | acc     0.76 | train_ae_norm     1.00\n",
      "[132/200][4299/4361] Loss_D: 0.00397553 (Loss_D_real: 0.00288323 Loss_D_fake: 0.00109230) Loss_G: 0.52335596 Loss_Enh_Dec: -1.93166733\n",
      "| epoch 132 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  2.18 | ppl     8.84 | acc     0.74 | train_ae_norm     1.00\n",
      "| end of epoch 132 | time: 1853.64s | test loss  2.28 | test ppl  9.79 | acc 0.780\n",
      "bleu_self:  [4.67329949e-01 2.19556216e-01 1.88286673e-06 5.93003665e-09\n",
      " 2.09333684e-10]\n",
      "bleu_test:  [8.76325757e-01 3.69464113e-01 2.79220439e-06 8.44300205e-09\n",
      " 5.39587382e-09]\n",
      "bleu_self: [0.46732995,0.21955622,0.00000188,0.00000001,0.00000000]\n",
      "bleu_test: [0.87632576,0.36946411,0.00000279,0.00000001,0.00000001]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 133 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.698\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.490\n",
      "  Test Loss: 4.764\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 133 |     0/ 4361 batches | lr 0.000000 | ms/batch 862.60 | loss  0.02 | ppl     1.02 | acc     0.77 | train_ae_norm     1.00\n",
      "[133/200][99/4361] Loss_D: 0.00811152 (Loss_D_real: 0.00533745 Loss_D_fake: 0.00277407) Loss_G: 0.42464018 Loss_Enh_Dec: -1.92641103\n",
      "| epoch 133 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  2.15 | ppl     8.57 | acc     0.71 | train_ae_norm     1.00\n",
      "[133/200][199/4361] Loss_D: 0.00268985 (Loss_D_real: 0.00102590 Loss_D_fake: 0.00166395) Loss_G: 0.38347450 Loss_Enh_Dec: -2.29724884\n",
      "| epoch 133 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.17 | ppl     8.72 | acc     0.72 | train_ae_norm     1.00\n",
      "[133/200][299/4361] Loss_D: 0.00201804 (Loss_D_real: 0.00023556 Loss_D_fake: 0.00178248) Loss_G: 0.40706205 Loss_Enh_Dec: -2.13085341\n",
      "| epoch 133 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.83 | loss  2.17 | ppl     8.80 | acc     0.67 | train_ae_norm     1.00\n",
      "[133/200][399/4361] Loss_D: 0.00213534 (Loss_D_real: 0.00088694 Loss_D_fake: 0.00124840) Loss_G: 0.40745354 Loss_Enh_Dec: -2.17974830\n",
      "| epoch 133 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  2.10 | ppl     8.16 | acc     0.72 | train_ae_norm     1.00\n",
      "[133/200][499/4361] Loss_D: 0.00045668 (Loss_D_real: 0.00036145 Loss_D_fake: 0.00009523) Loss_G: 0.45993811 Loss_Enh_Dec: -2.25819206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 133 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.01 | loss  2.17 | ppl     8.76 | acc     0.76 | train_ae_norm     1.00\n",
      "[133/200][599/4361] Loss_D: 0.00104350 (Loss_D_real: 0.00021107 Loss_D_fake: 0.00083243) Loss_G: 0.44417977 Loss_Enh_Dec: -2.21389937\n",
      "| epoch 133 |   600/ 4361 batches | lr 0.000000 | ms/batch 400.72 | loss  2.14 | ppl     8.47 | acc     0.71 | train_ae_norm     1.00\n",
      "[133/200][699/4361] Loss_D: 0.00177826 (Loss_D_real: 0.00057362 Loss_D_fake: 0.00120464) Loss_G: 0.46847567 Loss_Enh_Dec: -1.91332710\n",
      "| epoch 133 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.43 | loss  2.19 | ppl     8.95 | acc     0.73 | train_ae_norm     1.00\n",
      "[133/200][799/4361] Loss_D: 0.00856328 (Loss_D_real: 0.00718834 Loss_D_fake: 0.00137494) Loss_G: 0.38241440 Loss_Enh_Dec: -2.38376689\n",
      "| epoch 133 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.18 | ppl     8.88 | acc     0.71 | train_ae_norm     1.00\n",
      "[133/200][899/4361] Loss_D: 0.00681831 (Loss_D_real: 0.00005755 Loss_D_fake: 0.00676076) Loss_G: 0.42846107 Loss_Enh_Dec: -2.27275395\n",
      "| epoch 133 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  2.20 | ppl     9.04 | acc     0.74 | train_ae_norm     1.00\n",
      "[133/200][999/4361] Loss_D: 0.00054338 (Loss_D_real: 0.00020663 Loss_D_fake: 0.00033675) Loss_G: 0.38632986 Loss_Enh_Dec: -2.19292116\n",
      "| epoch 133 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  2.16 | ppl     8.70 | acc     0.74 | train_ae_norm     1.00\n",
      "[133/200][1099/4361] Loss_D: 0.00097109 (Loss_D_real: 0.00093635 Loss_D_fake: 0.00003474) Loss_G: 0.73979110 Loss_Enh_Dec: -2.12455058\n",
      "| epoch 133 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  2.14 | ppl     8.47 | acc     0.73 | train_ae_norm     1.00\n",
      "[133/200][1199/4361] Loss_D: 0.00461186 (Loss_D_real: 0.00201382 Loss_D_fake: 0.00259804) Loss_G: 0.45889169 Loss_Enh_Dec: -2.21072459\n",
      "| epoch 133 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.80 | loss  2.16 | ppl     8.71 | acc     0.76 | train_ae_norm     1.00\n",
      "[133/200][1299/4361] Loss_D: 0.00476757 (Loss_D_real: 0.00345087 Loss_D_fake: 0.00131671) Loss_G: 0.44432202 Loss_Enh_Dec: -1.92038119\n",
      "| epoch 133 |  1300/ 4361 batches | lr 0.000000 | ms/batch 400.62 | loss  2.20 | ppl     9.02 | acc     0.74 | train_ae_norm     1.00\n",
      "[133/200][1399/4361] Loss_D: 0.00105090 (Loss_D_real: 0.00038476 Loss_D_fake: 0.00066614) Loss_G: 0.44065547 Loss_Enh_Dec: -2.12043238\n",
      "| epoch 133 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.12 | loss  2.17 | ppl     8.76 | acc     0.71 | train_ae_norm     1.00\n",
      "[133/200][1499/4361] Loss_D: 0.00575070 (Loss_D_real: 0.00461145 Loss_D_fake: 0.00113925) Loss_G: 0.42618972 Loss_Enh_Dec: -2.19421077\n",
      "| epoch 133 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  2.23 | ppl     9.29 | acc     0.70 | train_ae_norm     1.00\n",
      "[133/200][1599/4361] Loss_D: 0.00177637 (Loss_D_real: 0.00136568 Loss_D_fake: 0.00041070) Loss_G: 0.67996532 Loss_Enh_Dec: -2.37677765\n",
      "| epoch 133 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.10 | loss  2.22 | ppl     9.16 | acc     0.71 | train_ae_norm     1.00\n",
      "[133/200][1699/4361] Loss_D: 0.00204733 (Loss_D_real: 0.00129173 Loss_D_fake: 0.00075561) Loss_G: 0.46424219 Loss_Enh_Dec: -2.34773374\n",
      "| epoch 133 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.16 | ppl     8.66 | acc     0.72 | train_ae_norm     1.00\n",
      "[133/200][1799/4361] Loss_D: 0.00648169 (Loss_D_real: 0.00438646 Loss_D_fake: 0.00209523) Loss_G: 0.38698611 Loss_Enh_Dec: -1.98658943\n",
      "| epoch 133 |  1800/ 4361 batches | lr 0.000000 | ms/batch 400.83 | loss  2.17 | ppl     8.72 | acc     0.74 | train_ae_norm     1.00\n",
      "[133/200][1899/4361] Loss_D: 0.00533119 (Loss_D_real: 0.00244788 Loss_D_fake: 0.00288331) Loss_G: 0.44479686 Loss_Enh_Dec: -2.33821845\n",
      "| epoch 133 |  1900/ 4361 batches | lr 0.000000 | ms/batch 400.93 | loss  2.20 | ppl     9.00 | acc     0.73 | train_ae_norm     1.00\n",
      "[133/200][1999/4361] Loss_D: 0.00054339 (Loss_D_real: 0.00013870 Loss_D_fake: 0.00040469) Loss_G: 0.45557880 Loss_Enh_Dec: -1.97511983\n",
      "| epoch 133 |  2000/ 4361 batches | lr 0.000000 | ms/batch 400.36 | loss  2.18 | ppl     8.84 | acc     0.76 | train_ae_norm     1.00\n",
      "[133/200][2099/4361] Loss_D: 0.00237950 (Loss_D_real: 0.00136792 Loss_D_fake: 0.00101158) Loss_G: 0.42111006 Loss_Enh_Dec: -2.71830797\n",
      "| epoch 133 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.20 | loss  2.23 | ppl     9.28 | acc     0.75 | train_ae_norm     1.00\n",
      "[133/200][2199/4361] Loss_D: 0.00281636 (Loss_D_real: 0.00063609 Loss_D_fake: 0.00218027) Loss_G: 0.37714368 Loss_Enh_Dec: -2.01016212\n",
      "| epoch 133 |  2200/ 4361 batches | lr 0.000000 | ms/batch 400.83 | loss  2.22 | ppl     9.17 | acc     0.74 | train_ae_norm     1.00\n",
      "[133/200][2299/4361] Loss_D: 0.02358195 (Loss_D_real: 0.00072251 Loss_D_fake: 0.02285944) Loss_G: 0.60107738 Loss_Enh_Dec: -2.08135152\n",
      "| epoch 133 |  2300/ 4361 batches | lr 0.000000 | ms/batch 400.75 | loss  2.22 | ppl     9.17 | acc     0.77 | train_ae_norm     1.00\n",
      "[133/200][2399/4361] Loss_D: 0.00261141 (Loss_D_real: 0.00020728 Loss_D_fake: 0.00240414) Loss_G: 0.38069427 Loss_Enh_Dec: -2.37361145\n",
      "| epoch 133 |  2400/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  2.20 | ppl     9.07 | acc     0.69 | train_ae_norm     1.00\n",
      "[133/200][2499/4361] Loss_D: 0.00581031 (Loss_D_real: 0.00273611 Loss_D_fake: 0.00307420) Loss_G: 0.43770075 Loss_Enh_Dec: -2.48304653\n",
      "| epoch 133 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  2.30 | ppl     9.98 | acc     0.71 | train_ae_norm     1.00\n",
      "[133/200][2599/4361] Loss_D: 0.00148997 (Loss_D_real: 0.00109517 Loss_D_fake: 0.00039480) Loss_G: 0.58386117 Loss_Enh_Dec: -2.37941337\n",
      "| epoch 133 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  2.33 | ppl    10.29 | acc     0.70 | train_ae_norm     1.00\n",
      "[133/200][2699/4361] Loss_D: 0.04341477 (Loss_D_real: 0.00078042 Loss_D_fake: 0.04263435) Loss_G: 0.68514931 Loss_Enh_Dec: -2.13337135\n",
      "| epoch 133 |  2700/ 4361 batches | lr 0.000000 | ms/batch 400.99 | loss  2.26 | ppl     9.61 | acc     0.69 | train_ae_norm     1.00\n",
      "[133/200][2799/4361] Loss_D: 0.00368207 (Loss_D_real: 0.00085719 Loss_D_fake: 0.00282487) Loss_G: 0.46894750 Loss_Enh_Dec: -2.05163360\n",
      "| epoch 133 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.41 | loss  2.32 | ppl    10.16 | acc     0.70 | train_ae_norm     1.00\n",
      "[133/200][2899/4361] Loss_D: 0.00552496 (Loss_D_real: 0.00487371 Loss_D_fake: 0.00065125) Loss_G: 0.41094694 Loss_Enh_Dec: -2.34795070\n",
      "| epoch 133 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.11 | loss  2.31 | ppl    10.11 | acc     0.74 | train_ae_norm     1.00\n",
      "[133/200][2999/4361] Loss_D: 0.00123362 (Loss_D_real: 0.00048925 Loss_D_fake: 0.00074437) Loss_G: 0.39785385 Loss_Enh_Dec: -1.91266823\n",
      "| epoch 133 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  2.32 | ppl    10.18 | acc     0.72 | train_ae_norm     1.00\n",
      "[133/200][3099/4361] Loss_D: 0.00137794 (Loss_D_real: 0.00019413 Loss_D_fake: 0.00118381) Loss_G: 0.43771917 Loss_Enh_Dec: -2.07441854\n",
      "| epoch 133 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.97 | loss  2.31 | ppl    10.07 | acc     0.67 | train_ae_norm     1.00\n",
      "[133/200][3199/4361] Loss_D: 0.00764301 (Loss_D_real: 0.00661841 Loss_D_fake: 0.00102460) Loss_G: 0.53708398 Loss_Enh_Dec: -2.18312502\n",
      "| epoch 133 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.75 | loss  2.32 | ppl    10.17 | acc     0.73 | train_ae_norm     1.00\n",
      "[133/200][3299/4361] Loss_D: 0.00270461 (Loss_D_real: 0.00047167 Loss_D_fake: 0.00223294) Loss_G: 0.51037675 Loss_Enh_Dec: -2.06408620\n",
      "| epoch 133 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  2.31 | ppl    10.11 | acc     0.70 | train_ae_norm     1.00\n",
      "[133/200][3399/4361] Loss_D: 0.00145448 (Loss_D_real: 0.00047684 Loss_D_fake: 0.00097764) Loss_G: 0.45564365 Loss_Enh_Dec: -2.15216517\n",
      "| epoch 133 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  2.32 | ppl    10.19 | acc     0.70 | train_ae_norm     1.00\n",
      "[133/200][3499/4361] Loss_D: 0.00355616 (Loss_D_real: 0.00034384 Loss_D_fake: 0.00321233) Loss_G: 0.46132359 Loss_Enh_Dec: -1.52558672\n",
      "| epoch 133 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.08 | loss  2.43 | ppl    11.35 | acc     0.66 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[133/200][3599/4361] Loss_D: 0.05020875 (Loss_D_real: 0.04852283 Loss_D_fake: 0.00168593) Loss_G: 0.55194718 Loss_Enh_Dec: -2.27728462\n",
      "| epoch 133 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  2.40 | ppl    11.02 | acc     0.71 | train_ae_norm     1.00\n",
      "[133/200][3699/4361] Loss_D: 0.00191730 (Loss_D_real: 0.00107763 Loss_D_fake: 0.00083967) Loss_G: 0.53486776 Loss_Enh_Dec: -2.45393491\n",
      "| epoch 133 |  3700/ 4361 batches | lr 0.000000 | ms/batch 400.93 | loss  2.36 | ppl    10.63 | acc     0.67 | train_ae_norm     1.00\n",
      "[133/200][3799/4361] Loss_D: 0.00337317 (Loss_D_real: 0.00299204 Loss_D_fake: 0.00038112) Loss_G: 0.54861772 Loss_Enh_Dec: -1.97809601\n",
      "| epoch 133 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  2.39 | ppl    10.96 | acc     0.74 | train_ae_norm     1.00\n",
      "[133/200][3899/4361] Loss_D: 0.01003810 (Loss_D_real: 0.00055497 Loss_D_fake: 0.00948313) Loss_G: 0.48389229 Loss_Enh_Dec: -1.69014728\n",
      "| epoch 133 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  2.40 | ppl    11.02 | acc     0.67 | train_ae_norm     1.00\n",
      "[133/200][3999/4361] Loss_D: 0.00138728 (Loss_D_real: 0.00029722 Loss_D_fake: 0.00109006) Loss_G: 0.42994571 Loss_Enh_Dec: -1.98234177\n",
      "| epoch 133 |  4000/ 4361 batches | lr 0.000000 | ms/batch 400.80 | loss  2.30 | ppl     9.93 | acc     0.71 | train_ae_norm     1.00\n",
      "[133/200][4099/4361] Loss_D: 0.00590264 (Loss_D_real: 0.00156254 Loss_D_fake: 0.00434010) Loss_G: 0.43899259 Loss_Enh_Dec: -2.48632884\n",
      "| epoch 133 |  4100/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  2.25 | ppl     9.44 | acc     0.69 | train_ae_norm     1.00\n",
      "[133/200][4199/4361] Loss_D: 0.01375192 (Loss_D_real: 0.01058524 Loss_D_fake: 0.00316669) Loss_G: 0.50113142 Loss_Enh_Dec: -1.89820731\n",
      "| epoch 133 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.28 | ppl     9.79 | acc     0.74 | train_ae_norm     1.00\n",
      "[133/200][4299/4361] Loss_D: 0.00233187 (Loss_D_real: 0.00130975 Loss_D_fake: 0.00102211) Loss_G: 0.37929773 Loss_Enh_Dec: -1.91780913\n",
      "| epoch 133 |  4300/ 4361 batches | lr 0.000000 | ms/batch 400.86 | loss  2.25 | ppl     9.47 | acc     0.74 | train_ae_norm     1.00\n",
      "| end of epoch 133 | time: 1851.53s | test loss  2.40 | test ppl 10.97 | acc 0.766\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 134 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.696\n",
      "  Average training loss discriminator: 0.700\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.495\n",
      "  Test Loss: 4.562\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 134 |     0/ 4361 batches | lr 0.000000 | ms/batch 864.28 | loss  0.02 | ppl     1.02 | acc     0.74 | train_ae_norm     1.00\n",
      "[134/200][99/4361] Loss_D: 0.00435957 (Loss_D_real: 0.00401964 Loss_D_fake: 0.00033994) Loss_G: 0.55271596 Loss_Enh_Dec: -2.13068819\n",
      "| epoch 134 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.79 | loss  2.23 | ppl     9.31 | acc     0.70 | train_ae_norm     1.00\n",
      "[134/200][199/4361] Loss_D: 0.03399287 (Loss_D_real: 0.03272874 Loss_D_fake: 0.00126413) Loss_G: 0.42182705 Loss_Enh_Dec: -1.94733238\n",
      "| epoch 134 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.39 | loss  2.29 | ppl     9.90 | acc     0.72 | train_ae_norm     1.00\n",
      "[134/200][299/4361] Loss_D: 0.00056054 (Loss_D_real: 0.00003531 Loss_D_fake: 0.00052524) Loss_G: 0.42597386 Loss_Enh_Dec: -2.45772386\n",
      "| epoch 134 |   300/ 4361 batches | lr 0.000000 | ms/batch 400.98 | loss  2.29 | ppl     9.86 | acc     0.67 | train_ae_norm     1.00\n",
      "[134/200][399/4361] Loss_D: 0.00351615 (Loss_D_real: 0.00131795 Loss_D_fake: 0.00219820) Loss_G: 0.43724284 Loss_Enh_Dec: -1.92838514\n",
      "| epoch 134 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.28 | ppl     9.75 | acc     0.69 | train_ae_norm     1.00\n",
      "[134/200][499/4361] Loss_D: 0.00125897 (Loss_D_real: 0.00043714 Loss_D_fake: 0.00082183) Loss_G: 0.45771155 Loss_Enh_Dec: -2.09794283\n",
      "| epoch 134 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  2.50 | ppl    12.19 | acc     0.73 | train_ae_norm     1.00\n",
      "[134/200][599/4361] Loss_D: 0.02583232 (Loss_D_real: 0.02453540 Loss_D_fake: 0.00129692) Loss_G: 0.44163123 Loss_Enh_Dec: -1.40207446\n",
      "| epoch 134 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  2.54 | ppl    12.70 | acc     0.63 | train_ae_norm     1.00\n",
      "[134/200][699/4361] Loss_D: 0.00111397 (Loss_D_real: 0.00071557 Loss_D_fake: 0.00039841) Loss_G: 0.62736714 Loss_Enh_Dec: -1.59987092\n",
      "| epoch 134 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.26 | loss  2.43 | ppl    11.35 | acc     0.71 | train_ae_norm     1.00\n",
      "[134/200][799/4361] Loss_D: 0.00296131 (Loss_D_real: 0.00255634 Loss_D_fake: 0.00040497) Loss_G: 0.41645923 Loss_Enh_Dec: -2.26412320\n",
      "| epoch 134 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.08 | loss  2.27 | ppl     9.67 | acc     0.69 | train_ae_norm     1.00\n",
      "[134/200][899/4361] Loss_D: 0.00278855 (Loss_D_real: 0.00145104 Loss_D_fake: 0.00133751) Loss_G: 0.46397263 Loss_Enh_Dec: -2.02766204\n",
      "| epoch 134 |   900/ 4361 batches | lr 0.000000 | ms/batch 400.70 | loss  2.25 | ppl     9.51 | acc     0.72 | train_ae_norm     1.00\n",
      "[134/200][999/4361] Loss_D: 0.00051057 (Loss_D_real: 0.00005222 Loss_D_fake: 0.00045835) Loss_G: 0.47742492 Loss_Enh_Dec: -1.89956117\n",
      "| epoch 134 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.28 | loss  2.31 | ppl    10.10 | acc     0.72 | train_ae_norm     1.00\n",
      "[134/200][1099/4361] Loss_D: 0.03768786 (Loss_D_real: 0.03757614 Loss_D_fake: 0.00011172) Loss_G: 0.69123250 Loss_Enh_Dec: -2.13630891\n",
      "| epoch 134 |  1100/ 4361 batches | lr 0.000000 | ms/batch 400.92 | loss  2.31 | ppl    10.06 | acc     0.70 | train_ae_norm     1.00\n",
      "[134/200][1199/4361] Loss_D: 0.00096946 (Loss_D_real: 0.00019815 Loss_D_fake: 0.00077131) Loss_G: 0.50622886 Loss_Enh_Dec: -2.15357041\n",
      "| epoch 134 |  1200/ 4361 batches | lr 0.000000 | ms/batch 400.48 | loss  2.26 | ppl     9.62 | acc     0.76 | train_ae_norm     1.00\n",
      "[134/200][1299/4361] Loss_D: 0.00185506 (Loss_D_real: 0.00046188 Loss_D_fake: 0.00139317) Loss_G: 0.40246230 Loss_Enh_Dec: -2.09901977\n",
      "| epoch 134 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.33 | ppl    10.28 | acc     0.72 | train_ae_norm     1.00\n",
      "[134/200][1399/4361] Loss_D: 0.00078544 (Loss_D_real: 0.00038200 Loss_D_fake: 0.00040345) Loss_G: 0.46650824 Loss_Enh_Dec: -1.67256153\n",
      "| epoch 134 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.52 | loss  2.28 | ppl     9.77 | acc     0.66 | train_ae_norm     1.00\n",
      "[134/200][1499/4361] Loss_D: 0.00243727 (Loss_D_real: 0.00127194 Loss_D_fake: 0.00116533) Loss_G: 0.36756507 Loss_Enh_Dec: -2.24787688\n",
      "| epoch 134 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.93 | loss  2.34 | ppl    10.38 | acc     0.68 | train_ae_norm     1.00\n",
      "[134/200][1599/4361] Loss_D: 0.00655978 (Loss_D_real: 0.00471974 Loss_D_fake: 0.00184004) Loss_G: 0.58419961 Loss_Enh_Dec: -2.06592917\n",
      "| epoch 134 |  1600/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  2.33 | ppl    10.32 | acc     0.72 | train_ae_norm     1.00\n",
      "[134/200][1699/4361] Loss_D: 0.00129827 (Loss_D_real: 0.00056564 Loss_D_fake: 0.00073263) Loss_G: 0.53698403 Loss_Enh_Dec: -1.82865608\n",
      "| epoch 134 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.28 | ppl     9.79 | acc     0.68 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[134/200][1799/4361] Loss_D: 0.00479154 (Loss_D_real: 0.00030795 Loss_D_fake: 0.00448359) Loss_G: 0.44760752 Loss_Enh_Dec: -2.28257990\n",
      "| epoch 134 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.81 | loss  2.32 | ppl    10.20 | acc     0.70 | train_ae_norm     1.00\n",
      "[134/200][1899/4361] Loss_D: 0.00525287 (Loss_D_real: 0.00390362 Loss_D_fake: 0.00134925) Loss_G: 0.47333080 Loss_Enh_Dec: -1.96432877\n",
      "| epoch 134 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.36 | loss  2.36 | ppl    10.63 | acc     0.69 | train_ae_norm     1.00\n",
      "[134/200][1999/4361] Loss_D: 0.00491713 (Loss_D_real: 0.00025276 Loss_D_fake: 0.00466437) Loss_G: 0.49947926 Loss_Enh_Dec: -1.97693121\n",
      "| epoch 134 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.91 | loss  2.29 | ppl     9.85 | acc     0.74 | train_ae_norm     1.00\n",
      "[134/200][2099/4361] Loss_D: 0.00160661 (Loss_D_real: 0.00140654 Loss_D_fake: 0.00020007) Loss_G: 0.57575434 Loss_Enh_Dec: -1.96254122\n",
      "| epoch 134 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.40 | loss  2.31 | ppl    10.12 | acc     0.73 | train_ae_norm     1.00\n",
      "[134/200][2199/4361] Loss_D: 0.00142638 (Loss_D_real: 0.00105205 Loss_D_fake: 0.00037433) Loss_G: 0.47266674 Loss_Enh_Dec: -2.39941621\n",
      "| epoch 134 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  2.29 | ppl     9.85 | acc     0.74 | train_ae_norm     1.00\n",
      "[134/200][2299/4361] Loss_D: 0.02492350 (Loss_D_real: 0.02187683 Loss_D_fake: 0.00304667) Loss_G: 0.36305705 Loss_Enh_Dec: -2.62859011\n",
      "| epoch 134 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  2.36 | ppl    10.55 | acc     0.72 | train_ae_norm     1.00\n",
      "[134/200][2399/4361] Loss_D: 0.00123927 (Loss_D_real: 0.00054114 Loss_D_fake: 0.00069813) Loss_G: 0.42501074 Loss_Enh_Dec: -2.19889998\n",
      "| epoch 134 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.37 | loss  2.27 | ppl     9.69 | acc     0.69 | train_ae_norm     1.00\n",
      "[134/200][2499/4361] Loss_D: 0.00441720 (Loss_D_real: 0.00187393 Loss_D_fake: 0.00254327) Loss_G: 0.56354326 Loss_Enh_Dec: -2.46648645\n",
      "| epoch 134 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.38 | loss  2.28 | ppl     9.74 | acc     0.72 | train_ae_norm     1.00\n",
      "[134/200][2599/4361] Loss_D: 0.00250256 (Loss_D_real: 0.00129158 Loss_D_fake: 0.00121099) Loss_G: 0.40476152 Loss_Enh_Dec: -2.70433164\n",
      "| epoch 134 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  2.28 | ppl     9.73 | acc     0.66 | train_ae_norm     1.00\n",
      "[134/200][2699/4361] Loss_D: 0.00364953 (Loss_D_real: 0.00020992 Loss_D_fake: 0.00343962) Loss_G: 0.34398246 Loss_Enh_Dec: -2.47836685\n",
      "| epoch 134 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.21 | loss  2.29 | ppl     9.87 | acc     0.70 | train_ae_norm     1.00\n",
      "[134/200][2799/4361] Loss_D: 0.00202415 (Loss_D_real: 0.00064786 Loss_D_fake: 0.00137629) Loss_G: 0.41736498 Loss_Enh_Dec: -2.78051734\n",
      "| epoch 134 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.25 | ppl     9.52 | acc     0.68 | train_ae_norm     1.00\n",
      "[134/200][2899/4361] Loss_D: 0.00241957 (Loss_D_real: 0.00021208 Loss_D_fake: 0.00220749) Loss_G: 0.39251214 Loss_Enh_Dec: -2.52886510\n",
      "| epoch 134 |  2900/ 4361 batches | lr 0.000000 | ms/batch 400.94 | loss  2.29 | ppl     9.88 | acc     0.71 | train_ae_norm     1.00\n",
      "[134/200][2999/4361] Loss_D: 0.00132168 (Loss_D_real: 0.00009017 Loss_D_fake: 0.00123151) Loss_G: 0.48851520 Loss_Enh_Dec: -2.30218005\n",
      "| epoch 134 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  2.28 | ppl     9.80 | acc     0.74 | train_ae_norm     1.00\n",
      "[134/200][3099/4361] Loss_D: 0.00019479 (Loss_D_real: 0.00009506 Loss_D_fake: 0.00009972) Loss_G: 0.72140354 Loss_Enh_Dec: -2.25797391\n",
      "| epoch 134 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  2.28 | ppl     9.80 | acc     0.71 | train_ae_norm     1.00\n",
      "[134/200][3199/4361] Loss_D: 0.00134376 (Loss_D_real: 0.00022666 Loss_D_fake: 0.00111710) Loss_G: 0.40254354 Loss_Enh_Dec: -2.40256047\n",
      "| epoch 134 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  2.29 | ppl     9.90 | acc     0.73 | train_ae_norm     1.00\n",
      "[134/200][3299/4361] Loss_D: 0.01088099 (Loss_D_real: 0.00832390 Loss_D_fake: 0.00255709) Loss_G: 0.41869688 Loss_Enh_Dec: -2.49590731\n",
      "| epoch 134 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.69 | loss  2.36 | ppl    10.57 | acc     0.69 | train_ae_norm     1.00\n",
      "[134/200][3399/4361] Loss_D: 0.01100862 (Loss_D_real: 0.00860095 Loss_D_fake: 0.00240767) Loss_G: 0.38009864 Loss_Enh_Dec: -1.90969682\n",
      "| epoch 134 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  2.33 | ppl    10.29 | acc     0.68 | train_ae_norm     1.00\n",
      "[134/200][3499/4361] Loss_D: 0.02234324 (Loss_D_real: 0.00253665 Loss_D_fake: 0.01980660) Loss_G: 0.43438074 Loss_Enh_Dec: -2.05082369\n",
      "| epoch 134 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  2.23 | ppl     9.34 | acc     0.71 | train_ae_norm     1.00\n",
      "[134/200][3599/4361] Loss_D: 0.00109735 (Loss_D_real: 0.00002616 Loss_D_fake: 0.00107119) Loss_G: 0.44121489 Loss_Enh_Dec: -2.43471146\n",
      "| epoch 134 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.86 | loss  2.23 | ppl     9.28 | acc     0.71 | train_ae_norm     1.00\n",
      "[134/200][3699/4361] Loss_D: 0.00268804 (Loss_D_real: 0.00017347 Loss_D_fake: 0.00251457) Loss_G: 0.68167651 Loss_Enh_Dec: -1.68535042\n",
      "| epoch 134 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  2.25 | ppl     9.52 | acc     0.70 | train_ae_norm     1.00\n",
      "[134/200][3799/4361] Loss_D: 0.00190559 (Loss_D_real: 0.00054466 Loss_D_fake: 0.00136093) Loss_G: 0.42114505 Loss_Enh_Dec: -2.30058908\n",
      "| epoch 134 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  2.25 | ppl     9.53 | acc     0.76 | train_ae_norm     1.00\n",
      "[134/200][3899/4361] Loss_D: 0.00718683 (Loss_D_real: 0.00308632 Loss_D_fake: 0.00410051) Loss_G: 0.44503069 Loss_Enh_Dec: -2.02596402\n",
      "| epoch 134 |  3900/ 4361 batches | lr 0.000000 | ms/batch 400.96 | loss  2.27 | ppl     9.65 | acc     0.72 | train_ae_norm     1.00\n",
      "[134/200][3999/4361] Loss_D: 0.04392846 (Loss_D_real: 0.04242709 Loss_D_fake: 0.00150137) Loss_G: 0.42007065 Loss_Enh_Dec: -2.13972783\n",
      "| epoch 134 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.22 | ppl     9.21 | acc     0.72 | train_ae_norm     1.00\n",
      "[134/200][4099/4361] Loss_D: 0.00137128 (Loss_D_real: 0.00003357 Loss_D_fake: 0.00133771) Loss_G: 0.41398826 Loss_Enh_Dec: -1.71691287\n",
      "| epoch 134 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.85 | loss  2.17 | ppl     8.78 | acc     0.73 | train_ae_norm     1.00\n",
      "[134/200][4199/4361] Loss_D: 0.00417368 (Loss_D_real: 0.00047150 Loss_D_fake: 0.00370219) Loss_G: 0.35348925 Loss_Enh_Dec: -1.68806922\n",
      "| epoch 134 |  4200/ 4361 batches | lr 0.000000 | ms/batch 400.97 | loss  2.26 | ppl     9.58 | acc     0.73 | train_ae_norm     1.00\n",
      "[134/200][4299/4361] Loss_D: 0.00511127 (Loss_D_real: 0.00039811 Loss_D_fake: 0.00471316) Loss_G: 0.39726758 Loss_Enh_Dec: -1.47652924\n",
      "| epoch 134 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  2.23 | ppl     9.28 | acc     0.75 | train_ae_norm     1.00\n",
      "| end of epoch 134 | time: 1852.76s | test loss  2.33 | test ppl 10.24 | acc 0.779\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 135 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:52.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:07.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:11.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:22.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.698\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.492\n",
      "  Test Loss: 4.735\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 135 |     0/ 4361 batches | lr 0.000000 | ms/batch 862.46 | loss  0.02 | ppl     1.02 | acc     0.74 | train_ae_norm     1.00\n",
      "[135/200][99/4361] Loss_D: 0.00246258 (Loss_D_real: 0.00206942 Loss_D_fake: 0.00039316) Loss_G: 0.49520737 Loss_Enh_Dec: -1.77977026\n",
      "| epoch 135 |   100/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  2.18 | ppl     8.88 | acc     0.69 | train_ae_norm     1.00\n",
      "[135/200][199/4361] Loss_D: 0.00325027 (Loss_D_real: 0.00125289 Loss_D_fake: 0.00199738) Loss_G: 0.42977086 Loss_Enh_Dec: -1.85090530\n",
      "| epoch 135 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.30 | loss  2.22 | ppl     9.20 | acc     0.77 | train_ae_norm     1.00\n",
      "[135/200][299/4361] Loss_D: 0.00553098 (Loss_D_real: 0.00150873 Loss_D_fake: 0.00402225) Loss_G: 0.70355409 Loss_Enh_Dec: -2.20447612\n",
      "| epoch 135 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.62 | loss  2.21 | ppl     9.08 | acc     0.68 | train_ae_norm     1.00\n",
      "[135/200][399/4361] Loss_D: 0.00359888 (Loss_D_real: 0.00071503 Loss_D_fake: 0.00288384) Loss_G: 0.37701264 Loss_Enh_Dec: -2.18188453\n",
      "| epoch 135 |   400/ 4361 batches | lr 0.000000 | ms/batch 400.86 | loss  2.11 | ppl     8.29 | acc     0.74 | train_ae_norm     1.00\n",
      "[135/200][499/4361] Loss_D: 0.00341561 (Loss_D_real: 0.00042020 Loss_D_fake: 0.00299541) Loss_G: 0.34463987 Loss_Enh_Dec: -2.59125400\n",
      "| epoch 135 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.20 | ppl     9.00 | acc     0.75 | train_ae_norm     1.00\n",
      "[135/200][599/4361] Loss_D: 0.00217256 (Loss_D_real: 0.00008678 Loss_D_fake: 0.00208578) Loss_G: 0.37190139 Loss_Enh_Dec: -1.68468213\n",
      "| epoch 135 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  2.15 | ppl     8.62 | acc     0.71 | train_ae_norm     1.00\n",
      "[135/200][699/4361] Loss_D: 0.00358753 (Loss_D_real: 0.00205289 Loss_D_fake: 0.00153464) Loss_G: 0.41678330 Loss_Enh_Dec: -1.66701412\n",
      "| epoch 135 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.21 | ppl     9.09 | acc     0.74 | train_ae_norm     1.00\n",
      "[135/200][799/4361] Loss_D: 0.00295270 (Loss_D_real: 0.00002630 Loss_D_fake: 0.00292641) Loss_G: 0.40755448 Loss_Enh_Dec: -1.73224318\n",
      "| epoch 135 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.65 | loss  2.25 | ppl     9.50 | acc     0.72 | train_ae_norm     1.00\n",
      "[135/200][899/4361] Loss_D: 0.00686694 (Loss_D_real: 0.00280245 Loss_D_fake: 0.00406449) Loss_G: 0.47674465 Loss_Enh_Dec: -1.75449884\n",
      "| epoch 135 |   900/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  2.29 | ppl     9.88 | acc     0.72 | train_ae_norm     1.00\n",
      "[135/200][999/4361] Loss_D: 0.00430870 (Loss_D_real: 0.00032555 Loss_D_fake: 0.00398315) Loss_G: 0.42243490 Loss_Enh_Dec: -1.77168655\n",
      "| epoch 135 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  2.25 | ppl     9.44 | acc     0.73 | train_ae_norm     1.00\n",
      "[135/200][1099/4361] Loss_D: 0.00354624 (Loss_D_real: 0.00072773 Loss_D_fake: 0.00281850) Loss_G: 0.42455396 Loss_Enh_Dec: -2.25762939\n",
      "| epoch 135 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.92 | loss  2.24 | ppl     9.38 | acc     0.70 | train_ae_norm     1.00\n",
      "[135/200][1199/4361] Loss_D: 0.00322174 (Loss_D_real: 0.00101292 Loss_D_fake: 0.00220882) Loss_G: 0.60746688 Loss_Enh_Dec: -1.72035444\n",
      "| epoch 135 |  1200/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  2.19 | ppl     8.96 | acc     0.76 | train_ae_norm     1.00\n",
      "[135/200][1299/4361] Loss_D: 0.00104546 (Loss_D_real: 0.00003805 Loss_D_fake: 0.00100742) Loss_G: 0.37451875 Loss_Enh_Dec: -2.31144404\n",
      "| epoch 135 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.99 | loss  2.21 | ppl     9.15 | acc     0.70 | train_ae_norm     1.00\n",
      "[135/200][1399/4361] Loss_D: 0.00159560 (Loss_D_real: 0.00024090 Loss_D_fake: 0.00135471) Loss_G: 0.39939952 Loss_Enh_Dec: -1.86173522\n",
      "| epoch 135 |  1400/ 4361 batches | lr 0.000000 | ms/batch 401.16 | loss  2.21 | ppl     9.10 | acc     0.72 | train_ae_norm     1.00\n",
      "[135/200][1499/4361] Loss_D: 0.00256287 (Loss_D_real: 0.00027917 Loss_D_fake: 0.00228370) Loss_G: 0.41348752 Loss_Enh_Dec: -2.21308899\n",
      "| epoch 135 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.26 | ppl     9.58 | acc     0.70 | train_ae_norm     1.00\n",
      "[135/200][1599/4361] Loss_D: 0.00121744 (Loss_D_real: 0.00006988 Loss_D_fake: 0.00114756) Loss_G: 0.57414019 Loss_Enh_Dec: -2.09108281\n",
      "| epoch 135 |  1600/ 4361 batches | lr 0.000000 | ms/batch 402.18 | loss  2.24 | ppl     9.36 | acc     0.71 | train_ae_norm     1.00\n",
      "[135/200][1699/4361] Loss_D: 0.00596315 (Loss_D_real: 0.00165298 Loss_D_fake: 0.00431018) Loss_G: 0.40052515 Loss_Enh_Dec: -1.71183622\n",
      "| epoch 135 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  2.21 | ppl     9.10 | acc     0.68 | train_ae_norm     1.00\n",
      "[135/200][1799/4361] Loss_D: 0.00161442 (Loss_D_real: 0.00111706 Loss_D_fake: 0.00049735) Loss_G: 0.79944038 Loss_Enh_Dec: -1.72729003\n",
      "| epoch 135 |  1800/ 4361 batches | lr 0.000000 | ms/batch 401.91 | loss  2.19 | ppl     8.97 | acc     0.72 | train_ae_norm     1.00\n",
      "[135/200][1899/4361] Loss_D: 0.00190161 (Loss_D_real: 0.00069931 Loss_D_fake: 0.00120230) Loss_G: 0.52377844 Loss_Enh_Dec: -1.99013102\n",
      "| epoch 135 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  2.22 | ppl     9.18 | acc     0.74 | train_ae_norm     1.00\n",
      "[135/200][1999/4361] Loss_D: 0.04382186 (Loss_D_real: 0.04310162 Loss_D_fake: 0.00072024) Loss_G: 0.43058202 Loss_Enh_Dec: -1.92011321\n",
      "| epoch 135 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.70 | loss  2.20 | ppl     8.98 | acc     0.75 | train_ae_norm     1.00\n",
      "[135/200][2099/4361] Loss_D: 0.00054537 (Loss_D_real: 0.00014854 Loss_D_fake: 0.00039684) Loss_G: 0.41612846 Loss_Enh_Dec: -2.10627341\n",
      "| epoch 135 |  2100/ 4361 batches | lr 0.000000 | ms/batch 401.63 | loss  2.21 | ppl     9.15 | acc     0.78 | train_ae_norm     1.00\n",
      "[135/200][2199/4361] Loss_D: 0.00256396 (Loss_D_real: 0.00049186 Loss_D_fake: 0.00207210) Loss_G: 0.38241982 Loss_Enh_Dec: -2.18592572\n",
      "| epoch 135 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  2.24 | ppl     9.43 | acc     0.69 | train_ae_norm     1.00\n",
      "[135/200][2299/4361] Loss_D: 0.00678600 (Loss_D_real: 0.00520045 Loss_D_fake: 0.00158554) Loss_G: 0.50411129 Loss_Enh_Dec: -2.41370845\n",
      "| epoch 135 |  2300/ 4361 batches | lr 0.000000 | ms/batch 402.35 | loss  2.59 | ppl    13.35 | acc     0.66 | train_ae_norm     1.00\n",
      "[135/200][2399/4361] Loss_D: 0.00110722 (Loss_D_real: 0.00043854 Loss_D_fake: 0.00066868) Loss_G: 0.55340552 Loss_Enh_Dec: -1.86608374\n",
      "| epoch 135 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  2.27 | ppl     9.73 | acc     0.65 | train_ae_norm     1.00\n",
      "[135/200][2499/4361] Loss_D: 0.00250179 (Loss_D_real: 0.00068601 Loss_D_fake: 0.00181577) Loss_G: 0.38503096 Loss_Enh_Dec: -1.61365795\n",
      "| epoch 135 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.66 | loss  2.25 | ppl     9.47 | acc     0.73 | train_ae_norm     1.00\n",
      "[135/200][2599/4361] Loss_D: 0.00135492 (Loss_D_real: 0.00011269 Loss_D_fake: 0.00124223) Loss_G: 0.44707814 Loss_Enh_Dec: -2.42161250\n",
      "| epoch 135 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.89 | loss  2.21 | ppl     9.13 | acc     0.68 | train_ae_norm     1.00\n",
      "[135/200][2699/4361] Loss_D: 0.00287809 (Loss_D_real: 0.00069144 Loss_D_fake: 0.00218666) Loss_G: 0.41686270 Loss_Enh_Dec: -1.87036955\n",
      "| epoch 135 |  2700/ 4361 batches | lr 0.000000 | ms/batch 402.03 | loss  2.21 | ppl     9.15 | acc     0.72 | train_ae_norm     1.00\n",
      "[135/200][2799/4361] Loss_D: 0.00182898 (Loss_D_real: 0.00014937 Loss_D_fake: 0.00167961) Loss_G: 0.51725322 Loss_Enh_Dec: -2.04579520\n",
      "| epoch 135 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  2.19 | ppl     8.95 | acc     0.71 | train_ae_norm     1.00\n",
      "[135/200][2899/4361] Loss_D: 0.00113796 (Loss_D_real: 0.00018018 Loss_D_fake: 0.00095778) Loss_G: 0.41988984 Loss_Enh_Dec: -1.88640904\n",
      "| epoch 135 |  2900/ 4361 batches | lr 0.000000 | ms/batch 401.72 | loss  2.18 | ppl     8.89 | acc     0.73 | train_ae_norm     1.00\n",
      "[135/200][2999/4361] Loss_D: 0.00484995 (Loss_D_real: 0.00181792 Loss_D_fake: 0.00303204) Loss_G: 0.44155475 Loss_Enh_Dec: -2.17812419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 135 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  2.23 | ppl     9.26 | acc     0.73 | train_ae_norm     1.00\n",
      "[135/200][3099/4361] Loss_D: 0.00186326 (Loss_D_real: 0.00032626 Loss_D_fake: 0.00153701) Loss_G: 0.45083547 Loss_Enh_Dec: -2.67118120\n",
      "| epoch 135 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  2.21 | ppl     9.16 | acc     0.72 | train_ae_norm     1.00\n",
      "[135/200][3199/4361] Loss_D: 0.00308181 (Loss_D_real: 0.00115974 Loss_D_fake: 0.00192207) Loss_G: 0.47002441 Loss_Enh_Dec: -2.28861547\n",
      "| epoch 135 |  3200/ 4361 batches | lr 0.000000 | ms/batch 401.99 | loss  2.27 | ppl     9.67 | acc     0.75 | train_ae_norm     1.00\n",
      "[135/200][3299/4361] Loss_D: 0.00171173 (Loss_D_real: 0.00158885 Loss_D_fake: 0.00012288) Loss_G: 0.84073633 Loss_Enh_Dec: -2.24127316\n",
      "| epoch 135 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  2.29 | ppl     9.85 | acc     0.72 | train_ae_norm     1.00\n",
      "[135/200][3399/4361] Loss_D: 0.01980634 (Loss_D_real: 0.01807225 Loss_D_fake: 0.00173409) Loss_G: 0.45794064 Loss_Enh_Dec: -1.82156301\n",
      "| epoch 135 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  2.26 | ppl     9.60 | acc     0.71 | train_ae_norm     1.00\n",
      "[135/200][3499/4361] Loss_D: 0.00969208 (Loss_D_real: 0.00860656 Loss_D_fake: 0.00108551) Loss_G: 0.38645592 Loss_Enh_Dec: -2.25773501\n",
      "| epoch 135 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.24 | loss  2.17 | ppl     8.80 | acc     0.70 | train_ae_norm     1.00\n",
      "[135/200][3599/4361] Loss_D: 0.00114952 (Loss_D_real: 0.00022040 Loss_D_fake: 0.00092913) Loss_G: 0.39223117 Loss_Enh_Dec: -1.69533277\n",
      "| epoch 135 |  3600/ 4361 batches | lr 0.000000 | ms/batch 401.17 | loss  2.21 | ppl     9.13 | acc     0.73 | train_ae_norm     1.00\n",
      "[135/200][3699/4361] Loss_D: 0.00235504 (Loss_D_real: 0.00158964 Loss_D_fake: 0.00076540) Loss_G: 0.38930807 Loss_Enh_Dec: -1.87426949\n",
      "| epoch 135 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.98 | loss  2.23 | ppl     9.34 | acc     0.69 | train_ae_norm     1.00\n",
      "[135/200][3799/4361] Loss_D: 0.00057202 (Loss_D_real: 0.00018119 Loss_D_fake: 0.00039083) Loss_G: 0.69954842 Loss_Enh_Dec: -1.96309650\n",
      "| epoch 135 |  3800/ 4361 batches | lr 0.000000 | ms/batch 401.88 | loss  2.23 | ppl     9.28 | acc     0.79 | train_ae_norm     1.00\n",
      "[135/200][3899/4361] Loss_D: 0.00158041 (Loss_D_real: 0.00046398 Loss_D_fake: 0.00111642) Loss_G: 0.44857174 Loss_Enh_Dec: -1.50174427\n",
      "| epoch 135 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  2.24 | ppl     9.39 | acc     0.70 | train_ae_norm     1.00\n",
      "[135/200][3999/4361] Loss_D: 0.03929899 (Loss_D_real: 0.03812889 Loss_D_fake: 0.00117010) Loss_G: 0.45176935 Loss_Enh_Dec: -1.10486102\n",
      "| epoch 135 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.61 | loss  2.22 | ppl     9.23 | acc     0.73 | train_ae_norm     1.00\n",
      "[135/200][4099/4361] Loss_D: 0.00387051 (Loss_D_real: 0.00066992 Loss_D_fake: 0.00320058) Loss_G: 0.39617202 Loss_Enh_Dec: -1.48340404\n",
      "| epoch 135 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.13 | loss  2.19 | ppl     8.96 | acc     0.73 | train_ae_norm     1.00\n",
      "[135/200][4199/4361] Loss_D: 0.00116048 (Loss_D_real: 0.00002380 Loss_D_fake: 0.00113668) Loss_G: 0.42985150 Loss_Enh_Dec: -1.78089702\n",
      "| epoch 135 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  2.23 | ppl     9.33 | acc     0.75 | train_ae_norm     1.00\n",
      "[135/200][4299/4361] Loss_D: 0.00314977 (Loss_D_real: 0.00164255 Loss_D_fake: 0.00150723) Loss_G: 0.47666690 Loss_Enh_Dec: -1.71157002\n",
      "| epoch 135 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.94 | loss  2.21 | ppl     9.08 | acc     0.76 | train_ae_norm     1.00\n",
      "| end of epoch 135 | time: 1853.42s | test loss  2.39 | test ppl 10.91 | acc 0.764\n",
      "bleu_self:  [3.38080266e-01 2.22965872e-01 7.80255522e-02 5.60837164e-02\n",
      " 4.68454745e-05]\n",
      "bleu_test:  [8.38121254e-01 1.22641062e-01 8.29550022e-07 2.20303630e-09\n",
      " 6.39570606e-11]\n",
      "bleu_self: [0.33808027,0.22296587,0.07802555,0.05608372,0.00004685]\n",
      "bleu_test: [0.83812125,0.12264106,0.00000083,0.00000000,0.00000000]\n",
      "New saving model: epoch 135.\n",
      "Saving models to ./results/yahoo_merge_assigned_results\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 136 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:27.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:57.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:01.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:16.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:20.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.697\n",
      "  Average training loss discriminator: 0.697\n",
      "  Training epcoh took: 0:01:27\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.485\n",
      "  Test Loss: 4.760\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 136 |     0/ 4361 batches | lr 0.000000 | ms/batch 873.34 | loss  0.02 | ppl     1.02 | acc     0.76 | train_ae_norm     1.00\n",
      "[136/200][99/4361] Loss_D: 0.00504399 (Loss_D_real: 0.00413190 Loss_D_fake: 0.00091209) Loss_G: 0.39728984 Loss_Enh_Dec: -1.82715690\n",
      "| epoch 136 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.40 | loss  2.20 | ppl     9.04 | acc     0.69 | train_ae_norm     1.00\n",
      "[136/200][199/4361] Loss_D: 0.00106954 (Loss_D_real: 0.00013565 Loss_D_fake: 0.00093389) Loss_G: 0.42418367 Loss_Enh_Dec: -1.57574928\n",
      "| epoch 136 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.77 | loss  2.24 | ppl     9.37 | acc     0.76 | train_ae_norm     1.00\n",
      "[136/200][299/4361] Loss_D: 0.00232438 (Loss_D_real: 0.00031565 Loss_D_fake: 0.00200873) Loss_G: 0.46705833 Loss_Enh_Dec: -1.97665155\n",
      "| epoch 136 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.47 | loss  2.23 | ppl     9.32 | acc     0.66 | train_ae_norm     1.00\n",
      "[136/200][399/4361] Loss_D: 0.00315019 (Loss_D_real: 0.00004618 Loss_D_fake: 0.00310401) Loss_G: 0.41326308 Loss_Enh_Dec: -1.59947503\n",
      "| epoch 136 |   400/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  2.16 | ppl     8.67 | acc     0.72 | train_ae_norm     1.00\n",
      "[136/200][499/4361] Loss_D: 0.00140094 (Loss_D_real: 0.00088923 Loss_D_fake: 0.00051171) Loss_G: 0.43835783 Loss_Enh_Dec: -2.33585715\n",
      "| epoch 136 |   500/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.25 | ppl     9.47 | acc     0.73 | train_ae_norm     1.00\n",
      "[136/200][599/4361] Loss_D: 0.00432331 (Loss_D_real: 0.00313514 Loss_D_fake: 0.00118817) Loss_G: 0.57789969 Loss_Enh_Dec: -1.57625651\n",
      "| epoch 136 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.87 | loss  2.21 | ppl     9.14 | acc     0.70 | train_ae_norm     1.00\n",
      "[136/200][699/4361] Loss_D: 0.00134179 (Loss_D_real: 0.00017062 Loss_D_fake: 0.00117117) Loss_G: 0.44195476 Loss_Enh_Dec: -1.94218481\n",
      "| epoch 136 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.18 | loss  2.27 | ppl     9.65 | acc     0.71 | train_ae_norm     1.00\n",
      "[136/200][799/4361] Loss_D: 0.00095129 (Loss_D_real: 0.00008698 Loss_D_fake: 0.00086431) Loss_G: 0.44507438 Loss_Enh_Dec: -1.50286472\n",
      "| epoch 136 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  2.24 | ppl     9.44 | acc     0.71 | train_ae_norm     1.00\n",
      "[136/200][899/4361] Loss_D: 0.00102870 (Loss_D_real: 0.00005958 Loss_D_fake: 0.00096912) Loss_G: 0.45204288 Loss_Enh_Dec: -1.78559911\n",
      "| epoch 136 |   900/ 4361 batches | lr 0.000000 | ms/batch 402.36 | loss  2.29 | ppl     9.86 | acc     0.75 | train_ae_norm     1.00\n",
      "[136/200][999/4361] Loss_D: 0.00205672 (Loss_D_real: 0.00117995 Loss_D_fake: 0.00087676) Loss_G: 0.65412015 Loss_Enh_Dec: -1.90246606\n",
      "| epoch 136 |  1000/ 4361 batches | lr 0.000000 | ms/batch 401.67 | loss  2.27 | ppl     9.68 | acc     0.71 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[136/200][1099/4361] Loss_D: 0.00328347 (Loss_D_real: 0.00023525 Loss_D_fake: 0.00304822) Loss_G: 0.41478512 Loss_Enh_Dec: -1.43626010\n",
      "| epoch 136 |  1100/ 4361 batches | lr 0.000000 | ms/batch 401.33 | loss  2.22 | ppl     9.24 | acc     0.70 | train_ae_norm     1.00\n",
      "[136/200][1199/4361] Loss_D: 0.01666483 (Loss_D_real: 0.01371816 Loss_D_fake: 0.00294667) Loss_G: 0.43029147 Loss_Enh_Dec: -1.38708448\n",
      "| epoch 136 |  1200/ 4361 batches | lr 0.000000 | ms/batch 402.09 | loss  2.23 | ppl     9.35 | acc     0.76 | train_ae_norm     1.00\n",
      "[136/200][1299/4361] Loss_D: 0.00196626 (Loss_D_real: 0.00080932 Loss_D_fake: 0.00115695) Loss_G: 0.52029794 Loss_Enh_Dec: -1.62013018\n",
      "| epoch 136 |  1300/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  2.26 | ppl     9.62 | acc     0.72 | train_ae_norm     1.00\n",
      "[136/200][1399/4361] Loss_D: 0.00046280 (Loss_D_real: 0.00007985 Loss_D_fake: 0.00038295) Loss_G: 0.48628169 Loss_Enh_Dec: -1.73763025\n",
      "| epoch 136 |  1400/ 4361 batches | lr 0.000000 | ms/batch 402.18 | loss  2.24 | ppl     9.38 | acc     0.70 | train_ae_norm     1.00\n",
      "[136/200][1499/4361] Loss_D: 0.00141816 (Loss_D_real: 0.00005017 Loss_D_fake: 0.00136800) Loss_G: 0.49460974 Loss_Enh_Dec: -2.13160944\n",
      "| epoch 136 |  1500/ 4361 batches | lr 0.000000 | ms/batch 401.57 | loss  2.29 | ppl     9.92 | acc     0.70 | train_ae_norm     1.00\n",
      "[136/200][1599/4361] Loss_D: 0.00405683 (Loss_D_real: 0.00040093 Loss_D_fake: 0.00365590) Loss_G: 0.41164294 Loss_Enh_Dec: -2.19234324\n",
      "| epoch 136 |  1600/ 4361 batches | lr 0.000000 | ms/batch 402.03 | loss  2.27 | ppl     9.65 | acc     0.72 | train_ae_norm     1.00\n",
      "[136/200][1699/4361] Loss_D: 0.01598130 (Loss_D_real: 0.01519700 Loss_D_fake: 0.00078430) Loss_G: 0.40542960 Loss_Enh_Dec: -2.09522104\n",
      "| epoch 136 |  1700/ 4361 batches | lr 0.000000 | ms/batch 401.55 | loss  2.22 | ppl     9.25 | acc     0.68 | train_ae_norm     1.00\n",
      "[136/200][1799/4361] Loss_D: 0.00293067 (Loss_D_real: 0.00017787 Loss_D_fake: 0.00275280) Loss_G: 0.51172686 Loss_Enh_Dec: -2.44903493\n",
      "| epoch 136 |  1800/ 4361 batches | lr 0.000000 | ms/batch 402.28 | loss  2.21 | ppl     9.15 | acc     0.71 | train_ae_norm     1.00\n",
      "[136/200][1899/4361] Loss_D: 0.00367520 (Loss_D_real: 0.00083601 Loss_D_fake: 0.00283918) Loss_G: 0.38172171 Loss_Enh_Dec: -2.08409858\n",
      "| epoch 136 |  1900/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  2.25 | ppl     9.52 | acc     0.71 | train_ae_norm     1.00\n",
      "[136/200][1999/4361] Loss_D: 0.02815480 (Loss_D_real: 0.02729710 Loss_D_fake: 0.00085770) Loss_G: 0.54533821 Loss_Enh_Dec: -2.27200699\n",
      "| epoch 136 |  2000/ 4361 batches | lr 0.000000 | ms/batch 401.23 | loss  2.20 | ppl     9.07 | acc     0.75 | train_ae_norm     1.00\n",
      "[136/200][2099/4361] Loss_D: 0.00997580 (Loss_D_real: 0.00839871 Loss_D_fake: 0.00157709) Loss_G: 0.45199957 Loss_Enh_Dec: -2.17137003\n",
      "| epoch 136 |  2100/ 4361 batches | lr 0.000000 | ms/batch 402.08 | loss  2.23 | ppl     9.34 | acc     0.75 | train_ae_norm     1.00\n",
      "[136/200][2199/4361] Loss_D: 0.00514215 (Loss_D_real: 0.00264709 Loss_D_fake: 0.00249506) Loss_G: 0.43439552 Loss_Enh_Dec: -2.03112149\n",
      "| epoch 136 |  2200/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  2.24 | ppl     9.44 | acc     0.74 | train_ae_norm     1.00\n",
      "[136/200][2299/4361] Loss_D: 0.00318271 (Loss_D_real: 0.00153605 Loss_D_fake: 0.00164666) Loss_G: 0.50259662 Loss_Enh_Dec: -2.19961452\n",
      "| epoch 136 |  2300/ 4361 batches | lr 0.000000 | ms/batch 401.95 | loss  2.25 | ppl     9.45 | acc     0.74 | train_ae_norm     1.00\n",
      "[136/200][2399/4361] Loss_D: 0.00172844 (Loss_D_real: 0.00079682 Loss_D_fake: 0.00093162) Loss_G: 0.43774197 Loss_Enh_Dec: -1.87754822\n",
      "| epoch 136 |  2400/ 4361 batches | lr 0.000000 | ms/batch 401.73 | loss  2.24 | ppl     9.38 | acc     0.69 | train_ae_norm     1.00\n",
      "[136/200][2499/4361] Loss_D: 0.00131024 (Loss_D_real: 0.00045870 Loss_D_fake: 0.00085153) Loss_G: 0.49397984 Loss_Enh_Dec: -2.01938081\n",
      "| epoch 136 |  2500/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  2.41 | ppl    11.14 | acc     0.71 | train_ae_norm     1.00\n",
      "[136/200][2599/4361] Loss_D: 0.00268753 (Loss_D_real: 0.00068793 Loss_D_fake: 0.00199960) Loss_G: 0.53453326 Loss_Enh_Dec: -2.70836639\n",
      "| epoch 136 |  2600/ 4361 batches | lr 0.000000 | ms/batch 401.48 | loss  2.23 | ppl     9.32 | acc     0.69 | train_ae_norm     1.00\n",
      "[136/200][2699/4361] Loss_D: 0.00280375 (Loss_D_real: 0.00204019 Loss_D_fake: 0.00076355) Loss_G: 0.41446385 Loss_Enh_Dec: -2.67081881\n",
      "| epoch 136 |  2700/ 4361 batches | lr 0.000000 | ms/batch 401.22 | loss  2.23 | ppl     9.26 | acc     0.70 | train_ae_norm     1.00\n",
      "[136/200][2799/4361] Loss_D: 0.00086078 (Loss_D_real: 0.00017221 Loss_D_fake: 0.00068857) Loss_G: 0.43845469 Loss_Enh_Dec: -2.49347138\n",
      "| epoch 136 |  2800/ 4361 batches | lr 0.000000 | ms/batch 401.83 | loss  2.19 | ppl     8.90 | acc     0.73 | train_ae_norm     1.00\n",
      "[136/200][2899/4361] Loss_D: 0.00242051 (Loss_D_real: 0.00063412 Loss_D_fake: 0.00178638) Loss_G: 0.42459425 Loss_Enh_Dec: -2.79903150\n",
      "| epoch 136 |  2900/ 4361 batches | lr 0.000000 | ms/batch 402.19 | loss  2.23 | ppl     9.30 | acc     0.74 | train_ae_norm     1.00\n",
      "[136/200][2999/4361] Loss_D: 0.00356350 (Loss_D_real: 0.00326484 Loss_D_fake: 0.00029866) Loss_G: 0.65359652 Loss_Enh_Dec: -2.34849763\n",
      "| epoch 136 |  3000/ 4361 batches | lr 0.000000 | ms/batch 401.82 | loss  2.28 | ppl     9.74 | acc     0.71 | train_ae_norm     1.00\n",
      "[136/200][3099/4361] Loss_D: 0.00049504 (Loss_D_real: 0.00003592 Loss_D_fake: 0.00045912) Loss_G: 0.58099574 Loss_Enh_Dec: -1.98668945\n",
      "| epoch 136 |  3100/ 4361 batches | lr 0.000000 | ms/batch 401.60 | loss  2.24 | ppl     9.42 | acc     0.73 | train_ae_norm     1.00\n",
      "[136/200][3199/4361] Loss_D: 0.00086166 (Loss_D_real: 0.00021506 Loss_D_fake: 0.00064660) Loss_G: 0.42167035 Loss_Enh_Dec: -2.43810415\n",
      "| epoch 136 |  3200/ 4361 batches | lr 0.000000 | ms/batch 402.25 | loss  2.22 | ppl     9.24 | acc     0.72 | train_ae_norm     1.00\n",
      "[136/200][3299/4361] Loss_D: 0.00051899 (Loss_D_real: 0.00007198 Loss_D_fake: 0.00044701) Loss_G: 0.44378453 Loss_Enh_Dec: -2.46227574\n",
      "| epoch 136 |  3300/ 4361 batches | lr 0.000000 | ms/batch 401.27 | loss  2.23 | ppl     9.30 | acc     0.72 | train_ae_norm     1.00\n",
      "[136/200][3399/4361] Loss_D: 0.00908786 (Loss_D_real: 0.00585537 Loss_D_fake: 0.00323249) Loss_G: 0.50705260 Loss_Enh_Dec: -2.50573850\n",
      "| epoch 136 |  3400/ 4361 batches | lr 0.000000 | ms/batch 401.68 | loss  2.25 | ppl     9.53 | acc     0.72 | train_ae_norm     1.00\n",
      "[136/200][3499/4361] Loss_D: 0.00098305 (Loss_D_real: 0.00021836 Loss_D_fake: 0.00076469) Loss_G: 0.48307857 Loss_Enh_Dec: -2.64035463\n",
      "| epoch 136 |  3500/ 4361 batches | lr 0.000000 | ms/batch 401.80 | loss  2.30 | ppl     9.99 | acc     0.69 | train_ae_norm     1.00\n",
      "[136/200][3599/4361] Loss_D: 0.00153352 (Loss_D_real: 0.00058683 Loss_D_fake: 0.00094669) Loss_G: 0.48014307 Loss_Enh_Dec: -2.49560046\n",
      "| epoch 136 |  3600/ 4361 batches | lr 0.000000 | ms/batch 402.43 | loss  2.30 | ppl    10.02 | acc     0.71 | train_ae_norm     1.00\n",
      "[136/200][3699/4361] Loss_D: 0.00857420 (Loss_D_real: 0.00632575 Loss_D_fake: 0.00224844) Loss_G: 0.43591666 Loss_Enh_Dec: -2.67681956\n",
      "| epoch 136 |  3700/ 4361 batches | lr 0.000000 | ms/batch 401.31 | loss  2.25 | ppl     9.51 | acc     0.70 | train_ae_norm     1.00\n",
      "[136/200][3799/4361] Loss_D: 0.00071378 (Loss_D_real: 0.00012738 Loss_D_fake: 0.00058640) Loss_G: 0.46989736 Loss_Enh_Dec: -2.64026070\n",
      "| epoch 136 |  3800/ 4361 batches | lr 0.000000 | ms/batch 402.43 | loss  2.23 | ppl     9.30 | acc     0.77 | train_ae_norm     1.00\n",
      "[136/200][3899/4361] Loss_D: 0.00134358 (Loss_D_real: 0.00020917 Loss_D_fake: 0.00113440) Loss_G: 0.49876246 Loss_Enh_Dec: -2.28178382\n",
      "| epoch 136 |  3900/ 4361 batches | lr 0.000000 | ms/batch 401.56 | loss  2.23 | ppl     9.33 | acc     0.71 | train_ae_norm     1.00\n",
      "[136/200][3999/4361] Loss_D: 0.00574841 (Loss_D_real: 0.00533716 Loss_D_fake: 0.00041126) Loss_G: 0.80161476 Loss_Enh_Dec: -2.36447310\n",
      "| epoch 136 |  4000/ 4361 batches | lr 0.000000 | ms/batch 401.74 | loss  2.27 | ppl     9.64 | acc     0.72 | train_ae_norm     1.00\n",
      "[136/200][4099/4361] Loss_D: 0.00319594 (Loss_D_real: 0.00255232 Loss_D_fake: 0.00064363) Loss_G: 0.48317620 Loss_Enh_Dec: -2.02067542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 136 |  4100/ 4361 batches | lr 0.000000 | ms/batch 401.46 | loss  2.27 | ppl     9.64 | acc     0.71 | train_ae_norm     1.00\n",
      "[136/200][4199/4361] Loss_D: 0.00198124 (Loss_D_real: 0.00029583 Loss_D_fake: 0.00168541) Loss_G: 0.47951147 Loss_Enh_Dec: -2.11502814\n",
      "| epoch 136 |  4200/ 4361 batches | lr 0.000000 | ms/batch 401.44 | loss  2.31 | ppl    10.11 | acc     0.72 | train_ae_norm     1.00\n",
      "[136/200][4299/4361] Loss_D: 0.00231893 (Loss_D_real: 0.00017068 Loss_D_fake: 0.00214825) Loss_G: 0.59530884 Loss_Enh_Dec: -2.36191773\n",
      "| epoch 136 |  4300/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.28 | ppl     9.79 | acc     0.70 | train_ae_norm     1.00\n",
      "| end of epoch 136 | time: 1855.04s | test loss  2.40 | test ppl 11.04 | acc 0.764\n",
      "bleu_self:  [2.22938381e-01 3.88491377e-09 1.04452896e-11 5.57819826e-13\n",
      " 9.88323447e-14]\n",
      "bleu_test:  [8.20486111e-01 9.40558459e-02 6.77875076e-07 1.95407268e-09\n",
      " 6.27957307e-11]\n",
      "bleu_self: [0.22293838,0.00000000,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.82048611,0.09405585,0.00000068,0.00000000,0.00000000]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 137 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    230.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    230.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    230.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    230.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    230.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    230.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    230.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    230.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    230.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    230.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    230.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    230.    Elapsed: 0:00:45.\n",
      "  Batch   130  of    230.    Elapsed: 0:00:49.\n",
      "  Batch   140  of    230.    Elapsed: 0:00:53.\n",
      "  Batch   150  of    230.    Elapsed: 0:00:56.\n",
      "  Batch   160  of    230.    Elapsed: 0:01:00.\n",
      "  Batch   170  of    230.    Elapsed: 0:01:04.\n",
      "  Batch   180  of    230.    Elapsed: 0:01:08.\n",
      "  Batch   190  of    230.    Elapsed: 0:01:12.\n",
      "  Batch   200  of    230.    Elapsed: 0:01:15.\n",
      "  Batch   210  of    230.    Elapsed: 0:01:19.\n",
      "  Batch   220  of    230.    Elapsed: 0:01:23.\n",
      "\n",
      "  Average training loss generetor: 0.696\n",
      "  Average training loss discriminator: 0.699\n",
      "  Training epcoh took: 0:01:26\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.487\n",
      "  Test Loss: 4.652\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch 137 |     0/ 4361 batches | lr 0.000000 | ms/batch 868.05 | loss  0.02 | ppl     1.02 | acc     0.73 | train_ae_norm     1.00\n",
      "[137/200][99/4361] Loss_D: 0.00113681 (Loss_D_real: 0.00009903 Loss_D_fake: 0.00103779) Loss_G: 0.42654634 Loss_Enh_Dec: -2.57526565\n",
      "| epoch 137 |   100/ 4361 batches | lr 0.000000 | ms/batch 402.00 | loss  2.42 | ppl    11.26 | acc     0.66 | train_ae_norm     1.00\n",
      "[137/200][199/4361] Loss_D: 0.02618849 (Loss_D_real: 0.02539206 Loss_D_fake: 0.00079643) Loss_G: 0.36659107 Loss_Enh_Dec: -2.39126754\n",
      "| epoch 137 |   200/ 4361 batches | lr 0.000000 | ms/batch 401.32 | loss  2.36 | ppl    10.59 | acc     0.75 | train_ae_norm     1.00\n",
      "[137/200][299/4361] Loss_D: 0.00470670 (Loss_D_real: 0.00391083 Loss_D_fake: 0.00079588) Loss_G: 0.47519752 Loss_Enh_Dec: -2.51590943\n",
      "| epoch 137 |   300/ 4361 batches | lr 0.000000 | ms/batch 401.51 | loss  2.25 | ppl     9.45 | acc     0.69 | train_ae_norm     1.00\n",
      "[137/200][399/4361] Loss_D: 0.00147709 (Loss_D_real: 0.00004651 Loss_D_fake: 0.00143059) Loss_G: 0.37441483 Loss_Enh_Dec: -2.72263861\n",
      "| epoch 137 |   400/ 4361 batches | lr 0.000000 | ms/batch 402.19 | loss  2.20 | ppl     9.01 | acc     0.74 | train_ae_norm     1.00\n",
      "[137/200][499/4361] Loss_D: 0.01412804 (Loss_D_real: 0.01247586 Loss_D_fake: 0.00165218) Loss_G: 0.47191009 Loss_Enh_Dec: -2.48798895\n",
      "| epoch 137 |   500/ 4361 batches | lr 0.000000 | ms/batch 402.05 | loss  2.35 | ppl    10.50 | acc     0.60 | train_ae_norm     1.00\n",
      "[137/200][599/4361] Loss_D: 0.00254642 (Loss_D_real: 0.00014295 Loss_D_fake: 0.00240347) Loss_G: 0.42194286 Loss_Enh_Dec: -2.46369529\n",
      "| epoch 137 |   600/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  3.29 | ppl    26.87 | acc     0.47 | train_ae_norm     1.00\n",
      "[137/200][699/4361] Loss_D: 0.02416392 (Loss_D_real: 0.00022952 Loss_D_fake: 0.02393441) Loss_G: 0.45449361 Loss_Enh_Dec: -2.28770709\n",
      "| epoch 137 |   700/ 4361 batches | lr 0.000000 | ms/batch 401.42 | loss  3.46 | ppl    31.70 | acc     0.51 | train_ae_norm     1.00\n",
      "[137/200][799/4361] Loss_D: 0.00071796 (Loss_D_real: 0.00012215 Loss_D_fake: 0.00059581) Loss_G: 0.45946169 Loss_Enh_Dec: -2.49747062\n",
      "| epoch 137 |   800/ 4361 batches | lr 0.000000 | ms/batch 401.99 | loss  2.51 | ppl    12.27 | acc     0.70 | train_ae_norm     1.00\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d879385",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6f00b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(accuracy_array))\n",
    "#0.4025 previous best accuracy (bert-base-cased, epoch 50, 5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294b4517",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_array[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0c30df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_save = pd.DataFrame(accuracy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84fb59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_save.to_csv('accuracy_array_assigned_yahoo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339f9940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
