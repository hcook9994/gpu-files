{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "673b1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils import to_gpu, Corpus, batchify, train_ngram_lm, get_ppl, create_exp_dir\n",
    "from models import Seq2Seq, MLP_D, MLP_D_local, MLP_G\n",
    "from bleu_self import *\n",
    "from bleu_test import *\n",
    "import datetime\n",
    "now_time = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f208b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='TILGAN for unconditional generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "520324a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "228c8e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path=\"data/MS_COCO\"\n",
    "data_path=\"../../TREC\"\n",
    "save=\"./results/TREC_results\"\n",
    "maxlen=24\n",
    "batch_size=100\n",
    "eval_batch_size = 16\n",
    "noise_seq_length = 15\n",
    "add_noise=True #what does this do? - question applies to most parameters\n",
    "emsize=512\n",
    "nhidden=512\n",
    "nlayers=2\n",
    "nheads=4\n",
    "nff=1024\n",
    "aehidden=56\n",
    "noise_r=0.05\n",
    "hidden_init=True\n",
    "dropout=0.3\n",
    "gpu=True\n",
    "z_size=100\n",
    "arch_g='300-300'\n",
    "gan_g_activation=True\n",
    "arch_d='300-300'\n",
    "gan_d_local=True\n",
    "gan_d_local_windowsize=3\n",
    "arch_d_local='300-300'\n",
    "lr_ae=0.12\n",
    "lr_gan_e=1e-04\n",
    "beta1=0.5\n",
    "lr_gan_g=4e-04\n",
    "lr_gan_d=1e-04\n",
    "epochs=20\n",
    "sample=True\n",
    "clip=1\n",
    "log_interval=100\n",
    "gan_lambda=0.1\n",
    "niters_gan_d=1\n",
    "niters_gan_g=1\n",
    "niters_gan_ae=1\n",
    "niters_gan_dec=1\n",
    "niters_gan_schedule=''\n",
    "niters_ae=1\n",
    "gan_type='kl'\n",
    "enhance_dec=True\n",
    "gan_gp_lambda=1\n",
    "vocab_size=0\n",
    "lowercase=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24e5f2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original vocab 8678; pruned to 8682\n",
      "Number of sentences dropped from ../../TREC/train.txt: 29 out of 5452 total\n",
      "Number of sentences dropped from ../../TREC/test.txt: 0 out of 500 total\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(data_path,\n",
    "                maxlen=maxlen,\n",
    "                vocab_size=vocab_size,\n",
    "                lowercase=lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daa4ee98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 8682\n"
     ]
    }
   ],
   "source": [
    "# save arguments\n",
    "ntokens = len(corpus.dictionary.word2idx)\n",
    "print(\"Vocabulary Size: {}\".format(ntokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f66f2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : ./results/TREC_results\n"
     ]
    }
   ],
   "source": [
    "# exp dir\n",
    "create_exp_dir(os.path.join(save), ['train.py', 'models.py', 'utils.py'],\n",
    "        dict=corpus.dictionary.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a05112cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging(str, to_stdout=True):\n",
    "    with open(os.path.join(save, 'log.txt'), 'a') as f:\n",
    "        f.write(str + '\\n')\n",
    "    if to_stdout:\n",
    "        print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4042220b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data!\n"
     ]
    }
   ],
   "source": [
    "test_data = batchify(corpus.test, eval_batch_size, maxlen, shuffle=False)\n",
    "train_data = batchify(corpus.train, batch_size, maxlen,  shuffle=True)\n",
    "\n",
    "print(\"Loaded data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51cc7067",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Build the models\n",
    "###############################################################################\n",
    "autoencoder = Seq2Seq(add_noise=add_noise,\n",
    "                      emsize=emsize,\n",
    "                      nhidden=nhidden,\n",
    "                      ntokens=ntokens,\n",
    "                      nlayers=nlayers,\n",
    "                      nheads=nheads,\n",
    "                      nff=nff,\n",
    "                      aehidden=aehidden,\n",
    "                      noise_r=noise_r,\n",
    "                      hidden_init=hidden_init,\n",
    "                      dropout=dropout,\n",
    "                      gpu=True)\n",
    "nlatent = aehidden * (maxlen+1)\n",
    "gan_gen = MLP_G(ninput=z_size, noutput=nlatent, layers=arch_g, gan_g_activation=gan_g_activation)\n",
    "gan_disc = MLP_D(ninput=nlatent, noutput=1, layers=arch_d)\n",
    "gan_disc_local = MLP_D_local(ninput=gan_d_local_windowsize * aehidden, noutput=1, layers=arch_d_local)\n",
    "\n",
    "optimizer_ae = optim.SGD(autoencoder.parameters(), lr=lr_ae)\n",
    "\n",
    "\n",
    "optimizer_gan_e = optim.Adam(autoencoder.encoder.parameters(),\n",
    "                             lr=lr_gan_e,\n",
    "                             betas=(beta1, 0.999))\n",
    "optimizer_gan_g = optim.Adam(gan_gen.parameters(),\n",
    "                             lr=lr_gan_g,\n",
    "                             betas=(beta1, 0.999))\n",
    "optimizer_gan_d = optim.Adam(gan_disc.parameters(),\n",
    "                             lr=lr_gan_d,\n",
    "                             betas=(beta1, 0.999))\n",
    "optimizer_gan_d_local = optim.Adam(gan_disc_local.parameters(),\n",
    "                             lr=lr_gan_d,\n",
    "                             betas=(beta1, 0.999))\n",
    "optimizer_gan_dec = optim.Adam(autoencoder.decoder.parameters(),\n",
    "                             lr=lr_gan_e,\n",
    "                             betas=(beta1, 0.999))\n",
    "\n",
    "autoencoder = autoencoder.to(device)\n",
    "gan_gen = gan_gen.to(device)\n",
    "gan_disc = gan_disc.to(device)\n",
    "gan_disc_local = gan_disc_local.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42a2dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    print(\"Saving models to {}\".format(save))\n",
    "    torch.save({\n",
    "        \"ae\": autoencoder.state_dict(),\n",
    "        \"gan_g\": gan_gen.state_dict(),\n",
    "        \"gan_d\": gan_disc.state_dict(),\n",
    "        \"gan_d_local\": gan_disc_local.state_dict()\n",
    "\n",
    "        },\n",
    "        os.path.join(save, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b528fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        param_norm = p.grad.data.norm(2)\n",
    "        total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** (1. / 2)\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7a9fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    model_args = json.load(open(os.path.join(save, 'options.json'), 'r'))\n",
    "    word2idx = json.load(open(os.path.join(save, 'vocab.json'), 'r'))\n",
    "    idx2word = {v: k for k, v in word2idx.items()}\n",
    "\n",
    "    print('Loading models from {}'.format(save))\n",
    "    loaded = torch.load(os.path.join(save, \"model.pt\"))\n",
    "    autoencoder.load_state_dict(loaded.get('ae'))\n",
    "    gan_gen.load_state_dict(loaded.get('gan_g'))\n",
    "    gan_disc.load_state_dict(loaded.get('gan_d'))\n",
    "    gan_disc_local.load_state_dict(loaded.get('gan_d_local'))\n",
    "    return model_args, idx2word, autoencoder, gan_gen, gan_disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03edc3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_autoencoder(data_source, epoch):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    autoencoder.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary.word2idx)\n",
    "    all_accuracies = 0\n",
    "    bcnt = 0\n",
    "    for i, batch in enumerate(data_source):\n",
    "        source, target, lengths = batch\n",
    "        with torch.no_grad():\n",
    "            source = Variable(source.to(device))\n",
    "            target = Variable(target.to(device))\n",
    "            mask = target.gt(0)\n",
    "            masked_target = target.masked_select(mask)\n",
    "            # examples x ntokens\n",
    "            output_mask = mask.unsqueeze(1).expand(mask.size(0), ntokens)\n",
    "\n",
    "            # output: batch x seq_len x ntokens\n",
    "            output = autoencoder(source, lengths, source, add_noise=add_noise, soft=False)\n",
    "            flattened_output = output.view(-1, ntokens)\n",
    "\n",
    "            masked_output = \\\n",
    "                flattened_output.masked_select(output_mask).view(-1, ntokens)\n",
    "            total_loss += F.cross_entropy(masked_output, masked_target)\n",
    "\n",
    "            # accuracy\n",
    "            max_vals, max_indices = torch.max(masked_output, 1)\n",
    "            accuracy = torch.mean(max_indices.eq(masked_target).float()).data.item()\n",
    "            all_accuracies += accuracy\n",
    "            bcnt += 1\n",
    "\n",
    "        aeoutf = os.path.join(save, \"autoencoder.txt\")\n",
    "        with open(aeoutf, \"w\") as f:\n",
    "            max_values, max_indices = torch.max(output, 2)\n",
    "            max_indices = \\\n",
    "                max_indices.view(output.size(0), -1).data.cpu().numpy()\n",
    "            target = target.view(output.size(0), -1).data.cpu().numpy()\n",
    "            for t, idx in zip(target, max_indices):\n",
    "                # real sentence\n",
    "                chars = \" \".join([corpus.dictionary.idx2word[x] for x in t])\n",
    "                f.write(chars + '\\n')\n",
    "                # autoencoder output sentence\n",
    "                chars = \" \".join([corpus.dictionary.idx2word[x] for x in idx])\n",
    "                f.write(chars + '\\n'*2)\n",
    "\n",
    "    return total_loss.item() / len(data_source), all_accuracies/bcnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b838544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fixed_noise(noise, to_save):\n",
    "    gan_gen.eval()\n",
    "    autoencoder.eval()\n",
    "\n",
    "    fake_hidden = gan_gen(noise)\n",
    "    max_indices = autoencoder.generate(fake_hidden, maxlen, sample=sample)\n",
    "\n",
    "    with open(to_save, \"w\") as f:\n",
    "        max_indices = max_indices.data.cpu().numpy()\n",
    "        for idx in max_indices:\n",
    "            # generated sentence\n",
    "            words = [corpus.dictionary.idx2word[x] for x in idx]\n",
    "            # truncate sentences to first occurrence of <eos>\n",
    "            truncated_sent = []\n",
    "            for w in words:\n",
    "                if w != '<eos>':\n",
    "                    truncated_sent.append(w)\n",
    "                else:\n",
    "                    break\n",
    "            chars = \" \".join(truncated_sent)\n",
    "            f.write(chars + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58dced27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(gen_text_savepath):\n",
    "    selfbleu = bleu_self(gen_text_savepath)\n",
    "    real_text = os.path.join(data_path, \"test.txt\")\n",
    "    testbleu = bleu_test(real_text, gen_text_savepath)\n",
    "    return selfbleu, testbleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "124dcc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae(epoch, batch, total_loss_ae, start_time, i):\n",
    "    '''Train AE with the negative log-likelihood loss'''\n",
    "    autoencoder.train()\n",
    "    optimizer_ae.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = Variable(source.to(device))\n",
    "    target = Variable(target.to(device))\n",
    "    output = autoencoder(source, lengths, source, add_noise=add_noise, soft=False)\n",
    "\n",
    "    mask = target.gt(0)\n",
    "    masked_target = target.masked_select(mask)\n",
    "    output_mask = mask.unsqueeze(1).expand(mask.size(0), ntokens)\n",
    "    flat_output = output.view(-1, ntokens)\n",
    "    masked_output = flat_output.masked_select(output_mask).view(-1, ntokens)\n",
    "    loss = F.cross_entropy(masked_output, masked_target)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(autoencoder.parameters(), clip)\n",
    "    train_ae_norm = cal_norm(autoencoder)\n",
    "    optimizer_ae.step()\n",
    "\n",
    "    total_loss_ae += loss.data.item()\n",
    "    if i % log_interval == 0:\n",
    "        probs = F.softmax(masked_output, dim=-1)\n",
    "        max_vals, max_indices = torch.max(probs, 1)\n",
    "        accuracy = torch.mean(max_indices.eq(masked_target).float()).data.item()\n",
    "        cur_loss = total_loss_ae / log_interval\n",
    "        elapsed = time.time() - start_time\n",
    "        logging('| epoch {:3d} | {:5d}/{:5d} batches | lr {:08.6f} | ms/batch {:5.2f} | '\n",
    "                'loss {:5.2f} | ppl {:8.2f} | acc {:8.2f} | train_ae_norm {:8.2f}'.format(\n",
    "                epoch, i, len(train_data), 0,\n",
    "                elapsed * 1000 / log_interval,\n",
    "                cur_loss, math.exp(cur_loss), accuracy, train_ae_norm))\n",
    "\n",
    "        total_loss_ae = 0\n",
    "        start_time = time.time()\n",
    "    return total_loss_ae, start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64870e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_g(gan_type='kl'):\n",
    "    gan_gen.train()\n",
    "    optimizer_gan_g.zero_grad()\n",
    "\n",
    "    z = Variable(torch.Tensor(batch_size, z_size).normal_(0, 1).to(device))\n",
    "    fake_hidden = gan_gen(z)\n",
    "    fake_score = gan_disc(fake_hidden)\n",
    "\n",
    "    if gan_d_local:\n",
    "        idx = random.randint(0, maxlen - gan_d_local_windowsize)\n",
    "        fake_hidden_local = fake_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        fake_score_local = gan_disc_local(fake_hidden_local)\n",
    "\n",
    "        if gan_type == 'kl':\n",
    "            errG = -(torch.exp(fake_score.detach()).clamp(0.5, 2) * fake_score).mean() -(torch.exp(fake_score_local.detach()).clamp(0.5, 2) * fake_score_local).mean()\n",
    "        else:\n",
    "            errG = -fake_score.mean() -fake_score_local.mean()\n",
    "    else:\n",
    "        if gan_type == 'kl':\n",
    "            errG = -(torch.exp(fake_score.detach()).clamp(0.5, 2) * fake_score).mean()\n",
    "        else:\n",
    "            errG = -fake_score.mean()\n",
    "\n",
    "\n",
    "    errG *= gan_lambda\n",
    "    errG.backward()\n",
    "    optimizer_gan_g.step()\n",
    "\n",
    "    return errG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33244527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_dec(gan_type='kl'):\n",
    "    autoencoder.decoder.train()\n",
    "    optimizer_gan_dec.zero_grad()\n",
    "\n",
    "    z = Variable(torch.Tensor(batch_size, z_size).normal_(0, 1).to(device))\n",
    "    fake_hidden = gan_gen(z)\n",
    "\n",
    "    # 1. decoder  - soft distribution\n",
    "    enhance_source, max_indices= autoencoder.generate_enh_dec(fake_hidden, maxlen, sample=sample)\n",
    "    # 2. soft distribution - > encoder  -> fake_hidden\n",
    "    enhance_hidden = autoencoder(enhance_source, None, max_indices, add_noise=add_noise, soft=True, encode_only=True)\n",
    "    fake_score = gan_disc(enhance_hidden)\n",
    "\n",
    "    if gan_d_local:\n",
    "        idx = random.randint(0, maxlen - gan_d_local_windowsize)\n",
    "        fake_hidden_local = fake_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        fake_score_local = gan_disc_local(fake_hidden_local)\n",
    "\n",
    "        if gan_type == 'kl':\n",
    "            errG = -(torch.exp(fake_score.detach()).clamp(0.5, 2) * fake_score).mean() -(torch.exp(fake_score_local.detach()).clamp(0.5, 2) * fake_score_local).mean()\n",
    "        else:\n",
    "            errG = -fake_score.mean() -fake_score_local.mean()\n",
    "    else:\n",
    "        if gan_type == 'kl':\n",
    "            errG = -(torch.exp(fake_score.detach()).clamp(0.5, 2) * fake_score).mean()\n",
    "        else:\n",
    "            errG = -fake_score.mean()\n",
    "\n",
    "\n",
    "    errG *= gan_lambda\n",
    "    errG.backward()\n",
    "    optimizer_gan_dec.step()\n",
    "\n",
    "    return errG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce1e7869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_hook(grad):\n",
    "    return grad * gan_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5cf668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Steal from https://github.com/caogang/wgan-gp/blob/master/gan_cifar10.py '''\n",
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    bsz = real_data.size(0)\n",
    "    alpha = torch.rand(bsz, 1)\n",
    "    alpha = alpha.expand(bsz, real_data.size(1))  # only works for 2D XXX\n",
    "    alpha = alpha.to(device)\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                                    grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
    "                                    create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * gan_gp_lambda\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e65e6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_d(batch, gan_type='kl'):\n",
    "    gan_disc.train()\n",
    "    gan_disc_local.train()\n",
    "    optimizer_gan_d.zero_grad()\n",
    "    optimizer_gan_d_local.zero_grad()\n",
    "\n",
    "    # + samples\n",
    "    source, target, lengths = batch\n",
    "    source = Variable(source.to(device))\n",
    "    target = Variable(target.to(device))\n",
    "    real_hidden = autoencoder(source, lengths, source, add_noise=add_noise, soft=False, encode_only=True)\n",
    "    real_score = gan_disc(real_hidden.detach())\n",
    "\n",
    "    idx = random.randint(0, maxlen - gan_d_local_windowsize)\n",
    "    if gan_d_local:\n",
    "        real_hidden_local = real_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        real_score_local = gan_disc_local(real_hidden_local)\n",
    "        real_score += real_score_local\n",
    "\n",
    "\n",
    "    if gan_type == 'wgan':\n",
    "        errD_real = -real_score.mean()\n",
    "    else: # kl or all\n",
    "        errD_real = F.softplus(-real_score).mean()\n",
    "    errD_real.backward()\n",
    "\n",
    "    # - samples\n",
    "    z = Variable(torch.Tensor(batch_size, z_size).normal_(0, 1).to(device))\n",
    "    fake_hidden = gan_gen(z)\n",
    "    fake_score = gan_disc(fake_hidden.detach())\n",
    "\n",
    "    if gan_d_local:\n",
    "        fake_hidden_local = fake_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        fake_score_local = gan_disc_local(fake_hidden_local)\n",
    "        fake_score += fake_score_local\n",
    "\n",
    "    if gan_type == 'wgan':\n",
    "        errD_fake = fake_score.mean()\n",
    "    else:  # kl or all\n",
    "        errD_fake = F.softplus(fake_score).mean()\n",
    "    errD_fake.backward()\n",
    "\n",
    "    # gradient penalty\n",
    "    if gan_type == 'wgan':\n",
    "        gradient_penalty = calc_gradient_penalty(gan_disc, real_hidden.data, fake_hidden.data)\n",
    "        gradient_penalty.backward()\n",
    "\n",
    "    optimizer_gan_d.step()\n",
    "    optimizer_gan_d_local.step()\n",
    "    return errD_real + errD_fake, errD_real, errD_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f12ead25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_d_into_ae(batch):\n",
    "    autoencoder.train()\n",
    "    optimizer_gan_e.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = Variable(source.to(device))\n",
    "    target = Variable(target.to(device))\n",
    "    real_hidden = autoencoder(source, lengths, source, add_noise=add_noise, soft=False, encode_only=True)\n",
    "\n",
    "    if gan_d_local:\n",
    "        idx = random.randint(0, maxlen - gan_d_local_windowsize)\n",
    "        real_hidden_local = real_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        real_score_local = gan_disc_local(real_hidden_local)\n",
    "        errD_real = gan_disc(real_hidden).mean() + real_score_local.mean()\n",
    "    else:\n",
    "        errD_real = gan_disc(real_hidden).mean()\n",
    "\n",
    "    errD_real *= gan_lambda\n",
    "    errD_real.backward()\n",
    "    torch.nn.utils.clip_grad_norm(autoencoder.parameters(), clip)\n",
    "\n",
    "    optimizer_gan_e.step()\n",
    "    return errD_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44e7895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    logging(\"Training\")\n",
    "    train_data = batchify(corpus.train, batch_size, maxlen, shuffle=True)\n",
    "\n",
    "    # gan: preparation\n",
    "    if niters_gan_schedule != \"\":\n",
    "        gan_schedule = [int(x) for x in niters_gan_schedule.split(\"-\")]\n",
    "    else:\n",
    "        gan_schedule = []\n",
    "    niter_gan = 1\n",
    "    fixed_noise = Variable(torch.ones(eval_batch_size, z_size).normal_(0, 1).to(device))\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # update gan training schedule\n",
    "        if epoch in gan_schedule:\n",
    "            niter_gan += 1\n",
    "            logging(\"GAN training loop schedule: {}\".format(niter_gan))\n",
    "\n",
    "        total_loss_ae = 0\n",
    "        epoch_start_time = time.time()\n",
    "        start_time = time.time()\n",
    "        niter = 0\n",
    "        niter_g = 1\n",
    "\n",
    "        while niter < len(train_data):\n",
    "            # train ae\n",
    "            for i in range(niters_ae):\n",
    "                if niter >= len(train_data):\n",
    "                    break  # end of epoch\n",
    "                total_loss_ae, start_time = train_ae(epoch, train_data[niter],\n",
    "                                total_loss_ae, start_time, niter)\n",
    "                niter += 1\n",
    "            # train gan\n",
    "            for k in range(niter_gan):\n",
    "                for i in range(niters_gan_d):\n",
    "                    errD, errD_real, errD_fake = train_gan_d(\n",
    "                            train_data[random.randint(0, len(train_data)-1)], gan_type)\n",
    "                for i in range(niters_gan_ae):\n",
    "                    train_gan_d_into_ae(train_data[random.randint(0, len(train_data)-1)])\n",
    "                for i in range(niters_gan_g):\n",
    "                    errG = train_gan_g(gan_type)\n",
    "                if enhance_dec:\n",
    "                    for i in range(niters_gan_dec):\n",
    "                        errG_enh_dec = train_gan_dec()\n",
    "                else:\n",
    "                    errG_enh_dec = torch.Tensor([0])\n",
    "\n",
    "            niter_g += 1\n",
    "            if niter_g % log_interval == 0:\n",
    "                logging('[{}/{}][{}/{}] Loss_D: {:.8f} (Loss_D_real: {:.8f} '\n",
    "                        'Loss_D_fake: {:.8f}) Loss_G: {:.8f} Loss_Enh_Dec: {:.8f}'.format(\n",
    "                         epoch, epochs, niter, len(train_data),\n",
    "                         errD.data.item(), errD_real.data.item(),\n",
    "                         errD_fake.data.item(), errG.data.item(), errG_enh_dec.data.item()))\n",
    "        # eval\n",
    "        test_loss, accuracy = evaluate_autoencoder(test_data, epoch)\n",
    "        logging('| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} | '\n",
    "                'test ppl {:5.2f} | acc {:3.3f}'.format(epoch,\n",
    "                (time.time() - epoch_start_time), test_loss,\n",
    "                math.exp(test_loss), accuracy))\n",
    "\n",
    "        gen_text_savepath = os.path.join(save, \"{:03d}_examplar_gen\".format(epoch))\n",
    "        gen_fixed_noise(fixed_noise, gen_text_savepath)\n",
    "        if epoch % 5 == 0 or epoch % 4 == 0 or (epochs - epoch) <=2:\n",
    "            selfbleu, testbleu = eval_bleu(gen_text_savepath)\n",
    "            logging('bleu_self: [{:.8f},{:.8f},{:.8f},{:.8f},{:.8f}]'.format(selfbleu[0], selfbleu[1], selfbleu[2], selfbleu[3], selfbleu[4]))\n",
    "            logging('bleu_test: [{:.8f},{:.8f},{:.8f},{:.8f},{:.8f}]'.format(testbleu[0], testbleu[1], testbleu[2], testbleu[3], testbleu[4]))\n",
    "\n",
    "        if epoch % 15 == 0 or epoch == epochs-1:\n",
    "            logging(\"New saving model: epoch {:03d}.\".format(epoch))\n",
    "            save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a24898f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "| epoch   1 |     0/   54 batches | lr 0.000000 | ms/batch  0.68 | loss  0.10 | ppl     1.11 | acc     0.00 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harry/venv/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/harry/venv/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| end of epoch   1 | time: 15.41s | test loss  4.79 | test ppl 120.38 | acc 0.421\n",
      "| epoch   2 |     0/   54 batches | lr 0.000000 | ms/batch  0.62 | loss  0.06 | ppl     1.06 | acc     0.28 | train_ae_norm     1.00\n",
      "| end of epoch   2 | time: 15.42s | test loss  4.63 | test ppl 102.21 | acc 0.438\n",
      "| epoch   3 |     0/   54 batches | lr 0.000000 | ms/batch  0.62 | loss  0.06 | ppl     1.06 | acc     0.29 | train_ae_norm     1.00\n",
      "| end of epoch   3 | time: 15.38s | test loss  4.42 | test ppl 82.82 | acc 0.459\n",
      "| epoch   4 |     0/   54 batches | lr 0.000000 | ms/batch  0.62 | loss  0.05 | ppl     1.06 | acc     0.31 | train_ae_norm     1.00\n",
      "| end of epoch   4 | time: 15.43s | test loss  4.35 | test ppl 77.74 | acc 0.459\n",
      "bleu_self:  [6.51619258e-01 4.32848541e-01 3.04401075e-01 4.92311679e-05\n",
      " 2.78968574e-07]\n",
      "bleu_test:  [7.71500721e-01 5.16702270e-01 2.91626020e-01 4.49211215e-05\n",
      " 2.44743301e-07]\n",
      "bleu_self: [0.65161926,0.43284854,0.30440108,0.00004923,0.00000028]\n",
      "bleu_test: [0.77150072,0.51670227,0.29162602,0.00004492,0.00000024]\n",
      "| epoch   5 |     0/   54 batches | lr 0.000000 | ms/batch  0.63 | loss  0.05 | ppl     1.06 | acc     0.31 | train_ae_norm     1.00\n",
      "| end of epoch   5 | time: 15.41s | test loss  4.28 | test ppl 72.39 | acc 0.452\n",
      "bleu_self:  [6.54809858e-01 4.89492452e-01 3.08173484e-01 7.26692497e-02\n",
      " 7.65969308e-05]\n",
      "bleu_test:  [6.92857143e-01 5.40922593e-01 1.13415687e-01 1.87797346e-05\n",
      " 3.98467072e-06]\n",
      "bleu_self: [0.65480986,0.48949245,0.30817348,0.07266925,0.00007660]\n",
      "bleu_test: [0.69285714,0.54092259,0.11341569,0.00001878,0.00000398]\n",
      "| epoch   6 |     0/   54 batches | lr 0.000000 | ms/batch  0.62 | loss  0.05 | ppl     1.05 | acc     0.32 | train_ae_norm     1.00\n",
      "| end of epoch   6 | time: 15.42s | test loss  4.22 | test ppl 68.18 | acc 0.475\n",
      "| epoch   7 |     0/   54 batches | lr 0.000000 | ms/batch  0.62 | loss  0.05 | ppl     1.05 | acc     0.35 | train_ae_norm     1.00\n",
      "| end of epoch   7 | time: 15.42s | test loss  4.25 | test ppl 70.11 | acc 0.475\n",
      "| epoch   8 |     0/   54 batches | lr 0.000000 | ms/batch  0.62 | loss  0.05 | ppl     1.05 | acc     0.34 | train_ae_norm     1.00\n",
      "| end of epoch   8 | time: 15.41s | test loss  4.06 | test ppl 57.74 | acc 0.485\n",
      "bleu_self:  [0.66898036 0.47261037 0.29196306 0.07741122 0.06650937]\n",
      "bleu_test:  [7.39797008e-01 5.27968366e-01 3.06616604e-01 4.64188535e-05\n",
      " 2.52264997e-07]\n",
      "bleu_self: [0.66898036,0.47261037,0.29196306,0.07741122,0.06650937]\n",
      "bleu_test: [0.73979701,0.52796837,0.30661660,0.00004642,0.00000025]\n",
      "| epoch   9 |     0/   54 batches | lr 0.000000 | ms/batch  0.63 | loss  0.05 | ppl     1.05 | acc     0.35 | train_ae_norm     1.00\n",
      "| end of epoch   9 | time: 15.42s | test loss  4.20 | test ppl 66.55 | acc 0.458\n",
      "| epoch  10 |     0/   54 batches | lr 0.000000 | ms/batch  0.62 | loss  0.05 | ppl     1.05 | acc     0.32 | train_ae_norm     1.00\n",
      "| end of epoch  10 | time: 15.41s | test loss  4.03 | test ppl 56.11 | acc 0.484\n",
      "bleu_self:  [0.78035714 0.6235478  0.50815495 0.12506777 0.12500044]\n",
      "bleu_test:  [8.66071428e-01 5.28011040e-01 1.38588370e-01 2.23372937e-05\n",
      " 1.36368343e-07]\n",
      "bleu_self: [0.78035714,0.62354780,0.50815495,0.12506777,0.12500044]\n",
      "bleu_test: [0.86607143,0.52801104,0.13858837,0.00002234,0.00000014]\n",
      "| epoch  11 |     0/   54 batches | lr 0.000000 | ms/batch  0.66 | loss  0.05 | ppl     1.05 | acc     0.33 | train_ae_norm     1.00\n",
      "| end of epoch  11 | time: 15.47s | test loss  4.00 | test ppl 54.47 | acc 0.493\n",
      "| epoch  12 |     0/   54 batches | lr 0.000000 | ms/batch  0.62 | loss  0.05 | ppl     1.05 | acc     0.35 | train_ae_norm     1.00\n",
      "| end of epoch  12 | time: 15.44s | test loss  4.10 | test ppl 60.61 | acc 0.473\n",
      "bleu_self:  [7.33333333e-01 4.37500007e-01 4.37500002e-03 4.37500096e-04\n",
      " 1.09895445e-04]\n",
      "bleu_test:  [4.91835933e-01 1.28779830e-08 1.61208574e-08 2.86939180e-08\n",
      " 4.07412755e-08]\n",
      "bleu_self: [0.73333333,0.43750001,0.00437500,0.00043750,0.00010990]\n",
      "bleu_test: [0.49183593,0.00000001,0.00000002,0.00000003,0.00000004]\n",
      "| epoch  13 |     0/   54 batches | lr 0.000000 | ms/batch  0.62 | loss  0.05 | ppl     1.05 | acc     0.34 | train_ae_norm     1.00\n",
      "| end of epoch  13 | time: 15.45s | test loss  3.99 | test ppl 54.23 | acc 0.486\n",
      "| epoch  14 |     0/   54 batches | lr 0.000000 | ms/batch  0.62 | loss  0.05 | ppl     1.05 | acc     0.35 | train_ae_norm     1.00\n",
      "| end of epoch  14 | time: 15.39s | test loss  4.20 | test ppl 66.45 | acc 0.468\n",
      "| epoch  15 |     0/   54 batches | lr 0.000000 | ms/batch  0.62 | loss  0.05 | ppl     1.05 | acc     0.35 | train_ae_norm     1.00\n",
      "| end of epoch  15 | time: 15.41s | test loss  4.22 | test ppl 68.02 | acc 0.443\n",
      "bleu_self:  [1. 1. 1. 1. 1.]\n",
      "bleu_test:  [4.16666667e-02 1.34595476e-09 4.35056920e-12 2.50238851e-13\n",
      " 4.55488892e-14]\n",
      "bleu_self: [1.00000000,1.00000000,1.00000000,1.00000000,1.00000000]\n",
      "bleu_test: [0.04166667,0.00000000,0.00000000,0.00000000,0.00000000]\n",
      "New saving model: epoch 015.\n",
      "Saving models to ./results/TREC_results\n",
      "| epoch  16 |     0/   54 batches | lr 0.000000 | ms/batch  0.62 | loss  0.05 | ppl     1.05 | acc     0.34 | train_ae_norm     1.00\n",
      "| end of epoch  16 | time: 15.56s | test loss  4.24 | test ppl 69.19 | acc 0.432\n",
      "bleu_self:  [1. 1. 1. 1. 1.]\n",
      "bleu_test:  [4.16666667e-02 1.34595476e-09 4.35056920e-12 2.50238851e-13\n",
      " 4.55488892e-14]\n",
      "bleu_self: [1.00000000,1.00000000,1.00000000,1.00000000,1.00000000]\n",
      "bleu_test: [0.04166667,0.00000000,0.00000000,0.00000000,0.00000000]\n",
      "| epoch  17 |     0/   54 batches | lr 0.000000 | ms/batch  0.62 | loss  0.05 | ppl     1.05 | acc     0.33 | train_ae_norm     1.00\n",
      "| end of epoch  17 | time: 15.45s | test loss  4.24 | test ppl 69.45 | acc 0.437\n",
      "| epoch  18 |     0/   54 batches | lr 0.000000 | ms/batch  0.62 | loss  0.05 | ppl     1.05 | acc     0.34 | train_ae_norm     1.00\n",
      "| end of epoch  18 | time: 15.46s | test loss  4.21 | test ppl 67.55 | acc 0.454\n",
      "bleu_self:  [0.8737275  0.85784468 0.84286844 0.84046328 0.83860354]\n",
      "bleu_test:  [9.30059524e-02 2.99286266e-02 2.35435292e-07 6.95307172e-10\n",
      " 2.22155895e-11]\n",
      "bleu_self: [0.87372750,0.85784468,0.84286844,0.84046328,0.83860354]\n",
      "bleu_test: [0.09300595,0.02992863,0.00000024,0.00000000,0.00000000]\n",
      "| epoch  19 |     0/   54 batches | lr 0.000000 | ms/batch  0.64 | loss  0.05 | ppl     1.05 | acc     0.34 | train_ae_norm     1.00\n",
      "| end of epoch  19 | time: 15.47s | test loss  4.16 | test ppl 63.85 | acc 0.459\n",
      "bleu_self:  [3.30729167e-01 7.76694378e-02 5.12681287e-07 1.33562531e-09\n",
      " 3.80188907e-11]\n",
      "bleu_test:  [1.01562500e-01 6.51607724e-03 4.94145556e-08 1.37926400e-10\n",
      " 4.12246829e-12]\n",
      "bleu_self: [0.33072917,0.07766944,0.00000051,0.00000000,0.00000000]\n",
      "bleu_test: [0.10156250,0.00651608,0.00000005,0.00000000,0.00000000]\n",
      "New saving model: epoch 019.\n",
      "Saving models to ./results/TREC_results\n",
      "| epoch  20 |     0/   54 batches | lr 0.000000 | ms/batch  0.67 | loss  0.05 | ppl     1.05 | acc     0.34 | train_ae_norm     1.00\n",
      "| end of epoch  20 | time: 15.43s | test loss  4.41 | test ppl 81.97 | acc 0.409\n",
      "bleu_self:  [0.85416667 0.85202847 0.41592522 0.16510149 0.00902379]\n",
      "bleu_test:  [5.09858441e-01 3.12379076e-06 3.27287011e-07 1.27172448e-07\n",
      " 9.03714724e-08]\n",
      "bleu_self: [0.85416667,0.85202847,0.41592522,0.16510149,0.00902379]\n",
      "bleu_test: [0.50985844,0.00000312,0.00000033,0.00000013,0.00000009]\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d879385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
