{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "673b1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils import to_gpu, train_ngram_lm, get_ppl, create_exp_dir, Dictionary, length_sort\n",
    "from models import Seq2Seq, MLP_D, MLP_D_local, MLP_G\n",
    "from bleu_self import *\n",
    "from bleu_test import *\n",
    "import datetime\n",
    "now_time = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f208b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='TILGAN for unconditional generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d96c0a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import *\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "520324a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=4\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "228c8e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path=\"data/MS_COCO\"\n",
    "data_path=\"../../yahoo/unlabelled_small\"\n",
    "save=\"./results/yahoo_merge_results\"\n",
    "maxlen=20\n",
    "batch_size=48\n",
    "eval_batch_size = 16\n",
    "noise_seq_length = 15\n",
    "add_noise=True #what does this do? - question applies to most parameters\n",
    "emsize=512\n",
    "nhidden=512\n",
    "nlayers=2\n",
    "nheads=4\n",
    "nff=1024\n",
    "aehidden=56\n",
    "noise_r=0.05\n",
    "hidden_init=True\n",
    "dropout=0.3\n",
    "gpu=True\n",
    "z_size=100\n",
    "arch_g='300-300'\n",
    "gan_g_activation=True\n",
    "arch_d='300-300'\n",
    "gan_d_local=False\n",
    "gan_d_local_windowsize=3\n",
    "arch_d_local='300-300'\n",
    "lr_ae=0.12\n",
    "lr_gan_e=1e-04\n",
    "beta1=0.5\n",
    "lr_gan_g=4e-04\n",
    "lr_gan_d=1e-04\n",
    "epochs=200\n",
    "sample=True\n",
    "clip=1\n",
    "log_interval=100\n",
    "gan_lambda=0.1\n",
    "niters_gan_d=1\n",
    "niters_gan_g=1\n",
    "niters_gan_ae=1\n",
    "niters_gan_dec=1\n",
    "niters_gan_schedule=''\n",
    "niters_ae=1\n",
    "gan_type='kl'\n",
    "enhance_dec=True\n",
    "gan_gp_lambda=1\n",
    "vocab_size=0\n",
    "lowercase=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a437b200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_1</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65990</th>\n",
       "      <td>whats love?</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41929</th>\n",
       "      <td>facts about \"welgang bayn\"?</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53950</th>\n",
       "      <td>GOP ticket in '08?</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42594</th>\n",
       "      <td>What is the average lifespan of a Yahoo Questi...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17619</th>\n",
       "      <td>How can i learn spanish fast ?</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   question_1  label\n",
       "Unnamed: 0                                                          \n",
       "65990                                             whats love?      9\n",
       "41929                             facts about \"welgang bayn\"?      4\n",
       "53950                                      GOP ticket in '08?     10\n",
       "42594       What is the average lifespan of a Yahoo Questi...      5\n",
       "17619                          How can i learn spanish fast ?      4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=\"../../yahoo/yahoo_everything.csv\"\n",
    "\n",
    "label_list = [\"UNK\",1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "df_yahoo = pd.read_csv(data)\n",
    "#df_yahoo=df_yahoo.rename(columns = {\"Unnamed: 0\":'label'})\n",
    "df_yahoo=df_yahoo.set_index(\"Unnamed: 0\")\n",
    "df = df_yahoo.sample(frac=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c82811f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers=list(df['label'].unique())\n",
    "list_zeros = [0]*len(numbers)\n",
    "count_dictionary = dict(zip(numbers, list_zeros))\n",
    "\n",
    "values_array_train_labelled=[]\n",
    "values_array_test_labelled=[]\n",
    "values_array_test_unlabelled=[]\n",
    "values_array_train_unlabelled=[]\n",
    "values_array_unlabelled=[]\n",
    "data_all=[]\n",
    "for index, row in df.iterrows():\n",
    "    if count_dictionary[row['label']]<20:\n",
    "        count_dictionary[row['label']]=count_dictionary[row['label']]+1\n",
    "        values_array_train_labelled.append((row['question_1'],row['label']))\n",
    "    elif count_dictionary[row['label']]<60:\n",
    "        count_dictionary[row['label']]=count_dictionary[row['label']]+1\n",
    "        values_array_test_labelled.append((row['question_1'],row['label']))\n",
    "    elif count_dictionary[row['label']]<600:\n",
    "        count_dictionary[row['label']]=count_dictionary[row['label']]+1\n",
    "        values_array_unlabelled.append((row['question_1'],'UNK'))\n",
    "    elif count_dictionary[row['label']]<1600:\n",
    "        count_dictionary[row['label']]=count_dictionary[row['label']]+1\n",
    "        values_array_test_unlabelled.append((row['question_1'],'UNK'))\n",
    "    elif count_dictionary[row['label']]<7600:\n",
    "        count_dictionary[row['label']]=count_dictionary[row['label']]+1\n",
    "        values_array_train_unlabelled.append((row['question_1'],'UNK'))\n",
    "    data_all.append(row['question_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aaa3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_l =  values_array_train_labelled\n",
    "test_l = values_array_test_labelled\n",
    "test_u = values_array_test_unlabelled\n",
    "train_u = values_array_train_unlabelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3d439b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, data_all, train_l, test_l, train_u, test_u, maxlen, vocab_size=11000, lowercase=False):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.maxlen = maxlen\n",
    "        self.lowercase = lowercase\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.data_all = data_all\n",
    "        self.train_l = train_l\n",
    "        self.test_l = test_l\n",
    "        self.train_u = train_u\n",
    "        self.test_u = test_u\n",
    "\n",
    "        # make the vocabulary from training set\n",
    "        self.make_vocab()\n",
    "        \n",
    "        self.train_l_tok = self.tokenize(self.train_l)\n",
    "        self.test_l_tok = self.tokenize(self.test_l)\n",
    "        self.train_u_tok = self.tokenize(self.train_u)\n",
    "        self.test_u_tok = self.tokenize(self.test_u)\n",
    "\n",
    "    def make_vocab(self):\n",
    "        # Add words to the dictionary\n",
    "        print(len(self.data_all))\n",
    "        print(self.data_all[0])\n",
    "        for line in self.data_all:\n",
    "            if self.lowercase:\n",
    "                # -1 to get rid of \\n character\n",
    "                words = line[:-1].lower().split(\" \")\n",
    "            else:\n",
    "                words = line[:-1].split(\" \")\n",
    "            for word in words:\n",
    "                self.dictionary.add_word(word)\n",
    "\n",
    "        # prune the vocabulary\n",
    "        self.dictionary.prune_vocab(k=self.vocab_size, cnt=True)\n",
    "\n",
    "    def tokenize(self, data):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        dropped = 0\n",
    "        #with open(path, 'r') as f:\n",
    "        linecount = 0\n",
    "        lines = []\n",
    "        for line, label in data:\n",
    "            linecount += 1\n",
    "            if self.lowercase:\n",
    "                words = line[:-1].lower().strip().split(\" \")\n",
    "            else:\n",
    "                words = line[:-1].strip().split(\" \")\n",
    "            if len(words) > self.maxlen:\n",
    "                dropped += 1\n",
    "                continue\n",
    "            words = ['<sos>'] + words\n",
    "            words += ['<eos>']\n",
    "            # vectorize\n",
    "            vocab = self.dictionary.word2idx\n",
    "            unk_idx = vocab['<oov>']\n",
    "            indices = [vocab[w] if w in vocab else unk_idx for w in words]\n",
    "            lines.append(indices)\n",
    "\n",
    "        print(\"Number of sentences dropped: {} out of {} total\".\n",
    "              format(dropped, linecount))\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24e5f2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000\n",
      "whats love?\n",
      "original vocab 62673; pruned to 62677\n",
      "Number of sentences dropped: 0 out of 200 total\n",
      "Number of sentences dropped: 0 out of 400 total\n",
      "Number of sentences dropped: 4 out of 60000 total\n",
      "Number of sentences dropped: 1 out of 10000 total\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(data_all = data_all,\n",
    "                train_l=train_l,\n",
    "                test_l=test_l,\n",
    "                train_u=train_u,\n",
    "                test_u=test_u,\n",
    "                maxlen=maxlen,\n",
    "                vocab_size=vocab_size,\n",
    "                lowercase=lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daa4ee98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 62677\n"
     ]
    }
   ],
   "source": [
    "# save arguments\n",
    "ntokens = len(corpus.dictionary.word2idx)\n",
    "print(\"Vocabulary Size: {}\".format(ntokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f66f2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : ./results/yahoo_merge_results\n"
     ]
    }
   ],
   "source": [
    "# exp dir\n",
    "create_exp_dir(os.path.join(save), ['train.py', 'models.py', 'utils.py'],\n",
    "        dict=corpus.dictionary.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a05112cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging(str, to_stdout=True):\n",
    "    with open(os.path.join(save, 'log.txt'), 'a') as f:\n",
    "        f.write(str + '\\n')\n",
    "    if to_stdout:\n",
    "        print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86dcf57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz, max_len, shuffle=False, gpu=False):\n",
    "    if shuffle:\n",
    "        random.shuffle(data)\n",
    "    nbatch = len(data) // bsz\n",
    "    batches = []\n",
    "\n",
    "    for i in range(nbatch):\n",
    "        maxlen = max_len+1\n",
    "        # Pad batches to maximum sequence length in batch\n",
    "        batch = data[i*bsz:(i+1)*bsz]\n",
    "        # subtract 1 from lengths b/c includes BOTH starts & end symbols\n",
    "        lengths = [len(x)-1 for x in batch]\n",
    "\n",
    "        # sort items by length (decreasing)\n",
    "        batch, lengths = length_sort(batch, lengths)\n",
    "\n",
    "        # source has no end symbol\n",
    "        source = [x[:-1] for x in batch]\n",
    "        # target has no start symbol\n",
    "        target = [x[1:] for x in batch]\n",
    "\n",
    "\n",
    "        for x, y in zip(source, target):\n",
    "            zeros = (maxlen-len(x))*[0]\n",
    "            x += zeros\n",
    "            y += zeros\n",
    "        source = torch.LongTensor(np.array(source))\n",
    "        target = torch.LongTensor(np.array(target)).view(-1)\n",
    "\n",
    "        if gpu:\n",
    "            source = source.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "        batches.append((source, target, lengths))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4042220b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data!\n"
     ]
    }
   ],
   "source": [
    "test_data = batchify(corpus.test_u_tok, eval_batch_size, maxlen, shuffle=False)\n",
    "train_data = batchify(corpus.train_u_tok, batch_size, maxlen,  shuffle=True)\n",
    "\n",
    "print(\"Loaded data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2bb5ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624\n",
      "1249\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51cc7067",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Build the models\n",
    "###############################################################################\n",
    "autoencoder = Seq2Seq(add_noise=add_noise,\n",
    "                      emsize=emsize,\n",
    "                      nhidden=nhidden,\n",
    "                      ntokens=ntokens,\n",
    "                      nlayers=nlayers,\n",
    "                      nheads=nheads,\n",
    "                      nff=nff,\n",
    "                      aehidden=aehidden,\n",
    "                      noise_r=noise_r,\n",
    "                      hidden_init=hidden_init,\n",
    "                      dropout=dropout,\n",
    "                      gpu=True)\n",
    "nlatent = aehidden * (maxlen+1)\n",
    "gan_gen = MLP_G(ninput=z_size, noutput=nlatent, layers=arch_g, gan_g_activation=gan_g_activation)\n",
    "gan_disc = MLP_D(ninput=nlatent, noutput=1, layers=arch_d)\n",
    "gan_disc_local = MLP_D_local(ninput=gan_d_local_windowsize * aehidden, noutput=1, layers=arch_d_local)\n",
    "\n",
    "optimizer_ae = optim.SGD(autoencoder.parameters(), lr=lr_ae)\n",
    "\n",
    "\n",
    "optimizer_gan_e = optim.Adam(autoencoder.encoder.parameters(),\n",
    "                             lr=lr_gan_e,\n",
    "                             betas=(beta1, 0.999))\n",
    "optimizer_gan_g = optim.Adam(gan_gen.parameters(),\n",
    "                             lr=lr_gan_g,\n",
    "                             betas=(beta1, 0.999))\n",
    "optimizer_gan_d = optim.Adam(gan_disc.parameters(),\n",
    "                             lr=lr_gan_d,\n",
    "                             betas=(beta1, 0.999))\n",
    "optimizer_gan_d_local = optim.Adam(gan_disc_local.parameters(),\n",
    "                             lr=lr_gan_d,\n",
    "                             betas=(beta1, 0.999))\n",
    "optimizer_gan_dec = optim.Adam(autoencoder.decoder.parameters(),\n",
    "                             lr=lr_gan_e,\n",
    "                             betas=(beta1, 0.999))\n",
    "\n",
    "autoencoder = autoencoder.to(device)\n",
    "gan_gen = gan_gen.to(device)\n",
    "gan_disc = gan_disc.to(device)\n",
    "gan_disc_local = gan_disc_local.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42a2dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    print(\"Saving models to {}\".format(save))\n",
    "    torch.save({\n",
    "        \"ae\": autoencoder.state_dict(),\n",
    "        \"gan_g\": gan_gen.state_dict(),\n",
    "        \"gan_d\": gan_disc.state_dict(),\n",
    "        \"gan_d_local\": gan_disc_local.state_dict()\n",
    "\n",
    "        },\n",
    "        os.path.join(save, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b528fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        param_norm = p.grad.data.norm(2)\n",
    "        total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** (1. / 2)\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7a9fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    model_args = json.load(open(os.path.join(save, 'options.json'), 'r'))\n",
    "    word2idx = json.load(open(os.path.join(save, 'vocab.json'), 'r'))\n",
    "    idx2word = {v: k for k, v in word2idx.items()}\n",
    "\n",
    "    print('Loading models from {}'.format(save))\n",
    "    loaded = torch.load(os.path.join(save, \"model.pt\"))\n",
    "    autoencoder.load_state_dict(loaded.get('ae'))\n",
    "    gan_gen.load_state_dict(loaded.get('gan_g'))\n",
    "    gan_disc.load_state_dict(loaded.get('gan_d'))\n",
    "    gan_disc_local.load_state_dict(loaded.get('gan_d_local'))\n",
    "    return model_args, idx2word, autoencoder, gan_gen, gan_disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03edc3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_autoencoder(data_source, epoch):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    autoencoder.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary.word2idx)\n",
    "    all_accuracies = 0\n",
    "    bcnt = 0\n",
    "    for i, batch in enumerate(data_source):\n",
    "        source, target, lengths = batch\n",
    "        with torch.no_grad():\n",
    "            source = Variable(source.to(device))\n",
    "            target = Variable(target.to(device))\n",
    "            mask = target.gt(0)\n",
    "            masked_target = target.masked_select(mask)\n",
    "            # examples x ntokens\n",
    "            output_mask = mask.unsqueeze(1).expand(mask.size(0), ntokens)\n",
    "\n",
    "            # output: batch x seq_len x ntokens\n",
    "            output = autoencoder(source, lengths, source, add_noise=add_noise, soft=False)\n",
    "            flattened_output = output.view(-1, ntokens)\n",
    "\n",
    "            masked_output = \\\n",
    "                flattened_output.masked_select(output_mask).view(-1, ntokens)\n",
    "            total_loss += F.cross_entropy(masked_output, masked_target)\n",
    "\n",
    "            # accuracy\n",
    "            max_vals, max_indices = torch.max(masked_output, 1)\n",
    "            accuracy = torch.mean(max_indices.eq(masked_target).float()).data.item()\n",
    "            all_accuracies += accuracy\n",
    "            bcnt += 1\n",
    "\n",
    "        aeoutf = os.path.join(save, \"autoencoder.txt\")\n",
    "        with open(aeoutf, \"w\") as f:\n",
    "            max_values, max_indices = torch.max(output, 2)\n",
    "            max_indices = \\\n",
    "                max_indices.view(output.size(0), -1).data.cpu().numpy()\n",
    "            target = target.view(output.size(0), -1).data.cpu().numpy()\n",
    "            for t, idx in zip(target, max_indices):\n",
    "                # real sentence\n",
    "                chars = \" \".join([corpus.dictionary.idx2word[x] for x in t])\n",
    "                f.write(chars + '\\n')\n",
    "                # autoencoder output sentence\n",
    "                chars = \" \".join([corpus.dictionary.idx2word[x] for x in idx])\n",
    "                f.write(chars + '\\n'*2)\n",
    "\n",
    "    return total_loss.item() / len(data_source), all_accuracies/bcnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b838544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fixed_noise(noise, to_save):\n",
    "    gan_gen.eval()\n",
    "    autoencoder.eval()\n",
    "\n",
    "    fake_hidden = gan_gen(noise)\n",
    "    max_indices = autoencoder.generate(fake_hidden, maxlen, sample=sample)\n",
    "\n",
    "    with open(to_save, \"w\") as f:\n",
    "        max_indices = max_indices.data.cpu().numpy()\n",
    "        for idx in max_indices:\n",
    "            # generated sentence\n",
    "            words = [corpus.dictionary.idx2word[x] for x in idx]\n",
    "            # truncate sentences to first occurrence of <eos>\n",
    "            truncated_sent = []\n",
    "            for w in words:\n",
    "                if w != '<eos>':\n",
    "                    truncated_sent.append(w)\n",
    "                else:\n",
    "                    break\n",
    "            chars = \" \".join(truncated_sent)\n",
    "            f.write(chars + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "184db856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fixed_noise_new(noise):\n",
    "    gan_gen.eval()\n",
    "    autoencoder.eval()\n",
    "\n",
    "    fake_hidden = gan_gen(noise)\n",
    "    max_indices = autoencoder.generate(fake_hidden, maxlen, sample=sample)\n",
    "    \n",
    "    sent_list = []\n",
    "    \n",
    "    #with open(to_save, \"w\") as f:\n",
    "    max_indices = max_indices.data.cpu().numpy()\n",
    "    for idx in max_indices:\n",
    "        # generated sentence\n",
    "        words = [corpus.dictionary.idx2word[x] for x in idx]\n",
    "        # truncate sentences to first occurrence of <eos>\n",
    "        truncated_sent = []\n",
    "        for w in words:\n",
    "            if w != '<eos>':\n",
    "                truncated_sent.append(w)\n",
    "            else:\n",
    "                break\n",
    "        chars = \" \".join(truncated_sent)\n",
    "        #f.write(chars + '\\n')\n",
    "        sent_list.append(chars)\n",
    "    #print(sent_list)\n",
    "    return sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58dced27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(gen_text_savepath):\n",
    "    selfbleu = bleu_self(gen_text_savepath)\n",
    "    real_text = os.path.join(data_path, \"test.txt\")\n",
    "    testbleu = bleu_test(real_text, gen_text_savepath)\n",
    "    return selfbleu, testbleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "124dcc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae(epoch, batch, total_loss_ae, start_time, i):\n",
    "    '''Train AE with the negative log-likelihood loss'''\n",
    "    autoencoder.train()\n",
    "    optimizer_ae.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = Variable(source.to(device))\n",
    "    target = Variable(target.to(device))\n",
    "    output = autoencoder(source, lengths, source, add_noise=add_noise, soft=False)\n",
    "\n",
    "    mask = target.gt(0)\n",
    "    masked_target = target.masked_select(mask)\n",
    "    output_mask = mask.unsqueeze(1).expand(mask.size(0), ntokens)\n",
    "    flat_output = output.view(-1, ntokens)\n",
    "    masked_output = flat_output.masked_select(output_mask).view(-1, ntokens)\n",
    "    loss = F.cross_entropy(masked_output, masked_target)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(autoencoder.parameters(), clip)\n",
    "    train_ae_norm = cal_norm(autoencoder)\n",
    "    optimizer_ae.step()\n",
    "\n",
    "    total_loss_ae += loss.data.item()\n",
    "    if i % log_interval == 0:\n",
    "        probs = F.softmax(masked_output, dim=-1)\n",
    "        max_vals, max_indices = torch.max(probs, 1)\n",
    "        accuracy = torch.mean(max_indices.eq(masked_target).float()).data.item()\n",
    "        cur_loss = total_loss_ae / log_interval\n",
    "        elapsed = time.time() - start_time\n",
    "        logging('| epoch {:3d} | {:5d}/{:5d} batches | lr {:08.6f} | ms/batch {:5.2f} | '\n",
    "                'loss {:5.2f} | ppl {:8.2f} | acc {:8.2f} | train_ae_norm {:8.2f}'.format(\n",
    "                epoch, i, len(train_data), 0,\n",
    "                elapsed * 1000 / log_interval,\n",
    "                cur_loss, math.exp(cur_loss), accuracy, train_ae_norm))\n",
    "\n",
    "        total_loss_ae = 0\n",
    "        start_time = time.time()\n",
    "    return total_loss_ae, start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64870e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_g(gan_type='kl'):\n",
    "    gan_gen.train()\n",
    "    optimizer_gan_g.zero_grad()\n",
    "\n",
    "    z = Variable(torch.Tensor(batch_size, z_size).normal_(0, 1).to(device))\n",
    "    fake_hidden = gan_gen(z)\n",
    "    fake_score = gan_disc(fake_hidden)\n",
    "\n",
    "    if gan_d_local:\n",
    "        idx = random.randint(0, maxlen - gan_d_local_windowsize)\n",
    "        fake_hidden_local = fake_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        fake_score_local = gan_disc_local(fake_hidden_local)\n",
    "\n",
    "        if gan_type == 'kl':\n",
    "            errG = -(torch.exp(fake_score.detach()).clamp(0.5, 2) * fake_score).mean() -(torch.exp(fake_score_local.detach()).clamp(0.5, 2) * fake_score_local).mean()\n",
    "        else:\n",
    "            errG = -fake_score.mean() -fake_score_local.mean()\n",
    "    else:\n",
    "        if gan_type == 'kl':\n",
    "            errG = -(torch.exp(fake_score.detach()).clamp(0.5, 2) * fake_score).mean()\n",
    "        else:\n",
    "            errG = -fake_score.mean()\n",
    "\n",
    "\n",
    "    errG *= gan_lambda\n",
    "    errG.backward()\n",
    "    optimizer_gan_g.step()\n",
    "\n",
    "    return errG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33244527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_dec(gan_type='kl'):\n",
    "    autoencoder.decoder.train()\n",
    "    optimizer_gan_dec.zero_grad()\n",
    "\n",
    "    z = Variable(torch.Tensor(batch_size, z_size).normal_(0, 1).to(device))\n",
    "    fake_hidden = gan_gen(z)\n",
    "\n",
    "    # 1. decoder  - soft distribution\n",
    "    enhance_source, max_indices= autoencoder.generate_enh_dec(fake_hidden, maxlen, sample=sample)\n",
    "    # 2. soft distribution - > encoder  -> fake_hidden\n",
    "    enhance_hidden = autoencoder(enhance_source, None, max_indices, add_noise=add_noise, soft=True, encode_only=True)\n",
    "    fake_score = gan_disc(enhance_hidden)\n",
    "\n",
    "    if gan_d_local:\n",
    "        idx = random.randint(0, maxlen - gan_d_local_windowsize)\n",
    "        fake_hidden_local = fake_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        fake_score_local = gan_disc_local(fake_hidden_local)\n",
    "\n",
    "        if gan_type == 'kl':\n",
    "            errG = -(torch.exp(fake_score.detach()).clamp(0.5, 2) * fake_score).mean() -(torch.exp(fake_score_local.detach()).clamp(0.5, 2) * fake_score_local).mean()\n",
    "        else:\n",
    "            errG = -fake_score.mean() -fake_score_local.mean()\n",
    "    else:\n",
    "        if gan_type == 'kl':\n",
    "            errG = -(torch.exp(fake_score.detach()).clamp(0.5, 2) * fake_score).mean()\n",
    "        else:\n",
    "            errG = -fake_score.mean()\n",
    "\n",
    "\n",
    "    errG *= gan_lambda\n",
    "    errG.backward()\n",
    "    optimizer_gan_dec.step()\n",
    "\n",
    "    return errG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce1e7869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_hook(grad):\n",
    "    return grad * gan_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5cf668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Steal from https://github.com/caogang/wgan-gp/blob/master/gan_cifar10.py '''\n",
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    bsz = real_data.size(0)\n",
    "    alpha = torch.rand(bsz, 1)\n",
    "    alpha = alpha.expand(bsz, real_data.size(1))  # only works for 2D XXX\n",
    "    alpha = alpha.to(device)\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                                    grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
    "                                    create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * gan_gp_lambda\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e65e6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_d(batch, gan_type='kl'):\n",
    "    gan_disc.train()\n",
    "    gan_disc_local.train()\n",
    "    optimizer_gan_d.zero_grad()\n",
    "    optimizer_gan_d_local.zero_grad()\n",
    "\n",
    "    # + samples\n",
    "    source, target, lengths = batch\n",
    "    source = Variable(source.to(device))\n",
    "    target = Variable(target.to(device))\n",
    "    real_hidden = autoencoder(source, lengths, source, add_noise=add_noise, soft=False, encode_only=True)\n",
    "    real_score = gan_disc(real_hidden.detach())\n",
    "\n",
    "    idx = random.randint(0, maxlen - gan_d_local_windowsize)\n",
    "    if gan_d_local:\n",
    "        real_hidden_local = real_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        real_score_local = gan_disc_local(real_hidden_local)\n",
    "        real_score += real_score_local\n",
    "\n",
    "\n",
    "    if gan_type == 'wgan':\n",
    "        errD_real = -real_score.mean()\n",
    "    else: # kl or all\n",
    "        errD_real = F.softplus(-real_score).mean()\n",
    "    errD_real.backward()\n",
    "\n",
    "    # - samples\n",
    "    z = Variable(torch.Tensor(batch_size, z_size).normal_(0, 1).to(device))\n",
    "    fake_hidden = gan_gen(z)\n",
    "    fake_score = gan_disc(fake_hidden.detach())\n",
    "\n",
    "    if gan_d_local:\n",
    "        fake_hidden_local = fake_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        fake_score_local = gan_disc_local(fake_hidden_local)\n",
    "        fake_score += fake_score_local\n",
    "\n",
    "    if gan_type == 'wgan':\n",
    "        errD_fake = fake_score.mean()\n",
    "    else:  # kl or all\n",
    "        errD_fake = F.softplus(fake_score).mean()\n",
    "    errD_fake.backward()\n",
    "\n",
    "    # gradient penalty\n",
    "    if gan_type == 'wgan':\n",
    "        gradient_penalty = calc_gradient_penalty(gan_disc, real_hidden.data, fake_hidden.data)\n",
    "        gradient_penalty.backward()\n",
    "\n",
    "    optimizer_gan_d.step()\n",
    "    optimizer_gan_d_local.step()\n",
    "    return errD_real + errD_fake, errD_real, errD_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f12ead25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_d_into_ae(batch):\n",
    "    autoencoder.train()\n",
    "    optimizer_gan_e.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = Variable(source.to(device))\n",
    "    target = Variable(target.to(device))\n",
    "    real_hidden = autoencoder(source, lengths, source, add_noise=add_noise, soft=False, encode_only=True)\n",
    "\n",
    "    if gan_d_local:\n",
    "        idx = random.randint(0, maxlen - gan_d_local_windowsize)\n",
    "        real_hidden_local = real_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        real_score_local = gan_disc_local(real_hidden_local)\n",
    "        errD_real = gan_disc(real_hidden).mean() + real_score_local.mean()\n",
    "    else:\n",
    "        errD_real = gan_disc(real_hidden).mean()\n",
    "\n",
    "    errD_real *= gan_lambda\n",
    "    errD_real.backward()\n",
    "    torch.nn.utils.clip_grad_norm(autoencoder.parameters(), clip)\n",
    "\n",
    "    optimizer_gan_e.step()\n",
    "    return errD_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd8488e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "#  Transformer parameters\n",
    "#--------------------------------\n",
    "max_seq_length = 20\n",
    "batch_size_d = 48\n",
    "\n",
    "#--------------------------------\n",
    "#  GAN-BERT specific parameters\n",
    "#--------------------------------\n",
    "# number of hidden layers in the generator, \n",
    "# each of the size of the output space\n",
    "#num_hidden_layers_g = 1; \n",
    "# number of hidden layers in the discriminator, \n",
    "# each of the size of the input space\n",
    "num_hidden_layers_d = 1; \n",
    "# size of the generator's input noisy vectors\n",
    "noise_size = 100\n",
    "# dropout to be applied to discriminator's input vectors\n",
    "out_dropout_rate = 0.2\n",
    "\n",
    "# Replicate labeled data to balance poorly represented datasets, \n",
    "# e.g., less than 1% of labeled material\n",
    "apply_balance = True\n",
    "\n",
    "#--------------------------------\n",
    "#  Optimization parameters\n",
    "#--------------------------------\n",
    "learning_rate_discriminator = 5e-6 #5e-6?\n",
    "#learning_rate_generator = 5e-5\n",
    "epsilon = 1e-8\n",
    "num_train_epochs = 50\n",
    "multi_gpu = True\n",
    "# Scheduler\n",
    "apply_scheduler = False\n",
    "warmup_proportion = 0.1\n",
    "# Print\n",
    "print_each_n_step = 10\n",
    "\n",
    "#--------------------------------\n",
    "#  Adopted Tranformer model\n",
    "#--------------------------------\n",
    "# Since this version is compatible with Huggingface transformers, you can uncomment\n",
    "# (or add) transformer models compatible with GAN\n",
    "\n",
    "#model_name = \"bert-base-cased\"\n",
    "#model_name = \"bert-base-uncased\"\n",
    "#model_name = \"roberta-base\"\n",
    "#model_name = \"albert-base-v2\"\n",
    "#model_name = \"xlm-roberta-base\"\n",
    "#model_name = \"amazon/bort\"\n",
    "#model_name=\"google/electra-large-discriminator\"\n",
    "#model_name=\"google/electra-small-discriminator\"\n",
    "#model_name=\"microsoft/deberta-v2-xxlarge\"\n",
    "#model_name=\"microsoft/deberta-v3-base\"\n",
    "model_name = \"google/electra-base-discriminator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90e59fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/electra-base-discriminator/resolve/main/config.json from cache at /home/harry/.cache/huggingface/transformers/7d1569a4df2372d67341bda716bce4e3edf3e3ffadb97251bc4b6b35d459f624.57c13443a51769ce892714c93bb3ee3952bad66d7d9662d9de382b808377c3f8\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/electra-base-discriminator/resolve/main/pytorch_model.bin from cache at /home/harry/.cache/huggingface/transformers/aed576b8aec823c870feda40d60bd803ac8e40056ecb7d7f43dd0b2bfd82e373.db390a2059e53ead2bb00e1a2f8cd50b0a47e1969d180cd70339ec3f6f29dce1\n",
      "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at google/electra-base-discriminator.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/google/electra-base-discriminator/resolve/main/config.json from cache at /home/harry/.cache/huggingface/transformers/7d1569a4df2372d67341bda716bce4e3edf3e3ffadb97251bc4b6b35d459f624.57c13443a51769ce892714c93bb3ee3952bad66d7d9662d9de382b808377c3f8\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/google/electra-base-discriminator/resolve/main/vocab.txt from cache at /home/harry/.cache/huggingface/transformers/fe616facc71d8e3afc69de3edac76bf1e4a0a741e80d9a99a2cc6a9a8f5f74b5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/google/electra-base-discriminator/resolve/main/tokenizer.json from cache at /home/harry/.cache/huggingface/transformers/81840ac426bf0d690bfb69a4ec7d706e8853d8ab309e7decb6b72ab939d6682e.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/google/electra-base-discriminator/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/google/electra-base-discriminator/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/google/electra-base-discriminator/resolve/main/tokenizer_config.json from cache at /home/harry/.cache/huggingface/transformers/6f8b3f5095b6f44f5c75cee3c56b971b3208b08132ba2f9fb775a4a7b7140942.4f2213f5603276adf12967b32e4444c0f187f34ca4f8b22a65f03e13514589e9\n",
      "loading configuration file https://huggingface.co/google/electra-base-discriminator/resolve/main/config.json from cache at /home/harry/.cache/huggingface/transformers/7d1569a4df2372d67341bda716bce4e3edf3e3ffadb97251bc4b6b35d459f624.57c13443a51769ce892714c93bb3ee3952bad66d7d9662d9de382b808377c3f8\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ea605ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_loader(input_examples, label_masks, label_map, do_shuffle = False, balance_label_examples = False):\n",
    "  '''\n",
    "  Generate a Dataloader given the input examples, eventually masked if they are \n",
    "  to be considered NOT labeled.\n",
    "  '''\n",
    "  examples = []\n",
    "\n",
    "  # Count the percentage of labeled examples  \n",
    "  num_labeled_examples = 0\n",
    "  for label_mask in label_masks:\n",
    "    if label_mask: \n",
    "      num_labeled_examples += 1\n",
    "  label_mask_rate = num_labeled_examples/len(input_examples)\n",
    "\n",
    "  # if required it applies the balance\n",
    "  for index, ex in enumerate(input_examples): \n",
    "    if label_mask_rate == 1 or not balance_label_examples:\n",
    "      examples.append((ex, label_masks[index]))\n",
    "    else:\n",
    "      # IT SIMULATE A LABELED EXAMPLE\n",
    "      if label_masks[index]:\n",
    "        balance = int(1/label_mask_rate)\n",
    "        balance = int(math.log(balance,2))\n",
    "        if balance < 1:\n",
    "          balance = 1\n",
    "        for b in range(0, int(balance)):\n",
    "          examples.append((ex, label_masks[index]))\n",
    "      else:\n",
    "        examples.append((ex, label_masks[index]))\n",
    "  \n",
    "  #-----------------------------------------------\n",
    "  # Generate input examples to the Transformer\n",
    "  #-----------------------------------------------\n",
    "  input_ids = []\n",
    "  input_mask_array = []\n",
    "  label_mask_array = []\n",
    "  label_id_array = []\n",
    "\n",
    "  # Tokenization \n",
    "  for (text, label_mask) in examples:\n",
    "    encoded_sent = tokenizer.encode(text[0], add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
    "    input_ids.append(encoded_sent)\n",
    "    label_id_array.append(label_map[text[1]])\n",
    "    label_mask_array.append(label_mask)\n",
    "  \n",
    "  # Attention to token (to ignore padded input wordpieces)\n",
    "  for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]                          \n",
    "    input_mask_array.append(att_mask)\n",
    "  # Convertion to Tensor\n",
    "  input_ids = torch.tensor(input_ids) \n",
    "  input_mask_array = torch.tensor(input_mask_array)\n",
    "  label_id_array = torch.tensor(label_id_array, dtype=torch.long)\n",
    "  label_mask_array = torch.tensor(label_mask_array)\n",
    "\n",
    "  # Building the TensorDataset\n",
    "  dataset = TensorDataset(input_ids, input_mask_array, label_id_array, label_mask_array)\n",
    "\n",
    "  if do_shuffle:\n",
    "    sampler = RandomSampler\n",
    "  else:\n",
    "    sampler = SequentialSampler\n",
    "\n",
    "  # Building the DataLoader\n",
    "  return DataLoader(\n",
    "              dataset,  # The training samples.\n",
    "              sampler = sampler(dataset), \n",
    "              batch_size = batch_size) # Trains with this batch size.\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d50e8d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_fake(input_examples):\n",
    "  '''\n",
    "  Generate a Dataloader given the input examples, eventually masked if they are \n",
    "  to be considered NOT labeled.\n",
    "  '''\n",
    "  \n",
    "  #-----------------------------------------------\n",
    "  # Generate input examples to the Transformer\n",
    "  #-----------------------------------------------\n",
    "  input_ids = []\n",
    "  input_mask_array = []\n",
    "\n",
    "  # Tokenization \n",
    "  for text in input_examples:\n",
    "    encoded_sent = tokenizer.encode(text, add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
    "    input_ids.append(encoded_sent)\n",
    "  \n",
    "  # Attention to token (to ignore padded input wordpieces)\n",
    "  for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]                          \n",
    "    input_mask_array.append(att_mask)\n",
    "  # Convertion to Tensor\n",
    "  input_ids = torch.tensor(input_ids) \n",
    "  input_mask_array = torch.tensor(input_mask_array)\n",
    "\n",
    "  # Building the DataLoader\n",
    "  return input_ids, input_mask_array # Trains with this batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3fa1e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the examples\n",
    "labeled_examples = train_l\n",
    "unlabeled_examples = values_array_unlabelled\n",
    "test_examples = test_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc1767ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {}\n",
    "for (i, label) in enumerate(label_list):\n",
    "  label_map[label] = i\n",
    "#------------------------------\n",
    "#   Load the train dataset\n",
    "#------------------------------\n",
    "train_examples = labeled_examples\n",
    "#The labeled (train) dataset is assigned with a mask set to True\n",
    "train_label_masks = np.ones(len(labeled_examples), dtype=bool)\n",
    "#If unlabel examples are available\n",
    "if unlabeled_examples:\n",
    "  train_examples = train_examples + unlabeled_examples\n",
    "  #The unlabeled (train) dataset is assigned with a mask set to False\n",
    "  tmp_masks = np.zeros(len(unlabeled_examples), dtype=bool)\n",
    "  train_label_masks = np.concatenate([train_label_masks,tmp_masks])\n",
    "\n",
    "train_dataloader = generate_data_loader(train_examples, train_label_masks, label_map, do_shuffle = True, balance_label_examples = apply_balance)\n",
    "\n",
    "#------------------------------\n",
    "#   Load the test dataset\n",
    "#------------------------------\n",
    "#The labeled (test) dataset is assigned with a mask set to True\n",
    "test_label_masks = np.ones(len(test_examples), dtype=bool)\n",
    "\n",
    "test_dataloader = generate_data_loader(test_examples, test_label_masks, label_map, do_shuffle = False, balance_label_examples = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "771c13ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------\n",
    "#   The Discriminator\n",
    "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
    "#   https://github.com/crux82/ganbert\n",
    "#------------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
    "        layers = []\n",
    "        hidden_sizes = [input_size] + hidden_sizes\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
    "\n",
    "        self.layers = nn.Sequential(*layers) #per il flatten\n",
    "        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1) # +1 for the probability of this sample being fake/real.\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_rep):\n",
    "        input_rep = self.input_dropout(input_rep)\n",
    "        last_rep = self.layers(input_rep)\n",
    "        logits = self.logit(last_rep)\n",
    "        probs = self.softmax(logits)\n",
    "        return last_rep, logits, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e797014b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/electra-base-discriminator/resolve/main/config.json from cache at /home/harry/.cache/huggingface/transformers/7d1569a4df2372d67341bda716bce4e3edf3e3ffadb97251bc4b6b35d459f624.57c13443a51769ce892714c93bb3ee3952bad66d7d9662d9de382b808377c3f8\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The config file is required to get the dimension of the vector produced by \n",
    "# the underlying transformer\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "hidden_size = int(config.hidden_size)\n",
    "# Define the number and width of hidden layers\n",
    "#hidden_levels_g = [hidden_size for i in range(0, num_hidden_layers_g)]\n",
    "hidden_levels_d = [hidden_size for i in range(0, num_hidden_layers_d)]\n",
    "\n",
    "#-------------------------------------------------\n",
    "#   Instantiate the Generator and Discriminator\n",
    "#-------------------------------------------------\n",
    "#generator = Generator(noise_size=noise_size, output_size=hidden_size, hidden_sizes=hidden_levels_g, dropout_rate=out_dropout_rate)\n",
    "discriminator = Discriminator(input_size=hidden_size, hidden_sizes=hidden_levels_d, num_labels=len(label_list), dropout_rate=out_dropout_rate)\n",
    "\n",
    "# Put everything in the GPU if available\n",
    "if torch.cuda.is_available():    \n",
    "  #generator.cuda()\n",
    "  discriminator.cuda()\n",
    "  transformer.cuda()\n",
    "  if multi_gpu:\n",
    "    transformer = torch.nn.DataParallel(transformer)\n",
    "\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5f542a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {}\n",
    "for (i, label) in enumerate(label_list):\n",
    "  label_map[label] = i\n",
    "#------------------------------\n",
    "#   Load the train dataset\n",
    "#------------------------------\n",
    "train_examples = labeled_examples\n",
    "#The labeled (train) dataset is assigned with a mask set to True\n",
    "train_label_masks = np.ones(len(labeled_examples), dtype=bool)\n",
    "#If unlabel examples are available\n",
    "if unlabeled_examples:\n",
    "  train_examples = train_examples + unlabeled_examples\n",
    "  #The unlabeled (train) dataset is assigned with a mask set to False\n",
    "  tmp_masks = np.zeros(len(unlabeled_examples), dtype=bool)\n",
    "  train_label_masks = np.concatenate([train_label_masks,tmp_masks])\n",
    "\n",
    "train_dataloader = generate_data_loader(train_examples, train_label_masks, label_map, do_shuffle = True, balance_label_examples = apply_balance)\n",
    "\n",
    "#------------------------------\n",
    "#   Load the test dataset\n",
    "#------------------------------\n",
    "#The labeled (test) dataset is assigned with a mask set to True\n",
    "test_label_masks = np.ones(len(test_examples), dtype=bool)\n",
    "\n",
    "test_dataloader = generate_data_loader(test_examples, test_label_masks, label_map, do_shuffle = False, balance_label_examples = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad127360",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stats = []\n",
    "\n",
    "accuracy_array=[]\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "#models parameters\n",
    "transformer_vars = [i for i in transformer.parameters()]\n",
    "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
    "#g_vars = [v for v in generator.parameters()]\n",
    "\n",
    "#optimizer\n",
    "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
    "#gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator) \n",
    "\n",
    "#scheduler\n",
    "if apply_scheduler:\n",
    "  num_train_examples = len(train_examples)\n",
    "  num_train_steps = int(num_train_examples / batch_size * num_train_epochs)\n",
    "  num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
    "\n",
    "  scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n",
    "                                           num_warmup_steps = num_warmup_steps)\n",
    "  scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n",
    "                                           num_warmup_steps = num_warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44e7895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    logging(\"Training\")\n",
    "    train_data = batchify(corpus.train_u_tok, batch_size, maxlen, shuffle=True)\n",
    "\n",
    "    # gan: preparation\n",
    "    if niters_gan_schedule != \"\":\n",
    "        gan_schedule = [int(x) for x in niters_gan_schedule.split(\"-\")]\n",
    "    else:\n",
    "        gan_schedule = []\n",
    "    niter_gan = 1\n",
    "    fixed_noise = Variable(torch.ones(eval_batch_size, z_size).normal_(0, 1).to(device))\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # update gan training schedule\n",
    "        if epoch in gan_schedule:\n",
    "            niter_gan += 1\n",
    "            logging(\"GAN training loop schedule: {}\".format(niter_gan))\n",
    "\n",
    "        total_loss_ae = 0\n",
    "        epoch_start_time = time.time()\n",
    "        start_time = time.time()\n",
    "        niter = 0\n",
    "        niter_g = 1\n",
    "        print(\"Train classification discriminator\")\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        # Perform one full pass over the training set.\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        tr_g_loss = 0\n",
    "        tr_d_loss = 0\n",
    "\n",
    "        # Put the model into training mode.\n",
    "        transformer.train() \n",
    "        #generator.train()\n",
    "        discriminator.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Progress update every print_each_n_step batches.\n",
    "            if step % print_each_n_step == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            b_label_mask = batch[3].to(device)\n",
    "\n",
    "            real_batch_size = b_input_ids.shape[0]\n",
    "\n",
    "            # Encode real data in the Transformer\n",
    "            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
    "            hidden_states = model_outputs.last_hidden_state[:,0,:] \n",
    "            #hidden_states = model_outputs[-1]\n",
    "            #print(\"  Number of real sentences (labelled and unlabelled): {}\".format(len(hidden_states)))\n",
    "            \n",
    "            # Generate fake data that should have the same distribution of the ones\n",
    "            # encoded by the transformer. \n",
    "            # First noisy input are used in input to the Generator\n",
    "            fixed_noise = Variable(torch.ones(real_batch_size, 100).normal_(0, 1).to(device))\n",
    "            fake_sentences = gen_fixed_noise_new(fixed_noise)\n",
    "            #print(\"  Number of generated sentences: {}\".format(len(fake_sentences)))\n",
    "\n",
    "            b_input_ids_fake, b_input_mask_fake = generate_data_fake(fake_sentences)\n",
    "            model_outputs_fake = transformer(b_input_ids_fake, attention_mask=b_input_mask_fake)\n",
    "            hidden_states_fake = model_outputs_fake.last_hidden_state[:,0,:] \n",
    "            #hidden_states_fake = model_outputs_fake[-1]\n",
    "\n",
    "            #noise = torch.zeros(real_batch_size, noise_size, device=device).uniform_(0, 1)\n",
    "            # Gnerate Fake data\n",
    "            #gen_rep = generator(noise)\n",
    "            #print(\"Length of generator output {}\".format(len(gen_rep)))\n",
    "            #print(\"Length of single generator output {}\".format(len(gen_rep[0])))\n",
    "\n",
    "            # Generate the output of the Discriminator for real and fake data.\n",
    "            # First, we put together the output of the tranformer and the generator\n",
    "            disciminator_input = torch.cat([hidden_states, hidden_states_fake], dim=0)\n",
    "            # Then, we select the output of the disciminator\n",
    "            features, logits, probs = discriminator(disciminator_input)\n",
    "\n",
    "            # Finally, we separate the discriminator's output for the real and fake\n",
    "            # data\n",
    "            features_list = torch.split(features, real_batch_size)\n",
    "            D_real_features = features_list[0]\n",
    "            D_fake_features = features_list[1]\n",
    "\n",
    "            logits_list = torch.split(logits, real_batch_size)\n",
    "            D_real_logits = logits_list[0]\n",
    "            D_fake_logits = logits_list[1]\n",
    "\n",
    "            probs_list = torch.split(probs, real_batch_size)\n",
    "            D_real_probs = probs_list[0]\n",
    "            D_fake_probs = probs_list[1]\n",
    "\n",
    "            #---------------------------------\n",
    "            #  LOSS evaluation\n",
    "            #---------------------------------\n",
    "            # Generator's LOSS estimation\n",
    "            g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + epsilon))\n",
    "            g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n",
    "            g_loss = g_loss_d + g_feat_reg\n",
    "\n",
    "            # Disciminator's LOSS estimation\n",
    "            logits = D_real_logits[:,0:-1]\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            # The discriminator provides an output for labeled and unlabeled real data\n",
    "            # so the loss evaluated for unlabeled data is ignored (masked)\n",
    "            label2one_hot = torch.nn.functional.one_hot(b_labels, len(label_list))\n",
    "            per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n",
    "            per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n",
    "            labeled_example_count = per_example_loss.type(torch.float32).numel()\n",
    "\n",
    "            # It may be the case that a batch does not contain labeled examples, \n",
    "            # so the \"supervised loss\" in this case is not evaluated\n",
    "            if labeled_example_count == 0:\n",
    "              D_L_Supervised = 0\n",
    "            else:\n",
    "              D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n",
    "\n",
    "            D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n",
    "            D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n",
    "            d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n",
    "\n",
    "            #---------------------------------\n",
    "            #  OPTIMIZATION\n",
    "            #---------------------------------\n",
    "            # Avoid gradient accumulation\n",
    "            #gen_optimizer.zero_grad()\n",
    "            dis_optimizer.zero_grad()\n",
    "\n",
    "            # Calculate weigth updates\n",
    "            # retain_graph=True is required since the underlying graph will be deleted after backward\n",
    "            g_loss.backward(retain_graph=True)\n",
    "            d_loss.backward() \n",
    "\n",
    "            # Apply modifications\n",
    "            #gen_optimizer.step()\n",
    "            dis_optimizer.step()\n",
    "\n",
    "            # A detail log of the individual losses\n",
    "            #print(\"{0:.4f}\\t{1:.4f}\\t{2:.4f}\\t{3:.4f}\\t{4:.4f}\".\n",
    "            #      format(D_L_Supervised, D_L_unsupervised1U, D_L_unsupervised2U,\n",
    "            #             g_loss_d, g_feat_reg))\n",
    "\n",
    "            # Save the losses to print them later\n",
    "            tr_g_loss += g_loss.item()\n",
    "            tr_d_loss += d_loss.item()\n",
    "\n",
    "            # Update the learning rate with the scheduler\n",
    "            if apply_scheduler:\n",
    "              scheduler_d.step()\n",
    "              #scheduler_g.step()\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss_g = tr_g_loss / len(train_dataloader)\n",
    "        avg_train_loss_d = tr_d_loss / len(train_dataloader)             \n",
    "\n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss generetor: {0:.3f}\".format(avg_train_loss_g))\n",
    "        print(\"  Average training loss discriminator: {0:.3f}\".format(avg_train_loss_d))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "        # ========================================\n",
    "        #     TEST ON THE EVALUATION DATASET\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our test set.\n",
    "        print(\"\")\n",
    "        print(\"Running Test...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        transformer.eval() #maybe redundant\n",
    "        discriminator.eval()\n",
    "        #generator.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_test_accuracy = 0\n",
    "\n",
    "        total_test_loss = 0\n",
    "        nb_test_steps = 0\n",
    "\n",
    "        all_preds = []\n",
    "        all_labels_ids = []\n",
    "\n",
    "        #loss\n",
    "        nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in test_dataloader:\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "                model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
    "                hidden_states = model_outputs.last_hidden_state[:,0,:] \n",
    "                #hidden_states = model_outputs[-1]\n",
    "                _, logits, probs = discriminator(hidden_states)\n",
    "                ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n",
    "                filtered_logits = logits[:,0:-1]\n",
    "                # Accumulate the test loss.\n",
    "                total_test_loss += nll_loss(filtered_logits, b_labels)\n",
    "\n",
    "            # Accumulate the predictions and the input labels\n",
    "            _, preds = torch.max(filtered_logits, 1)\n",
    "            all_preds += preds.detach().cpu()\n",
    "            all_labels_ids += b_labels.detach().cpu()\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        all_preds = torch.stack(all_preds).numpy()\n",
    "        all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
    "        test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
    "        print(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "        avg_test_loss = avg_test_loss.item()\n",
    "\n",
    "        # Measure how long the validation run took.\n",
    "        test_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"  Test Loss: {0:.3f}\".format(avg_test_loss))\n",
    "        print(\"  Test took: {:}\".format(test_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch + 1,\n",
    "                'Training Loss generator': avg_train_loss_g,\n",
    "                'Training Loss discriminator': avg_train_loss_d,\n",
    "                'Valid. Loss': avg_test_loss,\n",
    "                'Valid. Accur.': test_accuracy,\n",
    "                'Training Time': training_time,\n",
    "                'Test Time': test_time\n",
    "            }\n",
    "        )\n",
    "\n",
    "        accuracy_array.append(test_accuracy)\n",
    "        print(\"Train other shit\")\n",
    "        while niter < len(train_data):\n",
    "            # train ae\n",
    "            for i in range(niters_ae):\n",
    "                if niter >= len(train_data):\n",
    "                    break  # end of epoch\n",
    "                total_loss_ae, start_time = train_ae(epoch, train_data[niter],\n",
    "                                total_loss_ae, start_time, niter)\n",
    "                niter += 1\n",
    "            # train gan\n",
    "            for k in range(niter_gan):\n",
    "                for i in range(niters_gan_d):\n",
    "                    errD, errD_real, errD_fake = train_gan_d(\n",
    "                            train_data[random.randint(0, len(train_data)-1)], gan_type)\n",
    "                for i in range(niters_gan_ae):\n",
    "                    train_gan_d_into_ae(train_data[random.randint(0, len(train_data)-1)])\n",
    "                for i in range(niters_gan_g):\n",
    "                    errG = train_gan_g(gan_type)\n",
    "                if enhance_dec:\n",
    "                    for i in range(niters_gan_dec):\n",
    "                        errG_enh_dec = train_gan_dec()\n",
    "                else:\n",
    "                    errG_enh_dec = torch.Tensor([0])\n",
    "\n",
    "            niter_g += 1\n",
    "            if niter_g % log_interval == 0:\n",
    "                logging('[{}/{}][{}/{}] Loss_D: {:.8f} (Loss_D_real: {:.8f} '\n",
    "                        'Loss_D_fake: {:.8f}) Loss_G: {:.8f} Loss_Enh_Dec: {:.8f}'.format(\n",
    "                         epoch, epochs, niter, len(train_data),\n",
    "                         errD.data.item(), errD_real.data.item(),\n",
    "                         errD_fake.data.item(), errG.data.item(), errG_enh_dec.data.item()))\n",
    "        # eval\n",
    "        test_loss, accuracy = evaluate_autoencoder(test_data, epoch)\n",
    "        logging('| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} | '\n",
    "                'test ppl {:5.2f} | acc {:3.3f}'.format(epoch,\n",
    "                (time.time() - epoch_start_time), test_loss,\n",
    "                math.exp(test_loss), accuracy))\n",
    "\n",
    "        gen_text_savepath = os.path.join(save, \"{:03d}_examplar_gen\".format(epoch))\n",
    "        gen_fixed_noise(fixed_noise, gen_text_savepath)\n",
    "        if epoch % 5 == 0 or epoch % 4 == 0 or (epochs - epoch) <=2:\n",
    "            selfbleu, testbleu = eval_bleu(gen_text_savepath)\n",
    "            logging('bleu_self: [{:.8f},{:.8f},{:.8f},{:.8f},{:.8f}]'.format(selfbleu[0], selfbleu[1], selfbleu[2], selfbleu[3], selfbleu[4]))\n",
    "            logging('bleu_test: [{:.8f},{:.8f},{:.8f},{:.8f},{:.8f}]'.format(testbleu[0], testbleu[1], testbleu[2], testbleu[3], testbleu[4]))\n",
    "\n",
    "        if epoch % 15 == 0 or epoch == epochs-1:\n",
    "            logging(\"New saving model: epoch {:03d}.\".format(epoch))\n",
    "            save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a24898f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 1 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:27.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:31.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:46.\n",
      "\n",
      "  Average training loss generetor: 0.393\n",
      "  Average training loss discriminator: 3.839\n",
      "  Training epcoh took: 0:00:50\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.115\n",
      "  Test Loss: 2.369\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   1 |     0/ 1249 batches | lr 0.000000 | ms/batch 499.00 | loss  0.12 | ppl     1.13 | acc     0.00 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harry/venv/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/harry/venv/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/200][99/1249] Loss_D: 1.38573277 (Loss_D_real: 0.69176692 Loss_D_fake: 0.69396585) Loss_G: -0.00011238 Loss_Enh_Dec: -0.00027536\n",
      "| epoch   1 |   100/ 1249 batches | lr 0.000000 | ms/batch 353.77 | loss  8.33 | ppl  4144.12 | acc     0.18 | train_ae_norm     1.00\n",
      "[1/200][199/1249] Loss_D: 1.38554406 (Loss_D_real: 0.69211912 Loss_D_fake: 0.69342494) Loss_G: -0.00007557 Loss_Enh_Dec: -0.00020225\n",
      "| epoch   1 |   200/ 1249 batches | lr 0.000000 | ms/batch 353.28 | loss  7.38 | ppl  1609.59 | acc     0.19 | train_ae_norm     1.00\n",
      "[1/200][299/1249] Loss_D: 1.38596964 (Loss_D_real: 0.69255108 Loss_D_fake: 0.69341862) Loss_G: -0.00002267 Loss_Enh_Dec: -0.00013771\n",
      "| epoch   1 |   300/ 1249 batches | lr 0.000000 | ms/batch 352.96 | loss  7.09 | ppl  1204.07 | acc     0.22 | train_ae_norm     1.00\n",
      "[1/200][399/1249] Loss_D: 1.38600349 (Loss_D_real: 0.69271266 Loss_D_fake: 0.69329089) Loss_G: 0.00000278 Loss_Enh_Dec: -0.00009203\n",
      "| epoch   1 |   400/ 1249 batches | lr 0.000000 | ms/batch 353.42 | loss  6.85 | ppl   947.62 | acc     0.22 | train_ae_norm     1.00\n",
      "[1/200][499/1249] Loss_D: 1.38614798 (Loss_D_real: 0.69286746 Loss_D_fake: 0.69328046) Loss_G: 0.00001945 Loss_Enh_Dec: -0.00009352\n",
      "| epoch   1 |   500/ 1249 batches | lr 0.000000 | ms/batch 353.81 | loss  6.74 | ppl   848.38 | acc     0.26 | train_ae_norm     1.00\n",
      "[1/200][599/1249] Loss_D: 1.38586545 (Loss_D_real: 0.69279432 Loss_D_fake: 0.69307107) Loss_G: 0.00003009 Loss_Enh_Dec: -0.00005886\n",
      "| epoch   1 |   600/ 1249 batches | lr 0.000000 | ms/batch 354.03 | loss  6.57 | ppl   715.49 | acc     0.27 | train_ae_norm     1.00\n",
      "[1/200][699/1249] Loss_D: 1.38613462 (Loss_D_real: 0.69314623 Loss_D_fake: 0.69298834) Loss_G: 0.00005031 Loss_Enh_Dec: 0.00002175\n",
      "| epoch   1 |   700/ 1249 batches | lr 0.000000 | ms/batch 353.89 | loss  6.48 | ppl   649.32 | acc     0.24 | train_ae_norm     1.00\n",
      "[1/200][799/1249] Loss_D: 1.38596237 (Loss_D_real: 0.69344103 Loss_D_fake: 0.69252133) Loss_G: 0.00012481 Loss_Enh_Dec: 0.00006164\n",
      "| epoch   1 |   800/ 1249 batches | lr 0.000000 | ms/batch 353.03 | loss  6.42 | ppl   615.73 | acc     0.23 | train_ae_norm     1.00\n",
      "[1/200][899/1249] Loss_D: 1.38612354 (Loss_D_real: 0.69314307 Loss_D_fake: 0.69298047) Loss_G: -0.00000556 Loss_Enh_Dec: -0.00001811\n",
      "| epoch   1 |   900/ 1249 batches | lr 0.000000 | ms/batch 352.44 | loss  6.29 | ppl   540.24 | acc     0.30 | train_ae_norm     1.00\n",
      "[1/200][999/1249] Loss_D: 1.38624072 (Loss_D_real: 0.69323230 Loss_D_fake: 0.69300842) Loss_G: -0.00000124 Loss_Enh_Dec: 0.00003547\n",
      "| epoch   1 |  1000/ 1249 batches | lr 0.000000 | ms/batch 352.36 | loss  6.23 | ppl   506.63 | acc     0.28 | train_ae_norm     1.00\n",
      "[1/200][1099/1249] Loss_D: 1.38625669 (Loss_D_real: 0.69307852 Loss_D_fake: 0.69317818) Loss_G: 0.00003644 Loss_Enh_Dec: 0.00000750\n",
      "| epoch   1 |  1100/ 1249 batches | lr 0.000000 | ms/batch 352.38 | loss  6.16 | ppl   471.40 | acc     0.25 | train_ae_norm     1.00\n",
      "[1/200][1199/1249] Loss_D: 1.38641214 (Loss_D_real: 0.69310737 Loss_D_fake: 0.69330472) Loss_G: -0.00000749 Loss_Enh_Dec: 0.00000114\n",
      "| epoch   1 |  1200/ 1249 batches | lr 0.000000 | ms/batch 352.62 | loss  6.09 | ppl   439.90 | acc     0.26 | train_ae_norm     1.00\n",
      "| end of epoch   1 | time: 497.44s | test loss  5.96 | test ppl 388.60 | acc 0.306\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 2 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.577\n",
      "  Average training loss discriminator: 3.381\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.138\n",
      "  Test Loss: 2.331\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   2 |     0/ 1249 batches | lr 0.000000 | ms/batch 488.40 | loss  0.06 | ppl     1.06 | acc     0.25 | train_ae_norm     1.00\n",
      "[2/200][99/1249] Loss_D: 1.38625169 (Loss_D_real: 0.69300401 Loss_D_fake: 0.69324768) Loss_G: -0.00004052 Loss_Enh_Dec: 0.00000620\n",
      "| epoch   2 |   100/ 1249 batches | lr 0.000000 | ms/batch 353.38 | loss  6.02 | ppl   413.31 | acc     0.31 | train_ae_norm     1.00\n",
      "[2/200][199/1249] Loss_D: 1.38648999 (Loss_D_real: 0.69333071 Loss_D_fake: 0.69315928) Loss_G: 0.00002103 Loss_Enh_Dec: 0.00003799\n",
      "| epoch   2 |   200/ 1249 batches | lr 0.000000 | ms/batch 352.97 | loss  5.94 | ppl   379.05 | acc     0.29 | train_ae_norm     1.00\n",
      "[2/200][299/1249] Loss_D: 1.38645172 (Loss_D_real: 0.69313550 Loss_D_fake: 0.69331622) Loss_G: -0.00005075 Loss_Enh_Dec: 0.00003113\n",
      "| epoch   2 |   300/ 1249 batches | lr 0.000000 | ms/batch 353.25 | loss  5.86 | ppl   351.62 | acc     0.33 | train_ae_norm     1.00\n",
      "[2/200][399/1249] Loss_D: 1.38643181 (Loss_D_real: 0.69332230 Loss_D_fake: 0.69310951) Loss_G: -0.00000801 Loss_Enh_Dec: 0.00004462\n",
      "| epoch   2 |   400/ 1249 batches | lr 0.000000 | ms/batch 353.81 | loss  5.78 | ppl   322.86 | acc     0.31 | train_ae_norm     1.00\n",
      "[2/200][499/1249] Loss_D: 1.38661790 (Loss_D_real: 0.69330806 Loss_D_fake: 0.69330990) Loss_G: -0.00002048 Loss_Enh_Dec: 0.00000382\n",
      "| epoch   2 |   500/ 1249 batches | lr 0.000000 | ms/batch 353.26 | loss  5.74 | ppl   311.16 | acc     0.31 | train_ae_norm     1.00\n",
      "[2/200][599/1249] Loss_D: 1.38645601 (Loss_D_real: 0.69311893 Loss_D_fake: 0.69333714) Loss_G: -0.00003899 Loss_Enh_Dec: -0.00001682\n",
      "| epoch   2 |   600/ 1249 batches | lr 0.000000 | ms/batch 353.87 | loss  5.66 | ppl   286.61 | acc     0.36 | train_ae_norm     1.00\n",
      "[2/200][699/1249] Loss_D: 1.38634324 (Loss_D_real: 0.69322145 Loss_D_fake: 0.69312179) Loss_G: 0.00000696 Loss_Enh_Dec: 0.00001446\n",
      "| epoch   2 |   700/ 1249 batches | lr 0.000000 | ms/batch 352.88 | loss  5.57 | ppl   262.60 | acc     0.35 | train_ae_norm     1.00\n",
      "[2/200][799/1249] Loss_D: 1.38644552 (Loss_D_real: 0.69329232 Loss_D_fake: 0.69315326) Loss_G: 0.00001668 Loss_Enh_Dec: 0.00003025\n",
      "| epoch   2 |   800/ 1249 batches | lr 0.000000 | ms/batch 353.64 | loss  5.55 | ppl   257.43 | acc     0.35 | train_ae_norm     1.00\n",
      "[2/200][899/1249] Loss_D: 1.38633180 (Loss_D_real: 0.69314957 Loss_D_fake: 0.69318223) Loss_G: 0.00000227 Loss_Enh_Dec: 0.00000348\n",
      "| epoch   2 |   900/ 1249 batches | lr 0.000000 | ms/batch 353.00 | loss  5.44 | ppl   230.69 | acc     0.39 | train_ae_norm     1.00\n",
      "[2/200][999/1249] Loss_D: 1.38637853 (Loss_D_real: 0.69320738 Loss_D_fake: 0.69317120) Loss_G: -0.00000133 Loss_Enh_Dec: 0.00001606\n",
      "| epoch   2 |  1000/ 1249 batches | lr 0.000000 | ms/batch 353.11 | loss  5.38 | ppl   217.62 | acc     0.42 | train_ae_norm     1.00\n",
      "[2/200][1099/1249] Loss_D: 1.38636732 (Loss_D_real: 0.69319391 Loss_D_fake: 0.69317341) Loss_G: 0.00000268 Loss_Enh_Dec: 0.00001415\n",
      "| epoch   2 |  1100/ 1249 batches | lr 0.000000 | ms/batch 353.31 | loss  5.33 | ppl   207.32 | acc     0.39 | train_ae_norm     1.00\n",
      "[2/200][1199/1249] Loss_D: 1.38633716 (Loss_D_real: 0.69316089 Loss_D_fake: 0.69317627) Loss_G: -0.00001397 Loss_Enh_Dec: 0.00000947\n",
      "| epoch   2 |  1200/ 1249 batches | lr 0.000000 | ms/batch 353.46 | loss  5.25 | ppl   190.84 | acc     0.38 | train_ae_norm     1.00\n",
      "| end of epoch   2 | time: 496.61s | test loss  5.03 | test ppl 152.50 | acc 0.436\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 3 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.678\n",
      "  Average training loss discriminator: 3.093\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.212\n",
      "  Test Loss: 2.280\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   3 |     0/ 1249 batches | lr 0.000000 | ms/batch 489.59 | loss  0.05 | ppl     1.06 | acc     0.37 | train_ae_norm     1.00\n",
      "[3/200][99/1249] Loss_D: 1.38630676 (Loss_D_real: 0.69315100 Loss_D_fake: 0.69315571) Loss_G: -0.00000482 Loss_Enh_Dec: 0.00000140\n",
      "| epoch   3 |   100/ 1249 batches | lr 0.000000 | ms/batch 353.73 | loss  5.18 | ppl   177.67 | acc     0.42 | train_ae_norm     1.00\n",
      "[3/200][199/1249] Loss_D: 1.38629365 (Loss_D_real: 0.69313359 Loss_D_fake: 0.69316006) Loss_G: -0.00000641 Loss_Enh_Dec: 0.00000215\n",
      "| epoch   3 |   200/ 1249 batches | lr 0.000000 | ms/batch 353.34 | loss  5.11 | ppl   165.56 | acc     0.42 | train_ae_norm     1.00\n",
      "[3/200][299/1249] Loss_D: 1.38630569 (Loss_D_real: 0.69316089 Loss_D_fake: 0.69314480) Loss_G: 0.00000386 Loss_Enh_Dec: 0.00000562\n",
      "| epoch   3 |   300/ 1249 batches | lr 0.000000 | ms/batch 351.83 | loss  5.05 | ppl   155.38 | acc     0.43 | train_ae_norm     1.00\n",
      "[3/200][399/1249] Loss_D: 1.38630199 (Loss_D_real: 0.69317639 Loss_D_fake: 0.69312561) Loss_G: 0.00000504 Loss_Enh_Dec: 0.00000517\n",
      "| epoch   3 |   400/ 1249 batches | lr 0.000000 | ms/batch 352.66 | loss  4.96 | ppl   142.17 | acc     0.47 | train_ae_norm     1.00\n",
      "[3/200][499/1249] Loss_D: 1.38629568 (Loss_D_real: 0.69315737 Loss_D_fake: 0.69313830) Loss_G: 0.00000019 Loss_Enh_Dec: 0.00000143\n",
      "| epoch   3 |   500/ 1249 batches | lr 0.000000 | ms/batch 351.90 | loss  4.97 | ppl   143.88 | acc     0.45 | train_ae_norm     1.00\n",
      "[3/200][599/1249] Loss_D: 1.38628936 (Loss_D_real: 0.69314444 Loss_D_fake: 0.69314492) Loss_G: 0.00000019 Loss_Enh_Dec: -0.00000015\n",
      "| epoch   3 |   600/ 1249 batches | lr 0.000000 | ms/batch 352.51 | loss  4.90 | ppl   134.52 | acc     0.45 | train_ae_norm     1.00\n",
      "[3/200][699/1249] Loss_D: 1.38628626 (Loss_D_real: 0.69315231 Loss_D_fake: 0.69313401) Loss_G: 0.00000402 Loss_Enh_Dec: -0.00000011\n",
      "| epoch   3 |   700/ 1249 batches | lr 0.000000 | ms/batch 353.25 | loss  4.84 | ppl   126.78 | acc     0.44 | train_ae_norm     1.00\n",
      "[3/200][799/1249] Loss_D: 1.38630605 (Loss_D_real: 0.69314361 Loss_D_fake: 0.69316244) Loss_G: -0.00000287 Loss_Enh_Dec: -0.00000118\n",
      "| epoch   3 |   800/ 1249 batches | lr 0.000000 | ms/batch 352.79 | loss  4.86 | ppl   129.65 | acc     0.44 | train_ae_norm     1.00\n",
      "[3/200][899/1249] Loss_D: 1.38630486 (Loss_D_real: 0.69313258 Loss_D_fake: 0.69317234) Loss_G: -0.00000280 Loss_Enh_Dec: -0.00000146\n",
      "| epoch   3 |   900/ 1249 batches | lr 0.000000 | ms/batch 354.07 | loss  4.79 | ppl   120.20 | acc     0.46 | train_ae_norm     1.00\n",
      "[3/200][999/1249] Loss_D: 1.38630140 (Loss_D_real: 0.69315070 Loss_D_fake: 0.69315070) Loss_G: -0.00000030 Loss_Enh_Dec: 0.00000180\n",
      "| epoch   3 |  1000/ 1249 batches | lr 0.000000 | ms/batch 353.76 | loss  4.74 | ppl   114.38 | acc     0.50 | train_ae_norm     1.00\n",
      "[3/200][1099/1249] Loss_D: 1.38630474 (Loss_D_real: 0.69314277 Loss_D_fake: 0.69316196) Loss_G: -0.00000059 Loss_Enh_Dec: -0.00000102\n",
      "| epoch   3 |  1100/ 1249 batches | lr 0.000000 | ms/batch 353.52 | loss  4.71 | ppl   110.83 | acc     0.47 | train_ae_norm     1.00\n",
      "[3/200][1199/1249] Loss_D: 1.38628542 (Loss_D_real: 0.69313765 Loss_D_fake: 0.69314778) Loss_G: -0.00000110 Loss_Enh_Dec: 0.00000043\n",
      "| epoch   3 |  1200/ 1249 batches | lr 0.000000 | ms/batch 353.15 | loss  4.65 | ppl   104.24 | acc     0.49 | train_ae_norm     1.00\n",
      "| end of epoch   3 | time: 496.34s | test loss  4.40 | test ppl 81.65 | acc 0.524\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 4 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.680\n",
      "  Average training loss discriminator: 2.870\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.217\n",
      "  Test Loss: 2.223\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   4 |     0/ 1249 batches | lr 0.000000 | ms/batch 489.75 | loss  0.05 | ppl     1.05 | acc     0.46 | train_ae_norm     1.00\n",
      "[4/200][99/1249] Loss_D: 1.38630605 (Loss_D_real: 0.69316500 Loss_D_fake: 0.69314110) Loss_G: 0.00000251 Loss_Enh_Dec: 0.00000313\n",
      "| epoch   4 |   100/ 1249 batches | lr 0.000000 | ms/batch 353.50 | loss  4.61 | ppl   100.52 | acc     0.49 | train_ae_norm     1.00\n",
      "[4/200][199/1249] Loss_D: 1.38630366 (Loss_D_real: 0.69315982 Loss_D_fake: 0.69314378) Loss_G: 0.00000091 Loss_Enh_Dec: 0.00000118\n",
      "| epoch   4 |   200/ 1249 batches | lr 0.000000 | ms/batch 353.26 | loss  4.55 | ppl    94.37 | acc     0.51 | train_ae_norm     1.00\n",
      "[4/200][299/1249] Loss_D: 1.38629830 (Loss_D_real: 0.69314492 Loss_D_fake: 0.69315338) Loss_G: -0.00000254 Loss_Enh_Dec: 0.00000033\n",
      "| epoch   4 |   300/ 1249 batches | lr 0.000000 | ms/batch 352.47 | loss  4.48 | ppl    88.34 | acc     0.50 | train_ae_norm     1.00\n",
      "[4/200][399/1249] Loss_D: 1.38628578 (Loss_D_real: 0.69313347 Loss_D_fake: 0.69315231) Loss_G: -0.00000292 Loss_Enh_Dec: 0.00000088\n",
      "| epoch   4 |   400/ 1249 batches | lr 0.000000 | ms/batch 353.35 | loss  4.42 | ppl    82.96 | acc     0.56 | train_ae_norm     1.00\n",
      "[4/200][499/1249] Loss_D: 1.38628900 (Loss_D_real: 0.69312763 Loss_D_fake: 0.69316137) Loss_G: 0.00000069 Loss_Enh_Dec: -0.00000036\n",
      "| epoch   4 |   500/ 1249 batches | lr 0.000000 | ms/batch 353.86 | loss  4.44 | ppl    84.79 | acc     0.50 | train_ae_norm     1.00\n",
      "[4/200][599/1249] Loss_D: 1.38630199 (Loss_D_real: 0.69314784 Loss_D_fake: 0.69315422) Loss_G: -0.00000078 Loss_Enh_Dec: 0.00000017\n",
      "| epoch   4 |   600/ 1249 batches | lr 0.000000 | ms/batch 353.10 | loss  4.38 | ppl    79.97 | acc     0.53 | train_ae_norm     1.00\n",
      "[4/200][699/1249] Loss_D: 1.38629127 (Loss_D_real: 0.69313043 Loss_D_fake: 0.69316089) Loss_G: -0.00000087 Loss_Enh_Dec: -0.00000271\n",
      "| epoch   4 |   700/ 1249 batches | lr 0.000000 | ms/batch 352.26 | loss  4.32 | ppl    75.30 | acc     0.49 | train_ae_norm     1.00\n",
      "[4/200][799/1249] Loss_D: 1.38629723 (Loss_D_real: 0.69316143 Loss_D_fake: 0.69313574) Loss_G: 0.00000034 Loss_Enh_Dec: -0.00000023\n",
      "| epoch   4 |   800/ 1249 batches | lr 0.000000 | ms/batch 353.38 | loss  4.35 | ppl    77.32 | acc     0.49 | train_ae_norm     1.00\n",
      "[4/200][899/1249] Loss_D: 1.38629878 (Loss_D_real: 0.69314671 Loss_D_fake: 0.69315207) Loss_G: 0.00000084 Loss_Enh_Dec: -0.00000032\n",
      "| epoch   4 |   900/ 1249 batches | lr 0.000000 | ms/batch 353.15 | loss  4.26 | ppl    70.88 | acc     0.53 | train_ae_norm     1.00\n",
      "[4/200][999/1249] Loss_D: 1.38628614 (Loss_D_real: 0.69314313 Loss_D_fake: 0.69314301) Loss_G: 0.00000020 Loss_Enh_Dec: -0.00000076\n",
      "| epoch   4 |  1000/ 1249 batches | lr 0.000000 | ms/batch 352.62 | loss  4.23 | ppl    68.67 | acc     0.55 | train_ae_norm     1.00\n",
      "[4/200][1099/1249] Loss_D: 1.38629007 (Loss_D_real: 0.69313401 Loss_D_fake: 0.69315612) Loss_G: -0.00000229 Loss_Enh_Dec: -0.00000353\n",
      "| epoch   4 |  1100/ 1249 batches | lr 0.000000 | ms/batch 353.53 | loss  4.21 | ppl    67.48 | acc     0.54 | train_ae_norm     1.00\n",
      "[4/200][1199/1249] Loss_D: 1.38629842 (Loss_D_real: 0.69314218 Loss_D_fake: 0.69315624) Loss_G: -0.00000160 Loss_Enh_Dec: -0.00000094\n",
      "| epoch   4 |  1200/ 1249 batches | lr 0.000000 | ms/batch 353.67 | loss  4.16 | ppl    64.10 | acc     0.53 | train_ae_norm     1.00\n",
      "| end of epoch   4 | time: 496.53s | test loss  3.87 | test ppl 48.06 | acc 0.583\n",
      "bleu_self:  [0.58839286 0.49550715 0.32931843 0.30739408 0.27197323]\n",
      "bleu_test:  [0.85446429 0.78134458 0.62901847 0.31777356 0.27924565]\n",
      "bleu_self: [0.58839286,0.49550715,0.32931843,0.30739408,0.27197323]\n",
      "bleu_test: [0.85446429,0.78134458,0.62901847,0.31777356,0.27924565]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 5 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.691\n",
      "  Average training loss discriminator: 2.619\n",
      "  Training epcoh took: 0:00:48\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.270\n",
      "  Test Loss: 2.166\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   5 |     0/ 1249 batches | lr 0.000000 | ms/batch 488.05 | loss  0.04 | ppl     1.04 | acc     0.51 | train_ae_norm     1.00\n",
      "[5/200][99/1249] Loss_D: 1.38629651 (Loss_D_real: 0.69315243 Loss_D_fake: 0.69314402) Loss_G: -0.00000150 Loss_Enh_Dec: -0.00000043\n",
      "| epoch   5 |   100/ 1249 batches | lr 0.000000 | ms/batch 352.95 | loss  4.12 | ppl    61.35 | acc     0.56 | train_ae_norm     1.00\n",
      "[5/200][199/1249] Loss_D: 1.38629913 (Loss_D_real: 0.69314361 Loss_D_fake: 0.69315547) Loss_G: 0.00000069 Loss_Enh_Dec: -0.00000028\n",
      "| epoch   5 |   200/ 1249 batches | lr 0.000000 | ms/batch 353.22 | loss  4.08 | ppl    58.88 | acc     0.55 | train_ae_norm     1.00\n",
      "[5/200][299/1249] Loss_D: 1.38630056 (Loss_D_real: 0.69315189 Loss_D_fake: 0.69314873) Loss_G: 0.00000033 Loss_Enh_Dec: -0.00000114\n",
      "| epoch   5 |   300/ 1249 batches | lr 0.000000 | ms/batch 352.71 | loss  4.01 | ppl    55.16 | acc     0.58 | train_ae_norm     1.00\n",
      "[5/200][399/1249] Loss_D: 1.38629484 (Loss_D_real: 0.69314790 Loss_D_fake: 0.69314688) Loss_G: -0.00000069 Loss_Enh_Dec: -0.00000182\n",
      "| epoch   5 |   400/ 1249 batches | lr 0.000000 | ms/batch 353.36 | loss  3.95 | ppl    51.78 | acc     0.60 | train_ae_norm     1.00\n",
      "[5/200][499/1249] Loss_D: 1.38628888 (Loss_D_real: 0.69315815 Loss_D_fake: 0.69313073) Loss_G: 0.00000215 Loss_Enh_Dec: 0.00000085\n",
      "| epoch   5 |   500/ 1249 batches | lr 0.000000 | ms/batch 353.33 | loss  3.97 | ppl    53.06 | acc     0.56 | train_ae_norm     1.00\n",
      "[5/200][599/1249] Loss_D: 1.38629735 (Loss_D_real: 0.69315076 Loss_D_fake: 0.69314659) Loss_G: -0.00000327 Loss_Enh_Dec: -0.00000335\n",
      "| epoch   5 |   600/ 1249 batches | lr 0.000000 | ms/batch 353.04 | loss  3.92 | ppl    50.58 | acc     0.59 | train_ae_norm     1.00\n",
      "[5/200][699/1249] Loss_D: 1.38628662 (Loss_D_real: 0.69315171 Loss_D_fake: 0.69313490) Loss_G: -0.00000218 Loss_Enh_Dec: -0.00000328\n",
      "| epoch   5 |   700/ 1249 batches | lr 0.000000 | ms/batch 351.76 | loss  3.87 | ppl    47.81 | acc     0.57 | train_ae_norm     1.00\n",
      "[5/200][799/1249] Loss_D: 1.38630128 (Loss_D_real: 0.69314384 Loss_D_fake: 0.69315737) Loss_G: 0.00000270 Loss_Enh_Dec: 0.00000099\n",
      "| epoch   5 |   800/ 1249 batches | lr 0.000000 | ms/batch 352.42 | loss  3.89 | ppl    48.93 | acc     0.53 | train_ae_norm     1.00\n",
      "[5/200][899/1249] Loss_D: 1.38630390 (Loss_D_real: 0.69315284 Loss_D_fake: 0.69315100) Loss_G: 0.00000025 Loss_Enh_Dec: -0.00000063\n",
      "| epoch   5 |   900/ 1249 batches | lr 0.000000 | ms/batch 353.52 | loss  3.82 | ppl    45.63 | acc     0.57 | train_ae_norm     1.00\n",
      "[5/200][999/1249] Loss_D: 1.38629162 (Loss_D_real: 0.69315135 Loss_D_fake: 0.69314027) Loss_G: 0.00000238 Loss_Enh_Dec: 0.00000276\n",
      "| epoch   5 |  1000/ 1249 batches | lr 0.000000 | ms/batch 353.44 | loss  3.79 | ppl    44.19 | acc     0.57 | train_ae_norm     1.00\n",
      "[5/200][1099/1249] Loss_D: 1.38630366 (Loss_D_real: 0.69314718 Loss_D_fake: 0.69315642) Loss_G: -0.00000201 Loss_Enh_Dec: -0.00000150\n",
      "| epoch   5 |  1100/ 1249 batches | lr 0.000000 | ms/batch 353.29 | loss  3.78 | ppl    43.70 | acc     0.58 | train_ae_norm     1.00\n",
      "[5/200][1199/1249] Loss_D: 1.38630354 (Loss_D_real: 0.69314384 Loss_D_fake: 0.69315970) Loss_G: -0.00000114 Loss_Enh_Dec: -0.00000489\n",
      "| epoch   5 |  1200/ 1249 batches | lr 0.000000 | ms/batch 353.82 | loss  3.71 | ppl    40.88 | acc     0.58 | train_ae_norm     1.00\n",
      "| end of epoch   5 | time: 496.24s | test loss  3.40 | test ppl 30.03 | acc 0.630\n",
      "bleu_self:  [4.55681818e-01 2.78247602e-01 2.02956591e-01 1.03906715e-01\n",
      " 9.38965983e-05]\n",
      "bleu_test:  [9.00000000e-01 6.20318388e-01 2.84178751e-01 4.49425696e-02\n",
      " 3.75248696e-05]\n",
      "bleu_self: [0.45568182,0.27824760,0.20295659,0.10390672,0.00009390]\n",
      "bleu_test: [0.90000000,0.62031839,0.28417875,0.04494257,0.00003752]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 6 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.693\n",
      "  Average training loss discriminator: 2.361\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.275\n",
      "  Test Loss: 2.107\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   6 |     0/ 1249 batches | lr 0.000000 | ms/batch 489.92 | loss  0.04 | ppl     1.04 | acc     0.58 | train_ae_norm     1.00\n",
      "[6/200][99/1249] Loss_D: 1.38629222 (Loss_D_real: 0.69315612 Loss_D_fake: 0.69313610) Loss_G: -0.00000028 Loss_Enh_Dec: -0.00000138\n",
      "| epoch   6 |   100/ 1249 batches | lr 0.000000 | ms/batch 353.33 | loss  3.69 | ppl    40.15 | acc     0.60 | train_ae_norm     1.00\n",
      "[6/200][199/1249] Loss_D: 1.38631117 (Loss_D_real: 0.69314927 Loss_D_fake: 0.69316190) Loss_G: 0.00000143 Loss_Enh_Dec: 0.00000242\n",
      "| epoch   6 |   200/ 1249 batches | lr 0.000000 | ms/batch 354.13 | loss  3.65 | ppl    38.41 | acc     0.62 | train_ae_norm     1.00\n",
      "[6/200][299/1249] Loss_D: 1.38629770 (Loss_D_real: 0.69314790 Loss_D_fake: 0.69314975) Loss_G: -0.00000045 Loss_Enh_Dec: -0.00000345\n",
      "| epoch   6 |   300/ 1249 batches | lr 0.000000 | ms/batch 353.27 | loss  3.62 | ppl    37.51 | acc     0.60 | train_ae_norm     1.00\n",
      "[6/200][399/1249] Loss_D: 1.38629937 (Loss_D_real: 0.69314933 Loss_D_fake: 0.69315004) Loss_G: 0.00000087 Loss_Enh_Dec: -0.00000026\n",
      "| epoch   6 |   400/ 1249 batches | lr 0.000000 | ms/batch 353.39 | loss  3.55 | ppl    34.66 | acc     0.63 | train_ae_norm     1.00\n",
      "[6/200][499/1249] Loss_D: 1.38629472 (Loss_D_real: 0.69314611 Loss_D_fake: 0.69314861) Loss_G: -0.00000329 Loss_Enh_Dec: -0.00000299\n",
      "| epoch   6 |   500/ 1249 batches | lr 0.000000 | ms/batch 353.84 | loss  3.55 | ppl    34.83 | acc     0.60 | train_ae_norm     1.00\n",
      "[6/200][599/1249] Loss_D: 1.38630795 (Loss_D_real: 0.69315070 Loss_D_fake: 0.69315720) Loss_G: 0.00000244 Loss_Enh_Dec: 0.00000359\n",
      "| epoch   6 |   600/ 1249 batches | lr 0.000000 | ms/batch 353.57 | loss  3.51 | ppl    33.42 | acc     0.61 | train_ae_norm     1.00\n",
      "[6/200][699/1249] Loss_D: 1.38629806 (Loss_D_real: 0.69315076 Loss_D_fake: 0.69314730) Loss_G: -0.00000107 Loss_Enh_Dec: 0.00000076\n",
      "| epoch   6 |   700/ 1249 batches | lr 0.000000 | ms/batch 354.03 | loss  3.51 | ppl    33.34 | acc     0.59 | train_ae_norm     1.00\n",
      "[6/200][799/1249] Loss_D: 1.38629985 (Loss_D_real: 0.69315475 Loss_D_fake: 0.69314516) Loss_G: -0.00000229 Loss_Enh_Dec: -0.00000173\n",
      "| epoch   6 |   800/ 1249 batches | lr 0.000000 | ms/batch 353.35 | loss  3.51 | ppl    33.58 | acc     0.57 | train_ae_norm     1.00\n",
      "[6/200][899/1249] Loss_D: 1.38629532 (Loss_D_real: 0.69315070 Loss_D_fake: 0.69314468) Loss_G: -0.00000047 Loss_Enh_Dec: -0.00000268\n",
      "| epoch   6 |   900/ 1249 batches | lr 0.000000 | ms/batch 354.56 | loss  3.43 | ppl    30.90 | acc     0.59 | train_ae_norm     1.00\n",
      "[6/200][999/1249] Loss_D: 1.38629985 (Loss_D_real: 0.69316912 Loss_D_fake: 0.69313067) Loss_G: -0.00000326 Loss_Enh_Dec: -0.00000727\n",
      "| epoch   6 |  1000/ 1249 batches | lr 0.000000 | ms/batch 354.72 | loss  3.42 | ppl    30.49 | acc     0.61 | train_ae_norm     1.00\n",
      "[6/200][1099/1249] Loss_D: 1.38630033 (Loss_D_real: 0.69314861 Loss_D_fake: 0.69315171) Loss_G: -0.00000147 Loss_Enh_Dec: -0.00000432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   6 |  1100/ 1249 batches | lr 0.000000 | ms/batch 353.43 | loss  3.42 | ppl    30.45 | acc     0.61 | train_ae_norm     1.00\n",
      "[6/200][1199/1249] Loss_D: 1.38628948 (Loss_D_real: 0.69315422 Loss_D_fake: 0.69313526) Loss_G: -0.00000109 Loss_Enh_Dec: -0.00000313\n",
      "| epoch   6 |  1200/ 1249 batches | lr 0.000000 | ms/batch 352.83 | loss  3.37 | ppl    29.18 | acc     0.60 | train_ae_norm     1.00\n",
      "| end of epoch   6 | time: 497.21s | test loss  3.03 | test ppl 20.61 | acc 0.665\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 7 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.714\n",
      "  Average training loss discriminator: 2.045\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.292\n",
      "  Test Loss: 2.067\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   7 |     0/ 1249 batches | lr 0.000000 | ms/batch 490.81 | loss  0.03 | ppl     1.03 | acc     0.60 | train_ae_norm     1.00\n",
      "[7/200][99/1249] Loss_D: 1.38630331 (Loss_D_real: 0.69315785 Loss_D_fake: 0.69314545) Loss_G: 0.00000150 Loss_Enh_Dec: 0.00000100\n",
      "| epoch   7 |   100/ 1249 batches | lr 0.000000 | ms/batch 353.16 | loss  3.37 | ppl    28.97 | acc     0.63 | train_ae_norm     1.00\n",
      "[7/200][199/1249] Loss_D: 1.38629496 (Loss_D_real: 0.69314694 Loss_D_fake: 0.69314802) Loss_G: -0.00000068 Loss_Enh_Dec: 0.00000045\n",
      "| epoch   7 |   200/ 1249 batches | lr 0.000000 | ms/batch 353.69 | loss  3.35 | ppl    28.61 | acc     0.64 | train_ae_norm     1.00\n",
      "[7/200][299/1249] Loss_D: 1.38630068 (Loss_D_real: 0.69314021 Loss_D_fake: 0.69316047) Loss_G: 0.00000093 Loss_Enh_Dec: -0.00000498\n",
      "| epoch   7 |   300/ 1249 batches | lr 0.000000 | ms/batch 354.24 | loss  3.34 | ppl    28.22 | acc     0.63 | train_ae_norm     1.00\n",
      "[7/200][399/1249] Loss_D: 1.38629305 (Loss_D_real: 0.69314492 Loss_D_fake: 0.69314814) Loss_G: -0.00000184 Loss_Enh_Dec: -0.00000728\n",
      "| epoch   7 |   400/ 1249 batches | lr 0.000000 | ms/batch 353.84 | loss  3.29 | ppl    26.97 | acc     0.66 | train_ae_norm     1.00\n",
      "[7/200][499/1249] Loss_D: 1.38630056 (Loss_D_real: 0.69315505 Loss_D_fake: 0.69314545) Loss_G: -0.00000298 Loss_Enh_Dec: -0.00000705\n",
      "| epoch   7 |   500/ 1249 batches | lr 0.000000 | ms/batch 354.09 | loss  3.31 | ppl    27.31 | acc     0.60 | train_ae_norm     1.00\n",
      "[7/200][599/1249] Loss_D: 1.38629568 (Loss_D_real: 0.69313705 Loss_D_fake: 0.69315863) Loss_G: 0.00000138 Loss_Enh_Dec: -0.00000151\n",
      "| epoch   7 |   600/ 1249 batches | lr 0.000000 | ms/batch 353.66 | loss  3.24 | ppl    25.42 | acc     0.64 | train_ae_norm     1.00\n",
      "[7/200][699/1249] Loss_D: 1.38629460 (Loss_D_real: 0.69313306 Loss_D_fake: 0.69316149) Loss_G: -0.00000028 Loss_Enh_Dec: -0.00000113\n",
      "| epoch   7 |   700/ 1249 batches | lr 0.000000 | ms/batch 353.67 | loss  3.19 | ppl    24.19 | acc     0.63 | train_ae_norm     1.00\n",
      "[7/200][799/1249] Loss_D: 1.38629556 (Loss_D_real: 0.69314212 Loss_D_fake: 0.69315350) Loss_G: 0.00000109 Loss_Enh_Dec: 0.00000111\n",
      "| epoch   7 |   800/ 1249 batches | lr 0.000000 | ms/batch 352.83 | loss  3.18 | ppl    24.14 | acc     0.60 | train_ae_norm     1.00\n",
      "[7/200][899/1249] Loss_D: 1.38629484 (Loss_D_real: 0.69313848 Loss_D_fake: 0.69315636) Loss_G: 0.00000295 Loss_Enh_Dec: 0.00000004\n",
      "| epoch   7 |   900/ 1249 batches | lr 0.000000 | ms/batch 353.31 | loss  3.09 | ppl    22.09 | acc     0.61 | train_ae_norm     1.00\n",
      "[7/200][999/1249] Loss_D: 1.38629878 (Loss_D_real: 0.69314194 Loss_D_fake: 0.69315684) Loss_G: 0.00000204 Loss_Enh_Dec: 0.00000490\n",
      "| epoch   7 |  1000/ 1249 batches | lr 0.000000 | ms/batch 353.48 | loss  3.09 | ppl    22.00 | acc     0.66 | train_ae_norm     1.00\n",
      "[7/200][1099/1249] Loss_D: 1.38629627 (Loss_D_real: 0.69314086 Loss_D_fake: 0.69315547) Loss_G: 0.00000133 Loss_Enh_Dec: -0.00000055\n",
      "| epoch   7 |  1100/ 1249 batches | lr 0.000000 | ms/batch 353.48 | loss  3.10 | ppl    22.22 | acc     0.64 | train_ae_norm     1.00\n",
      "[7/200][1199/1249] Loss_D: 1.38629889 (Loss_D_real: 0.69315302 Loss_D_fake: 0.69314593) Loss_G: -0.00000010 Loss_Enh_Dec: 0.00000036\n",
      "| epoch   7 |  1200/ 1249 batches | lr 0.000000 | ms/batch 352.55 | loss  3.04 | ppl    20.89 | acc     0.63 | train_ae_norm     1.00\n",
      "| end of epoch   7 | time: 497.02s | test loss  2.72 | test ppl 15.25 | acc 0.695\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 8 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.710\n",
      "  Average training loss discriminator: 1.784\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.318\n",
      "  Test Loss: 2.009\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   8 |     0/ 1249 batches | lr 0.000000 | ms/batch 488.86 | loss  0.03 | ppl     1.03 | acc     0.66 | train_ae_norm     1.00\n",
      "[8/200][99/1249] Loss_D: 1.38629341 (Loss_D_real: 0.69313687 Loss_D_fake: 0.69315660) Loss_G: 0.00000187 Loss_Enh_Dec: -0.00000741\n",
      "| epoch   8 |   100/ 1249 batches | lr 0.000000 | ms/batch 352.82 | loss  3.00 | ppl    20.14 | acc     0.65 | train_ae_norm     1.00\n",
      "[8/200][199/1249] Loss_D: 1.38629615 (Loss_D_real: 0.69316024 Loss_D_fake: 0.69313592) Loss_G: -0.00000154 Loss_Enh_Dec: -0.00000088\n",
      "| epoch   8 |   200/ 1249 batches | lr 0.000000 | ms/batch 352.43 | loss  2.98 | ppl    19.63 | acc     0.68 | train_ae_norm     1.00\n",
      "[8/200][299/1249] Loss_D: 1.38629055 (Loss_D_real: 0.69313425 Loss_D_fake: 0.69315624) Loss_G: 0.00000533 Loss_Enh_Dec: 0.00000565\n",
      "| epoch   8 |   300/ 1249 batches | lr 0.000000 | ms/batch 352.63 | loss  2.96 | ppl    19.33 | acc     0.68 | train_ae_norm     1.00\n",
      "[8/200][399/1249] Loss_D: 1.38629651 (Loss_D_real: 0.69315356 Loss_D_fake: 0.69314301) Loss_G: -0.00000234 Loss_Enh_Dec: -0.00000719\n",
      "| epoch   8 |   400/ 1249 batches | lr 0.000000 | ms/batch 352.22 | loss  2.92 | ppl    18.49 | acc     0.70 | train_ae_norm     1.00\n",
      "[8/200][499/1249] Loss_D: 1.38628662 (Loss_D_real: 0.69312429 Loss_D_fake: 0.69316232) Loss_G: 0.00000502 Loss_Enh_Dec: 0.00000118\n",
      "| epoch   8 |   500/ 1249 batches | lr 0.000000 | ms/batch 353.00 | loss  2.91 | ppl    18.41 | acc     0.66 | train_ae_norm     1.00\n",
      "[8/200][599/1249] Loss_D: 1.38629580 (Loss_D_real: 0.69314301 Loss_D_fake: 0.69315279) Loss_G: 0.00000160 Loss_Enh_Dec: 0.00000173\n",
      "| epoch   8 |   600/ 1249 batches | lr 0.000000 | ms/batch 353.30 | loss  2.88 | ppl    17.85 | acc     0.69 | train_ae_norm     1.00\n",
      "[8/200][699/1249] Loss_D: 1.38629425 (Loss_D_real: 0.69314545 Loss_D_fake: 0.69314879) Loss_G: 0.00000153 Loss_Enh_Dec: -0.00000034\n",
      "| epoch   8 |   700/ 1249 batches | lr 0.000000 | ms/batch 353.57 | loss  2.85 | ppl    17.26 | acc     0.68 | train_ae_norm     1.00\n",
      "[8/200][799/1249] Loss_D: 1.38628280 (Loss_D_real: 0.69313955 Loss_D_fake: 0.69314325) Loss_G: 0.00000154 Loss_Enh_Dec: -0.00000157\n",
      "| epoch   8 |   800/ 1249 batches | lr 0.000000 | ms/batch 353.20 | loss  2.87 | ppl    17.57 | acc     0.65 | train_ae_norm     1.00\n",
      "[8/200][899/1249] Loss_D: 1.38629651 (Loss_D_real: 0.69314897 Loss_D_fake: 0.69314760) Loss_G: 0.00000115 Loss_Enh_Dec: -0.00000300\n",
      "| epoch   8 |   900/ 1249 batches | lr 0.000000 | ms/batch 352.42 | loss  2.82 | ppl    16.75 | acc     0.65 | train_ae_norm     1.00\n",
      "[8/200][999/1249] Loss_D: 1.38629675 (Loss_D_real: 0.69315857 Loss_D_fake: 0.69313812) Loss_G: -0.00000145 Loss_Enh_Dec: -0.00000156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   8 |  1000/ 1249 batches | lr 0.000000 | ms/batch 353.21 | loss  2.81 | ppl    16.55 | acc     0.68 | train_ae_norm     1.00\n",
      "[8/200][1099/1249] Loss_D: 1.38629317 (Loss_D_real: 0.69313973 Loss_D_fake: 0.69315350) Loss_G: 0.00000174 Loss_Enh_Dec: 0.00000175\n",
      "| epoch   8 |  1100/ 1249 batches | lr 0.000000 | ms/batch 352.53 | loss  2.79 | ppl    16.32 | acc     0.68 | train_ae_norm     1.00\n",
      "[8/200][1199/1249] Loss_D: 1.38629639 (Loss_D_real: 0.69312990 Loss_D_fake: 0.69316649) Loss_G: 0.00000252 Loss_Enh_Dec: -0.00000541\n",
      "| epoch   8 |  1200/ 1249 batches | lr 0.000000 | ms/batch 353.47 | loss  2.75 | ppl    15.65 | acc     0.66 | train_ae_norm     1.00\n",
      "| end of epoch   8 | time: 496.08s | test loss  2.49 | test ppl 12.01 | acc 0.720\n",
      "bleu_self:  [1.         1.         1.         0.03162278 0.00398107]\n",
      "bleu_test:  [9.99999999e-01 2.23606798e-08 7.93700525e-11 8.40896415e-10\n",
      " 3.46572421e-09]\n",
      "bleu_self: [1.00000000,1.00000000,1.00000000,0.03162278,0.00398107]\n",
      "bleu_test: [1.00000000,0.00000002,0.00000000,0.00000000,0.00000000]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 9 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.715\n",
      "  Average training loss discriminator: 1.528\n",
      "  Training epcoh took: 0:00:48\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.333\n",
      "  Test Loss: 1.982\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch   9 |     0/ 1249 batches | lr 0.000000 | ms/batch 487.01 | loss  0.03 | ppl     1.03 | acc     0.70 | train_ae_norm     1.00\n",
      "[9/200][99/1249] Loss_D: 1.38629293 (Loss_D_real: 0.69311559 Loss_D_fake: 0.69317740) Loss_G: 0.00000561 Loss_Enh_Dec: -0.00000888\n",
      "| epoch   9 |   100/ 1249 batches | lr 0.000000 | ms/batch 352.32 | loss  2.78 | ppl    16.07 | acc     0.68 | train_ae_norm     1.00\n",
      "[9/200][199/1249] Loss_D: 1.38630056 (Loss_D_real: 0.69315815 Loss_D_fake: 0.69314235) Loss_G: -0.00000180 Loss_Enh_Dec: -0.00000395\n",
      "| epoch   9 |   200/ 1249 batches | lr 0.000000 | ms/batch 352.95 | loss  2.79 | ppl    16.33 | acc     0.70 | train_ae_norm     1.00\n",
      "[9/200][299/1249] Loss_D: 1.38629246 (Loss_D_real: 0.69314188 Loss_D_fake: 0.69315052) Loss_G: -0.00000045 Loss_Enh_Dec: -0.00000481\n",
      "| epoch   9 |   300/ 1249 batches | lr 0.000000 | ms/batch 351.78 | loss  2.77 | ppl    15.97 | acc     0.70 | train_ae_norm     1.00\n",
      "[9/200][399/1249] Loss_D: 1.38629460 (Loss_D_real: 0.69315737 Loss_D_fake: 0.69313717) Loss_G: -0.00000298 Loss_Enh_Dec: -0.00000190\n",
      "| epoch   9 |   400/ 1249 batches | lr 0.000000 | ms/batch 352.87 | loss  2.72 | ppl    15.14 | acc     0.72 | train_ae_norm     1.00\n",
      "[9/200][499/1249] Loss_D: 1.38630819 (Loss_D_real: 0.69314587 Loss_D_fake: 0.69316232) Loss_G: -0.00000036 Loss_Enh_Dec: 0.00000179\n",
      "| epoch   9 |   500/ 1249 batches | lr 0.000000 | ms/batch 353.54 | loss  2.73 | ppl    15.27 | acc     0.67 | train_ae_norm     1.00\n",
      "[9/200][599/1249] Loss_D: 1.38630533 (Loss_D_real: 0.69315290 Loss_D_fake: 0.69315243) Loss_G: 0.00000035 Loss_Enh_Dec: -0.00000165\n",
      "| epoch   9 |   600/ 1249 batches | lr 0.000000 | ms/batch 352.97 | loss  2.70 | ppl    14.88 | acc     0.70 | train_ae_norm     1.00\n",
      "[9/200][699/1249] Loss_D: 1.38629448 (Loss_D_real: 0.69314283 Loss_D_fake: 0.69315165) Loss_G: 0.00000020 Loss_Enh_Dec: -0.00000106\n",
      "| epoch   9 |   700/ 1249 batches | lr 0.000000 | ms/batch 352.63 | loss  2.66 | ppl    14.25 | acc     0.69 | train_ae_norm     1.00\n",
      "[9/200][799/1249] Loss_D: 1.38629651 (Loss_D_real: 0.69315803 Loss_D_fake: 0.69313848) Loss_G: -0.00000130 Loss_Enh_Dec: -0.00000232\n",
      "| epoch   9 |   800/ 1249 batches | lr 0.000000 | ms/batch 354.34 | loss  2.66 | ppl    14.29 | acc     0.65 | train_ae_norm     1.00\n",
      "[9/200][899/1249] Loss_D: 1.38629258 (Loss_D_real: 0.69315577 Loss_D_fake: 0.69313681) Loss_G: -0.00000257 Loss_Enh_Dec: -0.00000395\n",
      "| epoch   9 |   900/ 1249 batches | lr 0.000000 | ms/batch 352.57 | loss  2.60 | ppl    13.45 | acc     0.66 | train_ae_norm     1.00\n",
      "[9/200][999/1249] Loss_D: 1.38629913 (Loss_D_real: 0.69314003 Loss_D_fake: 0.69315910) Loss_G: 0.00000305 Loss_Enh_Dec: 0.00000120\n",
      "| epoch   9 |  1000/ 1249 batches | lr 0.000000 | ms/batch 353.05 | loss  2.60 | ppl    13.43 | acc     0.71 | train_ae_norm     1.00\n",
      "[9/200][1099/1249] Loss_D: 1.38629639 (Loss_D_real: 0.69314611 Loss_D_fake: 0.69315028) Loss_G: 0.00000201 Loss_Enh_Dec: 0.00000247\n",
      "| epoch   9 |  1100/ 1249 batches | lr 0.000000 | ms/batch 353.69 | loss  2.60 | ppl    13.51 | acc     0.68 | train_ae_norm     1.00\n",
      "[9/200][1199/1249] Loss_D: 1.38629043 (Loss_D_real: 0.69314909 Loss_D_fake: 0.69314134) Loss_G: -0.00000160 Loss_Enh_Dec: -0.00000182\n",
      "| epoch   9 |  1200/ 1249 batches | lr 0.000000 | ms/batch 353.07 | loss  2.55 | ppl    12.76 | acc     0.69 | train_ae_norm     1.00\n",
      "| end of epoch   9 | time: 496.01s | test loss  2.31 | test ppl 10.03 | acc 0.740\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 10 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.717\n",
      "  Average training loss discriminator: 1.332\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.330\n",
      "  Test Loss: 1.964\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  10 |     0/ 1249 batches | lr 0.000000 | ms/batch 490.83 | loss  0.03 | ppl     1.03 | acc     0.69 | train_ae_norm     1.00\n",
      "[10/200][99/1249] Loss_D: 1.38629043 (Loss_D_real: 0.69315690 Loss_D_fake: 0.69313353) Loss_G: -0.00000154 Loss_Enh_Dec: -0.00000357\n",
      "| epoch  10 |   100/ 1249 batches | lr 0.000000 | ms/batch 353.89 | loss  2.51 | ppl    12.36 | acc     0.71 | train_ae_norm     1.00\n",
      "[10/200][199/1249] Loss_D: 1.38630366 (Loss_D_real: 0.69312972 Loss_D_fake: 0.69317389) Loss_G: 0.00000217 Loss_Enh_Dec: -0.00000038\n",
      "| epoch  10 |   200/ 1249 batches | lr 0.000000 | ms/batch 353.39 | loss  2.50 | ppl    12.20 | acc     0.73 | train_ae_norm     1.00\n",
      "[10/200][299/1249] Loss_D: 1.38629508 (Loss_D_real: 0.69316244 Loss_D_fake: 0.69313258) Loss_G: -0.00000283 Loss_Enh_Dec: -0.00000575\n",
      "| epoch  10 |   300/ 1249 batches | lr 0.000000 | ms/batch 352.51 | loss  2.49 | ppl    12.10 | acc     0.70 | train_ae_norm     1.00\n",
      "[10/200][399/1249] Loss_D: 1.38629663 (Loss_D_real: 0.69314611 Loss_D_fake: 0.69315052) Loss_G: -0.00000068 Loss_Enh_Dec: 0.00000051\n",
      "| epoch  10 |   400/ 1249 batches | lr 0.000000 | ms/batch 354.12 | loss  2.48 | ppl    11.91 | acc     0.74 | train_ae_norm     1.00\n",
      "[10/200][499/1249] Loss_D: 1.38629866 (Loss_D_real: 0.69317174 Loss_D_fake: 0.69312686) Loss_G: -0.00000289 Loss_Enh_Dec: -0.00000571\n",
      "| epoch  10 |   500/ 1249 batches | lr 0.000000 | ms/batch 354.02 | loss  2.47 | ppl    11.86 | acc     0.70 | train_ae_norm     1.00\n",
      "[10/200][599/1249] Loss_D: 1.38629723 (Loss_D_real: 0.69316733 Loss_D_fake: 0.69312990) Loss_G: -0.00000445 Loss_Enh_Dec: -0.00000532\n",
      "| epoch  10 |   600/ 1249 batches | lr 0.000000 | ms/batch 354.32 | loss  2.44 | ppl    11.48 | acc     0.72 | train_ae_norm     1.00\n",
      "[10/200][699/1249] Loss_D: 1.38629913 (Loss_D_real: 0.69312966 Loss_D_fake: 0.69316953) Loss_G: 0.00000437 Loss_Enh_Dec: 0.00000461\n",
      "| epoch  10 |   700/ 1249 batches | lr 0.000000 | ms/batch 354.27 | loss  2.42 | ppl    11.28 | acc     0.70 | train_ae_norm     1.00\n",
      "[10/200][799/1249] Loss_D: 1.38629448 (Loss_D_real: 0.69315737 Loss_D_fake: 0.69313711) Loss_G: 0.00000042 Loss_Enh_Dec: -0.00000058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  10 |   800/ 1249 batches | lr 0.000000 | ms/batch 352.80 | loss  2.44 | ppl    11.43 | acc     0.71 | train_ae_norm     1.00\n",
      "[10/200][899/1249] Loss_D: 1.38629770 (Loss_D_real: 0.69315356 Loss_D_fake: 0.69314408) Loss_G: 0.00000221 Loss_Enh_Dec: -0.00000053\n",
      "| epoch  10 |   900/ 1249 batches | lr 0.000000 | ms/batch 353.56 | loss  2.40 | ppl    11.08 | acc     0.68 | train_ae_norm     1.00\n",
      "[10/200][999/1249] Loss_D: 1.38630509 (Loss_D_real: 0.69315958 Loss_D_fake: 0.69314545) Loss_G: -0.00000108 Loss_Enh_Dec: -0.00000076\n",
      "| epoch  10 |  1000/ 1249 batches | lr 0.000000 | ms/batch 354.20 | loss  2.45 | ppl    11.57 | acc     0.72 | train_ae_norm     1.00\n",
      "[10/200][1099/1249] Loss_D: 1.38630176 (Loss_D_real: 0.69314688 Loss_D_fake: 0.69315481) Loss_G: 0.00000132 Loss_Enh_Dec: 0.00000213\n",
      "| epoch  10 |  1100/ 1249 batches | lr 0.000000 | ms/batch 353.37 | loss  2.42 | ppl    11.27 | acc     0.70 | train_ae_norm     1.00\n",
      "[10/200][1199/1249] Loss_D: 1.38629174 (Loss_D_real: 0.69315743 Loss_D_fake: 0.69313431) Loss_G: -0.00000160 Loss_Enh_Dec: -0.00000380\n",
      "| epoch  10 |  1200/ 1249 batches | lr 0.000000 | ms/batch 353.65 | loss  2.36 | ppl    10.64 | acc     0.72 | train_ae_norm     1.00\n",
      "| end of epoch  10 | time: 497.26s | test loss  2.16 | test ppl  8.64 | acc 0.758\n",
      "bleu_self:  [5.99702381e-01 4.88887832e-01 2.87315619e-01 4.66560456e-05\n",
      " 2.65373617e-07]\n",
      "bleu_test:  [9.50892857e-01 5.60341887e-01 8.04607640e-02 6.35166030e-02\n",
      " 5.83807608e-05]\n",
      "bleu_self: [0.59970238,0.48888783,0.28731562,0.00004666,0.00000027]\n",
      "bleu_test: [0.95089286,0.56034189,0.08046076,0.06351660,0.00005838]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 11 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.711\n",
      "  Average training loss discriminator: 1.195\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.343\n",
      "  Test Loss: 2.015\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  11 |     0/ 1249 batches | lr 0.000000 | ms/batch 489.00 | loss  0.02 | ppl     1.02 | acc     0.72 | train_ae_norm     1.00\n",
      "[11/200][99/1249] Loss_D: 1.38629854 (Loss_D_real: 0.69313395 Loss_D_fake: 0.69316459) Loss_G: 0.00000303 Loss_Enh_Dec: 0.00000136\n",
      "| epoch  11 |   100/ 1249 batches | lr 0.000000 | ms/batch 354.06 | loss  2.35 | ppl    10.45 | acc     0.73 | train_ae_norm     1.00\n",
      "[11/200][199/1249] Loss_D: 1.38629127 (Loss_D_real: 0.69316292 Loss_D_fake: 0.69312835) Loss_G: -0.00000403 Loss_Enh_Dec: -0.00000391\n",
      "| epoch  11 |   200/ 1249 batches | lr 0.000000 | ms/batch 353.31 | loss  2.32 | ppl    10.20 | acc     0.75 | train_ae_norm     1.00\n",
      "[11/200][299/1249] Loss_D: 1.38629675 (Loss_D_real: 0.69314611 Loss_D_fake: 0.69315064) Loss_G: -0.00000082 Loss_Enh_Dec: -0.00000454\n",
      "| epoch  11 |   300/ 1249 batches | lr 0.000000 | ms/batch 353.52 | loss  2.32 | ppl    10.14 | acc     0.73 | train_ae_norm     1.00\n",
      "[11/200][399/1249] Loss_D: 1.38629985 (Loss_D_real: 0.69316041 Loss_D_fake: 0.69313943) Loss_G: -0.00000347 Loss_Enh_Dec: -0.00000468\n",
      "| epoch  11 |   400/ 1249 batches | lr 0.000000 | ms/batch 353.25 | loss  2.30 | ppl     9.96 | acc     0.75 | train_ae_norm     1.00\n",
      "[11/200][499/1249] Loss_D: 1.38628554 (Loss_D_real: 0.69312400 Loss_D_fake: 0.69316149) Loss_G: 0.00000362 Loss_Enh_Dec: 0.00000424\n",
      "| epoch  11 |   500/ 1249 batches | lr 0.000000 | ms/batch 353.60 | loss  2.27 | ppl     9.71 | acc     0.72 | train_ae_norm     1.00\n",
      "[11/200][599/1249] Loss_D: 1.38629711 (Loss_D_real: 0.69315118 Loss_D_fake: 0.69314593) Loss_G: -0.00000114 Loss_Enh_Dec: -0.00000176\n",
      "| epoch  11 |   600/ 1249 batches | lr 0.000000 | ms/batch 353.66 | loss  2.25 | ppl     9.46 | acc     0.76 | train_ae_norm     1.00\n",
      "[11/200][699/1249] Loss_D: 1.38629723 (Loss_D_real: 0.69312382 Loss_D_fake: 0.69317335) Loss_G: 0.00000559 Loss_Enh_Dec: 0.00000537\n",
      "| epoch  11 |   700/ 1249 batches | lr 0.000000 | ms/batch 352.46 | loss  2.23 | ppl     9.26 | acc     0.73 | train_ae_norm     1.00\n",
      "[11/200][799/1249] Loss_D: 1.38629174 (Loss_D_real: 0.69319242 Loss_D_fake: 0.69309938) Loss_G: -0.00000761 Loss_Enh_Dec: -0.00000950\n",
      "| epoch  11 |   800/ 1249 batches | lr 0.000000 | ms/batch 353.19 | loss  2.23 | ppl     9.28 | acc     0.72 | train_ae_norm     1.00\n",
      "[11/200][899/1249] Loss_D: 1.38629699 (Loss_D_real: 0.69312525 Loss_D_fake: 0.69317174) Loss_G: 0.00000385 Loss_Enh_Dec: 0.00000321\n",
      "| epoch  11 |   900/ 1249 batches | lr 0.000000 | ms/batch 353.25 | loss  2.19 | ppl     8.93 | acc     0.71 | train_ae_norm     1.00\n",
      "[11/200][999/1249] Loss_D: 1.38629556 (Loss_D_real: 0.69317961 Loss_D_fake: 0.69311595) Loss_G: -0.00000467 Loss_Enh_Dec: -0.00000550\n",
      "| epoch  11 |  1000/ 1249 batches | lr 0.000000 | ms/batch 353.97 | loss  2.23 | ppl     9.32 | acc     0.73 | train_ae_norm     1.00\n",
      "[11/200][1099/1249] Loss_D: 1.38628840 (Loss_D_real: 0.69311583 Loss_D_fake: 0.69317257) Loss_G: 0.00000507 Loss_Enh_Dec: 0.00000482\n",
      "| epoch  11 |  1100/ 1249 batches | lr 0.000000 | ms/batch 352.48 | loss  2.23 | ppl     9.26 | acc     0.72 | train_ae_norm     1.00\n",
      "[11/200][1199/1249] Loss_D: 1.38629365 (Loss_D_real: 0.69314945 Loss_D_fake: 0.69314426) Loss_G: 0.00000041 Loss_Enh_Dec: -0.00000251\n",
      "| epoch  11 |  1200/ 1249 batches | lr 0.000000 | ms/batch 352.50 | loss  2.20 | ppl     8.98 | acc     0.71 | train_ae_norm     1.00\n",
      "| end of epoch  11 | time: 496.52s | test loss  2.00 | test ppl  7.41 | acc 0.776\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 12 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.712\n",
      "  Average training loss discriminator: 1.074\n",
      "  Training epcoh took: 0:00:48\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.343\n",
      "  Test Loss: 2.057\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  12 |     0/ 1249 batches | lr 0.000000 | ms/batch 487.29 | loss  0.02 | ppl     1.02 | acc     0.73 | train_ae_norm     1.00\n",
      "[12/200][99/1249] Loss_D: 1.38628948 (Loss_D_real: 0.69322920 Loss_D_fake: 0.69306028) Loss_G: -0.00001471 Loss_Enh_Dec: -0.00001765\n",
      "| epoch  12 |   100/ 1249 batches | lr 0.000000 | ms/batch 352.80 | loss  2.17 | ppl     8.77 | acc     0.73 | train_ae_norm     1.00\n",
      "[12/200][199/1249] Loss_D: 1.38629413 (Loss_D_real: 0.69313955 Loss_D_fake: 0.69315451) Loss_G: 0.00000133 Loss_Enh_Dec: 0.00000485\n",
      "| epoch  12 |   200/ 1249 batches | lr 0.000000 | ms/batch 353.34 | loss  2.17 | ppl     8.77 | acc     0.75 | train_ae_norm     1.00\n",
      "[12/200][299/1249] Loss_D: 1.38629675 (Loss_D_real: 0.69306940 Loss_D_fake: 0.69322741) Loss_G: 0.00001834 Loss_Enh_Dec: 0.00001630\n",
      "| epoch  12 |   300/ 1249 batches | lr 0.000000 | ms/batch 353.40 | loss  2.19 | ppl     8.91 | acc     0.73 | train_ae_norm     1.00\n",
      "[12/200][399/1249] Loss_D: 1.38629770 (Loss_D_real: 0.69322085 Loss_D_fake: 0.69307685) Loss_G: -0.00001622 Loss_Enh_Dec: -0.00001519\n",
      "| epoch  12 |   400/ 1249 batches | lr 0.000000 | ms/batch 352.67 | loss  2.12 | ppl     8.32 | acc     0.76 | train_ae_norm     1.00\n",
      "[12/200][499/1249] Loss_D: 1.38629389 (Loss_D_real: 0.69318295 Loss_D_fake: 0.69311088) Loss_G: -0.00000760 Loss_Enh_Dec: -0.00000914\n",
      "| epoch  12 |   500/ 1249 batches | lr 0.000000 | ms/batch 352.49 | loss  2.12 | ppl     8.36 | acc     0.74 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/200][599/1249] Loss_D: 1.38629889 (Loss_D_real: 0.69315815 Loss_D_fake: 0.69314069) Loss_G: 0.00000149 Loss_Enh_Dec: -0.00000282\n",
      "| epoch  12 |   600/ 1249 batches | lr 0.000000 | ms/batch 352.89 | loss  2.11 | ppl     8.25 | acc     0.76 | train_ae_norm     1.00\n",
      "[12/200][699/1249] Loss_D: 1.38629246 (Loss_D_real: 0.69312155 Loss_D_fake: 0.69317096) Loss_G: 0.00000595 Loss_Enh_Dec: 0.00000345\n",
      "| epoch  12 |   700/ 1249 batches | lr 0.000000 | ms/batch 352.64 | loss  2.09 | ppl     8.07 | acc     0.73 | train_ae_norm     1.00\n",
      "[12/200][799/1249] Loss_D: 1.38629842 (Loss_D_real: 0.69316685 Loss_D_fake: 0.69313163) Loss_G: -0.00000271 Loss_Enh_Dec: -0.00000454\n",
      "| epoch  12 |   800/ 1249 batches | lr 0.000000 | ms/batch 352.42 | loss  2.07 | ppl     7.95 | acc     0.73 | train_ae_norm     1.00\n",
      "[12/200][899/1249] Loss_D: 1.38629150 (Loss_D_real: 0.69311738 Loss_D_fake: 0.69317406) Loss_G: 0.00000494 Loss_Enh_Dec: 0.00000510\n",
      "| epoch  12 |   900/ 1249 batches | lr 0.000000 | ms/batch 352.95 | loss  2.03 | ppl     7.61 | acc     0.73 | train_ae_norm     1.00\n",
      "[12/200][999/1249] Loss_D: 1.38629341 (Loss_D_real: 0.69313109 Loss_D_fake: 0.69316238) Loss_G: 0.00000452 Loss_Enh_Dec: 0.00000498\n",
      "| epoch  12 |  1000/ 1249 batches | lr 0.000000 | ms/batch 353.26 | loss  2.03 | ppl     7.61 | acc     0.75 | train_ae_norm     1.00\n",
      "[12/200][1099/1249] Loss_D: 1.38628650 (Loss_D_real: 0.69303083 Loss_D_fake: 0.69325566) Loss_G: 0.00002362 Loss_Enh_Dec: 0.00002278\n",
      "| epoch  12 |  1100/ 1249 batches | lr 0.000000 | ms/batch 352.94 | loss  2.05 | ppl     7.77 | acc     0.74 | train_ae_norm     1.00\n",
      "[12/200][1199/1249] Loss_D: 1.38630390 (Loss_D_real: 0.69306225 Loss_D_fake: 0.69324160) Loss_G: 0.00001717 Loss_Enh_Dec: 0.00001478\n",
      "| epoch  12 |  1200/ 1249 batches | lr 0.000000 | ms/batch 353.41 | loss  2.01 | ppl     7.44 | acc     0.73 | train_ae_norm     1.00\n",
      "| end of epoch  12 | time: 496.01s | test loss  1.88 | test ppl  6.52 | acc 0.792\n",
      "bleu_self:  [0.95113636 0.93469085 0.91464557 0.88839771 0.85482602]\n",
      "bleu_test:  [7.56818182e-01 5.70372092e-01 1.95625468e-01 2.76566529e-05\n",
      " 1.39267460e-07]\n",
      "bleu_self: [0.95113636,0.93469085,0.91464557,0.88839771,0.85482602]\n",
      "bleu_test: [0.75681818,0.57037209,0.19562547,0.00002766,0.00000014]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 13 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.717\n",
      "  Average training loss discriminator: 0.984\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.372\n",
      "  Test Loss: 2.075\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  13 |     0/ 1249 batches | lr 0.000000 | ms/batch 490.29 | loss  0.02 | ppl     1.02 | acc     0.77 | train_ae_norm     1.00\n",
      "[13/200][99/1249] Loss_D: 1.38629866 (Loss_D_real: 0.69320619 Loss_D_fake: 0.69309253) Loss_G: -0.00001517 Loss_Enh_Dec: -0.00001758\n",
      "| epoch  13 |   100/ 1249 batches | lr 0.000000 | ms/batch 353.27 | loss  2.07 | ppl     7.96 | acc     0.74 | train_ae_norm     1.00\n",
      "[13/200][199/1249] Loss_D: 1.38631034 (Loss_D_real: 0.69316739 Loss_D_fake: 0.69314289) Loss_G: -0.00000144 Loss_Enh_Dec: 0.00000008\n",
      "| epoch  13 |   200/ 1249 batches | lr 0.000000 | ms/batch 352.69 | loss  2.13 | ppl     8.44 | acc     0.78 | train_ae_norm     1.00\n",
      "[13/200][299/1249] Loss_D: 1.38630438 (Loss_D_real: 0.69315523 Loss_D_fake: 0.69314909) Loss_G: 0.00000298 Loss_Enh_Dec: 0.00000224\n",
      "| epoch  13 |   300/ 1249 batches | lr 0.000000 | ms/batch 352.89 | loss  2.07 | ppl     7.94 | acc     0.74 | train_ae_norm     1.00\n",
      "[13/200][399/1249] Loss_D: 1.38629556 (Loss_D_real: 0.69313902 Loss_D_fake: 0.69315648) Loss_G: 0.00000071 Loss_Enh_Dec: -0.00000071\n",
      "| epoch  13 |   400/ 1249 batches | lr 0.000000 | ms/batch 352.80 | loss  2.02 | ppl     7.57 | acc     0.76 | train_ae_norm     1.00\n",
      "[13/200][499/1249] Loss_D: 1.38629794 (Loss_D_real: 0.69307649 Loss_D_fake: 0.69322145) Loss_G: 0.00000838 Loss_Enh_Dec: 0.00001026\n",
      "| epoch  13 |   500/ 1249 batches | lr 0.000000 | ms/batch 352.63 | loss  2.02 | ppl     7.53 | acc     0.75 | train_ae_norm     1.00\n",
      "[13/200][599/1249] Loss_D: 1.38628793 (Loss_D_real: 0.69312447 Loss_D_fake: 0.69316351) Loss_G: 0.00000366 Loss_Enh_Dec: 0.00000484\n",
      "| epoch  13 |   600/ 1249 batches | lr 0.000000 | ms/batch 352.34 | loss  2.01 | ppl     7.46 | acc     0.78 | train_ae_norm     1.00\n",
      "[13/200][699/1249] Loss_D: 1.38629222 (Loss_D_real: 0.69309127 Loss_D_fake: 0.69320095) Loss_G: 0.00000893 Loss_Enh_Dec: 0.00000816\n",
      "| epoch  13 |   700/ 1249 batches | lr 0.000000 | ms/batch 352.23 | loss  1.98 | ppl     7.26 | acc     0.73 | train_ae_norm     1.00\n",
      "[13/200][799/1249] Loss_D: 1.38629270 (Loss_D_real: 0.69313085 Loss_D_fake: 0.69316185) Loss_G: 0.00000517 Loss_Enh_Dec: 0.00000367\n",
      "| epoch  13 |   800/ 1249 batches | lr 0.000000 | ms/batch 352.28 | loss  1.99 | ppl     7.35 | acc     0.74 | train_ae_norm     1.00\n",
      "[13/200][899/1249] Loss_D: 1.38629711 (Loss_D_real: 0.69324148 Loss_D_fake: 0.69305563) Loss_G: -0.00001653 Loss_Enh_Dec: -0.00001788\n",
      "| epoch  13 |   900/ 1249 batches | lr 0.000000 | ms/batch 353.66 | loss  1.95 | ppl     7.06 | acc     0.73 | train_ae_norm     1.00\n",
      "[13/200][999/1249] Loss_D: 1.38629544 (Loss_D_real: 0.69313401 Loss_D_fake: 0.69316143) Loss_G: 0.00000343 Loss_Enh_Dec: 0.00000399\n",
      "| epoch  13 |  1000/ 1249 batches | lr 0.000000 | ms/batch 352.10 | loss  1.94 | ppl     6.95 | acc     0.76 | train_ae_norm     1.00\n",
      "[13/200][1099/1249] Loss_D: 1.38629496 (Loss_D_real: 0.69307101 Loss_D_fake: 0.69322395) Loss_G: 0.00001501 Loss_Enh_Dec: 0.00001519\n",
      "| epoch  13 |  1100/ 1249 batches | lr 0.000000 | ms/batch 351.87 | loss  1.95 | ppl     7.04 | acc     0.76 | train_ae_norm     1.00\n",
      "[13/200][1199/1249] Loss_D: 1.38629496 (Loss_D_real: 0.69313788 Loss_D_fake: 0.69315708) Loss_G: 0.00000106 Loss_Enh_Dec: 0.00000264\n",
      "| epoch  13 |  1200/ 1249 batches | lr 0.000000 | ms/batch 353.50 | loss  1.91 | ppl     6.73 | acc     0.75 | train_ae_norm     1.00\n",
      "| end of epoch  13 | time: 495.97s | test loss  1.78 | test ppl  5.93 | acc 0.803\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 14 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.710\n",
      "  Average training loss discriminator: 0.942\n",
      "  Training epcoh took: 0:00:48\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.362\n",
      "  Test Loss: 2.178\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  14 |     0/ 1249 batches | lr 0.000000 | ms/batch 487.45 | loss  0.02 | ppl     1.02 | acc     0.75 | train_ae_norm     1.00\n",
      "[14/200][99/1249] Loss_D: 1.38629186 (Loss_D_real: 0.69318551 Loss_D_fake: 0.69310635) Loss_G: -0.00000464 Loss_Enh_Dec: -0.00000571\n",
      "| epoch  14 |   100/ 1249 batches | lr 0.000000 | ms/batch 352.50 | loss  1.88 | ppl     6.54 | acc     0.77 | train_ae_norm     1.00\n",
      "[14/200][199/1249] Loss_D: 1.38629591 (Loss_D_real: 0.69313610 Loss_D_fake: 0.69315982) Loss_G: 0.00000300 Loss_Enh_Dec: 0.00000323\n",
      "| epoch  14 |   200/ 1249 batches | lr 0.000000 | ms/batch 352.76 | loss  1.88 | ppl     6.52 | acc     0.79 | train_ae_norm     1.00\n",
      "[14/200][299/1249] Loss_D: 1.38629878 (Loss_D_real: 0.69315141 Loss_D_fake: 0.69314736) Loss_G: -0.00000077 Loss_Enh_Dec: -0.00000092\n",
      "| epoch  14 |   300/ 1249 batches | lr 0.000000 | ms/batch 352.80 | loss  1.86 | ppl     6.45 | acc     0.75 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/200][399/1249] Loss_D: 1.38629365 (Loss_D_real: 0.69320267 Loss_D_fake: 0.69309092) Loss_G: -0.00000951 Loss_Enh_Dec: -0.00000850\n",
      "| epoch  14 |   400/ 1249 batches | lr 0.000000 | ms/batch 351.74 | loss  1.83 | ppl     6.25 | acc     0.79 | train_ae_norm     1.00\n",
      "[14/200][499/1249] Loss_D: 1.38629293 (Loss_D_real: 0.69311297 Loss_D_fake: 0.69317997) Loss_G: 0.00000762 Loss_Enh_Dec: 0.00000727\n",
      "| epoch  14 |   500/ 1249 batches | lr 0.000000 | ms/batch 351.53 | loss  1.85 | ppl     6.38 | acc     0.77 | train_ae_norm     1.00\n",
      "[14/200][599/1249] Loss_D: 1.38629687 (Loss_D_real: 0.69314760 Loss_D_fake: 0.69314927) Loss_G: 0.00000063 Loss_Enh_Dec: -0.00000137\n",
      "| epoch  14 |   600/ 1249 batches | lr 0.000000 | ms/batch 352.35 | loss  1.81 | ppl     6.11 | acc     0.79 | train_ae_norm     1.00\n",
      "[14/200][699/1249] Loss_D: 1.38629651 (Loss_D_real: 0.69311351 Loss_D_fake: 0.69318295) Loss_G: 0.00000667 Loss_Enh_Dec: 0.00000573\n",
      "| epoch  14 |   700/ 1249 batches | lr 0.000000 | ms/batch 353.03 | loss  1.80 | ppl     6.07 | acc     0.77 | train_ae_norm     1.00\n",
      "[14/200][799/1249] Loss_D: 1.38629341 (Loss_D_real: 0.69313592 Loss_D_fake: 0.69315743) Loss_G: 0.00000240 Loss_Enh_Dec: 0.00000295\n",
      "| epoch  14 |   800/ 1249 batches | lr 0.000000 | ms/batch 352.97 | loss  1.81 | ppl     6.10 | acc     0.76 | train_ae_norm     1.00\n",
      "[14/200][899/1249] Loss_D: 1.38629711 (Loss_D_real: 0.69315547 Loss_D_fake: 0.69314164) Loss_G: -0.00000248 Loss_Enh_Dec: -0.00000363\n",
      "| epoch  14 |   900/ 1249 batches | lr 0.000000 | ms/batch 353.02 | loss  1.76 | ppl     5.82 | acc     0.77 | train_ae_norm     1.00\n",
      "[14/200][999/1249] Loss_D: 1.38628745 (Loss_D_real: 0.69312811 Loss_D_fake: 0.69315934) Loss_G: 0.00000501 Loss_Enh_Dec: 0.00000172\n",
      "| epoch  14 |  1000/ 1249 batches | lr 0.000000 | ms/batch 352.63 | loss  1.75 | ppl     5.78 | acc     0.79 | train_ae_norm     1.00\n",
      "[14/200][1099/1249] Loss_D: 1.38630009 (Loss_D_real: 0.69310772 Loss_D_fake: 0.69319236) Loss_G: 0.00001054 Loss_Enh_Dec: 0.00000953\n",
      "| epoch  14 |  1100/ 1249 batches | lr 0.000000 | ms/batch 352.25 | loss  1.79 | ppl     5.98 | acc     0.78 | train_ae_norm     1.00\n",
      "[14/200][1199/1249] Loss_D: 1.38629889 (Loss_D_real: 0.69317377 Loss_D_fake: 0.69312513) Loss_G: -0.00000391 Loss_Enh_Dec: -0.00000525\n",
      "| epoch  14 |  1200/ 1249 batches | lr 0.000000 | ms/batch 352.55 | loss  1.73 | ppl     5.64 | acc     0.77 | train_ae_norm     1.00\n",
      "| end of epoch  14 | time: 495.56s | test loss  1.67 | test ppl  5.34 | acc 0.816\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 15 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.716\n",
      "  Average training loss discriminator: 0.893\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.395\n",
      "  Test Loss: 2.164\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  15 |     0/ 1249 batches | lr 0.000000 | ms/batch 488.70 | loss  0.02 | ppl     1.02 | acc     0.77 | train_ae_norm     1.00\n",
      "[15/200][99/1249] Loss_D: 1.38629103 (Loss_D_real: 0.69311368 Loss_D_fake: 0.69317740) Loss_G: 0.00000744 Loss_Enh_Dec: 0.00000837\n",
      "| epoch  15 |   100/ 1249 batches | lr 0.000000 | ms/batch 353.56 | loss  1.74 | ppl     5.67 | acc     0.78 | train_ae_norm     1.00\n",
      "[15/200][199/1249] Loss_D: 1.38629806 (Loss_D_real: 0.69312191 Loss_D_fake: 0.69317615) Loss_G: 0.00000501 Loss_Enh_Dec: 0.00000610\n",
      "| epoch  15 |   200/ 1249 batches | lr 0.000000 | ms/batch 353.34 | loss  1.72 | ppl     5.61 | acc     0.80 | train_ae_norm     1.00\n",
      "[15/200][299/1249] Loss_D: 1.38629699 (Loss_D_real: 0.69318354 Loss_D_fake: 0.69311351) Loss_G: -0.00000652 Loss_Enh_Dec: -0.00000625\n",
      "| epoch  15 |   300/ 1249 batches | lr 0.000000 | ms/batch 354.89 | loss  1.72 | ppl     5.58 | acc     0.78 | train_ae_norm     1.00\n",
      "[15/200][399/1249] Loss_D: 1.38629198 (Loss_D_real: 0.69316101 Loss_D_fake: 0.69313097) Loss_G: -0.00000568 Loss_Enh_Dec: -0.00000652\n",
      "| epoch  15 |   400/ 1249 batches | lr 0.000000 | ms/batch 353.36 | loss  1.68 | ppl     5.37 | acc     0.82 | train_ae_norm     1.00\n",
      "[15/200][499/1249] Loss_D: 1.38629174 (Loss_D_real: 0.69312942 Loss_D_fake: 0.69316232) Loss_G: 0.00000455 Loss_Enh_Dec: 0.00000216\n",
      "| epoch  15 |   500/ 1249 batches | lr 0.000000 | ms/batch 353.47 | loss  1.69 | ppl     5.44 | acc     0.78 | train_ae_norm     1.00\n",
      "[15/200][599/1249] Loss_D: 1.38629639 (Loss_D_real: 0.69309950 Loss_D_fake: 0.69319689) Loss_G: 0.00001031 Loss_Enh_Dec: 0.00000835\n",
      "| epoch  15 |   600/ 1249 batches | lr 0.000000 | ms/batch 353.05 | loss  1.68 | ppl     5.34 | acc     0.82 | train_ae_norm     1.00\n",
      "[15/200][699/1249] Loss_D: 1.38629222 (Loss_D_real: 0.69316506 Loss_D_fake: 0.69312716) Loss_G: -0.00000159 Loss_Enh_Dec: -0.00000301\n",
      "| epoch  15 |   700/ 1249 batches | lr 0.000000 | ms/batch 352.43 | loss  1.65 | ppl     5.22 | acc     0.79 | train_ae_norm     1.00\n",
      "[15/200][799/1249] Loss_D: 1.38629723 (Loss_D_real: 0.69311881 Loss_D_fake: 0.69317836) Loss_G: 0.00000535 Loss_Enh_Dec: 0.00000398\n",
      "| epoch  15 |   800/ 1249 batches | lr 0.000000 | ms/batch 351.91 | loss  1.66 | ppl     5.28 | acc     0.77 | train_ae_norm     1.00\n",
      "[15/200][899/1249] Loss_D: 1.38629282 (Loss_D_real: 0.69315481 Loss_D_fake: 0.69313800) Loss_G: -0.00000207 Loss_Enh_Dec: -0.00000279\n",
      "| epoch  15 |   900/ 1249 batches | lr 0.000000 | ms/batch 352.67 | loss  1.62 | ppl     5.03 | acc     0.79 | train_ae_norm     1.00\n",
      "[15/200][999/1249] Loss_D: 1.38630342 (Loss_D_real: 0.69320107 Loss_D_fake: 0.69310230) Loss_G: -0.00000878 Loss_Enh_Dec: -0.00000898\n",
      "| epoch  15 |  1000/ 1249 batches | lr 0.000000 | ms/batch 352.43 | loss  1.63 | ppl     5.11 | acc     0.80 | train_ae_norm     1.00\n",
      "[15/200][1099/1249] Loss_D: 1.38630056 (Loss_D_real: 0.69313139 Loss_D_fake: 0.69316912) Loss_G: 0.00000380 Loss_Enh_Dec: 0.00000387\n",
      "| epoch  15 |  1100/ 1249 batches | lr 0.000000 | ms/batch 353.19 | loss  1.66 | ppl     5.25 | acc     0.80 | train_ae_norm     1.00\n",
      "[15/200][1199/1249] Loss_D: 1.38629413 (Loss_D_real: 0.69313592 Loss_D_fake: 0.69315827) Loss_G: 0.00000318 Loss_Enh_Dec: 0.00000148\n",
      "| epoch  15 |  1200/ 1249 batches | lr 0.000000 | ms/batch 354.02 | loss  1.60 | ppl     4.98 | acc     0.81 | train_ae_norm     1.00\n",
      "| end of epoch  15 | time: 496.43s | test loss  1.56 | test ppl  4.78 | acc 0.829\n",
      "bleu_self:  [0.931446   0.90193499 0.87124074 0.83126256 0.76332837]\n",
      "bleu_test:  [5.89772727e-01 3.98112215e-01 2.94141706e-01 5.06015837e-02\n",
      " 4.25168069e-05]\n",
      "bleu_self: [0.93144600,0.90193499,0.87124074,0.83126256,0.76332837]\n",
      "bleu_test: [0.58977273,0.39811221,0.29414171,0.05060158,0.00004252]\n",
      "New saving model: epoch 015.\n",
      "Saving models to ./results/yahoo_merge_results\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 16 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.715\n",
      "  Average training loss discriminator: 0.857\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.393\n",
      "  Test Loss: 2.258\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  16 |     0/ 1249 batches | lr 0.000000 | ms/batch 491.36 | loss  0.02 | ppl     1.02 | acc     0.81 | train_ae_norm     1.00\n",
      "[16/200][99/1249] Loss_D: 1.38629472 (Loss_D_real: 0.69317603 Loss_D_fake: 0.69311869) Loss_G: -0.00000510 Loss_Enh_Dec: -0.00000636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  16 |   100/ 1249 batches | lr 0.000000 | ms/batch 353.24 | loss  1.58 | ppl     4.85 | acc     0.80 | train_ae_norm     1.00\n",
      "[16/200][199/1249] Loss_D: 1.38629651 (Loss_D_real: 0.69320887 Loss_D_fake: 0.69308770) Loss_G: -0.00001112 Loss_Enh_Dec: -0.00001216\n",
      "| epoch  16 |   200/ 1249 batches | lr 0.000000 | ms/batch 354.65 | loss  1.56 | ppl     4.76 | acc     0.80 | train_ae_norm     1.00\n",
      "[16/200][299/1249] Loss_D: 1.38629544 (Loss_D_real: 0.69316351 Loss_D_fake: 0.69313192) Loss_G: -0.00000136 Loss_Enh_Dec: -0.00000417\n",
      "| epoch  16 |   300/ 1249 batches | lr 0.000000 | ms/batch 354.33 | loss  1.57 | ppl     4.79 | acc     0.80 | train_ae_norm     1.00\n",
      "[16/200][399/1249] Loss_D: 1.38628507 (Loss_D_real: 0.69307268 Loss_D_fake: 0.69321245) Loss_G: 0.00001538 Loss_Enh_Dec: 0.00000760\n",
      "| epoch  16 |   400/ 1249 batches | lr 0.000000 | ms/batch 353.81 | loss  1.59 | ppl     4.91 | acc     0.79 | train_ae_norm     1.00\n",
      "[16/200][499/1249] Loss_D: 1.38630366 (Loss_D_real: 0.69314831 Loss_D_fake: 0.69315529) Loss_G: -0.00000162 Loss_Enh_Dec: -0.00000913\n",
      "| epoch  16 |   500/ 1249 batches | lr 0.000000 | ms/batch 353.22 | loss  1.63 | ppl     5.10 | acc     0.77 | train_ae_norm     1.00\n",
      "[16/200][599/1249] Loss_D: 1.38629031 (Loss_D_real: 0.69300342 Loss_D_fake: 0.69328684) Loss_G: 0.00003106 Loss_Enh_Dec: 0.00002721\n",
      "| epoch  16 |   600/ 1249 batches | lr 0.000000 | ms/batch 353.54 | loss  1.59 | ppl     4.92 | acc     0.82 | train_ae_norm     1.00\n",
      "[16/200][699/1249] Loss_D: 1.38629222 (Loss_D_real: 0.69308800 Loss_D_fake: 0.69320428) Loss_G: 0.00000908 Loss_Enh_Dec: 0.00000699\n",
      "| epoch  16 |   700/ 1249 batches | lr 0.000000 | ms/batch 353.99 | loss  1.56 | ppl     4.78 | acc     0.76 | train_ae_norm     1.00\n",
      "[16/200][799/1249] Loss_D: 1.38629103 (Loss_D_real: 0.69309115 Loss_D_fake: 0.69319981) Loss_G: 0.00000812 Loss_Enh_Dec: 0.00000835\n",
      "| epoch  16 |   800/ 1249 batches | lr 0.000000 | ms/batch 353.44 | loss  1.57 | ppl     4.81 | acc     0.79 | train_ae_norm     1.00\n",
      "[16/200][899/1249] Loss_D: 1.38629293 (Loss_D_real: 0.69302416 Loss_D_fake: 0.69326878) Loss_G: 0.00002100 Loss_Enh_Dec: 0.00001978\n",
      "| epoch  16 |   900/ 1249 batches | lr 0.000000 | ms/batch 353.23 | loss  1.55 | ppl     4.73 | acc     0.78 | train_ae_norm     1.00\n",
      "[16/200][999/1249] Loss_D: 1.38630021 (Loss_D_real: 0.69317693 Loss_D_fake: 0.69312328) Loss_G: -0.00000472 Loss_Enh_Dec: -0.00000338\n",
      "| epoch  16 |  1000/ 1249 batches | lr 0.000000 | ms/batch 354.01 | loss  1.58 | ppl     4.84 | acc     0.83 | train_ae_norm     1.00\n",
      "[16/200][1099/1249] Loss_D: 1.38629580 (Loss_D_real: 0.69316494 Loss_D_fake: 0.69313085) Loss_G: -0.00000222 Loss_Enh_Dec: -0.00000505\n",
      "| epoch  16 |  1100/ 1249 batches | lr 0.000000 | ms/batch 353.95 | loss  1.59 | ppl     4.91 | acc     0.81 | train_ae_norm     1.00\n",
      "[16/200][1199/1249] Loss_D: 1.38630271 (Loss_D_real: 0.69312376 Loss_D_fake: 0.69317889) Loss_G: 0.00000560 Loss_Enh_Dec: 0.00000489\n",
      "| epoch  16 |  1200/ 1249 batches | lr 0.000000 | ms/batch 353.43 | loss  1.54 | ppl     4.64 | acc     0.79 | train_ae_norm     1.00\n",
      "| end of epoch  16 | time: 497.47s | test loss  1.51 | test ppl  4.52 | acc 0.836\n",
      "bleu_self:  [8.33484929e-01 6.67491276e-01 1.37653879e-01 5.24710923e-04\n",
      " 1.26499239e-04]\n",
      "bleu_test:  [9.37499999e-01 7.16631353e-01 5.01408482e-03 5.03976239e-04\n",
      " 1.27602937e-04]\n",
      "bleu_self: [0.83348493,0.66749128,0.13765388,0.00052471,0.00012650]\n",
      "bleu_test: [0.93750000,0.71663135,0.00501408,0.00050398,0.00012760]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 17 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.709\n",
      "  Average training loss discriminator: 0.847\n",
      "  Training epcoh took: 0:00:48\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.390\n",
      "  Test Loss: 2.325\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  17 |     0/ 1249 batches | lr 0.000000 | ms/batch 487.55 | loss  0.02 | ppl     1.02 | acc     0.81 | train_ae_norm     1.00\n",
      "[17/200][99/1249] Loss_D: 1.38628662 (Loss_D_real: 0.69313115 Loss_D_fake: 0.69315547) Loss_G: 0.00000488 Loss_Enh_Dec: 0.00000394\n",
      "| epoch  17 |   100/ 1249 batches | lr 0.000000 | ms/batch 353.09 | loss  1.53 | ppl     4.63 | acc     0.80 | train_ae_norm     1.00\n",
      "[17/200][199/1249] Loss_D: 1.38628674 (Loss_D_real: 0.69312704 Loss_D_fake: 0.69315976) Loss_G: 0.00000089 Loss_Enh_Dec: 0.00000084\n",
      "| epoch  17 |   200/ 1249 batches | lr 0.000000 | ms/batch 353.51 | loss  1.52 | ppl     4.57 | acc     0.81 | train_ae_norm     1.00\n",
      "[17/200][299/1249] Loss_D: 1.38628983 (Loss_D_real: 0.69310999 Loss_D_fake: 0.69317985) Loss_G: 0.00000804 Loss_Enh_Dec: 0.00000216\n",
      "| epoch  17 |   300/ 1249 batches | lr 0.000000 | ms/batch 354.37 | loss  1.52 | ppl     4.55 | acc     0.80 | train_ae_norm     1.00\n",
      "[17/200][399/1249] Loss_D: 1.38629198 (Loss_D_real: 0.69312799 Loss_D_fake: 0.69316399) Loss_G: 0.00000516 Loss_Enh_Dec: 0.00000317\n",
      "| epoch  17 |   400/ 1249 batches | lr 0.000000 | ms/batch 353.25 | loss  1.52 | ppl     4.58 | acc     0.82 | train_ae_norm     1.00\n",
      "[17/200][499/1249] Loss_D: 1.38631260 (Loss_D_real: 0.69309253 Loss_D_fake: 0.69322008) Loss_G: 0.00001447 Loss_Enh_Dec: 0.00000689\n",
      "| epoch  17 |   500/ 1249 batches | lr 0.000000 | ms/batch 353.65 | loss  1.51 | ppl     4.52 | acc     0.79 | train_ae_norm     1.00\n",
      "[17/200][599/1249] Loss_D: 1.38629675 (Loss_D_real: 0.69314039 Loss_D_fake: 0.69315642) Loss_G: 0.00000058 Loss_Enh_Dec: 0.00000110\n",
      "| epoch  17 |   600/ 1249 batches | lr 0.000000 | ms/batch 353.51 | loss  1.50 | ppl     4.49 | acc     0.82 | train_ae_norm     1.00\n",
      "[17/200][699/1249] Loss_D: 1.38629460 (Loss_D_real: 0.69324541 Loss_D_fake: 0.69304913) Loss_G: -0.00001827 Loss_Enh_Dec: -0.00001954\n",
      "| epoch  17 |   700/ 1249 batches | lr 0.000000 | ms/batch 354.11 | loss  1.47 | ppl     4.34 | acc     0.78 | train_ae_norm     1.00\n",
      "[17/200][799/1249] Loss_D: 1.38629651 (Loss_D_real: 0.69321674 Loss_D_fake: 0.69307983) Loss_G: -0.00001375 Loss_Enh_Dec: -0.00001418\n",
      "| epoch  17 |   800/ 1249 batches | lr 0.000000 | ms/batch 355.31 | loss  1.44 | ppl     4.22 | acc     0.80 | train_ae_norm     1.00\n",
      "[17/200][899/1249] Loss_D: 1.38629842 (Loss_D_real: 0.69317865 Loss_D_fake: 0.69311976) Loss_G: -0.00000608 Loss_Enh_Dec: -0.00000803\n",
      "| epoch  17 |   900/ 1249 batches | lr 0.000000 | ms/batch 354.45 | loss  1.40 | ppl     4.07 | acc     0.81 | train_ae_norm     1.00\n",
      "[17/200][999/1249] Loss_D: 1.38629842 (Loss_D_real: 0.69307613 Loss_D_fake: 0.69322222) Loss_G: 0.00001384 Loss_Enh_Dec: 0.00001385\n",
      "| epoch  17 |  1000/ 1249 batches | lr 0.000000 | ms/batch 354.32 | loss  1.40 | ppl     4.04 | acc     0.83 | train_ae_norm     1.00\n",
      "[17/200][1099/1249] Loss_D: 1.38629174 (Loss_D_real: 0.69312525 Loss_D_fake: 0.69316643) Loss_G: 0.00000475 Loss_Enh_Dec: 0.00000436\n",
      "| epoch  17 |  1100/ 1249 batches | lr 0.000000 | ms/batch 353.68 | loss  1.41 | ppl     4.11 | acc     0.84 | train_ae_norm     1.00\n",
      "[17/200][1199/1249] Loss_D: 1.38629603 (Loss_D_real: 0.69309467 Loss_D_fake: 0.69320142) Loss_G: 0.00001019 Loss_Enh_Dec: 0.00000693\n",
      "| epoch  17 |  1200/ 1249 batches | lr 0.000000 | ms/batch 354.14 | loss  1.36 | ppl     3.90 | acc     0.83 | train_ae_norm     1.00\n",
      "| end of epoch  17 | time: 497.28s | test loss  1.41 | test ppl  4.10 | acc 0.848\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 18 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.713\n",
      "  Average training loss discriminator: 0.822\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.385\n",
      "  Test Loss: 2.388\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  18 |     0/ 1249 batches | lr 0.000000 | ms/batch 489.90 | loss  0.01 | ppl     1.01 | acc     0.83 | train_ae_norm     1.00\n",
      "[18/200][99/1249] Loss_D: 1.38629496 (Loss_D_real: 0.69325775 Loss_D_fake: 0.69303721) Loss_G: -0.00002340 Loss_Enh_Dec: -0.00002396\n",
      "| epoch  18 |   100/ 1249 batches | lr 0.000000 | ms/batch 353.62 | loss  1.35 | ppl     3.84 | acc     0.81 | train_ae_norm     1.00\n",
      "[18/200][199/1249] Loss_D: 1.38629246 (Loss_D_real: 0.69314349 Loss_D_fake: 0.69314897) Loss_G: 0.00000066 Loss_Enh_Dec: -0.00000315\n",
      "| epoch  18 |   200/ 1249 batches | lr 0.000000 | ms/batch 353.39 | loss  1.34 | ppl     3.80 | acc     0.83 | train_ae_norm     1.00\n",
      "[18/200][299/1249] Loss_D: 1.38629556 (Loss_D_real: 0.69313657 Loss_D_fake: 0.69315898) Loss_G: 0.00000169 Loss_Enh_Dec: -0.00000259\n",
      "| epoch  18 |   300/ 1249 batches | lr 0.000000 | ms/batch 353.41 | loss  1.34 | ppl     3.83 | acc     0.80 | train_ae_norm     1.00\n",
      "[18/200][399/1249] Loss_D: 1.38629365 (Loss_D_real: 0.69315189 Loss_D_fake: 0.69314170) Loss_G: -0.00000071 Loss_Enh_Dec: -0.00000094\n",
      "| epoch  18 |   400/ 1249 batches | lr 0.000000 | ms/batch 352.96 | loss  1.33 | ppl     3.77 | acc     0.86 | train_ae_norm     1.00\n",
      "[18/200][499/1249] Loss_D: 1.38629341 (Loss_D_real: 0.69321650 Loss_D_fake: 0.69307697) Loss_G: -0.00001401 Loss_Enh_Dec: -0.00001536\n",
      "| epoch  18 |   500/ 1249 batches | lr 0.000000 | ms/batch 353.00 | loss  1.33 | ppl     3.77 | acc     0.81 | train_ae_norm     1.00\n",
      "[18/200][599/1249] Loss_D: 1.38629365 (Loss_D_real: 0.69306517 Loss_D_fake: 0.69322842) Loss_G: 0.00001737 Loss_Enh_Dec: 0.00001643\n",
      "| epoch  18 |   600/ 1249 batches | lr 0.000000 | ms/batch 353.96 | loss  1.31 | ppl     3.70 | acc     0.85 | train_ae_norm     1.00\n",
      "[18/200][699/1249] Loss_D: 1.38629353 (Loss_D_real: 0.69319892 Loss_D_fake: 0.69309461) Loss_G: -0.00001063 Loss_Enh_Dec: -0.00001081\n",
      "| epoch  18 |   700/ 1249 batches | lr 0.000000 | ms/batch 353.49 | loss  1.32 | ppl     3.76 | acc     0.78 | train_ae_norm     1.00\n",
      "[18/200][799/1249] Loss_D: 1.38629448 (Loss_D_real: 0.69319928 Loss_D_fake: 0.69309521) Loss_G: -0.00001122 Loss_Enh_Dec: -0.00001117\n",
      "| epoch  18 |   800/ 1249 batches | lr 0.000000 | ms/batch 353.64 | loss  1.32 | ppl     3.73 | acc     0.82 | train_ae_norm     1.00\n",
      "[18/200][899/1249] Loss_D: 1.38629603 (Loss_D_real: 0.69316101 Loss_D_fake: 0.69313496) Loss_G: -0.00000115 Loss_Enh_Dec: -0.00000153\n",
      "| epoch  18 |   900/ 1249 batches | lr 0.000000 | ms/batch 354.17 | loss  1.27 | ppl     3.57 | acc     0.82 | train_ae_norm     1.00\n",
      "[18/200][999/1249] Loss_D: 1.38630891 (Loss_D_real: 0.69319212 Loss_D_fake: 0.69311684) Loss_G: -0.00000219 Loss_Enh_Dec: -0.00000118\n",
      "| epoch  18 |  1000/ 1249 batches | lr 0.000000 | ms/batch 353.03 | loss  1.33 | ppl     3.77 | acc     0.81 | train_ae_norm     1.00\n",
      "[18/200][1099/1249] Loss_D: 1.38629460 (Loss_D_real: 0.69315600 Loss_D_fake: 0.69313854) Loss_G: -0.00000205 Loss_Enh_Dec: -0.00000216\n",
      "| epoch  18 |  1100/ 1249 batches | lr 0.000000 | ms/batch 353.36 | loss  1.52 | ppl     4.58 | acc     0.80 | train_ae_norm     1.00\n",
      "[18/200][1199/1249] Loss_D: 1.38629317 (Loss_D_real: 0.69318122 Loss_D_fake: 0.69311190) Loss_G: -0.00000105 Loss_Enh_Dec: -0.00001311\n",
      "| epoch  18 |  1200/ 1249 batches | lr 0.000000 | ms/batch 353.47 | loss  1.62 | ppl     5.07 | acc     0.78 | train_ae_norm     1.00\n",
      "| end of epoch  18 | time: 496.80s | test loss  1.48 | test ppl  4.40 | acc 0.840\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 19 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.706\n",
      "  Average training loss discriminator: 0.829\n",
      "  Training epcoh took: 0:00:48\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.390\n",
      "  Test Loss: 2.409\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  19 |     0/ 1249 batches | lr 0.000000 | ms/batch 487.43 | loss  0.02 | ppl     1.02 | acc     0.77 | train_ae_norm     1.00\n",
      "[19/200][99/1249] Loss_D: 1.38628447 (Loss_D_real: 0.69310629 Loss_D_fake: 0.69317818) Loss_G: 0.00000144 Loss_Enh_Dec: -0.00000436\n",
      "| epoch  19 |   100/ 1249 batches | lr 0.000000 | ms/batch 352.75 | loss  1.74 | ppl     5.69 | acc     0.78 | train_ae_norm     1.00\n",
      "[19/200][199/1249] Loss_D: 1.38628674 (Loss_D_real: 0.69325656 Loss_D_fake: 0.69303024) Loss_G: -0.00001835 Loss_Enh_Dec: -0.00001597\n",
      "| epoch  19 |   200/ 1249 batches | lr 0.000000 | ms/batch 353.14 | loss  1.95 | ppl     7.02 | acc     0.71 | train_ae_norm     1.00\n",
      "[19/200][299/1249] Loss_D: 1.38637877 (Loss_D_real: 0.69320560 Loss_D_fake: 0.69317311) Loss_G: -0.00000062 Loss_Enh_Dec: -0.00000745\n",
      "| epoch  19 |   300/ 1249 batches | lr 0.000000 | ms/batch 353.30 | loss  1.97 | ppl     7.19 | acc     0.74 | train_ae_norm     1.00\n",
      "[19/200][399/1249] Loss_D: 1.38629031 (Loss_D_real: 0.69310570 Loss_D_fake: 0.69318461) Loss_G: 0.00000351 Loss_Enh_Dec: -0.00000785\n",
      "| epoch  19 |   400/ 1249 batches | lr 0.000000 | ms/batch 352.05 | loss  1.83 | ppl     6.21 | acc     0.77 | train_ae_norm     1.00\n",
      "[19/200][499/1249] Loss_D: 1.38631403 (Loss_D_real: 0.69313586 Loss_D_fake: 0.69317818) Loss_G: 0.00000375 Loss_Enh_Dec: -0.00001200\n",
      "| epoch  19 |   500/ 1249 batches | lr 0.000000 | ms/batch 352.79 | loss  1.72 | ppl     5.59 | acc     0.75 | train_ae_norm     1.00\n",
      "[19/200][599/1249] Loss_D: 1.38632393 (Loss_D_real: 0.69317043 Loss_D_fake: 0.69315356) Loss_G: -0.00000687 Loss_Enh_Dec: -0.00000793\n",
      "| epoch  19 |   600/ 1249 batches | lr 0.000000 | ms/batch 353.29 | loss  1.67 | ppl     5.33 | acc     0.79 | train_ae_norm     1.00\n",
      "[19/200][699/1249] Loss_D: 1.38629663 (Loss_D_real: 0.69318759 Loss_D_fake: 0.69310904) Loss_G: -0.00001284 Loss_Enh_Dec: -0.00000297\n",
      "| epoch  19 |   700/ 1249 batches | lr 0.000000 | ms/batch 352.67 | loss  1.59 | ppl     4.88 | acc     0.76 | train_ae_norm     1.00\n",
      "[19/200][799/1249] Loss_D: 1.38629591 (Loss_D_real: 0.69312090 Loss_D_fake: 0.69317502) Loss_G: 0.00000201 Loss_Enh_Dec: -0.00000528\n",
      "| epoch  19 |   800/ 1249 batches | lr 0.000000 | ms/batch 352.03 | loss  1.65 | ppl     5.22 | acc     0.76 | train_ae_norm     1.00\n",
      "[19/200][899/1249] Loss_D: 1.38632250 (Loss_D_real: 0.69316697 Loss_D_fake: 0.69315547) Loss_G: -0.00000479 Loss_Enh_Dec: -0.00001435\n",
      "| epoch  19 |   900/ 1249 batches | lr 0.000000 | ms/batch 352.73 | loss  1.56 | ppl     4.77 | acc     0.78 | train_ae_norm     1.00\n",
      "[19/200][999/1249] Loss_D: 1.38629341 (Loss_D_real: 0.69314015 Loss_D_fake: 0.69315326) Loss_G: -0.00000313 Loss_Enh_Dec: -0.00000393\n",
      "| epoch  19 |  1000/ 1249 batches | lr 0.000000 | ms/batch 352.54 | loss  1.57 | ppl     4.82 | acc     0.83 | train_ae_norm     1.00\n",
      "[19/200][1099/1249] Loss_D: 1.38630629 (Loss_D_real: 0.69308251 Loss_D_fake: 0.69322371) Loss_G: 0.00000677 Loss_Enh_Dec: -0.00000355\n",
      "| epoch  19 |  1100/ 1249 batches | lr 0.000000 | ms/batch 353.05 | loss  1.54 | ppl     4.69 | acc     0.79 | train_ae_norm     1.00\n",
      "[19/200][1199/1249] Loss_D: 1.38626897 (Loss_D_real: 0.69316977 Loss_D_fake: 0.69309920) Loss_G: -0.00000476 Loss_Enh_Dec: -0.00001364\n",
      "| epoch  19 |  1200/ 1249 batches | lr 0.000000 | ms/batch 352.78 | loss  1.49 | ppl     4.44 | acc     0.80 | train_ae_norm     1.00\n",
      "| end of epoch  19 | time: 495.76s | test loss  1.44 | test ppl  4.21 | acc 0.845\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 20 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.711\n",
      "  Average training loss discriminator: 0.794\n",
      "  Training epcoh took: 0:00:48\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.407\n",
      "  Test Loss: 2.489\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  20 |     0/ 1249 batches | lr 0.000000 | ms/batch 487.91 | loss  0.01 | ppl     1.01 | acc     0.80 | train_ae_norm     1.00\n",
      "[20/200][99/1249] Loss_D: 1.38632452 (Loss_D_real: 0.69316453 Loss_D_fake: 0.69316000) Loss_G: 0.00000354 Loss_Enh_Dec: -0.00001066\n",
      "| epoch  20 |   100/ 1249 batches | lr 0.000000 | ms/batch 352.53 | loss  1.47 | ppl     4.34 | acc     0.80 | train_ae_norm     1.00\n",
      "[20/200][199/1249] Loss_D: 1.38630235 (Loss_D_real: 0.69314402 Loss_D_fake: 0.69315833) Loss_G: 0.00000006 Loss_Enh_Dec: 0.00000435\n",
      "| epoch  20 |   200/ 1249 batches | lr 0.000000 | ms/batch 352.14 | loss  1.46 | ppl     4.29 | acc     0.83 | train_ae_norm     1.00\n",
      "[20/200][299/1249] Loss_D: 1.38629138 (Loss_D_real: 0.69315338 Loss_D_fake: 0.69313800) Loss_G: -0.00000069 Loss_Enh_Dec: -0.00000030\n",
      "| epoch  20 |   300/ 1249 batches | lr 0.000000 | ms/batch 352.16 | loss  1.44 | ppl     4.24 | acc     0.81 | train_ae_norm     1.00\n",
      "[20/200][399/1249] Loss_D: 1.38629937 (Loss_D_real: 0.69314283 Loss_D_fake: 0.69315648) Loss_G: 0.00000312 Loss_Enh_Dec: -0.00000039\n",
      "| epoch  20 |   400/ 1249 batches | lr 0.000000 | ms/batch 352.39 | loss  1.40 | ppl     4.05 | acc     0.83 | train_ae_norm     1.00\n",
      "[20/200][499/1249] Loss_D: 1.38629115 (Loss_D_real: 0.69314712 Loss_D_fake: 0.69314402) Loss_G: 0.00000042 Loss_Enh_Dec: -0.00000268\n",
      "| epoch  20 |   500/ 1249 batches | lr 0.000000 | ms/batch 353.43 | loss  1.37 | ppl     3.95 | acc     0.82 | train_ae_norm     1.00\n",
      "[20/200][599/1249] Loss_D: 1.38629997 (Loss_D_real: 0.69323337 Loss_D_fake: 0.69306660) Loss_G: -0.00001606 Loss_Enh_Dec: -0.00002011\n",
      "| epoch  20 |   600/ 1249 batches | lr 0.000000 | ms/batch 352.74 | loss  1.34 | ppl     3.82 | acc     0.84 | train_ae_norm     1.00\n",
      "[20/200][699/1249] Loss_D: 1.38629365 (Loss_D_real: 0.69322169 Loss_D_fake: 0.69307202) Loss_G: -0.00001111 Loss_Enh_Dec: -0.00001579\n",
      "| epoch  20 |   700/ 1249 batches | lr 0.000000 | ms/batch 352.36 | loss  1.35 | ppl     3.84 | acc     0.81 | train_ae_norm     1.00\n",
      "[20/200][799/1249] Loss_D: 1.38629258 (Loss_D_real: 0.69315118 Loss_D_fake: 0.69314140) Loss_G: -0.00000182 Loss_Enh_Dec: -0.00000400\n",
      "| epoch  20 |   800/ 1249 batches | lr 0.000000 | ms/batch 352.31 | loss  1.34 | ppl     3.83 | acc     0.83 | train_ae_norm     1.00\n",
      "[20/200][899/1249] Loss_D: 1.38629687 (Loss_D_real: 0.69314140 Loss_D_fake: 0.69315547) Loss_G: 0.00000033 Loss_Enh_Dec: -0.00000206\n",
      "| epoch  20 |   900/ 1249 batches | lr 0.000000 | ms/batch 352.26 | loss  1.29 | ppl     3.64 | acc     0.81 | train_ae_norm     1.00\n",
      "[20/200][999/1249] Loss_D: 1.38629413 (Loss_D_real: 0.69318902 Loss_D_fake: 0.69310516) Loss_G: -0.00000840 Loss_Enh_Dec: -0.00001147\n",
      "| epoch  20 |  1000/ 1249 batches | lr 0.000000 | ms/batch 351.88 | loss  1.31 | ppl     3.69 | acc     0.85 | train_ae_norm     1.00\n",
      "[20/200][1099/1249] Loss_D: 1.38630080 (Loss_D_real: 0.69317263 Loss_D_fake: 0.69312811) Loss_G: -0.00000247 Loss_Enh_Dec: -0.00000825\n",
      "| epoch  20 |  1100/ 1249 batches | lr 0.000000 | ms/batch 352.73 | loss  1.32 | ppl     3.75 | acc     0.85 | train_ae_norm     1.00\n",
      "[20/200][1199/1249] Loss_D: 1.38631439 (Loss_D_real: 0.69314647 Loss_D_fake: 0.69316787) Loss_G: 0.00000048 Loss_Enh_Dec: 0.00000040\n",
      "| epoch  20 |  1200/ 1249 batches | lr 0.000000 | ms/batch 352.73 | loss  1.29 | ppl     3.64 | acc     0.83 | train_ae_norm     1.00\n",
      "| end of epoch  20 | time: 495.55s | test loss  1.33 | test ppl  3.79 | acc 0.861\n",
      "bleu_self:  [0.75442383 0.66997026 0.58241173 0.42364892 0.26083545]\n",
      "bleu_test:  [8.28914141e-01 6.49768178e-01 2.79443502e-01 1.92081439e-01\n",
      " 1.57567358e-04]\n",
      "bleu_self: [0.75442383,0.66997026,0.58241173,0.42364892,0.26083545]\n",
      "bleu_test: [0.82891414,0.64976818,0.27944350,0.19208144,0.00015757]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 21 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.712\n",
      "  Average training loss discriminator: 0.803\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.415\n",
      "  Test Loss: 2.501\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  21 |     0/ 1249 batches | lr 0.000000 | ms/batch 491.05 | loss  0.01 | ppl     1.01 | acc     0.84 | train_ae_norm     1.00\n",
      "[21/200][99/1249] Loss_D: 1.38629365 (Loss_D_real: 0.69314402 Loss_D_fake: 0.69314957) Loss_G: 0.00000008 Loss_Enh_Dec: -0.00000090\n",
      "| epoch  21 |   100/ 1249 batches | lr 0.000000 | ms/batch 352.87 | loss  1.28 | ppl     3.59 | acc     0.82 | train_ae_norm     1.00\n",
      "[21/200][199/1249] Loss_D: 1.38629770 (Loss_D_real: 0.69315696 Loss_D_fake: 0.69314069) Loss_G: 0.00000193 Loss_Enh_Dec: 0.00000030\n",
      "| epoch  21 |   200/ 1249 batches | lr 0.000000 | ms/batch 352.90 | loss  1.26 | ppl     3.54 | acc     0.84 | train_ae_norm     1.00\n",
      "[21/200][299/1249] Loss_D: 1.38629508 (Loss_D_real: 0.69319105 Loss_D_fake: 0.69310397) Loss_G: -0.00000862 Loss_Enh_Dec: -0.00000915\n",
      "| epoch  21 |   300/ 1249 batches | lr 0.000000 | ms/batch 353.36 | loss  1.26 | ppl     3.54 | acc     0.82 | train_ae_norm     1.00\n",
      "[21/200][399/1249] Loss_D: 1.38629377 (Loss_D_real: 0.69314909 Loss_D_fake: 0.69314468) Loss_G: 0.00000112 Loss_Enh_Dec: 0.00000182\n",
      "| epoch  21 |   400/ 1249 batches | lr 0.000000 | ms/batch 353.98 | loss  1.23 | ppl     3.43 | acc     0.87 | train_ae_norm     1.00\n",
      "[21/200][499/1249] Loss_D: 1.38628948 (Loss_D_real: 0.69308269 Loss_D_fake: 0.69320679) Loss_G: 0.00000994 Loss_Enh_Dec: 0.00000974\n",
      "| epoch  21 |   500/ 1249 batches | lr 0.000000 | ms/batch 354.01 | loss  1.22 | ppl     3.38 | acc     0.81 | train_ae_norm     1.00\n",
      "[21/200][599/1249] Loss_D: 1.38629198 (Loss_D_real: 0.69315594 Loss_D_fake: 0.69313610) Loss_G: -0.00000248 Loss_Enh_Dec: -0.00000228\n",
      "| epoch  21 |   600/ 1249 batches | lr 0.000000 | ms/batch 353.69 | loss  1.20 | ppl     3.31 | acc     0.85 | train_ae_norm     1.00\n",
      "[21/200][699/1249] Loss_D: 1.38629937 (Loss_D_real: 0.69315356 Loss_D_fake: 0.69314575) Loss_G: -0.00000173 Loss_Enh_Dec: -0.00000265\n",
      "| epoch  21 |   700/ 1249 batches | lr 0.000000 | ms/batch 353.48 | loss  1.19 | ppl     3.29 | acc     0.81 | train_ae_norm     1.00\n",
      "[21/200][799/1249] Loss_D: 1.38629389 (Loss_D_real: 0.69310778 Loss_D_fake: 0.69318604) Loss_G: 0.00000772 Loss_Enh_Dec: 0.00000673\n",
      "| epoch  21 |   800/ 1249 batches | lr 0.000000 | ms/batch 352.76 | loss  1.20 | ppl     3.32 | acc     0.84 | train_ae_norm     1.00\n",
      "[21/200][899/1249] Loss_D: 1.38628745 (Loss_D_real: 0.69316900 Loss_D_fake: 0.69311845) Loss_G: -0.00000422 Loss_Enh_Dec: -0.00000557\n",
      "| epoch  21 |   900/ 1249 batches | lr 0.000000 | ms/batch 352.52 | loss  1.15 | ppl     3.15 | acc     0.83 | train_ae_norm     1.00\n",
      "[21/200][999/1249] Loss_D: 1.38629735 (Loss_D_real: 0.69316196 Loss_D_fake: 0.69313538) Loss_G: -0.00000338 Loss_Enh_Dec: -0.00000432\n",
      "| epoch  21 |  1000/ 1249 batches | lr 0.000000 | ms/batch 354.39 | loss  1.16 | ppl     3.18 | acc     0.85 | train_ae_norm     1.00\n",
      "[21/200][1099/1249] Loss_D: 1.38629651 (Loss_D_real: 0.69314927 Loss_D_fake: 0.69314718) Loss_G: -0.00000178 Loss_Enh_Dec: -0.00000343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  21 |  1100/ 1249 batches | lr 0.000000 | ms/batch 353.64 | loss  1.17 | ppl     3.23 | acc     0.85 | train_ae_norm     1.00\n",
      "[21/200][1199/1249] Loss_D: 1.38629842 (Loss_D_real: 0.69316745 Loss_D_fake: 0.69313091) Loss_G: -0.00000579 Loss_Enh_Dec: -0.00000680\n",
      "| epoch  21 |  1200/ 1249 batches | lr 0.000000 | ms/batch 354.01 | loss  1.14 | ppl     3.14 | acc     0.85 | train_ae_norm     1.00\n",
      "| end of epoch  21 | time: 497.04s | test loss  1.25 | test ppl  3.49 | acc 0.871\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 22 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.710\n",
      "  Average training loss discriminator: 0.779\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.380\n",
      "  Test Loss: 2.599\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  22 |     0/ 1249 batches | lr 0.000000 | ms/batch 490.19 | loss  0.01 | ppl     1.01 | acc     0.86 | train_ae_norm     1.00\n",
      "[22/200][99/1249] Loss_D: 1.38629985 (Loss_D_real: 0.69307441 Loss_D_fake: 0.69322538) Loss_G: 0.00001709 Loss_Enh_Dec: 0.00001738\n",
      "| epoch  22 |   100/ 1249 batches | lr 0.000000 | ms/batch 354.09 | loss  1.14 | ppl     3.12 | acc     0.86 | train_ae_norm     1.00\n",
      "[22/200][199/1249] Loss_D: 1.38629615 (Loss_D_real: 0.69317263 Loss_D_fake: 0.69312352) Loss_G: -0.00000500 Loss_Enh_Dec: -0.00000526\n",
      "| epoch  22 |   200/ 1249 batches | lr 0.000000 | ms/batch 353.85 | loss  1.12 | ppl     3.07 | acc     0.87 | train_ae_norm     1.00\n",
      "[22/200][299/1249] Loss_D: 1.38629520 (Loss_D_real: 0.69313204 Loss_D_fake: 0.69316316) Loss_G: 0.00000300 Loss_Enh_Dec: 0.00000258\n",
      "| epoch  22 |   300/ 1249 batches | lr 0.000000 | ms/batch 353.18 | loss  1.12 | ppl     3.06 | acc     0.85 | train_ae_norm     1.00\n",
      "[22/200][399/1249] Loss_D: 1.38629484 (Loss_D_real: 0.69317663 Loss_D_fake: 0.69311827) Loss_G: -0.00000517 Loss_Enh_Dec: -0.00000535\n",
      "| epoch  22 |   400/ 1249 batches | lr 0.000000 | ms/batch 353.65 | loss  1.09 | ppl     2.98 | acc     0.86 | train_ae_norm     1.00\n",
      "[22/200][499/1249] Loss_D: 1.38629007 (Loss_D_real: 0.69321746 Loss_D_fake: 0.69307256) Loss_G: -0.00001556 Loss_Enh_Dec: -0.00001877\n",
      "| epoch  22 |   500/ 1249 batches | lr 0.000000 | ms/batch 352.68 | loss  1.09 | ppl     2.99 | acc     0.82 | train_ae_norm     1.00\n",
      "[22/200][599/1249] Loss_D: 1.38629663 (Loss_D_real: 0.69317377 Loss_D_fake: 0.69312286) Loss_G: -0.00000582 Loss_Enh_Dec: -0.00000623\n",
      "| epoch  22 |   600/ 1249 batches | lr 0.000000 | ms/batch 353.94 | loss  1.08 | ppl     2.94 | acc     0.88 | train_ae_norm     1.00\n",
      "[22/200][699/1249] Loss_D: 1.38628805 (Loss_D_real: 0.69315624 Loss_D_fake: 0.69313180) Loss_G: 0.00000511 Loss_Enh_Dec: 0.00000269\n",
      "| epoch  22 |   700/ 1249 batches | lr 0.000000 | ms/batch 354.11 | loss  1.10 | ppl     3.02 | acc     0.81 | train_ae_norm     1.00\n",
      "[22/200][799/1249] Loss_D: 1.38629246 (Loss_D_real: 0.69313860 Loss_D_fake: 0.69315386) Loss_G: -0.00000013 Loss_Enh_Dec: -0.00000119\n",
      "| epoch  22 |   800/ 1249 batches | lr 0.000000 | ms/batch 354.24 | loss  1.11 | ppl     3.04 | acc     0.86 | train_ae_norm     1.00\n",
      "[22/200][899/1249] Loss_D: 1.38629341 (Loss_D_real: 0.69311619 Loss_D_fake: 0.69317716) Loss_G: 0.00000833 Loss_Enh_Dec: 0.00000578\n",
      "| epoch  22 |   900/ 1249 batches | lr 0.000000 | ms/batch 353.18 | loss  1.06 | ppl     2.88 | acc     0.84 | train_ae_norm     1.00\n",
      "[22/200][999/1249] Loss_D: 1.38628793 (Loss_D_real: 0.69303346 Loss_D_fake: 0.69325447) Loss_G: 0.00002162 Loss_Enh_Dec: 0.00001731\n",
      "| epoch  22 |  1000/ 1249 batches | lr 0.000000 | ms/batch 353.89 | loss  1.08 | ppl     2.94 | acc     0.85 | train_ae_norm     1.00\n",
      "[22/200][1099/1249] Loss_D: 1.38630176 (Loss_D_real: 0.69308186 Loss_D_fake: 0.69321984) Loss_G: 0.00000109 Loss_Enh_Dec: -0.00000486\n",
      "| epoch  22 |  1100/ 1249 batches | lr 0.000000 | ms/batch 354.24 | loss  1.12 | ppl     3.07 | acc     0.85 | train_ae_norm     1.00\n",
      "[22/200][1199/1249] Loss_D: 1.38628089 (Loss_D_real: 0.69312304 Loss_D_fake: 0.69315785) Loss_G: 0.00000001 Loss_Enh_Dec: -0.00000137\n",
      "| epoch  22 |  1200/ 1249 batches | lr 0.000000 | ms/batch 352.99 | loss  1.21 | ppl     3.36 | acc     0.85 | train_ae_norm     1.00\n",
      "| end of epoch  22 | time: 497.26s | test loss  1.26 | test ppl  3.52 | acc 0.871\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 23 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.710\n",
      "  Average training loss discriminator: 0.770\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.398\n",
      "  Test Loss: 2.657\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  23 |     0/ 1249 batches | lr 0.000000 | ms/batch 491.05 | loss  0.01 | ppl     1.01 | acc     0.84 | train_ae_norm     1.00\n",
      "[23/200][99/1249] Loss_D: 1.38628078 (Loss_D_real: 0.69312811 Loss_D_fake: 0.69315261) Loss_G: 0.00000847 Loss_Enh_Dec: 0.00000354\n",
      "| epoch  23 |   100/ 1249 batches | lr 0.000000 | ms/batch 353.72 | loss  1.23 | ppl     3.44 | acc     0.83 | train_ae_norm     1.00\n",
      "[23/200][199/1249] Loss_D: 1.38630068 (Loss_D_real: 0.69314539 Loss_D_fake: 0.69315529) Loss_G: 0.00000135 Loss_Enh_Dec: 0.00000008\n",
      "| epoch  23 |   200/ 1249 batches | lr 0.000000 | ms/batch 353.33 | loss  1.15 | ppl     3.17 | acc     0.85 | train_ae_norm     1.00\n",
      "[23/200][299/1249] Loss_D: 1.38629377 (Loss_D_real: 0.69315195 Loss_D_fake: 0.69314182) Loss_G: 0.00000031 Loss_Enh_Dec: -0.00000493\n",
      "| epoch  23 |   300/ 1249 batches | lr 0.000000 | ms/batch 353.45 | loss  1.14 | ppl     3.12 | acc     0.83 | train_ae_norm     1.00\n",
      "[23/200][399/1249] Loss_D: 1.38630033 (Loss_D_real: 0.69313836 Loss_D_fake: 0.69316196) Loss_G: 0.00000334 Loss_Enh_Dec: -0.00000128\n",
      "| epoch  23 |   400/ 1249 batches | lr 0.000000 | ms/batch 353.95 | loss  1.12 | ppl     3.06 | acc     0.87 | train_ae_norm     1.00\n",
      "[23/200][499/1249] Loss_D: 1.38629770 (Loss_D_real: 0.69296110 Loss_D_fake: 0.69333667) Loss_G: 0.00002652 Loss_Enh_Dec: 0.00002176\n",
      "| epoch  23 |   500/ 1249 batches | lr 0.000000 | ms/batch 352.58 | loss  1.13 | ppl     3.09 | acc     0.85 | train_ae_norm     1.00\n",
      "[23/200][599/1249] Loss_D: 1.38629615 (Loss_D_real: 0.69314992 Loss_D_fake: 0.69314623) Loss_G: -0.00000053 Loss_Enh_Dec: -0.00000382\n",
      "| epoch  23 |   600/ 1249 batches | lr 0.000000 | ms/batch 353.47 | loss  1.12 | ppl     3.08 | acc     0.87 | train_ae_norm     1.00\n",
      "[23/200][699/1249] Loss_D: 1.38629270 (Loss_D_real: 0.69316709 Loss_D_fake: 0.69312561) Loss_G: -0.00000424 Loss_Enh_Dec: -0.00000553\n",
      "| epoch  23 |   700/ 1249 batches | lr 0.000000 | ms/batch 352.49 | loss  1.11 | ppl     3.04 | acc     0.81 | train_ae_norm     1.00\n",
      "[23/200][799/1249] Loss_D: 1.38629854 (Loss_D_real: 0.69310856 Loss_D_fake: 0.69318998) Loss_G: 0.00000727 Loss_Enh_Dec: 0.00000489\n",
      "| epoch  23 |   800/ 1249 batches | lr 0.000000 | ms/batch 352.00 | loss  1.10 | ppl     3.02 | acc     0.86 | train_ae_norm     1.00\n",
      "[23/200][899/1249] Loss_D: 1.38630414 (Loss_D_real: 0.69313049 Loss_D_fake: 0.69317365) Loss_G: 0.00000175 Loss_Enh_Dec: 0.00000085\n",
      "| epoch  23 |   900/ 1249 batches | lr 0.000000 | ms/batch 353.63 | loss  1.08 | ppl     2.94 | acc     0.84 | train_ae_norm     1.00\n",
      "[23/200][999/1249] Loss_D: 1.38629413 (Loss_D_real: 0.69316453 Loss_D_fake: 0.69312966) Loss_G: -0.00000388 Loss_Enh_Dec: -0.00000713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  23 |  1000/ 1249 batches | lr 0.000000 | ms/batch 354.16 | loss  1.07 | ppl     2.91 | acc     0.87 | train_ae_norm     1.00\n",
      "[23/200][1099/1249] Loss_D: 1.38628793 (Loss_D_real: 0.69311970 Loss_D_fake: 0.69316828) Loss_G: 0.00000395 Loss_Enh_Dec: 0.00000496\n",
      "| epoch  23 |  1100/ 1249 batches | lr 0.000000 | ms/batch 352.89 | loss  1.10 | ppl     3.02 | acc     0.85 | train_ae_norm     1.00\n",
      "[23/200][1199/1249] Loss_D: 1.38630271 (Loss_D_real: 0.69306505 Loss_D_fake: 0.69323766) Loss_G: 0.00001759 Loss_Enh_Dec: 0.00001812\n",
      "| epoch  23 |  1200/ 1249 batches | lr 0.000000 | ms/batch 353.46 | loss  1.12 | ppl     3.06 | acc     0.85 | train_ae_norm     1.00\n",
      "| end of epoch  23 | time: 496.82s | test loss  1.24 | test ppl  3.46 | acc 0.874\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 24 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.710\n",
      "  Average training loss discriminator: 0.763\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.415\n",
      "  Test Loss: 2.693\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  24 |     0/ 1249 batches | lr 0.000000 | ms/batch 490.42 | loss  0.01 | ppl     1.01 | acc     0.84 | train_ae_norm     1.00\n",
      "[24/200][99/1249] Loss_D: 1.38630164 (Loss_D_real: 0.69314051 Loss_D_fake: 0.69316113) Loss_G: 0.00000138 Loss_Enh_Dec: 0.00000200\n",
      "| epoch  24 |   100/ 1249 batches | lr 0.000000 | ms/batch 353.44 | loss  1.21 | ppl     3.35 | acc     0.82 | train_ae_norm     1.00\n",
      "[24/200][199/1249] Loss_D: 1.38629675 (Loss_D_real: 0.69314229 Loss_D_fake: 0.69315445) Loss_G: 0.00000109 Loss_Enh_Dec: -0.00000080\n",
      "| epoch  24 |   200/ 1249 batches | lr 0.000000 | ms/batch 354.07 | loss  1.11 | ppl     3.05 | acc     0.85 | train_ae_norm     1.00\n",
      "[24/200][299/1249] Loss_D: 1.38630104 (Loss_D_real: 0.69308633 Loss_D_fake: 0.69321477) Loss_G: 0.00001432 Loss_Enh_Dec: 0.00001326\n",
      "| epoch  24 |   300/ 1249 batches | lr 0.000000 | ms/batch 352.86 | loss  1.10 | ppl     3.00 | acc     0.85 | train_ae_norm     1.00\n",
      "[24/200][399/1249] Loss_D: 1.38633049 (Loss_D_real: 0.69315213 Loss_D_fake: 0.69317836) Loss_G: 0.00000216 Loss_Enh_Dec: -0.00000240\n",
      "| epoch  24 |   400/ 1249 batches | lr 0.000000 | ms/batch 353.62 | loss  1.08 | ppl     2.95 | acc     0.88 | train_ae_norm     1.00\n",
      "[24/200][499/1249] Loss_D: 1.38629055 (Loss_D_real: 0.69318098 Loss_D_fake: 0.69310951) Loss_G: -0.00000822 Loss_Enh_Dec: -0.00001268\n",
      "| epoch  24 |   500/ 1249 batches | lr 0.000000 | ms/batch 353.19 | loss  1.10 | ppl     2.99 | acc     0.83 | train_ae_norm     1.00\n",
      "[24/200][599/1249] Loss_D: 1.38627267 (Loss_D_real: 0.69310796 Loss_D_fake: 0.69316477) Loss_G: 0.00000824 Loss_Enh_Dec: 0.00000809\n",
      "| epoch  24 |   600/ 1249 batches | lr 0.000000 | ms/batch 353.77 | loss  1.12 | ppl     3.07 | acc     0.86 | train_ae_norm     1.00\n",
      "[24/200][699/1249] Loss_D: 1.38631010 (Loss_D_real: 0.69317281 Loss_D_fake: 0.69313735) Loss_G: -0.00000050 Loss_Enh_Dec: -0.00000650\n",
      "| epoch  24 |   700/ 1249 batches | lr 0.000000 | ms/batch 354.14 | loss  1.16 | ppl     3.19 | acc     0.81 | train_ae_norm     1.00\n",
      "[24/200][799/1249] Loss_D: 1.38629866 (Loss_D_real: 0.69323099 Loss_D_fake: 0.69306767) Loss_G: -0.00001757 Loss_Enh_Dec: -0.00001890\n",
      "| epoch  24 |   800/ 1249 batches | lr 0.000000 | ms/batch 353.37 | loss  1.14 | ppl     3.14 | acc     0.86 | train_ae_norm     1.00\n",
      "[24/200][899/1249] Loss_D: 1.38629758 (Loss_D_real: 0.69309890 Loss_D_fake: 0.69319868) Loss_G: 0.00000812 Loss_Enh_Dec: 0.00000565\n",
      "| epoch  24 |   900/ 1249 batches | lr 0.000000 | ms/batch 353.47 | loss  1.10 | ppl     3.00 | acc     0.85 | train_ae_norm     1.00\n",
      "[24/200][999/1249] Loss_D: 1.38629329 (Loss_D_real: 0.69312286 Loss_D_fake: 0.69317043) Loss_G: 0.00000288 Loss_Enh_Dec: -0.00000125\n",
      "| epoch  24 |  1000/ 1249 batches | lr 0.000000 | ms/batch 353.87 | loss  1.09 | ppl     2.97 | acc     0.85 | train_ae_norm     1.00\n",
      "[24/200][1099/1249] Loss_D: 1.38628173 (Loss_D_real: 0.69315833 Loss_D_fake: 0.69312334) Loss_G: -0.00000429 Loss_Enh_Dec: -0.00000587\n",
      "| epoch  24 |  1100/ 1249 batches | lr 0.000000 | ms/batch 354.03 | loss  1.14 | ppl     3.11 | acc     0.85 | train_ae_norm     1.00\n",
      "[24/200][1199/1249] Loss_D: 1.38629508 (Loss_D_real: 0.69318366 Loss_D_fake: 0.69311136) Loss_G: -0.00000677 Loss_Enh_Dec: -0.00001476\n",
      "| epoch  24 |  1200/ 1249 batches | lr 0.000000 | ms/batch 353.96 | loss  1.12 | ppl     3.05 | acc     0.88 | train_ae_norm     1.00\n",
      "| end of epoch  24 | time: 497.23s | test loss  1.19 | test ppl  3.30 | acc 0.881\n",
      "bleu_self:  [0.81641414 0.75063625 0.67367587 0.57634752 0.47603968]\n",
      "bleu_test:  [7.21212121e-01 4.46492475e-01 1.09168595e-01 8.12386800e-02\n",
      " 7.00353784e-05]\n",
      "bleu_self: [0.81641414,0.75063625,0.67367587,0.57634752,0.47603968]\n",
      "bleu_test: [0.72121212,0.44649248,0.10916860,0.08123868,0.00007004]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 25 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.708\n",
      "  Average training loss discriminator: 0.758\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.425\n",
      "  Test Loss: 2.755\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  25 |     0/ 1249 batches | lr 0.000000 | ms/batch 491.12 | loss  0.01 | ppl     1.01 | acc     0.86 | train_ae_norm     1.00\n",
      "[25/200][99/1249] Loss_D: 1.38629508 (Loss_D_real: 0.69312990 Loss_D_fake: 0.69316518) Loss_G: 0.00000221 Loss_Enh_Dec: 0.00000109\n",
      "| epoch  25 |   100/ 1249 batches | lr 0.000000 | ms/batch 353.84 | loss  1.08 | ppl     2.95 | acc     0.87 | train_ae_norm     1.00\n",
      "[25/200][199/1249] Loss_D: 1.38626873 (Loss_D_real: 0.69303095 Loss_D_fake: 0.69323778) Loss_G: 0.00002040 Loss_Enh_Dec: 0.00001823\n",
      "| epoch  25 |   200/ 1249 batches | lr 0.000000 | ms/batch 353.22 | loss  1.06 | ppl     2.90 | acc     0.86 | train_ae_norm     1.00\n",
      "[25/200][299/1249] Loss_D: 1.38628340 (Loss_D_real: 0.69307899 Loss_D_fake: 0.69320434) Loss_G: 0.00001338 Loss_Enh_Dec: 0.00001237\n",
      "| epoch  25 |   300/ 1249 batches | lr 0.000000 | ms/batch 353.43 | loss  1.10 | ppl     3.00 | acc     0.86 | train_ae_norm     1.00\n",
      "[25/200][399/1249] Loss_D: 1.38629305 (Loss_D_real: 0.69321859 Loss_D_fake: 0.69307446) Loss_G: -0.00001412 Loss_Enh_Dec: -0.00001520\n",
      "| epoch  25 |   400/ 1249 batches | lr 0.000000 | ms/batch 353.24 | loss  1.06 | ppl     2.90 | acc     0.88 | train_ae_norm     1.00\n",
      "[25/200][499/1249] Loss_D: 1.38630033 (Loss_D_real: 0.69312131 Loss_D_fake: 0.69317907) Loss_G: 0.00000716 Loss_Enh_Dec: 0.00000564\n",
      "| epoch  25 |   500/ 1249 batches | lr 0.000000 | ms/batch 353.37 | loss  1.04 | ppl     2.83 | acc     0.83 | train_ae_norm     1.00\n",
      "[25/200][599/1249] Loss_D: 1.38629735 (Loss_D_real: 0.69317734 Loss_D_fake: 0.69312000) Loss_G: -0.00000660 Loss_Enh_Dec: -0.00000603\n",
      "| epoch  25 |   600/ 1249 batches | lr 0.000000 | ms/batch 353.86 | loss  1.01 | ppl     2.74 | acc     0.88 | train_ae_norm     1.00\n",
      "[25/200][699/1249] Loss_D: 1.38629580 (Loss_D_real: 0.69317645 Loss_D_fake: 0.69311941) Loss_G: -0.00000557 Loss_Enh_Dec: -0.00000612\n",
      "| epoch  25 |   700/ 1249 batches | lr 0.000000 | ms/batch 354.12 | loss  1.01 | ppl     2.75 | acc     0.81 | train_ae_norm     1.00\n",
      "[25/200][799/1249] Loss_D: 1.38629365 (Loss_D_real: 0.69308674 Loss_D_fake: 0.69320697) Loss_G: 0.00001099 Loss_Enh_Dec: 0.00001043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  25 |   800/ 1249 batches | lr 0.000000 | ms/batch 352.69 | loss  1.00 | ppl     2.72 | acc     0.87 | train_ae_norm     1.00\n",
      "[25/200][899/1249] Loss_D: 1.38629329 (Loss_D_real: 0.69303703 Loss_D_fake: 0.69325626) Loss_G: 0.00002546 Loss_Enh_Dec: 0.00002415\n",
      "| epoch  25 |   900/ 1249 batches | lr 0.000000 | ms/batch 355.06 | loss  0.96 | ppl     2.61 | acc     0.85 | train_ae_norm     1.00\n",
      "[25/200][999/1249] Loss_D: 1.38629425 (Loss_D_real: 0.69312763 Loss_D_fake: 0.69316661) Loss_G: 0.00000354 Loss_Enh_Dec: 0.00000412\n",
      "| epoch  25 |  1000/ 1249 batches | lr 0.000000 | ms/batch 353.53 | loss  0.95 | ppl     2.60 | acc     0.88 | train_ae_norm     1.00\n",
      "[25/200][1099/1249] Loss_D: 1.38629723 (Loss_D_real: 0.69318712 Loss_D_fake: 0.69311017) Loss_G: -0.00000839 Loss_Enh_Dec: -0.00000950\n",
      "| epoch  25 |  1100/ 1249 batches | lr 0.000000 | ms/batch 353.04 | loss  0.98 | ppl     2.67 | acc     0.87 | train_ae_norm     1.00\n",
      "[25/200][1199/1249] Loss_D: 1.38629055 (Loss_D_real: 0.69312996 Loss_D_fake: 0.69316053) Loss_G: 0.00000548 Loss_Enh_Dec: 0.00000713\n",
      "| epoch  25 |  1200/ 1249 batches | lr 0.000000 | ms/batch 353.49 | loss  0.99 | ppl     2.68 | acc     0.88 | train_ae_norm     1.00\n",
      "| end of epoch  25 | time: 497.11s | test loss  1.15 | test ppl  3.16 | acc 0.886\n",
      "bleu_self:  [9.37499999e-01 8.75000002e-01 8.75000991e-03 8.75018691e-04\n",
      " 2.19817397e-04]\n",
      "bleu_test:  [9.99999999e-01 8.75000003e-01 8.75001249e-03 8.75022228e-04\n",
      " 2.19821461e-04]\n",
      "bleu_self: [0.93750000,0.87500000,0.00875001,0.00087502,0.00021982]\n",
      "bleu_test: [1.00000000,0.87500000,0.00875001,0.00087502,0.00021982]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 26 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.707\n",
      "  Average training loss discriminator: 0.754\n",
      "  Training epcoh took: 0:00:48\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.432\n",
      "  Test Loss: 2.832\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  26 |     0/ 1249 batches | lr 0.000000 | ms/batch 486.92 | loss  0.01 | ppl     1.01 | acc     0.87 | train_ae_norm     1.00\n",
      "[26/200][99/1249] Loss_D: 1.38629699 (Loss_D_real: 0.69319528 Loss_D_fake: 0.69310176) Loss_G: -0.00000996 Loss_Enh_Dec: -0.00000946\n",
      "| epoch  26 |   100/ 1249 batches | lr 0.000000 | ms/batch 352.97 | loss  0.99 | ppl     2.70 | acc     0.86 | train_ae_norm     1.00\n",
      "[26/200][199/1249] Loss_D: 1.38629282 (Loss_D_real: 0.69307727 Loss_D_fake: 0.69321555) Loss_G: 0.00001448 Loss_Enh_Dec: 0.00001392\n",
      "| epoch  26 |   200/ 1249 batches | lr 0.000000 | ms/batch 352.96 | loss  0.96 | ppl     2.62 | acc     0.87 | train_ae_norm     1.00\n",
      "[26/200][299/1249] Loss_D: 1.38629651 (Loss_D_real: 0.69317067 Loss_D_fake: 0.69312590) Loss_G: -0.00000489 Loss_Enh_Dec: -0.00000561\n",
      "| epoch  26 |   300/ 1249 batches | lr 0.000000 | ms/batch 352.33 | loss  0.98 | ppl     2.66 | acc     0.85 | train_ae_norm     1.00\n",
      "[26/200][399/1249] Loss_D: 1.38631582 (Loss_D_real: 0.69315732 Loss_D_fake: 0.69315857) Loss_G: 0.00000326 Loss_Enh_Dec: 0.00000351\n",
      "| epoch  26 |   400/ 1249 batches | lr 0.000000 | ms/batch 352.92 | loss  0.96 | ppl     2.62 | acc     0.88 | train_ae_norm     1.00\n",
      "[26/200][499/1249] Loss_D: 1.38630462 (Loss_D_real: 0.69314957 Loss_D_fake: 0.69315499) Loss_G: -0.00000030 Loss_Enh_Dec: -0.00000268\n",
      "| epoch  26 |   500/ 1249 batches | lr 0.000000 | ms/batch 352.13 | loss  0.95 | ppl     2.59 | acc     0.85 | train_ae_norm     1.00\n",
      "[26/200][599/1249] Loss_D: 1.38630033 (Loss_D_real: 0.69317627 Loss_D_fake: 0.69312400) Loss_G: -0.00000591 Loss_Enh_Dec: -0.00000872\n",
      "| epoch  26 |   600/ 1249 batches | lr 0.000000 | ms/batch 353.07 | loss  0.94 | ppl     2.56 | acc     0.88 | train_ae_norm     1.00\n",
      "[26/200][699/1249] Loss_D: 1.38629770 (Loss_D_real: 0.69324654 Loss_D_fake: 0.69305110) Loss_G: -0.00002043 Loss_Enh_Dec: -0.00002354\n",
      "| epoch  26 |   700/ 1249 batches | lr 0.000000 | ms/batch 352.29 | loss  0.93 | ppl     2.54 | acc     0.83 | train_ae_norm     1.00\n",
      "[26/200][799/1249] Loss_D: 1.38628185 (Loss_D_real: 0.69310760 Loss_D_fake: 0.69317424) Loss_G: 0.00000817 Loss_Enh_Dec: 0.00000650\n",
      "| epoch  26 |   800/ 1249 batches | lr 0.000000 | ms/batch 352.13 | loss  0.95 | ppl     2.58 | acc     0.89 | train_ae_norm     1.00\n",
      "[26/200][899/1249] Loss_D: 1.38630211 (Loss_D_real: 0.69318259 Loss_D_fake: 0.69311953) Loss_G: -0.00000407 Loss_Enh_Dec: -0.00001017\n",
      "| epoch  26 |   900/ 1249 batches | lr 0.000000 | ms/batch 353.44 | loss  0.95 | ppl     2.57 | acc     0.85 | train_ae_norm     1.00\n",
      "[26/200][999/1249] Loss_D: 1.38629520 (Loss_D_real: 0.69316930 Loss_D_fake: 0.69312590) Loss_G: -0.00000310 Loss_Enh_Dec: -0.00000574\n",
      "| epoch  26 |  1000/ 1249 batches | lr 0.000000 | ms/batch 352.54 | loss  1.03 | ppl     2.79 | acc     0.85 | train_ae_norm     1.00\n",
      "[26/200][1099/1249] Loss_D: 1.38629425 (Loss_D_real: 0.69311142 Loss_D_fake: 0.69318283) Loss_G: 0.00000583 Loss_Enh_Dec: -0.00000694\n",
      "| epoch  26 |  1100/ 1249 batches | lr 0.000000 | ms/batch 352.85 | loss  1.07 | ppl     2.92 | acc     0.87 | train_ae_norm     1.00\n",
      "[26/200][1199/1249] Loss_D: 1.38629508 (Loss_D_real: 0.69311678 Loss_D_fake: 0.69317830) Loss_G: 0.00000550 Loss_Enh_Dec: 0.00000869\n",
      "| epoch  26 |  1200/ 1249 batches | lr 0.000000 | ms/batch 352.80 | loss  1.03 | ppl     2.81 | acc     0.88 | train_ae_norm     1.00\n",
      "| end of epoch  26 | time: 495.61s | test loss  1.14 | test ppl  3.12 | acc 0.889\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 27 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.708\n",
      "  Average training loss discriminator: 0.754\n",
      "  Training epcoh took: 0:00:48\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.435\n",
      "  Test Loss: 2.750\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  27 |     0/ 1249 batches | lr 0.000000 | ms/batch 487.45 | loss  0.01 | ppl     1.01 | acc     0.87 | train_ae_norm     1.00\n",
      "[27/200][99/1249] Loss_D: 1.38629854 (Loss_D_real: 0.69302464 Loss_D_fake: 0.69327390) Loss_G: 0.00002484 Loss_Enh_Dec: 0.00002280\n",
      "| epoch  27 |   100/ 1249 batches | lr 0.000000 | ms/batch 352.57 | loss  1.01 | ppl     2.74 | acc     0.86 | train_ae_norm     1.00\n",
      "[27/200][199/1249] Loss_D: 1.38629067 (Loss_D_real: 0.69308484 Loss_D_fake: 0.69320583) Loss_G: 0.00001661 Loss_Enh_Dec: 0.00000339\n",
      "| epoch  27 |   200/ 1249 batches | lr 0.000000 | ms/batch 353.39 | loss  0.99 | ppl     2.70 | acc     0.87 | train_ae_norm     1.00\n",
      "[27/200][299/1249] Loss_D: 1.38629603 (Loss_D_real: 0.69313240 Loss_D_fake: 0.69316357) Loss_G: 0.00000489 Loss_Enh_Dec: 0.00000353\n",
      "| epoch  27 |   300/ 1249 batches | lr 0.000000 | ms/batch 353.04 | loss  1.05 | ppl     2.87 | acc     0.85 | train_ae_norm     1.00\n",
      "[27/200][399/1249] Loss_D: 1.38629508 (Loss_D_real: 0.69316602 Loss_D_fake: 0.69312906) Loss_G: 0.00000218 Loss_Enh_Dec: -0.00000075\n",
      "| epoch  27 |   400/ 1249 batches | lr 0.000000 | ms/batch 352.62 | loss  1.03 | ppl     2.79 | acc     0.88 | train_ae_norm     1.00\n",
      "[27/200][499/1249] Loss_D: 1.38630426 (Loss_D_real: 0.69312227 Loss_D_fake: 0.69318199) Loss_G: 0.00001185 Loss_Enh_Dec: 0.00001681\n",
      "| epoch  27 |   500/ 1249 batches | lr 0.000000 | ms/batch 352.34 | loss  1.07 | ppl     2.92 | acc     0.82 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27/200][599/1249] Loss_D: 1.38628912 (Loss_D_real: 0.69316769 Loss_D_fake: 0.69312143) Loss_G: -0.00000876 Loss_Enh_Dec: -0.00002184\n",
      "| epoch  27 |   600/ 1249 batches | lr 0.000000 | ms/batch 353.19 | loss  1.08 | ppl     2.96 | acc     0.85 | train_ae_norm     1.00\n",
      "[27/200][699/1249] Loss_D: 1.38627887 (Loss_D_real: 0.69320083 Loss_D_fake: 0.69307798) Loss_G: -0.00000659 Loss_Enh_Dec: -0.00003320\n",
      "| epoch  27 |   700/ 1249 batches | lr 0.000000 | ms/batch 353.17 | loss  1.15 | ppl     3.15 | acc     0.79 | train_ae_norm     1.00\n",
      "[27/200][799/1249] Loss_D: 1.38629532 (Loss_D_real: 0.69317502 Loss_D_fake: 0.69312024) Loss_G: -0.00000457 Loss_Enh_Dec: -0.00000848\n",
      "| epoch  27 |   800/ 1249 batches | lr 0.000000 | ms/batch 353.20 | loss  1.13 | ppl     3.09 | acc     0.87 | train_ae_norm     1.00\n",
      "[27/200][899/1249] Loss_D: 1.38629389 (Loss_D_real: 0.69304246 Loss_D_fake: 0.69325149) Loss_G: 0.00002283 Loss_Enh_Dec: 0.00001082\n",
      "| epoch  27 |   900/ 1249 batches | lr 0.000000 | ms/batch 353.48 | loss  1.07 | ppl     2.92 | acc     0.83 | train_ae_norm     1.00\n",
      "[27/200][999/1249] Loss_D: 1.38632143 (Loss_D_real: 0.69320601 Loss_D_fake: 0.69311541) Loss_G: -0.00001028 Loss_Enh_Dec: -0.00003285\n",
      "| epoch  27 |  1000/ 1249 batches | lr 0.000000 | ms/batch 354.01 | loss  1.06 | ppl     2.90 | acc     0.86 | train_ae_norm     1.00\n",
      "[27/200][1099/1249] Loss_D: 1.38628256 (Loss_D_real: 0.69315982 Loss_D_fake: 0.69312274) Loss_G: -0.00000458 Loss_Enh_Dec: -0.00000345\n",
      "| epoch  27 |  1100/ 1249 batches | lr 0.000000 | ms/batch 353.73 | loss  1.13 | ppl     3.11 | acc     0.86 | train_ae_norm     1.00\n",
      "[27/200][1199/1249] Loss_D: 1.38628912 (Loss_D_real: 0.69316977 Loss_D_fake: 0.69311941) Loss_G: -0.00001063 Loss_Enh_Dec: -0.00001694\n",
      "| epoch  27 |  1200/ 1249 batches | lr 0.000000 | ms/batch 352.85 | loss  1.12 | ppl     3.08 | acc     0.83 | train_ae_norm     1.00\n",
      "| end of epoch  27 | time: 496.18s | test loss  1.20 | test ppl  3.33 | acc 0.882\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 28 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.705\n",
      "  Average training loss discriminator: 0.755\n",
      "  Training epcoh took: 0:00:48\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.435\n",
      "  Test Loss: 2.834\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  28 |     0/ 1249 batches | lr 0.000000 | ms/batch 487.36 | loss  0.01 | ppl     1.01 | acc     0.85 | train_ae_norm     1.00\n",
      "[28/200][99/1249] Loss_D: 1.38633287 (Loss_D_real: 0.69314307 Loss_D_fake: 0.69318980) Loss_G: 0.00000078 Loss_Enh_Dec: -0.00000596\n",
      "| epoch  28 |   100/ 1249 batches | lr 0.000000 | ms/batch 352.97 | loss  1.12 | ppl     3.08 | acc     0.82 | train_ae_norm     1.00\n",
      "[28/200][199/1249] Loss_D: 1.38633287 (Loss_D_real: 0.69300270 Loss_D_fake: 0.69333017) Loss_G: 0.00002021 Loss_Enh_Dec: -0.00001364\n",
      "| epoch  28 |   200/ 1249 batches | lr 0.000000 | ms/batch 353.85 | loss  1.18 | ppl     3.26 | acc     0.81 | train_ae_norm     1.00\n",
      "[28/200][299/1249] Loss_D: 1.38629770 (Loss_D_real: 0.69311088 Loss_D_fake: 0.69318676) Loss_G: 0.00000163 Loss_Enh_Dec: -0.00001420\n",
      "| epoch  28 |   300/ 1249 batches | lr 0.000000 | ms/batch 352.61 | loss  1.24 | ppl     3.47 | acc     0.83 | train_ae_norm     1.00\n",
      "[28/200][399/1249] Loss_D: 1.38628626 (Loss_D_real: 0.69316119 Loss_D_fake: 0.69312501) Loss_G: -0.00000302 Loss_Enh_Dec: -0.00000595\n",
      "| epoch  28 |   400/ 1249 batches | lr 0.000000 | ms/batch 352.60 | loss  1.17 | ppl     3.21 | acc     0.86 | train_ae_norm     1.00\n",
      "[28/200][499/1249] Loss_D: 1.38629878 (Loss_D_real: 0.69311190 Loss_D_fake: 0.69318688) Loss_G: 0.00000825 Loss_Enh_Dec: 0.00001015\n",
      "| epoch  28 |   500/ 1249 batches | lr 0.000000 | ms/batch 353.81 | loss  1.13 | ppl     3.11 | acc     0.82 | train_ae_norm     1.00\n",
      "[28/200][599/1249] Loss_D: 1.38628697 (Loss_D_real: 0.69306535 Loss_D_fake: 0.69322169) Loss_G: 0.00001613 Loss_Enh_Dec: 0.00000933\n",
      "| epoch  28 |   600/ 1249 batches | lr 0.000000 | ms/batch 354.33 | loss  1.08 | ppl     2.93 | acc     0.87 | train_ae_norm     1.00\n",
      "[28/200][699/1249] Loss_D: 1.38631010 (Loss_D_real: 0.69320315 Loss_D_fake: 0.69310689) Loss_G: -0.00001168 Loss_Enh_Dec: -0.00000878\n",
      "| epoch  28 |   700/ 1249 batches | lr 0.000000 | ms/batch 353.79 | loss  1.06 | ppl     2.87 | acc     0.82 | train_ae_norm     1.00\n",
      "[28/200][799/1249] Loss_D: 1.38626790 (Loss_D_real: 0.69308949 Loss_D_fake: 0.69317836) Loss_G: 0.00001198 Loss_Enh_Dec: 0.00000682\n",
      "| epoch  28 |   800/ 1249 batches | lr 0.000000 | ms/batch 354.04 | loss  1.03 | ppl     2.80 | acc     0.86 | train_ae_norm     1.00\n",
      "[28/200][899/1249] Loss_D: 1.38628411 (Loss_D_real: 0.69316691 Loss_D_fake: 0.69311726) Loss_G: -0.00000696 Loss_Enh_Dec: -0.00002034\n",
      "| epoch  28 |   900/ 1249 batches | lr 0.000000 | ms/batch 353.39 | loss  1.04 | ppl     2.84 | acc     0.83 | train_ae_norm     1.00\n",
      "[28/200][999/1249] Loss_D: 1.38631094 (Loss_D_real: 0.69316852 Loss_D_fake: 0.69314241) Loss_G: -0.00000376 Loss_Enh_Dec: -0.00001688\n",
      "| epoch  28 |  1000/ 1249 batches | lr 0.000000 | ms/batch 353.06 | loss  1.19 | ppl     3.28 | acc     0.81 | train_ae_norm     1.00\n",
      "[28/200][1099/1249] Loss_D: 1.38640475 (Loss_D_real: 0.69323146 Loss_D_fake: 0.69317329) Loss_G: -0.00003308 Loss_Enh_Dec: 0.00000200\n",
      "| epoch  28 |  1100/ 1249 batches | lr 0.000000 | ms/batch 354.59 | loss  1.74 | ppl     5.70 | acc     0.67 | train_ae_norm     1.00\n",
      "[28/200][1199/1249] Loss_D: 1.38622260 (Loss_D_real: 0.69311810 Loss_D_fake: 0.69310451) Loss_G: 0.00000832 Loss_Enh_Dec: -0.00003966\n",
      "| epoch  28 |  1200/ 1249 batches | lr 0.000000 | ms/batch 353.55 | loss  2.48 | ppl    11.96 | acc     0.66 | train_ae_norm     1.00\n",
      "| end of epoch  28 | time: 496.74s | test loss  1.79 | test ppl  5.97 | acc 0.788\n",
      "bleu_self:  [0.79614981 0.67393737 0.50613367 0.44078413 0.22684581]\n",
      "bleu_test:  [8.37500000e-01 4.52586948e-01 3.22121982e-06 9.01602077e-09\n",
      " 2.77470736e-10]\n",
      "bleu_self: [0.79614981,0.67393737,0.50613367,0.44078413,0.22684581]\n",
      "bleu_test: [0.83750000,0.45258695,0.00000322,0.00000001,0.00000000]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 29 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.708\n",
      "  Average training loss discriminator: 0.754\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.380\n",
      "  Test Loss: 2.931\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  29 |     0/ 1249 batches | lr 0.000000 | ms/batch 489.08 | loss  0.02 | ppl     1.02 | acc     0.67 | train_ae_norm     1.00\n",
      "[29/200][99/1249] Loss_D: 1.38590932 (Loss_D_real: 0.69279093 Loss_D_fake: 0.69311833) Loss_G: 0.00004111 Loss_Enh_Dec: 0.00000785\n",
      "| epoch  29 |   100/ 1249 batches | lr 0.000000 | ms/batch 353.58 | loss  2.52 | ppl    12.46 | acc     0.64 | train_ae_norm     1.00\n",
      "[29/200][199/1249] Loss_D: 1.38620758 (Loss_D_real: 0.69297814 Loss_D_fake: 0.69322944) Loss_G: 0.00000871 Loss_Enh_Dec: 0.00002182\n",
      "| epoch  29 |   200/ 1249 batches | lr 0.000000 | ms/batch 353.46 | loss  2.72 | ppl    15.11 | acc     0.69 | train_ae_norm     1.00\n",
      "[29/200][299/1249] Loss_D: 1.38565874 (Loss_D_real: 0.69292128 Loss_D_fake: 0.69273752) Loss_G: 0.00004348 Loss_Enh_Dec: -0.00004528\n",
      "| epoch  29 |   300/ 1249 batches | lr 0.000000 | ms/batch 353.71 | loss  2.73 | ppl    15.39 | acc     0.61 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29/200][399/1249] Loss_D: 1.38571048 (Loss_D_real: 0.69298697 Loss_D_fake: 0.69272345) Loss_G: 0.00010972 Loss_Enh_Dec: -0.00006403\n",
      "| epoch  29 |   400/ 1249 batches | lr 0.000000 | ms/batch 353.41 | loss  2.93 | ppl    18.70 | acc     0.60 | train_ae_norm     1.00\n",
      "[29/200][499/1249] Loss_D: 1.38494873 (Loss_D_real: 0.69262683 Loss_D_fake: 0.69232196) Loss_G: 0.00005831 Loss_Enh_Dec: -0.00030535\n",
      "| epoch  29 |   500/ 1249 batches | lr 0.000000 | ms/batch 353.14 | loss  3.61 | ppl    36.86 | acc     0.52 | train_ae_norm     1.00\n",
      "[29/200][599/1249] Loss_D: 1.38482058 (Loss_D_real: 0.69243616 Loss_D_fake: 0.69238442) Loss_G: 0.00009464 Loss_Enh_Dec: -0.00071409\n",
      "| epoch  29 |   600/ 1249 batches | lr 0.000000 | ms/batch 354.63 | loss  3.81 | ppl    45.09 | acc     0.49 | train_ae_norm     1.00\n",
      "[29/200][699/1249] Loss_D: 1.38429117 (Loss_D_real: 0.69229960 Loss_D_fake: 0.69199157) Loss_G: 0.00011192 Loss_Enh_Dec: -0.00076549\n",
      "| epoch  29 |   700/ 1249 batches | lr 0.000000 | ms/batch 355.10 | loss  3.68 | ppl    39.83 | acc     0.53 | train_ae_norm     1.00\n",
      "[29/200][799/1249] Loss_D: 1.38525522 (Loss_D_real: 0.69285190 Loss_D_fake: 0.69240332) Loss_G: 0.00016168 Loss_Enh_Dec: -0.00014879\n",
      "| epoch  29 |   800/ 1249 batches | lr 0.000000 | ms/batch 354.73 | loss  4.19 | ppl    66.28 | acc     0.39 | train_ae_norm     1.00\n",
      "[29/200][899/1249] Loss_D: 1.38469744 (Loss_D_real: 0.69179201 Loss_D_fake: 0.69290543) Loss_G: 0.00019733 Loss_Enh_Dec: -0.00034473\n",
      "| epoch  29 |   900/ 1249 batches | lr 0.000000 | ms/batch 353.35 | loss  5.31 | ppl   202.46 | acc     0.44 | train_ae_norm     1.00\n",
      "[29/200][999/1249] Loss_D: 1.38455999 (Loss_D_real: 0.69257593 Loss_D_fake: 0.69198406) Loss_G: 0.00035504 Loss_Enh_Dec: -0.00031716\n",
      "| epoch  29 |  1000/ 1249 batches | lr 0.000000 | ms/batch 354.48 | loss  4.93 | ppl   139.02 | acc     0.43 | train_ae_norm     1.00\n",
      "[29/200][1099/1249] Loss_D: 1.38383687 (Loss_D_real: 0.69130820 Loss_D_fake: 0.69252867) Loss_G: 0.00046398 Loss_Enh_Dec: 0.00062537\n",
      "| epoch  29 |  1100/ 1249 batches | lr 0.000000 | ms/batch 353.88 | loss  4.53 | ppl    92.51 | acc     0.45 | train_ae_norm     1.00\n",
      "[29/200][1199/1249] Loss_D: 1.38207030 (Loss_D_real: 0.69062585 Loss_D_fake: 0.69144440) Loss_G: 0.00018186 Loss_Enh_Dec: -0.00177231\n",
      "| epoch  29 |  1200/ 1249 batches | lr 0.000000 | ms/batch 354.72 | loss  4.41 | ppl    82.01 | acc     0.41 | train_ae_norm     1.00\n",
      "| end of epoch  29 | time: 497.53s | test loss  3.95 | test ppl 51.80 | acc 0.572\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 30 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.708\n",
      "  Average training loss discriminator: 0.746\n",
      "  Training epcoh took: 0:00:48\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.422\n",
      "  Test Loss: 2.986\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  30 |     0/ 1249 batches | lr 0.000000 | ms/batch 487.58 | loss  0.05 | ppl     1.05 | acc     0.40 | train_ae_norm     1.00\n",
      "[30/200][99/1249] Loss_D: 1.38091683 (Loss_D_real: 0.69023246 Loss_D_fake: 0.69068432) Loss_G: 0.00072161 Loss_Enh_Dec: -0.00282234\n",
      "| epoch  30 |   100/ 1249 batches | lr 0.000000 | ms/batch 355.05 | loss  4.95 | ppl   140.73 | acc     0.46 | train_ae_norm     1.00\n",
      "[30/200][199/1249] Loss_D: 1.37834358 (Loss_D_real: 0.68888509 Loss_D_fake: 0.68945855) Loss_G: 0.00105205 Loss_Enh_Dec: -0.00302894\n",
      "| epoch  30 |   200/ 1249 batches | lr 0.000000 | ms/batch 354.82 | loss  4.86 | ppl   129.45 | acc     0.39 | train_ae_norm     1.00\n",
      "[30/200][299/1249] Loss_D: 1.37880588 (Loss_D_real: 0.69030184 Loss_D_fake: 0.68850410) Loss_G: 0.00110684 Loss_Enh_Dec: -0.00474973\n",
      "| epoch  30 |   300/ 1249 batches | lr 0.000000 | ms/batch 355.28 | loss  4.81 | ppl   122.52 | acc     0.45 | train_ae_norm     1.00\n",
      "[30/200][399/1249] Loss_D: 1.37780595 (Loss_D_real: 0.68829697 Loss_D_fake: 0.68950891) Loss_G: 0.00100576 Loss_Enh_Dec: -0.00487051\n",
      "| epoch  30 |   400/ 1249 batches | lr 0.000000 | ms/batch 355.31 | loss  4.59 | ppl    98.58 | acc     0.47 | train_ae_norm     1.00\n",
      "[30/200][499/1249] Loss_D: 1.37249625 (Loss_D_real: 0.68655461 Loss_D_fake: 0.68594164) Loss_G: 0.00149165 Loss_Enh_Dec: -0.00149278\n",
      "| epoch  30 |   500/ 1249 batches | lr 0.000000 | ms/batch 355.27 | loss  4.61 | ppl   100.92 | acc     0.41 | train_ae_norm     1.00\n",
      "[30/200][599/1249] Loss_D: 1.36702144 (Loss_D_real: 0.68212122 Loss_D_fake: 0.68490022) Loss_G: 0.00207897 Loss_Enh_Dec: -0.00482153\n",
      "| epoch  30 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.08 | loss  4.50 | ppl    90.07 | acc     0.46 | train_ae_norm     1.00\n",
      "[30/200][699/1249] Loss_D: 1.37828493 (Loss_D_real: 0.69004941 Loss_D_fake: 0.68823546) Loss_G: 0.00113416 Loss_Enh_Dec: -0.00765854\n",
      "| epoch  30 |   700/ 1249 batches | lr 0.000000 | ms/batch 355.31 | loss  5.14 | ppl   170.59 | acc     0.33 | train_ae_norm     1.00\n",
      "[30/200][799/1249] Loss_D: 1.38511634 (Loss_D_real: 0.69179082 Loss_D_fake: 0.69332552) Loss_G: 0.00061682 Loss_Enh_Dec: -0.00623360\n",
      "| epoch  30 |   800/ 1249 batches | lr 0.000000 | ms/batch 355.67 | loss  5.32 | ppl   203.50 | acc     0.36 | train_ae_norm     1.00\n",
      "[30/200][899/1249] Loss_D: 1.37950838 (Loss_D_real: 0.68905437 Loss_D_fake: 0.69045401) Loss_G: 0.00002387 Loss_Enh_Dec: -0.00381994\n",
      "| epoch  30 |   900/ 1249 batches | lr 0.000000 | ms/batch 355.82 | loss  4.87 | ppl   129.92 | acc     0.41 | train_ae_norm     1.00\n",
      "[30/200][999/1249] Loss_D: 1.37313962 (Loss_D_real: 0.68739176 Loss_D_fake: 0.68574780) Loss_G: 0.00120534 Loss_Enh_Dec: 0.00117296\n",
      "| epoch  30 |  1000/ 1249 batches | lr 0.000000 | ms/batch 354.72 | loss  4.58 | ppl    97.83 | acc     0.46 | train_ae_norm     1.00\n",
      "[30/200][1099/1249] Loss_D: 1.36970627 (Loss_D_real: 0.68521917 Loss_D_fake: 0.68448710) Loss_G: 0.00204186 Loss_Enh_Dec: -0.00666242\n",
      "| epoch  30 |  1100/ 1249 batches | lr 0.000000 | ms/batch 355.73 | loss  4.38 | ppl    79.78 | acc     0.45 | train_ae_norm     1.00\n",
      "[30/200][1199/1249] Loss_D: 1.35910833 (Loss_D_real: 0.67594600 Loss_D_fake: 0.68316233) Loss_G: 0.00209998 Loss_Enh_Dec: -0.01808241\n",
      "| epoch  30 |  1200/ 1249 batches | lr 0.000000 | ms/batch 356.08 | loss  4.26 | ppl    70.97 | acc     0.49 | train_ae_norm     1.00\n",
      "| end of epoch  30 | time: 499.13s | test loss  3.50 | test ppl 33.18 | acc 0.603\n",
      "bleu_self:  [0.95       0.90817508 0.87251135 0.84616162 0.82766277]\n",
      "bleu_test:  [1.50000000e-01 2.78306039e-09 7.53398005e-12 3.97943835e-13\n",
      " 6.90039159e-14]\n",
      "bleu_self: [0.95000000,0.90817508,0.87251135,0.84616162,0.82766277]\n",
      "bleu_test: [0.15000000,0.00000000,0.00000000,0.00000000,0.00000000]\n",
      "New saving model: epoch 030.\n",
      "Saving models to ./results/yahoo_merge_results\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 31 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:12.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:27.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:31.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:46.\n",
      "\n",
      "  Average training loss generetor: 0.707\n",
      "  Average training loss discriminator: 0.738\n",
      "  Training epcoh took: 0:00:50\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.432\n",
      "  Test Loss: 3.087\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  31 |     0/ 1249 batches | lr 0.000000 | ms/batch 499.44 | loss  0.04 | ppl     1.04 | acc     0.48 | train_ae_norm     1.00\n",
      "[31/200][99/1249] Loss_D: 1.36914861 (Loss_D_real: 0.68482029 Loss_D_fake: 0.68432832) Loss_G: 0.00096282 Loss_Enh_Dec: -0.01128653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  31 |   100/ 1249 batches | lr 0.000000 | ms/batch 354.86 | loss  4.23 | ppl    68.61 | acc     0.51 | train_ae_norm     1.00\n",
      "[31/200][199/1249] Loss_D: 1.36536241 (Loss_D_real: 0.68001431 Loss_D_fake: 0.68534803) Loss_G: 0.00235914 Loss_Enh_Dec: -0.01709795\n",
      "| epoch  31 |   200/ 1249 batches | lr 0.000000 | ms/batch 355.17 | loss  4.13 | ppl    62.27 | acc     0.51 | train_ae_norm     1.00\n",
      "[31/200][299/1249] Loss_D: 1.37073183 (Loss_D_real: 0.68874335 Loss_D_fake: 0.68198842) Loss_G: 0.00282428 Loss_Enh_Dec: -0.00213090\n",
      "| epoch  31 |   300/ 1249 batches | lr 0.000000 | ms/batch 355.92 | loss  4.12 | ppl    61.53 | acc     0.53 | train_ae_norm     1.00\n",
      "[31/200][399/1249] Loss_D: 1.34511685 (Loss_D_real: 0.67432827 Loss_D_fake: 0.67078865) Loss_G: 0.00432229 Loss_Enh_Dec: 0.00117697\n",
      "| epoch  31 |   400/ 1249 batches | lr 0.000000 | ms/batch 355.19 | loss  4.12 | ppl    61.79 | acc     0.55 | train_ae_norm     1.00\n",
      "[31/200][499/1249] Loss_D: 1.37244022 (Loss_D_real: 0.68793368 Loss_D_fake: 0.68450654) Loss_G: 0.00229320 Loss_Enh_Dec: -0.02779173\n",
      "| epoch  31 |   500/ 1249 batches | lr 0.000000 | ms/batch 355.35 | loss  4.22 | ppl    68.35 | acc     0.48 | train_ae_norm     1.00\n",
      "[31/200][599/1249] Loss_D: 1.36495519 (Loss_D_real: 0.68033969 Loss_D_fake: 0.68461549) Loss_G: 0.00164597 Loss_Enh_Dec: -0.00200881\n",
      "| epoch  31 |   600/ 1249 batches | lr 0.000000 | ms/batch 354.61 | loss  4.14 | ppl    63.05 | acc     0.52 | train_ae_norm     1.00\n",
      "[31/200][699/1249] Loss_D: 1.36107469 (Loss_D_real: 0.67979240 Loss_D_fake: 0.68128222) Loss_G: 0.00194902 Loss_Enh_Dec: -0.01454428\n",
      "| epoch  31 |   700/ 1249 batches | lr 0.000000 | ms/batch 355.72 | loss  4.13 | ppl    62.35 | acc     0.52 | train_ae_norm     1.00\n",
      "[31/200][799/1249] Loss_D: 1.36568236 (Loss_D_real: 0.68350619 Loss_D_fake: 0.68217611) Loss_G: 0.00250928 Loss_Enh_Dec: -0.01328540\n",
      "| epoch  31 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.09 | loss  4.13 | ppl    62.25 | acc     0.49 | train_ae_norm     1.00\n",
      "[31/200][899/1249] Loss_D: 1.35568714 (Loss_D_real: 0.67760676 Loss_D_fake: 0.67808044) Loss_G: 0.00337798 Loss_Enh_Dec: -0.02415772\n",
      "| epoch  31 |   900/ 1249 batches | lr 0.000000 | ms/batch 355.37 | loss  4.05 | ppl    57.68 | acc     0.48 | train_ae_norm     1.00\n",
      "[31/200][999/1249] Loss_D: 1.34382010 (Loss_D_real: 0.67412490 Loss_D_fake: 0.66969526) Loss_G: 0.00464808 Loss_Enh_Dec: -0.00356898\n",
      "| epoch  31 |  1000/ 1249 batches | lr 0.000000 | ms/batch 355.99 | loss  4.02 | ppl    55.70 | acc     0.51 | train_ae_norm     1.00\n",
      "[31/200][1099/1249] Loss_D: 1.31516731 (Loss_D_real: 0.65592420 Loss_D_fake: 0.65924311) Loss_G: 0.00678865 Loss_Enh_Dec: -0.00561707\n",
      "| epoch  31 |  1100/ 1249 batches | lr 0.000000 | ms/batch 354.42 | loss  4.05 | ppl    57.21 | acc     0.48 | train_ae_norm     1.00\n",
      "[31/200][1199/1249] Loss_D: 1.34308457 (Loss_D_real: 0.67805552 Loss_D_fake: 0.66502911) Loss_G: 0.00617682 Loss_Enh_Dec: -0.01744593\n",
      "| epoch  31 |  1200/ 1249 batches | lr 0.000000 | ms/batch 353.86 | loss  4.00 | ppl    54.62 | acc     0.49 | train_ae_norm     1.00\n",
      "| end of epoch  31 | time: 500.02s | test loss  3.43 | test ppl 30.73 | acc 0.622\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 32 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:27.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:46.\n",
      "\n",
      "  Average training loss generetor: 0.706\n",
      "  Average training loss discriminator: 0.741\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.403\n",
      "  Test Loss: 3.151\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  32 |     0/ 1249 batches | lr 0.000000 | ms/batch 496.19 | loss  0.04 | ppl     1.04 | acc     0.46 | train_ae_norm     1.00\n",
      "[32/200][99/1249] Loss_D: 1.31381893 (Loss_D_real: 0.65588713 Loss_D_fake: 0.65793180) Loss_G: 0.00687323 Loss_Enh_Dec: -0.02595373\n",
      "| epoch  32 |   100/ 1249 batches | lr 0.000000 | ms/batch 354.93 | loss  4.21 | ppl    67.17 | acc     0.51 | train_ae_norm     1.00\n",
      "[32/200][199/1249] Loss_D: 1.30410218 (Loss_D_real: 0.65359056 Loss_D_fake: 0.65051162) Loss_G: 0.00674733 Loss_Enh_Dec: -0.02364112\n",
      "| epoch  32 |   200/ 1249 batches | lr 0.000000 | ms/batch 354.82 | loss  4.23 | ppl    69.00 | acc     0.51 | train_ae_norm     1.00\n",
      "[32/200][299/1249] Loss_D: 1.27728772 (Loss_D_real: 0.63771415 Loss_D_fake: 0.63957351) Loss_G: 0.00857913 Loss_Enh_Dec: -0.00346468\n",
      "| epoch  32 |   300/ 1249 batches | lr 0.000000 | ms/batch 355.18 | loss  4.14 | ppl    63.03 | acc     0.52 | train_ae_norm     1.00\n",
      "[32/200][399/1249] Loss_D: 1.29458833 (Loss_D_real: 0.65148783 Loss_D_fake: 0.64310044) Loss_G: 0.00851729 Loss_Enh_Dec: -0.03602950\n",
      "| epoch  32 |   400/ 1249 batches | lr 0.000000 | ms/batch 355.16 | loss  4.03 | ppl    56.22 | acc     0.55 | train_ae_norm     1.00\n",
      "[32/200][499/1249] Loss_D: 1.24874640 (Loss_D_real: 0.63199502 Loss_D_fake: 0.61675137) Loss_G: 0.01165988 Loss_Enh_Dec: -0.03587837\n",
      "| epoch  32 |   500/ 1249 batches | lr 0.000000 | ms/batch 355.49 | loss  4.24 | ppl    69.17 | acc     0.51 | train_ae_norm     1.00\n",
      "[32/200][599/1249] Loss_D: 1.26235616 (Loss_D_real: 0.62531626 Loss_D_fake: 0.63703990) Loss_G: 0.00748433 Loss_Enh_Dec: -0.03176068\n",
      "| epoch  32 |   600/ 1249 batches | lr 0.000000 | ms/batch 355.89 | loss  4.23 | ppl    68.81 | acc     0.50 | train_ae_norm     1.00\n",
      "[32/200][699/1249] Loss_D: 1.26432335 (Loss_D_real: 0.63750637 Loss_D_fake: 0.62681699) Loss_G: 0.01119854 Loss_Enh_Dec: -0.05868315\n",
      "| epoch  32 |   700/ 1249 batches | lr 0.000000 | ms/batch 355.52 | loss  4.48 | ppl    88.65 | acc     0.46 | train_ae_norm     1.00\n",
      "[32/200][799/1249] Loss_D: 1.26887870 (Loss_D_real: 0.63511503 Loss_D_fake: 0.63376367) Loss_G: 0.00765879 Loss_Enh_Dec: -0.02229037\n",
      "| epoch  32 |   800/ 1249 batches | lr 0.000000 | ms/batch 354.88 | loss  4.71 | ppl   111.13 | acc     0.40 | train_ae_norm     1.00\n",
      "[32/200][899/1249] Loss_D: 1.28005779 (Loss_D_real: 0.64526260 Loss_D_fake: 0.63479519) Loss_G: 0.01161254 Loss_Enh_Dec: -0.03136024\n",
      "| epoch  32 |   900/ 1249 batches | lr 0.000000 | ms/batch 355.37 | loss  4.78 | ppl   118.65 | acc     0.41 | train_ae_norm     1.00\n",
      "[32/200][999/1249] Loss_D: 1.25986111 (Loss_D_real: 0.62344313 Loss_D_fake: 0.63641798) Loss_G: 0.01008840 Loss_Enh_Dec: -0.02695369\n",
      "| epoch  32 |  1000/ 1249 batches | lr 0.000000 | ms/batch 355.85 | loss  4.83 | ppl   125.60 | acc     0.45 | train_ae_norm     1.00\n",
      "[32/200][1099/1249] Loss_D: 1.19634676 (Loss_D_real: 0.58369207 Loss_D_fake: 0.61265475) Loss_G: 0.01351535 Loss_Enh_Dec: -0.04834053\n",
      "| epoch  32 |  1100/ 1249 batches | lr 0.000000 | ms/batch 354.70 | loss  4.64 | ppl   103.81 | acc     0.45 | train_ae_norm     1.00\n",
      "[32/200][1199/1249] Loss_D: 1.12735367 (Loss_D_real: 0.54890579 Loss_D_fake: 0.57844782) Loss_G: 0.01847879 Loss_Enh_Dec: -0.13205288\n",
      "| epoch  32 |  1200/ 1249 batches | lr 0.000000 | ms/batch 354.62 | loss  4.32 | ppl    75.45 | acc     0.49 | train_ae_norm     1.00\n",
      "| end of epoch  32 | time: 499.74s | test loss  3.47 | test ppl 31.99 | acc 0.614\n",
      "bleu_self:  [0.9875     0.98048861 0.97285338 0.60133174 0.46456268]\n",
      "bleu_test:  [3.64583333e-01 1.28247384e-02 1.04563602e-07 5.73087570e-10\n",
      " 1.16071997e-09]\n",
      "bleu_self: [0.98750000,0.98048861,0.97285338,0.60133174,0.46456268]\n",
      "bleu_test: [0.36458333,0.01282474,0.00000010,0.00000000,0.00000000]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 33 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.706\n",
      "  Average training loss discriminator: 0.732\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.440\n",
      "  Test Loss: 3.121\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  33 |     0/ 1249 batches | lr 0.000000 | ms/batch 489.60 | loss  0.05 | ppl     1.05 | acc     0.49 | train_ae_norm     1.00\n",
      "[33/200][99/1249] Loss_D: 1.12409043 (Loss_D_real: 0.57516080 Loss_D_fake: 0.54892957) Loss_G: 0.02071385 Loss_Enh_Dec: -0.10895443\n",
      "| epoch  33 |   100/ 1249 batches | lr 0.000000 | ms/batch 355.56 | loss  4.16 | ppl    63.96 | acc     0.50 | train_ae_norm     1.00\n",
      "[33/200][199/1249] Loss_D: 1.11040294 (Loss_D_real: 0.57531106 Loss_D_fake: 0.53509188) Loss_G: 0.02508850 Loss_Enh_Dec: -0.09238788\n",
      "| epoch  33 |   200/ 1249 batches | lr 0.000000 | ms/batch 354.88 | loss  4.17 | ppl    64.55 | acc     0.52 | train_ae_norm     1.00\n",
      "[33/200][299/1249] Loss_D: 1.03895211 (Loss_D_real: 0.53223312 Loss_D_fake: 0.50671905) Loss_G: 0.02586054 Loss_Enh_Dec: -0.08570617\n",
      "| epoch  33 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.15 | loss  4.14 | ppl    62.83 | acc     0.57 | train_ae_norm     1.00\n",
      "[33/200][399/1249] Loss_D: 1.06514561 (Loss_D_real: 0.54235858 Loss_D_fake: 0.52278703) Loss_G: 0.02356573 Loss_Enh_Dec: -0.04072480\n",
      "| epoch  33 |   400/ 1249 batches | lr 0.000000 | ms/batch 355.26 | loss  4.06 | ppl    58.01 | acc     0.54 | train_ae_norm     1.00\n",
      "[33/200][499/1249] Loss_D: 1.13209581 (Loss_D_real: 0.60802561 Loss_D_fake: 0.52407014) Loss_G: 0.02018482 Loss_Enh_Dec: -0.06033670\n",
      "| epoch  33 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.58 | loss  4.23 | ppl    68.61 | acc     0.46 | train_ae_norm     1.00\n",
      "[33/200][599/1249] Loss_D: 1.15659714 (Loss_D_real: 0.57956529 Loss_D_fake: 0.57703179) Loss_G: 0.01962697 Loss_Enh_Dec: -0.15074442\n",
      "| epoch  33 |   600/ 1249 batches | lr 0.000000 | ms/batch 355.91 | loss  4.38 | ppl    79.51 | acc     0.49 | train_ae_norm     1.00\n",
      "[33/200][699/1249] Loss_D: 1.18393660 (Loss_D_real: 0.60965377 Loss_D_fake: 0.57428277) Loss_G: 0.01397290 Loss_Enh_Dec: -0.18928635\n",
      "| epoch  33 |   700/ 1249 batches | lr 0.000000 | ms/batch 355.15 | loss  4.61 | ppl   100.30 | acc     0.41 | train_ae_norm     1.00\n",
      "[33/200][799/1249] Loss_D: 1.08121693 (Loss_D_real: 0.55425882 Loss_D_fake: 0.52695811) Loss_G: 0.02542486 Loss_Enh_Dec: -0.25567037\n",
      "| epoch  33 |   800/ 1249 batches | lr 0.000000 | ms/batch 355.39 | loss  4.71 | ppl   111.30 | acc     0.43 | train_ae_norm     1.00\n",
      "[33/200][899/1249] Loss_D: 1.07077193 (Loss_D_real: 0.54207635 Loss_D_fake: 0.52869558) Loss_G: 0.02227091 Loss_Enh_Dec: -0.25845858\n",
      "| epoch  33 |   900/ 1249 batches | lr 0.000000 | ms/batch 355.35 | loss  4.52 | ppl    92.28 | acc     0.46 | train_ae_norm     1.00\n",
      "[33/200][999/1249] Loss_D: 1.04216766 (Loss_D_real: 0.53089195 Loss_D_fake: 0.51127577) Loss_G: 0.02596542 Loss_Enh_Dec: -0.16911988\n",
      "| epoch  33 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.01 | loss  4.77 | ppl   118.14 | acc     0.44 | train_ae_norm     1.00\n",
      "[33/200][1099/1249] Loss_D: 1.04615545 (Loss_D_real: 0.51299226 Loss_D_fake: 0.53316313) Loss_G: 0.02308302 Loss_Enh_Dec: -0.21489663\n",
      "| epoch  33 |  1100/ 1249 batches | lr 0.000000 | ms/batch 355.07 | loss  4.75 | ppl   115.74 | acc     0.41 | train_ae_norm     1.00\n",
      "[33/200][1199/1249] Loss_D: 1.02678621 (Loss_D_real: 0.51054978 Loss_D_fake: 0.51623642) Loss_G: 0.02554479 Loss_Enh_Dec: -0.22679399\n",
      "| epoch  33 |  1200/ 1249 batches | lr 0.000000 | ms/batch 355.05 | loss  4.73 | ppl   112.81 | acc     0.38 | train_ae_norm     1.00\n",
      "| end of epoch  33 | time: 499.48s | test loss  4.51 | test ppl 91.25 | acc 0.513\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 34 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:27.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:31.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:46.\n",
      "\n",
      "  Average training loss generetor: 0.706\n",
      "  Average training loss discriminator: 0.731\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.445\n",
      "  Test Loss: 3.157\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  34 |     0/ 1249 batches | lr 0.000000 | ms/batch 497.39 | loss  0.05 | ppl     1.05 | acc     0.36 | train_ae_norm     1.00\n",
      "[34/200][99/1249] Loss_D: 0.99929279 (Loss_D_real: 0.49721479 Loss_D_fake: 0.50207800) Loss_G: 0.02770106 Loss_Enh_Dec: -0.08212123\n",
      "| epoch  34 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.32 | loss  5.13 | ppl   169.64 | acc     0.46 | train_ae_norm     1.00\n",
      "[34/200][199/1249] Loss_D: 1.05514586 (Loss_D_real: 0.55517733 Loss_D_fake: 0.49996853) Loss_G: 0.02596402 Loss_Enh_Dec: -0.10917213\n",
      "| epoch  34 |   200/ 1249 batches | lr 0.000000 | ms/batch 355.90 | loss  4.89 | ppl   133.32 | acc     0.42 | train_ae_norm     1.00\n",
      "[34/200][299/1249] Loss_D: 0.90811855 (Loss_D_real: 0.45539534 Loss_D_fake: 0.45272321) Loss_G: 0.02936015 Loss_Enh_Dec: -0.27535686\n",
      "| epoch  34 |   300/ 1249 batches | lr 0.000000 | ms/batch 355.94 | loss  4.87 | ppl   130.18 | acc     0.42 | train_ae_norm     1.00\n",
      "[34/200][399/1249] Loss_D: 0.92939085 (Loss_D_real: 0.45513755 Loss_D_fake: 0.47425330) Loss_G: 0.03096966 Loss_Enh_Dec: -0.24242643\n",
      "| epoch  34 |   400/ 1249 batches | lr 0.000000 | ms/batch 356.33 | loss  4.66 | ppl   105.98 | acc     0.48 | train_ae_norm     1.00\n",
      "[34/200][499/1249] Loss_D: 1.00538230 (Loss_D_real: 0.54531479 Loss_D_fake: 0.46006757) Loss_G: 0.02860705 Loss_Enh_Dec: -0.21560590\n",
      "| epoch  34 |   500/ 1249 batches | lr 0.000000 | ms/batch 357.02 | loss  4.65 | ppl   104.26 | acc     0.44 | train_ae_norm     1.00\n",
      "[34/200][599/1249] Loss_D: 0.99899155 (Loss_D_real: 0.52444136 Loss_D_fake: 0.47455019) Loss_G: 0.03075703 Loss_Enh_Dec: -0.10237809\n",
      "| epoch  34 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.78 | loss  4.66 | ppl   105.44 | acc     0.45 | train_ae_norm     1.00\n",
      "[34/200][699/1249] Loss_D: 0.84384716 (Loss_D_real: 0.41155896 Loss_D_fake: 0.43228817) Loss_G: 0.03889737 Loss_Enh_Dec: -0.12648788\n",
      "| epoch  34 |   700/ 1249 batches | lr 0.000000 | ms/batch 355.18 | loss  4.55 | ppl    94.85 | acc     0.46 | train_ae_norm     1.00\n",
      "[34/200][799/1249] Loss_D: 0.78170061 (Loss_D_real: 0.38587773 Loss_D_fake: 0.39582285) Loss_G: 0.04010388 Loss_Enh_Dec: -0.22247629\n",
      "| epoch  34 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.16 | loss  4.62 | ppl   101.88 | acc     0.44 | train_ae_norm     1.00\n",
      "[34/200][899/1249] Loss_D: 0.81795669 (Loss_D_real: 0.40035892 Loss_D_fake: 0.41759777) Loss_G: 0.03550487 Loss_Enh_Dec: -0.24093643\n",
      "| epoch  34 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.24 | loss  4.44 | ppl    84.72 | acc     0.48 | train_ae_norm     1.00\n",
      "[34/200][999/1249] Loss_D: 0.69439971 (Loss_D_real: 0.32934380 Loss_D_fake: 0.36505592) Loss_G: 0.04748882 Loss_Enh_Dec: -0.20375672\n",
      "| epoch  34 |  1000/ 1249 batches | lr 0.000000 | ms/batch 355.41 | loss  4.39 | ppl    80.72 | acc     0.49 | train_ae_norm     1.00\n",
      "[34/200][1099/1249] Loss_D: 0.85492778 (Loss_D_real: 0.47282180 Loss_D_fake: 0.38210601) Loss_G: 0.04370383 Loss_Enh_Dec: -0.12117391\n",
      "| epoch  34 |  1100/ 1249 batches | lr 0.000000 | ms/batch 355.37 | loss  4.38 | ppl    79.74 | acc     0.46 | train_ae_norm     1.00\n",
      "[34/200][1199/1249] Loss_D: 0.82675344 (Loss_D_real: 0.38262129 Loss_D_fake: 0.44413215) Loss_G: 0.03547051 Loss_Enh_Dec: -0.22359923\n",
      "| epoch  34 |  1200/ 1249 batches | lr 0.000000 | ms/batch 356.61 | loss  4.49 | ppl    88.88 | acc     0.41 | train_ae_norm     1.00\n",
      "| end of epoch  34 | time: 501.02s | test loss  3.80 | test ppl 44.91 | acc 0.575\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 35 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.748\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.445\n",
      "  Test Loss: 3.176\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  35 |     0/ 1249 batches | lr 0.000000 | ms/batch 492.80 | loss  0.05 | ppl     1.05 | acc     0.43 | train_ae_norm     1.00\n",
      "[35/200][99/1249] Loss_D: 0.78994584 (Loss_D_real: 0.38142592 Loss_D_fake: 0.40851992) Loss_G: 0.03989179 Loss_Enh_Dec: -0.24198757\n",
      "| epoch  35 |   100/ 1249 batches | lr 0.000000 | ms/batch 357.03 | loss  4.70 | ppl   110.39 | acc     0.43 | train_ae_norm     1.00\n",
      "[35/200][199/1249] Loss_D: 0.80416971 (Loss_D_real: 0.41369817 Loss_D_fake: 0.39047155) Loss_G: 0.04323797 Loss_Enh_Dec: -0.29374123\n",
      "| epoch  35 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.87 | loss  4.90 | ppl   134.03 | acc     0.43 | train_ae_norm     1.00\n",
      "[35/200][299/1249] Loss_D: 0.80900341 (Loss_D_real: 0.41004324 Loss_D_fake: 0.39896017) Loss_G: 0.04185311 Loss_Enh_Dec: -0.25634953\n",
      "| epoch  35 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.45 | loss  4.97 | ppl   144.44 | acc     0.43 | train_ae_norm     1.00\n",
      "[35/200][399/1249] Loss_D: 0.71951813 (Loss_D_real: 0.33603105 Loss_D_fake: 0.38348708) Loss_G: 0.04376043 Loss_Enh_Dec: -0.30774900\n",
      "| epoch  35 |   400/ 1249 batches | lr 0.000000 | ms/batch 356.70 | loss  4.88 | ppl   131.00 | acc     0.46 | train_ae_norm     1.00\n",
      "[35/200][499/1249] Loss_D: 0.74623871 (Loss_D_real: 0.39597505 Loss_D_fake: 0.35026366) Loss_G: 0.04570349 Loss_Enh_Dec: -0.29713666\n",
      "| epoch  35 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.44 | loss  4.86 | ppl   128.83 | acc     0.39 | train_ae_norm     1.00\n",
      "[35/200][599/1249] Loss_D: 0.78847158 (Loss_D_real: 0.40070555 Loss_D_fake: 0.38776600) Loss_G: 0.04687440 Loss_Enh_Dec: -0.24215508\n",
      "| epoch  35 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.40 | loss  4.82 | ppl   124.19 | acc     0.44 | train_ae_norm     1.00\n",
      "[35/200][699/1249] Loss_D: 0.69449818 (Loss_D_real: 0.32386565 Loss_D_fake: 0.37063250) Loss_G: 0.04605671 Loss_Enh_Dec: -0.13200186\n",
      "| epoch  35 |   700/ 1249 batches | lr 0.000000 | ms/batch 355.47 | loss  4.76 | ppl   116.88 | acc     0.41 | train_ae_norm     1.00\n",
      "[35/200][799/1249] Loss_D: 0.75650966 (Loss_D_real: 0.39347383 Loss_D_fake: 0.36303586) Loss_G: 0.04431425 Loss_Enh_Dec: -0.26887795\n",
      "| epoch  35 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.93 | loss  4.74 | ppl   114.27 | acc     0.41 | train_ae_norm     1.00\n",
      "[35/200][899/1249] Loss_D: 0.62895453 (Loss_D_real: 0.29327202 Loss_D_fake: 0.33568248) Loss_G: 0.05653403 Loss_Enh_Dec: -0.27249900\n",
      "| epoch  35 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.40 | loss  4.63 | ppl   102.18 | acc     0.46 | train_ae_norm     1.00\n",
      "[35/200][999/1249] Loss_D: 0.57239687 (Loss_D_real: 0.29799670 Loss_D_fake: 0.27440020) Loss_G: 0.06463331 Loss_Enh_Dec: -0.11136599\n",
      "| epoch  35 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.81 | loss  4.48 | ppl    88.50 | acc     0.50 | train_ae_norm     1.00\n",
      "[35/200][1099/1249] Loss_D: 0.52394539 (Loss_D_real: 0.24632524 Loss_D_fake: 0.27762017) Loss_G: 0.06300249 Loss_Enh_Dec: -0.11141089\n",
      "| epoch  35 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.95 | loss  4.45 | ppl    85.64 | acc     0.48 | train_ae_norm     1.00\n",
      "[35/200][1199/1249] Loss_D: 0.51882136 (Loss_D_real: 0.26422340 Loss_D_fake: 0.25459796) Loss_G: 0.06545331 Loss_Enh_Dec: -0.33876130\n",
      "| epoch  35 |  1200/ 1249 batches | lr 0.000000 | ms/batch 356.67 | loss  4.29 | ppl    72.82 | acc     0.48 | train_ae_norm     1.00\n",
      "| end of epoch  35 | time: 501.13s | test loss  3.68 | test ppl 39.79 | acc 0.603\n",
      "bleu_self:  [0.91875    0.87921154 0.85354053 0.82981509 0.80893057]\n",
      "bleu_test:  [1.68750000e-01 1.28247320e-02 1.04537689e-07 3.03081737e-10\n",
      " 9.25324072e-12]\n",
      "bleu_self: [0.91875000,0.87921154,0.85354053,0.82981509,0.80893057]\n",
      "bleu_test: [0.16875000,0.01282473,0.00000010,0.00000000,0.00000000]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 36 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:27.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:46.\n",
      "\n",
      "  Average training loss generetor: 0.705\n",
      "  Average training loss discriminator: 0.728\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.445\n",
      "  Test Loss: 3.248\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  36 |     0/ 1249 batches | lr 0.000000 | ms/batch 496.78 | loss  0.04 | ppl     1.04 | acc     0.48 | train_ae_norm     1.00\n",
      "[36/200][99/1249] Loss_D: 0.55458772 (Loss_D_real: 0.29008996 Loss_D_fake: 0.26449776) Loss_G: 0.07354724 Loss_Enh_Dec: -0.29302615\n",
      "| epoch  36 |   100/ 1249 batches | lr 0.000000 | ms/batch 357.07 | loss  4.26 | ppl    70.70 | acc     0.51 | train_ae_norm     1.00\n",
      "[36/200][199/1249] Loss_D: 0.44847745 (Loss_D_real: 0.22950581 Loss_D_fake: 0.21897165) Loss_G: 0.07595682 Loss_Enh_Dec: -0.27051926\n",
      "| epoch  36 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.20 | loss  4.33 | ppl    75.80 | acc     0.51 | train_ae_norm     1.00\n",
      "[36/200][299/1249] Loss_D: 0.47601476 (Loss_D_real: 0.25500378 Loss_D_fake: 0.22101098) Loss_G: 0.07538863 Loss_Enh_Dec: -0.15850411\n",
      "| epoch  36 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.53 | loss  4.28 | ppl    72.43 | acc     0.53 | train_ae_norm     1.00\n",
      "[36/200][399/1249] Loss_D: 0.51396948 (Loss_D_real: 0.27562255 Loss_D_fake: 0.23834693) Loss_G: 0.07292961 Loss_Enh_Dec: -0.35527989\n",
      "| epoch  36 |   400/ 1249 batches | lr 0.000000 | ms/batch 356.81 | loss  4.36 | ppl    78.31 | acc     0.51 | train_ae_norm     1.00\n",
      "[36/200][499/1249] Loss_D: 0.51760066 (Loss_D_real: 0.27025414 Loss_D_fake: 0.24734649) Loss_G: 0.07227262 Loss_Enh_Dec: -0.37254906\n",
      "| epoch  36 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.71 | loss  4.36 | ppl    77.94 | acc     0.47 | train_ae_norm     1.00\n",
      "[36/200][599/1249] Loss_D: 0.55843437 (Loss_D_real: 0.29515338 Loss_D_fake: 0.26328102) Loss_G: 0.06724837 Loss_Enh_Dec: -0.31486699\n",
      "| epoch  36 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.87 | loss  4.39 | ppl    80.28 | acc     0.49 | train_ae_norm     1.00\n",
      "[36/200][699/1249] Loss_D: 0.44593659 (Loss_D_real: 0.24132967 Loss_D_fake: 0.20460692) Loss_G: 0.07471032 Loss_Enh_Dec: -0.31383732\n",
      "| epoch  36 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.21 | loss  4.34 | ppl    76.37 | acc     0.45 | train_ae_norm     1.00\n",
      "[36/200][799/1249] Loss_D: 0.44685447 (Loss_D_real: 0.23478982 Loss_D_fake: 0.21206467) Loss_G: 0.07870378 Loss_Enh_Dec: -0.24829245\n",
      "| epoch  36 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.70 | loss  4.40 | ppl    81.81 | acc     0.44 | train_ae_norm     1.00\n",
      "[36/200][899/1249] Loss_D: 0.54123080 (Loss_D_real: 0.28250828 Loss_D_fake: 0.25872254) Loss_G: 0.07197873 Loss_Enh_Dec: -0.29948673\n",
      "| epoch  36 |   900/ 1249 batches | lr 0.000000 | ms/batch 357.42 | loss  4.67 | ppl   106.50 | acc     0.42 | train_ae_norm     1.00\n",
      "[36/200][999/1249] Loss_D: 0.51561189 (Loss_D_real: 0.27286229 Loss_D_fake: 0.24274957) Loss_G: 0.07081111 Loss_Enh_Dec: -0.19775987\n",
      "| epoch  36 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.90 | loss  4.77 | ppl   117.47 | acc     0.45 | train_ae_norm     1.00\n",
      "[36/200][1099/1249] Loss_D: 0.43685746 (Loss_D_real: 0.21816586 Loss_D_fake: 0.21869159) Loss_G: 0.07875001 Loss_Enh_Dec: -0.33017191\n",
      "| epoch  36 |  1100/ 1249 batches | lr 0.000000 | ms/batch 357.54 | loss  4.67 | ppl   106.86 | acc     0.41 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36/200][1199/1249] Loss_D: 0.47056368 (Loss_D_real: 0.25015774 Loss_D_fake: 0.22040594) Loss_G: 0.07538592 Loss_Enh_Dec: -0.38232112\n",
      "| epoch  36 |  1200/ 1249 batches | lr 0.000000 | ms/batch 357.11 | loss  4.65 | ppl   104.71 | acc     0.45 | train_ae_norm     1.00\n",
      "| end of epoch  36 | time: 501.81s | test loss  4.00 | test ppl 54.63 | acc 0.575\n",
      "bleu_self:  [3.26219420e-01 9.77721897e-02 6.30527234e-07 1.63496280e-09\n",
      " 4.68315357e-11]\n",
      "bleu_test:  [9.14772727e-01 1.93988084e-01 3.05658528e-02 3.80875078e-06\n",
      " 1.75665191e-08]\n",
      "bleu_self: [0.32621942,0.09777219,0.00000063,0.00000000,0.00000000]\n",
      "bleu_test: [0.91477273,0.19398808,0.03056585,0.00000381,0.00000002]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 37 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.703\n",
      "  Average training loss discriminator: 0.734\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.435\n",
      "  Test Loss: 3.393\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  37 |     0/ 1249 batches | lr 0.000000 | ms/batch 492.87 | loss  0.05 | ppl     1.05 | acc     0.42 | train_ae_norm     1.00\n",
      "[37/200][99/1249] Loss_D: 0.60836613 (Loss_D_real: 0.31188253 Loss_D_fake: 0.29648358) Loss_G: 0.07160922 Loss_Enh_Dec: -0.35260031\n",
      "| epoch  37 |   100/ 1249 batches | lr 0.000000 | ms/batch 357.37 | loss  4.61 | ppl   100.28 | acc     0.44 | train_ae_norm     1.00\n",
      "[37/200][199/1249] Loss_D: 0.44905013 (Loss_D_real: 0.21761666 Loss_D_fake: 0.23143348) Loss_G: 0.07788768 Loss_Enh_Dec: -0.25104341\n",
      "| epoch  37 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.66 | loss  4.70 | ppl   110.37 | acc     0.44 | train_ae_norm     1.00\n",
      "[37/200][299/1249] Loss_D: 0.47616735 (Loss_D_real: 0.23314866 Loss_D_fake: 0.24301869) Loss_G: 0.07188310 Loss_Enh_Dec: -0.36469746\n",
      "| epoch  37 |   300/ 1249 batches | lr 0.000000 | ms/batch 357.21 | loss  4.79 | ppl   120.13 | acc     0.44 | train_ae_norm     1.00\n",
      "[37/200][399/1249] Loss_D: 0.38144761 (Loss_D_real: 0.19375846 Loss_D_fake: 0.18768916) Loss_G: 0.07903494 Loss_Enh_Dec: -0.38481250\n",
      "| epoch  37 |   400/ 1249 batches | lr 0.000000 | ms/batch 357.71 | loss  4.74 | ppl   113.97 | acc     0.45 | train_ae_norm     1.00\n",
      "[37/200][499/1249] Loss_D: 0.44588923 (Loss_D_real: 0.25060368 Loss_D_fake: 0.19528556) Loss_G: 0.08068067 Loss_Enh_Dec: -0.28379232\n",
      "| epoch  37 |   500/ 1249 batches | lr 0.000000 | ms/batch 357.74 | loss  4.65 | ppl   104.96 | acc     0.44 | train_ae_norm     1.00\n",
      "[37/200][599/1249] Loss_D: 0.33011603 (Loss_D_real: 0.16089299 Loss_D_fake: 0.16922306) Loss_G: 0.09428821 Loss_Enh_Dec: -0.33095884\n",
      "| epoch  37 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.64 | loss  4.58 | ppl    97.42 | acc     0.48 | train_ae_norm     1.00\n",
      "[37/200][699/1249] Loss_D: 0.39697331 (Loss_D_real: 0.22403893 Loss_D_fake: 0.17293438) Loss_G: 0.09257556 Loss_Enh_Dec: -0.25949016\n",
      "| epoch  37 |   700/ 1249 batches | lr 0.000000 | ms/batch 357.80 | loss  4.42 | ppl    83.14 | acc     0.46 | train_ae_norm     1.00\n",
      "[37/200][799/1249] Loss_D: 0.33355641 (Loss_D_real: 0.16185275 Loss_D_fake: 0.17170368) Loss_G: 0.10209844 Loss_Enh_Dec: -0.34905061\n",
      "| epoch  37 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.27 | loss  4.45 | ppl    85.25 | acc     0.46 | train_ae_norm     1.00\n",
      "[37/200][899/1249] Loss_D: 0.28455108 (Loss_D_real: 0.14752999 Loss_D_fake: 0.13702109) Loss_G: 0.10789373 Loss_Enh_Dec: -0.45207548\n",
      "| epoch  37 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.35 | loss  4.29 | ppl    72.98 | acc     0.49 | train_ae_norm     1.00\n",
      "[37/200][999/1249] Loss_D: 0.29777050 (Loss_D_real: 0.15770672 Loss_D_fake: 0.14006378) Loss_G: 0.10453041 Loss_Enh_Dec: -0.36450002\n",
      "| epoch  37 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.60 | loss  4.23 | ppl    68.55 | acc     0.51 | train_ae_norm     1.00\n",
      "[37/200][1099/1249] Loss_D: 0.30489087 (Loss_D_real: 0.14817309 Loss_D_fake: 0.15671776) Loss_G: 0.10859174 Loss_Enh_Dec: -0.30460793\n",
      "| epoch  37 |  1100/ 1249 batches | lr 0.000000 | ms/batch 357.04 | loss  4.20 | ppl    66.84 | acc     0.51 | train_ae_norm     1.00\n",
      "[37/200][1199/1249] Loss_D: 0.26888514 (Loss_D_real: 0.12548575 Loss_D_fake: 0.14339939) Loss_G: 0.10375787 Loss_Enh_Dec: -0.43428165\n",
      "| epoch  37 |  1200/ 1249 batches | lr 0.000000 | ms/batch 356.05 | loss  4.15 | ppl    63.70 | acc     0.52 | train_ae_norm     1.00\n",
      "| end of epoch  37 | time: 501.67s | test loss  3.56 | test ppl 35.15 | acc 0.610\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 38 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.749\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.420\n",
      "  Test Loss: 3.333\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  38 |     0/ 1249 batches | lr 0.000000 | ms/batch 489.80 | loss  0.04 | ppl     1.04 | acc     0.50 | train_ae_norm     1.00\n",
      "[38/200][99/1249] Loss_D: 0.26706666 (Loss_D_real: 0.11505918 Loss_D_fake: 0.15200748) Loss_G: 0.11401874 Loss_Enh_Dec: -0.41927701\n",
      "| epoch  38 |   100/ 1249 batches | lr 0.000000 | ms/batch 357.08 | loss  4.12 | ppl    61.69 | acc     0.54 | train_ae_norm     1.00\n",
      "[38/200][199/1249] Loss_D: 0.25112975 (Loss_D_real: 0.11853951 Loss_D_fake: 0.13259023) Loss_G: 0.11327143 Loss_Enh_Dec: -0.42184082\n",
      "| epoch  38 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.39 | loss  4.15 | ppl    63.20 | acc     0.52 | train_ae_norm     1.00\n",
      "[38/200][299/1249] Loss_D: 0.23795652 (Loss_D_real: 0.10940512 Loss_D_fake: 0.12855141) Loss_G: 0.11123555 Loss_Enh_Dec: -0.30917475\n",
      "| epoch  38 |   300/ 1249 batches | lr 0.000000 | ms/batch 357.26 | loss  4.18 | ppl    65.33 | acc     0.51 | train_ae_norm     1.00\n",
      "[38/200][399/1249] Loss_D: 0.30842769 (Loss_D_real: 0.12838019 Loss_D_fake: 0.18004751) Loss_G: 0.11024940 Loss_Enh_Dec: -0.44056389\n",
      "| epoch  38 |   400/ 1249 batches | lr 0.000000 | ms/batch 356.59 | loss  4.13 | ppl    62.09 | acc     0.53 | train_ae_norm     1.00\n",
      "[38/200][499/1249] Loss_D: 0.22243629 (Loss_D_real: 0.11940243 Loss_D_fake: 0.10303386) Loss_G: 0.12526599 Loss_Enh_Dec: -0.48155257\n",
      "| epoch  38 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.71 | loss  4.22 | ppl    67.91 | acc     0.49 | train_ae_norm     1.00\n",
      "[38/200][599/1249] Loss_D: 0.24877560 (Loss_D_real: 0.12795776 Loss_D_fake: 0.12081784) Loss_G: 0.13165458 Loss_Enh_Dec: -0.40525436\n",
      "| epoch  38 |   600/ 1249 batches | lr 0.000000 | ms/batch 357.54 | loss  4.16 | ppl    64.10 | acc     0.51 | train_ae_norm     1.00\n",
      "[38/200][699/1249] Loss_D: 0.21807759 (Loss_D_real: 0.08712919 Loss_D_fake: 0.13094839) Loss_G: 0.12663554 Loss_Enh_Dec: -0.39898181\n",
      "| epoch  38 |   700/ 1249 batches | lr 0.000000 | ms/batch 357.22 | loss  4.17 | ppl    65.01 | acc     0.48 | train_ae_norm     1.00\n",
      "[38/200][799/1249] Loss_D: 0.40328866 (Loss_D_real: 0.26581901 Loss_D_fake: 0.13746966) Loss_G: 0.11870741 Loss_Enh_Dec: -0.41460434\n",
      "| epoch  38 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.74 | loss  4.29 | ppl    72.95 | acc     0.49 | train_ae_norm     1.00\n",
      "[38/200][899/1249] Loss_D: 0.20656621 (Loss_D_real: 0.08871019 Loss_D_fake: 0.11785603) Loss_G: 0.13043590 Loss_Enh_Dec: -0.34401226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  38 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.42 | loss  4.21 | ppl    67.47 | acc     0.51 | train_ae_norm     1.00\n",
      "[38/200][999/1249] Loss_D: 0.28441811 (Loss_D_real: 0.08942701 Loss_D_fake: 0.19499111) Loss_G: 0.13090794 Loss_Enh_Dec: -0.56661940\n",
      "| epoch  38 |  1000/ 1249 batches | lr 0.000000 | ms/batch 358.34 | loss  4.16 | ppl    63.79 | acc     0.53 | train_ae_norm     1.00\n",
      "[38/200][1099/1249] Loss_D: 0.19971035 (Loss_D_real: 0.08100187 Loss_D_fake: 0.11870848) Loss_G: 0.11293755 Loss_Enh_Dec: -0.53395838\n",
      "| epoch  38 |  1100/ 1249 batches | lr 0.000000 | ms/batch 357.73 | loss  4.22 | ppl    67.94 | acc     0.49 | train_ae_norm     1.00\n",
      "[38/200][1199/1249] Loss_D: 0.24315208 (Loss_D_real: 0.12635913 Loss_D_fake: 0.11679294) Loss_G: 0.13575430 Loss_Enh_Dec: -0.49698314\n",
      "| epoch  38 |  1200/ 1249 batches | lr 0.000000 | ms/batch 357.30 | loss  4.21 | ppl    67.29 | acc     0.49 | train_ae_norm     1.00\n",
      "| end of epoch  38 | time: 501.51s | test loss  3.59 | test ppl 36.40 | acc 0.609\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 39 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.703\n",
      "  Average training loss discriminator: 0.726\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.445\n",
      "  Test Loss: 3.426\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  39 |     0/ 1249 batches | lr 0.000000 | ms/batch 490.94 | loss  0.04 | ppl     1.04 | acc     0.48 | train_ae_norm     1.00\n",
      "[39/200][99/1249] Loss_D: 0.25398290 (Loss_D_real: 0.11860834 Loss_D_fake: 0.13537455) Loss_G: 0.12417803 Loss_Enh_Dec: -0.47660151\n",
      "| epoch  39 |   100/ 1249 batches | lr 0.000000 | ms/batch 357.10 | loss  4.30 | ppl    73.69 | acc     0.51 | train_ae_norm     1.00\n",
      "[39/200][199/1249] Loss_D: 0.25599891 (Loss_D_real: 0.08400460 Loss_D_fake: 0.17199433) Loss_G: 0.12036089 Loss_Enh_Dec: -0.49832639\n",
      "| epoch  39 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.23 | loss  4.31 | ppl    74.77 | acc     0.49 | train_ae_norm     1.00\n",
      "[39/200][299/1249] Loss_D: 0.22826701 (Loss_D_real: 0.08823367 Loss_D_fake: 0.14003333) Loss_G: 0.12068126 Loss_Enh_Dec: -0.52995884\n",
      "| epoch  39 |   300/ 1249 batches | lr 0.000000 | ms/batch 357.73 | loss  4.40 | ppl    81.11 | acc     0.48 | train_ae_norm     1.00\n",
      "[39/200][399/1249] Loss_D: 0.28038898 (Loss_D_real: 0.13241401 Loss_D_fake: 0.14797497) Loss_G: 0.11008047 Loss_Enh_Dec: -0.51843256\n",
      "| epoch  39 |   400/ 1249 batches | lr 0.000000 | ms/batch 357.53 | loss  4.47 | ppl    87.69 | acc     0.50 | train_ae_norm     1.00\n",
      "[39/200][499/1249] Loss_D: 0.24595799 (Loss_D_real: 0.12467010 Loss_D_fake: 0.12128788) Loss_G: 0.12928143 Loss_Enh_Dec: -0.53031820\n",
      "| epoch  39 |   500/ 1249 batches | lr 0.000000 | ms/batch 358.22 | loss  4.49 | ppl    88.99 | acc     0.49 | train_ae_norm     1.00\n",
      "[39/200][599/1249] Loss_D: 0.28923070 (Loss_D_real: 0.13627699 Loss_D_fake: 0.15295373) Loss_G: 0.11714073 Loss_Enh_Dec: -0.56729794\n",
      "| epoch  39 |   600/ 1249 batches | lr 0.000000 | ms/batch 357.95 | loss  4.42 | ppl    82.88 | acc     0.49 | train_ae_norm     1.00\n",
      "[39/200][699/1249] Loss_D: 0.27675056 (Loss_D_real: 0.17707714 Loss_D_fake: 0.09967341) Loss_G: 0.13084097 Loss_Enh_Dec: -0.59855211\n",
      "| epoch  39 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.86 | loss  4.37 | ppl    79.04 | acc     0.44 | train_ae_norm     1.00\n",
      "[39/200][799/1249] Loss_D: 0.16238488 (Loss_D_real: 0.06969836 Loss_D_fake: 0.09268653) Loss_G: 0.12819779 Loss_Enh_Dec: -0.49307081\n",
      "| epoch  39 |   800/ 1249 batches | lr 0.000000 | ms/batch 357.39 | loss  4.31 | ppl    74.31 | acc     0.49 | train_ae_norm     1.00\n",
      "[39/200][899/1249] Loss_D: 0.14302552 (Loss_D_real: 0.07055547 Loss_D_fake: 0.07247005) Loss_G: 0.13799906 Loss_Enh_Dec: -0.43353686\n",
      "| epoch  39 |   900/ 1249 batches | lr 0.000000 | ms/batch 357.14 | loss  4.11 | ppl    60.99 | acc     0.51 | train_ae_norm     1.00\n",
      "[39/200][999/1249] Loss_D: 0.20414536 (Loss_D_real: 0.09770662 Loss_D_fake: 0.10643874) Loss_G: 0.14118746 Loss_Enh_Dec: -0.55510652\n",
      "| epoch  39 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.76 | loss  4.07 | ppl    58.46 | acc     0.54 | train_ae_norm     1.00\n",
      "[39/200][1099/1249] Loss_D: 0.19835950 (Loss_D_real: 0.09408686 Loss_D_fake: 0.10427265) Loss_G: 0.13427852 Loss_Enh_Dec: -0.56348652\n",
      "| epoch  39 |  1100/ 1249 batches | lr 0.000000 | ms/batch 357.52 | loss  4.13 | ppl    61.98 | acc     0.45 | train_ae_norm     1.00\n",
      "[39/200][1199/1249] Loss_D: 0.20742868 (Loss_D_real: 0.12455080 Loss_D_fake: 0.08287787) Loss_G: 0.13507915 Loss_Enh_Dec: -0.61265320\n",
      "| epoch  39 |  1200/ 1249 batches | lr 0.000000 | ms/batch 356.62 | loss  4.15 | ppl    63.50 | acc     0.49 | train_ae_norm     1.00\n",
      "| end of epoch  39 | time: 501.81s | test loss  3.53 | test ppl 34.05 | acc 0.607\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 40 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.703\n",
      "  Average training loss discriminator: 0.723\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.427\n",
      "  Test Loss: 3.499\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  40 |     0/ 1249 batches | lr 0.000000 | ms/batch 492.95 | loss  0.04 | ppl     1.04 | acc     0.48 | train_ae_norm     1.00\n",
      "[40/200][99/1249] Loss_D: 0.16921151 (Loss_D_real: 0.08547837 Loss_D_fake: 0.08373314) Loss_G: 0.15469573 Loss_Enh_Dec: -0.58650732\n",
      "| epoch  40 |   100/ 1249 batches | lr 0.000000 | ms/batch 357.26 | loss  4.10 | ppl    60.49 | acc     0.53 | train_ae_norm     1.00\n",
      "[40/200][199/1249] Loss_D: 0.14181437 (Loss_D_real: 0.07697839 Loss_D_fake: 0.06483598) Loss_G: 0.15161528 Loss_Enh_Dec: -0.52763408\n",
      "| epoch  40 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.20 | loss  4.10 | ppl    60.59 | acc     0.52 | train_ae_norm     1.00\n",
      "[40/200][299/1249] Loss_D: 0.19942525 (Loss_D_real: 0.12190521 Loss_D_fake: 0.07752004) Loss_G: 0.14909834 Loss_Enh_Dec: -0.51633942\n",
      "| epoch  40 |   300/ 1249 batches | lr 0.000000 | ms/batch 357.76 | loss  4.02 | ppl    55.95 | acc     0.54 | train_ae_norm     1.00\n",
      "[40/200][399/1249] Loss_D: 0.20322490 (Loss_D_real: 0.10978520 Loss_D_fake: 0.09343969) Loss_G: 0.15518184 Loss_Enh_Dec: -0.35457513\n",
      "| epoch  40 |   400/ 1249 batches | lr 0.000000 | ms/batch 356.51 | loss  3.87 | ppl    47.84 | acc     0.60 | train_ae_norm     1.00\n",
      "[40/200][499/1249] Loss_D: 0.13148767 (Loss_D_real: 0.07800220 Loss_D_fake: 0.05348546) Loss_G: 0.16944484 Loss_Enh_Dec: -0.56255764\n",
      "| epoch  40 |   500/ 1249 batches | lr 0.000000 | ms/batch 355.96 | loss  3.83 | ppl    46.08 | acc     0.57 | train_ae_norm     1.00\n",
      "[40/200][599/1249] Loss_D: 0.13974282 (Loss_D_real: 0.07364150 Loss_D_fake: 0.06610131) Loss_G: 0.16212834 Loss_Enh_Dec: -0.55832976\n",
      "| epoch  40 |   600/ 1249 batches | lr 0.000000 | ms/batch 357.17 | loss  3.79 | ppl    44.24 | acc     0.60 | train_ae_norm     1.00\n",
      "[40/200][699/1249] Loss_D: 0.16615319 (Loss_D_real: 0.08226483 Loss_D_fake: 0.08388835) Loss_G: 0.14586666 Loss_Enh_Dec: -0.58609825\n",
      "| epoch  40 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.58 | loss  3.75 | ppl    42.65 | acc     0.55 | train_ae_norm     1.00\n",
      "[40/200][799/1249] Loss_D: 0.15741777 (Loss_D_real: 0.05654967 Loss_D_fake: 0.10086810) Loss_G: 0.14896670 Loss_Enh_Dec: -0.59768164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  40 |   800/ 1249 batches | lr 0.000000 | ms/batch 357.04 | loss  3.74 | ppl    42.05 | acc     0.52 | train_ae_norm     1.00\n",
      "[40/200][899/1249] Loss_D: 0.18862905 (Loss_D_real: 0.09039322 Loss_D_fake: 0.09823583) Loss_G: 0.17271402 Loss_Enh_Dec: -0.51773590\n",
      "| epoch  40 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.85 | loss  3.65 | ppl    38.53 | acc     0.57 | train_ae_norm     1.00\n",
      "[40/200][999/1249] Loss_D: 0.13490355 (Loss_D_real: 0.05707258 Loss_D_fake: 0.07783097) Loss_G: 0.16513656 Loss_Enh_Dec: -0.64228326\n",
      "| epoch  40 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.58 | loss  3.70 | ppl    40.36 | acc     0.56 | train_ae_norm     1.00\n",
      "[40/200][1099/1249] Loss_D: 0.13241655 (Loss_D_real: 0.08390574 Loss_D_fake: 0.04851080) Loss_G: 0.19564545 Loss_Enh_Dec: -0.75887316\n",
      "| epoch  40 |  1100/ 1249 batches | lr 0.000000 | ms/batch 357.49 | loss  3.79 | ppl    44.46 | acc     0.55 | train_ae_norm     1.00\n",
      "[40/200][1199/1249] Loss_D: 0.09983705 (Loss_D_real: 0.04599342 Loss_D_fake: 0.05384363) Loss_G: 0.17642461 Loss_Enh_Dec: -0.70885944\n",
      "| epoch  40 |  1200/ 1249 batches | lr 0.000000 | ms/batch 357.16 | loss  3.74 | ppl    41.93 | acc     0.55 | train_ae_norm     1.00\n",
      "| end of epoch  40 | time: 501.51s | test loss  3.13 | test ppl 22.94 | acc 0.656\n",
      "bleu_self:  [0.95       0.89748238 0.81643438 0.70473704 0.58911959]\n",
      "bleu_test:  [3.68750000e-01 1.21049969e-01 7.90207514e-07 2.05608369e-09\n",
      " 5.86382478e-11]\n",
      "bleu_self: [0.95000000,0.89748238,0.81643438,0.70473704,0.58911959]\n",
      "bleu_test: [0.36875000,0.12104997,0.00000079,0.00000000,0.00000000]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 41 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:27.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:31.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:46.\n",
      "\n",
      "  Average training loss generetor: 0.704\n",
      "  Average training loss discriminator: 0.720\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.435\n",
      "  Test Loss: 3.582\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  41 |     0/ 1249 batches | lr 0.000000 | ms/batch 498.11 | loss  0.04 | ppl     1.04 | acc     0.54 | train_ae_norm     1.00\n",
      "[41/200][99/1249] Loss_D: 0.15144107 (Loss_D_real: 0.08947997 Loss_D_fake: 0.06196109) Loss_G: 0.16522382 Loss_Enh_Dec: -0.49155673\n",
      "| epoch  41 |   100/ 1249 batches | lr 0.000000 | ms/batch 357.35 | loss  3.72 | ppl    41.34 | acc     0.55 | train_ae_norm     1.00\n",
      "[41/200][199/1249] Loss_D: 0.10083668 (Loss_D_real: 0.05949429 Loss_D_fake: 0.04134239) Loss_G: 0.17763226 Loss_Enh_Dec: -0.65452856\n",
      "| epoch  41 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.84 | loss  3.75 | ppl    42.35 | acc     0.57 | train_ae_norm     1.00\n",
      "[41/200][299/1249] Loss_D: 0.13499880 (Loss_D_real: 0.07610661 Loss_D_fake: 0.05889220) Loss_G: 0.17549156 Loss_Enh_Dec: -0.53982419\n",
      "| epoch  41 |   300/ 1249 batches | lr 0.000000 | ms/batch 357.33 | loss  3.75 | ppl    42.58 | acc     0.54 | train_ae_norm     1.00\n",
      "[41/200][399/1249] Loss_D: 0.19059899 (Loss_D_real: 0.12716167 Loss_D_fake: 0.06343732) Loss_G: 0.17236552 Loss_Enh_Dec: -0.77944297\n",
      "| epoch  41 |   400/ 1249 batches | lr 0.000000 | ms/batch 357.38 | loss  3.70 | ppl    40.45 | acc     0.59 | train_ae_norm     1.00\n",
      "[41/200][499/1249] Loss_D: 0.16444366 (Loss_D_real: 0.07989731 Loss_D_fake: 0.08454635) Loss_G: 0.16145058 Loss_Enh_Dec: -0.75217247\n",
      "| epoch  41 |   500/ 1249 batches | lr 0.000000 | ms/batch 357.07 | loss  3.77 | ppl    43.57 | acc     0.56 | train_ae_norm     1.00\n",
      "[41/200][599/1249] Loss_D: 0.10344292 (Loss_D_real: 0.03081871 Loss_D_fake: 0.07262421) Loss_G: 0.17336683 Loss_Enh_Dec: -0.75127602\n",
      "| epoch  41 |   600/ 1249 batches | lr 0.000000 | ms/batch 357.87 | loss  3.83 | ppl    46.24 | acc     0.58 | train_ae_norm     1.00\n",
      "[41/200][699/1249] Loss_D: 0.12096131 (Loss_D_real: 0.06017788 Loss_D_fake: 0.06078342) Loss_G: 0.17860955 Loss_Enh_Dec: -0.52585286\n",
      "| epoch  41 |   700/ 1249 batches | lr 0.000000 | ms/batch 357.09 | loss  3.76 | ppl    42.88 | acc     0.53 | train_ae_norm     1.00\n",
      "[41/200][799/1249] Loss_D: 0.09285916 (Loss_D_real: 0.03887966 Loss_D_fake: 0.05397950) Loss_G: 0.18584383 Loss_Enh_Dec: -0.57868880\n",
      "| epoch  41 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.70 | loss  3.73 | ppl    41.48 | acc     0.53 | train_ae_norm     1.00\n",
      "[41/200][899/1249] Loss_D: 0.09558903 (Loss_D_real: 0.05203494 Loss_D_fake: 0.04355408) Loss_G: 0.18190430 Loss_Enh_Dec: -0.58474237\n",
      "| epoch  41 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.72 | loss  3.61 | ppl    36.99 | acc     0.55 | train_ae_norm     1.00\n",
      "[41/200][999/1249] Loss_D: 0.08049558 (Loss_D_real: 0.04264021 Loss_D_fake: 0.03785538) Loss_G: 0.17520368 Loss_Enh_Dec: -0.50660652\n",
      "| epoch  41 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.35 | loss  3.69 | ppl    39.96 | acc     0.59 | train_ae_norm     1.00\n",
      "[41/200][1099/1249] Loss_D: 0.09859731 (Loss_D_real: 0.04182338 Loss_D_fake: 0.05677393) Loss_G: 0.18839058 Loss_Enh_Dec: -0.58017272\n",
      "| epoch  41 |  1100/ 1249 batches | lr 0.000000 | ms/batch 357.23 | loss  3.76 | ppl    42.88 | acc     0.55 | train_ae_norm     1.00\n",
      "[41/200][1199/1249] Loss_D: 0.07960299 (Loss_D_real: 0.03033568 Loss_D_fake: 0.04926732) Loss_G: 0.19892821 Loss_Enh_Dec: -0.63539135\n",
      "| epoch  41 |  1200/ 1249 batches | lr 0.000000 | ms/batch 356.99 | loss  3.75 | ppl    42.33 | acc     0.52 | train_ae_norm     1.00\n",
      "| end of epoch  41 | time: 502.36s | test loss  3.10 | test ppl 22.19 | acc 0.664\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 42 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.704\n",
      "  Average training loss discriminator: 0.719\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.443\n",
      "  Test Loss: 3.627\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  42 |     0/ 1249 batches | lr 0.000000 | ms/batch 490.01 | loss  0.04 | ppl     1.04 | acc     0.51 | train_ae_norm     1.00\n",
      "[42/200][99/1249] Loss_D: 0.10145765 (Loss_D_real: 0.06158686 Loss_D_fake: 0.03987078) Loss_G: 0.18519081 Loss_Enh_Dec: -0.64508265\n",
      "| epoch  42 |   100/ 1249 batches | lr 0.000000 | ms/batch 357.24 | loss  3.79 | ppl    44.22 | acc     0.55 | train_ae_norm     1.00\n",
      "[42/200][199/1249] Loss_D: 0.06446394 (Loss_D_real: 0.02614634 Loss_D_fake: 0.03831759) Loss_G: 0.18846312 Loss_Enh_Dec: -0.82214016\n",
      "| epoch  42 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.54 | loss  3.68 | ppl    39.76 | acc     0.61 | train_ae_norm     1.00\n",
      "[42/200][299/1249] Loss_D: 0.08489420 (Loss_D_real: 0.05357000 Loss_D_fake: 0.03132419) Loss_G: 0.19923300 Loss_Enh_Dec: -0.77365953\n",
      "| epoch  42 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.95 | loss  3.62 | ppl    37.22 | acc     0.58 | train_ae_norm     1.00\n",
      "[42/200][399/1249] Loss_D: 0.07262062 (Loss_D_real: 0.03179639 Loss_D_fake: 0.04082423) Loss_G: 0.19130313 Loss_Enh_Dec: -0.92847103\n",
      "| epoch  42 |   400/ 1249 batches | lr 0.000000 | ms/batch 357.15 | loss  3.55 | ppl    34.93 | acc     0.61 | train_ae_norm     1.00\n",
      "[42/200][499/1249] Loss_D: 0.08205017 (Loss_D_real: 0.04464642 Loss_D_fake: 0.03740375) Loss_G: 0.19085531 Loss_Enh_Dec: -0.88001043\n",
      "| epoch  42 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.85 | loss  3.57 | ppl    35.39 | acc     0.57 | train_ae_norm     1.00\n",
      "[42/200][599/1249] Loss_D: 0.05931380 (Loss_D_real: 0.03056374 Loss_D_fake: 0.02875006) Loss_G: 0.20018005 Loss_Enh_Dec: -0.87410641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  42 |   600/ 1249 batches | lr 0.000000 | ms/batch 357.36 | loss  3.58 | ppl    35.75 | acc     0.58 | train_ae_norm     1.00\n",
      "[42/200][699/1249] Loss_D: 0.05062533 (Loss_D_real: 0.01436819 Loss_D_fake: 0.03625714) Loss_G: 0.19437598 Loss_Enh_Dec: -0.93526602\n",
      "| epoch  42 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.16 | loss  3.59 | ppl    36.16 | acc     0.58 | train_ae_norm     1.00\n",
      "[42/200][799/1249] Loss_D: 0.05877088 (Loss_D_real: 0.03770074 Loss_D_fake: 0.02107013) Loss_G: 0.22612877 Loss_Enh_Dec: -0.79131383\n",
      "| epoch  42 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.56 | loss  3.58 | ppl    35.96 | acc     0.57 | train_ae_norm     1.00\n",
      "[42/200][899/1249] Loss_D: 0.05190156 (Loss_D_real: 0.02939953 Loss_D_fake: 0.02250202) Loss_G: 0.21896134 Loss_Enh_Dec: -1.01020658\n",
      "| epoch  42 |   900/ 1249 batches | lr 0.000000 | ms/batch 357.23 | loss  3.49 | ppl    32.87 | acc     0.57 | train_ae_norm     1.00\n",
      "[42/200][999/1249] Loss_D: 0.10998958 (Loss_D_real: 0.05558128 Loss_D_fake: 0.05440829) Loss_G: 0.18222727 Loss_Enh_Dec: -0.63414919\n",
      "| epoch  42 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.71 | loss  3.46 | ppl    31.73 | acc     0.60 | train_ae_norm     1.00\n",
      "[42/200][1099/1249] Loss_D: 0.04741922 (Loss_D_real: 0.02542767 Loss_D_fake: 0.02199154) Loss_G: 0.22278033 Loss_Enh_Dec: -0.89008343\n",
      "| epoch  42 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.61 | loss  3.44 | ppl    31.18 | acc     0.59 | train_ae_norm     1.00\n",
      "[42/200][1199/1249] Loss_D: 0.03902099 (Loss_D_real: 0.01488034 Loss_D_fake: 0.02414065) Loss_G: 0.20891391 Loss_Enh_Dec: -0.99497712\n",
      "| epoch  42 |  1200/ 1249 batches | lr 0.000000 | ms/batch 356.75 | loss  3.35 | ppl    28.40 | acc     0.61 | train_ae_norm     1.00\n",
      "| end of epoch  42 | time: 501.29s | test loss  2.84 | test ppl 17.06 | acc 0.689\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 43 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.722\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.445\n",
      "  Test Loss: 3.686\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  43 |     0/ 1249 batches | lr 0.000000 | ms/batch 489.17 | loss  0.03 | ppl     1.03 | acc     0.60 | train_ae_norm     1.00\n",
      "[43/200][99/1249] Loss_D: 0.05090203 (Loss_D_real: 0.02330797 Loss_D_fake: 0.02759406) Loss_G: 0.21483777 Loss_Enh_Dec: -0.83244818\n",
      "| epoch  43 |   100/ 1249 batches | lr 0.000000 | ms/batch 357.91 | loss  3.34 | ppl    28.29 | acc     0.60 | train_ae_norm     1.00\n",
      "[43/200][199/1249] Loss_D: 0.04173312 (Loss_D_real: 0.02342617 Loss_D_fake: 0.01830696) Loss_G: 0.21693146 Loss_Enh_Dec: -0.89970982\n",
      "| epoch  43 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.72 | loss  3.41 | ppl    30.30 | acc     0.63 | train_ae_norm     1.00\n",
      "[43/200][299/1249] Loss_D: 0.05986840 (Loss_D_real: 0.03200292 Loss_D_fake: 0.02786548) Loss_G: 0.22693899 Loss_Enh_Dec: -0.87307894\n",
      "| epoch  43 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.62 | loss  3.44 | ppl    31.13 | acc     0.60 | train_ae_norm     1.00\n",
      "[43/200][399/1249] Loss_D: 0.05247261 (Loss_D_real: 0.02216594 Loss_D_fake: 0.03030666) Loss_G: 0.20345514 Loss_Enh_Dec: -0.85152781\n",
      "| epoch  43 |   400/ 1249 batches | lr 0.000000 | ms/batch 357.20 | loss  3.44 | ppl    31.04 | acc     0.63 | train_ae_norm     1.00\n",
      "[43/200][499/1249] Loss_D: 0.05290226 (Loss_D_real: 0.02939511 Loss_D_fake: 0.02350714) Loss_G: 0.22187459 Loss_Enh_Dec: -0.96623671\n",
      "| epoch  43 |   500/ 1249 batches | lr 0.000000 | ms/batch 357.32 | loss  3.54 | ppl    34.32 | acc     0.56 | train_ae_norm     1.00\n",
      "[43/200][599/1249] Loss_D: 0.05451315 (Loss_D_real: 0.02193214 Loss_D_fake: 0.03258100) Loss_G: 0.20331180 Loss_Enh_Dec: -1.01007068\n",
      "| epoch  43 |   600/ 1249 batches | lr 0.000000 | ms/batch 357.87 | loss  3.67 | ppl    39.23 | acc     0.58 | train_ae_norm     1.00\n",
      "[43/200][699/1249] Loss_D: 0.04543883 (Loss_D_real: 0.01992255 Loss_D_fake: 0.02551628) Loss_G: 0.20458071 Loss_Enh_Dec: -0.94626886\n",
      "| epoch  43 |   700/ 1249 batches | lr 0.000000 | ms/batch 357.84 | loss  3.80 | ppl    44.72 | acc     0.57 | train_ae_norm     1.00\n",
      "[43/200][799/1249] Loss_D: 0.07507321 (Loss_D_real: 0.03064067 Loss_D_fake: 0.04443254) Loss_G: 0.17412648 Loss_Enh_Dec: -0.72107899\n",
      "| epoch  43 |   800/ 1249 batches | lr 0.000000 | ms/batch 357.47 | loss  3.77 | ppl    43.30 | acc     0.53 | train_ae_norm     1.00\n",
      "[43/200][899/1249] Loss_D: 0.11357590 (Loss_D_real: 0.01865774 Loss_D_fake: 0.09491816) Loss_G: 0.16601957 Loss_Enh_Dec: -0.65117216\n",
      "| epoch  43 |   900/ 1249 batches | lr 0.000000 | ms/batch 357.07 | loss  3.60 | ppl    36.75 | acc     0.55 | train_ae_norm     1.00\n",
      "[43/200][999/1249] Loss_D: 0.04710957 (Loss_D_real: 0.02781754 Loss_D_fake: 0.01929203) Loss_G: 0.20995159 Loss_Enh_Dec: -0.76660532\n",
      "| epoch  43 |  1000/ 1249 batches | lr 0.000000 | ms/batch 357.30 | loss  3.55 | ppl    34.96 | acc     0.61 | train_ae_norm     1.00\n",
      "[43/200][1099/1249] Loss_D: 0.10313140 (Loss_D_real: 0.03521256 Loss_D_fake: 0.06791884) Loss_G: 0.15037303 Loss_Enh_Dec: -0.49299279\n",
      "| epoch  43 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.83 | loss  3.60 | ppl    36.48 | acc     0.58 | train_ae_norm     1.00\n",
      "[43/200][1199/1249] Loss_D: 0.06889257 (Loss_D_real: 0.03078421 Loss_D_fake: 0.03810836) Loss_G: 0.18608151 Loss_Enh_Dec: -0.73015398\n",
      "| epoch  43 |  1200/ 1249 batches | lr 0.000000 | ms/batch 356.46 | loss  3.58 | ppl    35.86 | acc     0.58 | train_ae_norm     1.00\n",
      "| end of epoch  43 | time: 501.58s | test loss  2.92 | test ppl 18.51 | acc 0.683\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 44 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.703\n",
      "  Average training loss discriminator: 0.718\n",
      "  Training epcoh took: 0:00:48\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.448\n",
      "  Test Loss: 3.733\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  44 |     0/ 1249 batches | lr 0.000000 | ms/batch 488.34 | loss  0.03 | ppl     1.04 | acc     0.57 | train_ae_norm     1.00\n",
      "[44/200][99/1249] Loss_D: 0.16570579 (Loss_D_real: 0.04274677 Loss_D_fake: 0.12295901) Loss_G: 0.16933393 Loss_Enh_Dec: -0.55066937\n",
      "| epoch  44 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.83 | loss  3.53 | ppl    34.12 | acc     0.59 | train_ae_norm     1.00\n",
      "[44/200][199/1249] Loss_D: 0.09520647 (Loss_D_real: 0.06446316 Loss_D_fake: 0.03074331) Loss_G: 0.18931721 Loss_Enh_Dec: -0.62331247\n",
      "| epoch  44 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.29 | loss  3.53 | ppl    34.13 | acc     0.61 | train_ae_norm     1.00\n",
      "[44/200][299/1249] Loss_D: 0.07891278 (Loss_D_real: 0.05140715 Loss_D_fake: 0.02750563) Loss_G: 0.19101089 Loss_Enh_Dec: -0.69734812\n",
      "| epoch  44 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.94 | loss  3.59 | ppl    36.19 | acc     0.59 | train_ae_norm     1.00\n",
      "[44/200][399/1249] Loss_D: 0.04154974 (Loss_D_real: 0.01849497 Loss_D_fake: 0.02305477) Loss_G: 0.21578903 Loss_Enh_Dec: -0.82831383\n",
      "| epoch  44 |   400/ 1249 batches | lr 0.000000 | ms/batch 357.64 | loss  3.53 | ppl    34.19 | acc     0.64 | train_ae_norm     1.00\n",
      "[44/200][499/1249] Loss_D: 0.05527336 (Loss_D_real: 0.01915770 Loss_D_fake: 0.03611566) Loss_G: 0.17840789 Loss_Enh_Dec: -0.72573709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  44 |   500/ 1249 batches | lr 0.000000 | ms/batch 357.13 | loss  3.58 | ppl    35.73 | acc     0.58 | train_ae_norm     1.00\n",
      "[44/200][599/1249] Loss_D: 0.06739780 (Loss_D_real: 0.03142761 Loss_D_fake: 0.03597020) Loss_G: 0.17947040 Loss_Enh_Dec: -0.67941600\n",
      "| epoch  44 |   600/ 1249 batches | lr 0.000000 | ms/batch 357.16 | loss  3.54 | ppl    34.63 | acc     0.61 | train_ae_norm     1.00\n",
      "[44/200][699/1249] Loss_D: 0.04396740 (Loss_D_real: 0.02267001 Loss_D_fake: 0.02129739) Loss_G: 0.20674554 Loss_Enh_Dec: -0.78970259\n",
      "| epoch  44 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.93 | loss  3.54 | ppl    34.58 | acc     0.58 | train_ae_norm     1.00\n",
      "[44/200][799/1249] Loss_D: 0.05063368 (Loss_D_real: 0.02223824 Loss_D_fake: 0.02839543) Loss_G: 0.21188143 Loss_Enh_Dec: -0.75258160\n",
      "| epoch  44 |   800/ 1249 batches | lr 0.000000 | ms/batch 357.57 | loss  3.60 | ppl    36.54 | acc     0.53 | train_ae_norm     1.00\n",
      "[44/200][899/1249] Loss_D: 0.06624012 (Loss_D_real: 0.02978012 Loss_D_fake: 0.03646001) Loss_G: 0.18140063 Loss_Enh_Dec: -0.65351743\n",
      "| epoch  44 |   900/ 1249 batches | lr 0.000000 | ms/batch 358.14 | loss  3.54 | ppl    34.55 | acc     0.57 | train_ae_norm     1.00\n",
      "[44/200][999/1249] Loss_D: 0.04593639 (Loss_D_real: 0.02274627 Loss_D_fake: 0.02319012) Loss_G: 0.21495628 Loss_Enh_Dec: -0.74344152\n",
      "| epoch  44 |  1000/ 1249 batches | lr 0.000000 | ms/batch 357.33 | loss  3.55 | ppl    34.72 | acc     0.58 | train_ae_norm     1.00\n",
      "[44/200][1099/1249] Loss_D: 0.06208394 (Loss_D_real: 0.04088189 Loss_D_fake: 0.02120206) Loss_G: 0.21268158 Loss_Enh_Dec: -0.78113508\n",
      "| epoch  44 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.41 | loss  3.52 | ppl    33.87 | acc     0.59 | train_ae_norm     1.00\n",
      "[44/200][1199/1249] Loss_D: 0.08471440 (Loss_D_real: 0.06127747 Loss_D_fake: 0.02343693) Loss_G: 0.20186858 Loss_Enh_Dec: -0.96095431\n",
      "| epoch  44 |  1200/ 1249 batches | lr 0.000000 | ms/batch 357.26 | loss  3.54 | ppl    34.40 | acc     0.58 | train_ae_norm     1.00\n",
      "| end of epoch  44 | time: 501.46s | test loss  3.11 | test ppl 22.39 | acc 0.673\n",
      "bleu_self:  [0.79801257 0.64789386 0.56132738 0.50001228 0.50000471]\n",
      "bleu_test:  [8.99999999e-01 1.92353628e-08 2.50326129e-08 4.44589533e-08\n",
      " 6.28029918e-08]\n",
      "bleu_self: [0.79801257,0.64789386,0.56132738,0.50001228,0.50000471]\n",
      "bleu_test: [0.90000000,0.00000002,0.00000003,0.00000004,0.00000006]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 45 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.703\n",
      "  Average training loss discriminator: 0.719\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.455\n",
      "  Test Loss: 3.768\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  45 |     0/ 1249 batches | lr 0.000000 | ms/batch 488.91 | loss  0.04 | ppl     1.04 | acc     0.55 | train_ae_norm     1.00\n",
      "[45/200][99/1249] Loss_D: 0.05588941 (Loss_D_real: 0.03268333 Loss_D_fake: 0.02320608) Loss_G: 0.21177398 Loss_Enh_Dec: -0.93150848\n",
      "| epoch  45 |   100/ 1249 batches | lr 0.000000 | ms/batch 357.71 | loss  3.71 | ppl    40.67 | acc     0.56 | train_ae_norm     1.00\n",
      "[45/200][199/1249] Loss_D: 0.04787987 (Loss_D_real: 0.02288316 Loss_D_fake: 0.02499671) Loss_G: 0.19570599 Loss_Enh_Dec: -0.71770227\n",
      "| epoch  45 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.09 | loss  3.71 | ppl    40.80 | acc     0.57 | train_ae_norm     1.00\n",
      "[45/200][299/1249] Loss_D: 0.06858854 (Loss_D_real: 0.02874292 Loss_D_fake: 0.03984561) Loss_G: 0.17694372 Loss_Enh_Dec: -0.79050004\n",
      "| epoch  45 |   300/ 1249 batches | lr 0.000000 | ms/batch 357.08 | loss  3.79 | ppl    44.46 | acc     0.57 | train_ae_norm     1.00\n",
      "[45/200][399/1249] Loss_D: 0.05423962 (Loss_D_real: 0.03257889 Loss_D_fake: 0.02166073) Loss_G: 0.20010582 Loss_Enh_Dec: -0.70594704\n",
      "| epoch  45 |   400/ 1249 batches | lr 0.000000 | ms/batch 356.73 | loss  3.73 | ppl    41.67 | acc     0.60 | train_ae_norm     1.00\n",
      "[45/200][499/1249] Loss_D: 0.07816028 (Loss_D_real: 0.04323313 Loss_D_fake: 0.03492715) Loss_G: 0.18563865 Loss_Enh_Dec: -0.89942324\n",
      "| epoch  45 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.52 | loss  3.85 | ppl    46.91 | acc     0.53 | train_ae_norm     1.00\n",
      "[45/200][599/1249] Loss_D: 0.07030727 (Loss_D_real: 0.03408103 Loss_D_fake: 0.03622624) Loss_G: 0.18596525 Loss_Enh_Dec: -0.81646967\n",
      "| epoch  45 |   600/ 1249 batches | lr 0.000000 | ms/batch 357.92 | loss  3.81 | ppl    45.13 | acc     0.54 | train_ae_norm     1.00\n",
      "[45/200][699/1249] Loss_D: 0.04301514 (Loss_D_real: 0.02254594 Loss_D_fake: 0.02046920) Loss_G: 0.19143403 Loss_Enh_Dec: -0.90412807\n",
      "| epoch  45 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.49 | loss  3.87 | ppl    47.79 | acc     0.51 | train_ae_norm     1.00\n",
      "[45/200][799/1249] Loss_D: 0.08980344 (Loss_D_real: 0.04089205 Loss_D_fake: 0.04891139) Loss_G: 0.16637950 Loss_Enh_Dec: -0.86238205\n",
      "| epoch  45 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.56 | loss  3.90 | ppl    49.34 | acc     0.49 | train_ae_norm     1.00\n",
      "[45/200][899/1249] Loss_D: 0.07318797 (Loss_D_real: 0.04200551 Loss_D_fake: 0.03118246) Loss_G: 0.18805836 Loss_Enh_Dec: -0.89176112\n",
      "| epoch  45 |   900/ 1249 batches | lr 0.000000 | ms/batch 358.32 | loss  3.80 | ppl    44.49 | acc     0.57 | train_ae_norm     1.00\n",
      "[45/200][999/1249] Loss_D: 0.05044519 (Loss_D_real: 0.01798178 Loss_D_fake: 0.03246341) Loss_G: 0.17894106 Loss_Enh_Dec: -0.75079316\n",
      "| epoch  45 |  1000/ 1249 batches | lr 0.000000 | ms/batch 357.24 | loss  3.75 | ppl    42.67 | acc     0.57 | train_ae_norm     1.00\n",
      "[45/200][1099/1249] Loss_D: 0.05965701 (Loss_D_real: 0.02886932 Loss_D_fake: 0.03078769) Loss_G: 0.18060485 Loss_Enh_Dec: -0.78858995\n",
      "| epoch  45 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.76 | loss  3.76 | ppl    43.16 | acc     0.54 | train_ae_norm     1.00\n",
      "[45/200][1199/1249] Loss_D: 0.05054291 (Loss_D_real: 0.02065492 Loss_D_fake: 0.02988799) Loss_G: 0.18595642 Loss_Enh_Dec: -0.69852400\n",
      "| epoch  45 |  1200/ 1249 batches | lr 0.000000 | ms/batch 357.14 | loss  3.72 | ppl    41.31 | acc     0.57 | train_ae_norm     1.00\n",
      "| end of epoch  45 | time: 501.38s | test loss  3.04 | test ppl 21.00 | acc 0.674\n",
      "bleu_self:  [0.70040817 0.5094789  0.41307256 0.19746509 0.00070167]\n",
      "bleu_test:  [9.04166666e-01 4.85748882e-01 1.25354395e-03 1.26187274e-04\n",
      " 3.22656245e-05]\n",
      "bleu_self: [0.70040817,0.50947890,0.41307256,0.19746509,0.00070167]\n",
      "bleu_test: [0.90416667,0.48574888,0.00125354,0.00012619,0.00003227]\n",
      "New saving model: epoch 045.\n",
      "Saving models to ./results/yahoo_merge_results\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 46 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.720\n",
      "  Training epcoh took: 0:00:48\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.455\n",
      "  Test Loss: 3.724\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  46 |     0/ 1249 batches | lr 0.000000 | ms/batch 488.13 | loss  0.04 | ppl     1.04 | acc     0.55 | train_ae_norm     1.00\n",
      "[46/200][99/1249] Loss_D: 0.05364465 (Loss_D_real: 0.02587629 Loss_D_fake: 0.02776836) Loss_G: 0.20030589 Loss_Enh_Dec: -0.76187760\n",
      "| epoch  46 |   100/ 1249 batches | lr 0.000000 | ms/batch 358.14 | loss  3.66 | ppl    39.04 | acc     0.56 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46/200][199/1249] Loss_D: 0.18840572 (Loss_D_real: 0.16431154 Loss_D_fake: 0.02409418) Loss_G: 0.19087271 Loss_Enh_Dec: -0.79860795\n",
      "| epoch  46 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.25 | loss  3.64 | ppl    37.91 | acc     0.57 | train_ae_norm     1.00\n",
      "[46/200][299/1249] Loss_D: 0.05688477 (Loss_D_real: 0.02621974 Loss_D_fake: 0.03066503) Loss_G: 0.19085465 Loss_Enh_Dec: -0.75107664\n",
      "| epoch  46 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.69 | loss  3.60 | ppl    36.74 | acc     0.57 | train_ae_norm     1.00\n",
      "[46/200][399/1249] Loss_D: 0.04385205 (Loss_D_real: 0.01547341 Loss_D_fake: 0.02837864) Loss_G: 0.19840975 Loss_Enh_Dec: -0.81854773\n",
      "| epoch  46 |   400/ 1249 batches | lr 0.000000 | ms/batch 357.16 | loss  3.57 | ppl    35.49 | acc     0.60 | train_ae_norm     1.00\n",
      "[46/200][499/1249] Loss_D: 0.07061061 (Loss_D_real: 0.04076988 Loss_D_fake: 0.02984074) Loss_G: 0.18827093 Loss_Enh_Dec: -0.84002268\n",
      "| epoch  46 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.83 | loss  3.63 | ppl    37.73 | acc     0.56 | train_ae_norm     1.00\n",
      "[46/200][599/1249] Loss_D: 0.06481749 (Loss_D_real: 0.03667738 Loss_D_fake: 0.02814011) Loss_G: 0.19369029 Loss_Enh_Dec: -0.67774260\n",
      "| epoch  46 |   600/ 1249 batches | lr 0.000000 | ms/batch 357.42 | loss  3.58 | ppl    35.81 | acc     0.59 | train_ae_norm     1.00\n",
      "[46/200][699/1249] Loss_D: 0.04355845 (Loss_D_real: 0.01131379 Loss_D_fake: 0.03224465) Loss_G: 0.20881835 Loss_Enh_Dec: -0.86639768\n",
      "| epoch  46 |   700/ 1249 batches | lr 0.000000 | ms/batch 357.06 | loss  3.52 | ppl    33.88 | acc     0.58 | train_ae_norm     1.00\n",
      "[46/200][799/1249] Loss_D: 0.06792349 (Loss_D_real: 0.03133060 Loss_D_fake: 0.03659289) Loss_G: 0.19143255 Loss_Enh_Dec: -0.92383116\n",
      "| epoch  46 |   800/ 1249 batches | lr 0.000000 | ms/batch 357.28 | loss  3.58 | ppl    35.88 | acc     0.54 | train_ae_norm     1.00\n",
      "[46/200][899/1249] Loss_D: 0.05881540 (Loss_D_real: 0.02512903 Loss_D_fake: 0.03368637) Loss_G: 0.19178122 Loss_Enh_Dec: -0.49953994\n",
      "| epoch  46 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.61 | loss  3.51 | ppl    33.32 | acc     0.62 | train_ae_norm     1.00\n",
      "[46/200][999/1249] Loss_D: 0.07231349 (Loss_D_real: 0.04387693 Loss_D_fake: 0.02843656) Loss_G: 0.20205493 Loss_Enh_Dec: -0.84196550\n",
      "| epoch  46 |  1000/ 1249 batches | lr 0.000000 | ms/batch 357.35 | loss  3.51 | ppl    33.42 | acc     0.61 | train_ae_norm     1.00\n",
      "[46/200][1099/1249] Loss_D: 0.07947927 (Loss_D_real: 0.05002385 Loss_D_fake: 0.02945542) Loss_G: 0.20246266 Loss_Enh_Dec: -0.79948366\n",
      "| epoch  46 |  1100/ 1249 batches | lr 0.000000 | ms/batch 357.19 | loss  3.50 | ppl    33.13 | acc     0.57 | train_ae_norm     1.00\n",
      "[46/200][1199/1249] Loss_D: 0.04950331 (Loss_D_real: 0.01945079 Loss_D_fake: 0.03005252) Loss_G: 0.20446996 Loss_Enh_Dec: -0.60493135\n",
      "| epoch  46 |  1200/ 1249 batches | lr 0.000000 | ms/batch 356.51 | loss  3.45 | ppl    31.42 | acc     0.56 | train_ae_norm     1.00\n",
      "| end of epoch  46 | time: 501.17s | test loss  2.88 | test ppl 17.77 | acc 0.688\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 47 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.704\n",
      "  Average training loss discriminator: 0.802\n",
      "  Training epcoh took: 0:00:48\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.395\n",
      "  Test Loss: 3.832\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  47 |     0/ 1249 batches | lr 0.000000 | ms/batch 488.04 | loss  0.03 | ppl     1.04 | acc     0.59 | train_ae_norm     1.00\n",
      "[47/200][99/1249] Loss_D: 0.04756038 (Loss_D_real: 0.01736238 Loss_D_fake: 0.03019799) Loss_G: 0.20904522 Loss_Enh_Dec: -0.58934844\n",
      "| epoch  47 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.98 | loss  3.44 | ppl    31.12 | acc     0.59 | train_ae_norm     1.00\n",
      "[47/200][199/1249] Loss_D: 0.07615945 (Loss_D_real: 0.04455929 Loss_D_fake: 0.03160016) Loss_G: 0.20806503 Loss_Enh_Dec: -0.61613059\n",
      "| epoch  47 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.40 | loss  3.42 | ppl    30.54 | acc     0.62 | train_ae_norm     1.00\n",
      "[47/200][299/1249] Loss_D: 0.05619095 (Loss_D_real: 0.03109783 Loss_D_fake: 0.02509311) Loss_G: 0.20759879 Loss_Enh_Dec: -0.75593436\n",
      "| epoch  47 |   300/ 1249 batches | lr 0.000000 | ms/batch 358.74 | loss  3.43 | ppl    30.89 | acc     0.60 | train_ae_norm     1.00\n",
      "[47/200][399/1249] Loss_D: 0.05063687 (Loss_D_real: 0.02685781 Loss_D_fake: 0.02377906) Loss_G: 0.22321801 Loss_Enh_Dec: -0.76929855\n",
      "| epoch  47 |   400/ 1249 batches | lr 0.000000 | ms/batch 358.00 | loss  3.40 | ppl    29.95 | acc     0.63 | train_ae_norm     1.00\n",
      "[47/200][499/1249] Loss_D: 0.06339748 (Loss_D_real: 0.02669750 Loss_D_fake: 0.03669998) Loss_G: 0.20616730 Loss_Enh_Dec: -0.52191448\n",
      "| epoch  47 |   500/ 1249 batches | lr 0.000000 | ms/batch 357.99 | loss  3.44 | ppl    31.20 | acc     0.58 | train_ae_norm     1.00\n",
      "[47/200][599/1249] Loss_D: 0.08950838 (Loss_D_real: 0.05168283 Loss_D_fake: 0.03782555) Loss_G: 0.22198932 Loss_Enh_Dec: -0.87461913\n",
      "| epoch  47 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.99 | loss  3.49 | ppl    32.69 | acc     0.59 | train_ae_norm     1.00\n",
      "[47/200][699/1249] Loss_D: 0.03882294 (Loss_D_real: 0.01326820 Loss_D_fake: 0.02555474) Loss_G: 0.21233784 Loss_Enh_Dec: -0.76018012\n",
      "| epoch  47 |   700/ 1249 batches | lr 0.000000 | ms/batch 357.46 | loss  3.49 | ppl    32.65 | acc     0.56 | train_ae_norm     1.00\n",
      "[47/200][799/1249] Loss_D: 0.06079357 (Loss_D_real: 0.03047499 Loss_D_fake: 0.03031859) Loss_G: 0.24832748 Loss_Enh_Dec: -0.42943057\n",
      "| epoch  47 |   800/ 1249 batches | lr 0.000000 | ms/batch 357.09 | loss  3.59 | ppl    36.15 | acc     0.54 | train_ae_norm     1.00\n",
      "[47/200][899/1249] Loss_D: 0.04185921 (Loss_D_real: 0.02068933 Loss_D_fake: 0.02116988) Loss_G: 0.21556078 Loss_Enh_Dec: -0.52813107\n",
      "| epoch  47 |   900/ 1249 batches | lr 0.000000 | ms/batch 357.86 | loss  3.52 | ppl    33.78 | acc     0.59 | train_ae_norm     1.00\n",
      "[47/200][999/1249] Loss_D: 0.03236221 (Loss_D_real: 0.01016014 Loss_D_fake: 0.02220207) Loss_G: 0.22720101 Loss_Enh_Dec: -0.61734444\n",
      "| epoch  47 |  1000/ 1249 batches | lr 0.000000 | ms/batch 365.65 | loss  3.50 | ppl    33.23 | acc     0.61 | train_ae_norm     1.00\n",
      "[47/200][1099/1249] Loss_D: 0.04720050 (Loss_D_real: 0.01831350 Loss_D_fake: 0.02888700) Loss_G: 0.23048782 Loss_Enh_Dec: -0.70521063\n",
      "| epoch  47 |  1100/ 1249 batches | lr 0.000000 | ms/batch 362.19 | loss  3.50 | ppl    33.13 | acc     0.60 | train_ae_norm     1.00\n",
      "[47/200][1199/1249] Loss_D: 0.08586450 (Loss_D_real: 0.03726324 Loss_D_fake: 0.04860126) Loss_G: 0.21887128 Loss_Enh_Dec: -0.72579020\n",
      "| epoch  47 |  1200/ 1249 batches | lr 0.000000 | ms/batch 356.68 | loss  3.46 | ppl    31.86 | acc     0.59 | train_ae_norm     1.00\n",
      "| end of epoch  47 | time: 502.86s | test loss  2.94 | test ppl 18.89 | acc 0.686\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 48 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.732\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.393\n",
      "  Test Loss: 3.868\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  48 |     0/ 1249 batches | lr 0.000000 | ms/batch 489.20 | loss  0.03 | ppl     1.04 | acc     0.59 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48/200][99/1249] Loss_D: 0.04694033 (Loss_D_real: 0.02877193 Loss_D_fake: 0.01816840) Loss_G: 0.21951631 Loss_Enh_Dec: -0.52061576\n",
      "| epoch  48 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.33 | loss  3.46 | ppl    31.68 | acc     0.62 | train_ae_norm     1.00\n",
      "[48/200][199/1249] Loss_D: 0.04657981 (Loss_D_real: 0.02999825 Loss_D_fake: 0.01658156) Loss_G: 0.23256068 Loss_Enh_Dec: -0.63526899\n",
      "| epoch  48 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.93 | loss  3.45 | ppl    31.53 | acc     0.59 | train_ae_norm     1.00\n",
      "[48/200][299/1249] Loss_D: 0.05394544 (Loss_D_real: 0.02527929 Loss_D_fake: 0.02866614) Loss_G: 0.23336712 Loss_Enh_Dec: -0.64795130\n",
      "| epoch  48 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.62 | loss  3.44 | ppl    31.25 | acc     0.59 | train_ae_norm     1.00\n",
      "[48/200][399/1249] Loss_D: 0.02576209 (Loss_D_real: 0.01474451 Loss_D_fake: 0.01101758) Loss_G: 0.25245944 Loss_Enh_Dec: -0.79951608\n",
      "| epoch  48 |   400/ 1249 batches | lr 0.000000 | ms/batch 357.70 | loss  3.41 | ppl    30.39 | acc     0.63 | train_ae_norm     1.00\n",
      "[48/200][499/1249] Loss_D: 0.04596336 (Loss_D_real: 0.03307523 Loss_D_fake: 0.01288813) Loss_G: 0.27092919 Loss_Enh_Dec: -0.76867181\n",
      "| epoch  48 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.97 | loss  3.38 | ppl    29.27 | acc     0.62 | train_ae_norm     1.00\n",
      "[48/200][599/1249] Loss_D: 0.03007020 (Loss_D_real: 0.01340034 Loss_D_fake: 0.01666986) Loss_G: 0.27576765 Loss_Enh_Dec: -0.72866911\n",
      "| epoch  48 |   600/ 1249 batches | lr 0.000000 | ms/batch 357.17 | loss  3.40 | ppl    29.87 | acc     0.59 | train_ae_norm     1.00\n",
      "[48/200][699/1249] Loss_D: 0.03047952 (Loss_D_real: 0.01597738 Loss_D_fake: 0.01450214) Loss_G: 0.27692848 Loss_Enh_Dec: -0.51205522\n",
      "| epoch  48 |   700/ 1249 batches | lr 0.000000 | ms/batch 357.40 | loss  3.38 | ppl    29.33 | acc     0.62 | train_ae_norm     1.00\n",
      "[48/200][799/1249] Loss_D: 0.02157454 (Loss_D_real: 0.01045760 Loss_D_fake: 0.01111694) Loss_G: 0.27148187 Loss_Enh_Dec: -0.80157751\n",
      "| epoch  48 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.29 | loss  3.40 | ppl    29.99 | acc     0.58 | train_ae_norm     1.00\n",
      "[48/200][899/1249] Loss_D: 0.03177643 (Loss_D_real: 0.01741854 Loss_D_fake: 0.01435789) Loss_G: 0.25730759 Loss_Enh_Dec: -0.94737953\n",
      "| epoch  48 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.89 | loss  3.36 | ppl    28.70 | acc     0.59 | train_ae_norm     1.00\n",
      "[48/200][999/1249] Loss_D: 0.05864967 (Loss_D_real: 0.02213493 Loss_D_fake: 0.03651474) Loss_G: 0.16787994 Loss_Enh_Dec: -0.73897582\n",
      "| epoch  48 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.87 | loss  3.41 | ppl    30.22 | acc     0.62 | train_ae_norm     1.00\n",
      "[48/200][1099/1249] Loss_D: 0.05413605 (Loss_D_real: 0.02784654 Loss_D_fake: 0.02628951) Loss_G: 0.18239650 Loss_Enh_Dec: -0.71004325\n",
      "| epoch  48 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.62 | loss  3.45 | ppl    31.54 | acc     0.59 | train_ae_norm     1.00\n",
      "[48/200][1199/1249] Loss_D: 0.03252678 (Loss_D_real: 0.01355958 Loss_D_fake: 0.01896721) Loss_G: 0.19362107 Loss_Enh_Dec: -0.82232398\n",
      "| epoch  48 |  1200/ 1249 batches | lr 0.000000 | ms/batch 356.59 | loss  3.43 | ppl    30.76 | acc     0.61 | train_ae_norm     1.00\n",
      "| end of epoch  48 | time: 501.04s | test loss  2.83 | test ppl 17.00 | acc 0.695\n",
      "bleu_self:  [3.34599321e-01 9.73797268e-09 3.41749542e-11 2.37763870e-12\n",
      " 2.69839065e-11]\n",
      "bleu_test:  [6.26488095e-01 7.21687954e-02 5.45987791e-02 9.07474666e-06\n",
      " 5.31726277e-08]\n",
      "bleu_self: [0.33459932,0.00000001,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.62648809,0.07216880,0.05459878,0.00000907,0.00000005]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 49 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.734\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.375\n",
      "  Test Loss: 4.036\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  49 |     0/ 1249 batches | lr 0.000000 | ms/batch 488.75 | loss  0.03 | ppl     1.03 | acc     0.63 | train_ae_norm     1.00\n",
      "[49/200][99/1249] Loss_D: 0.04808549 (Loss_D_real: 0.03095778 Loss_D_fake: 0.01712771) Loss_G: 0.20112744 Loss_Enh_Dec: -0.91223568\n",
      "| epoch  49 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.95 | loss  3.34 | ppl    28.24 | acc     0.58 | train_ae_norm     1.00\n",
      "[49/200][199/1249] Loss_D: 0.02817450 (Loss_D_real: 0.01248985 Loss_D_fake: 0.01568465) Loss_G: 0.20960040 Loss_Enh_Dec: -0.96616596\n",
      "| epoch  49 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.05 | loss  3.28 | ppl    26.53 | acc     0.63 | train_ae_norm     1.00\n",
      "[49/200][299/1249] Loss_D: 0.03920696 (Loss_D_real: 0.02326732 Loss_D_fake: 0.01593964) Loss_G: 0.20948921 Loss_Enh_Dec: -0.81978434\n",
      "| epoch  49 |   300/ 1249 batches | lr 0.000000 | ms/batch 357.38 | loss  3.28 | ppl    26.67 | acc     0.62 | train_ae_norm     1.00\n",
      "[49/200][399/1249] Loss_D: 0.02518640 (Loss_D_real: 0.01140505 Loss_D_fake: 0.01378135) Loss_G: 0.21765284 Loss_Enh_Dec: -0.72547930\n",
      "| epoch  49 |   400/ 1249 batches | lr 0.000000 | ms/batch 357.66 | loss  3.21 | ppl    24.85 | acc     0.65 | train_ae_norm     1.00\n",
      "[49/200][499/1249] Loss_D: 0.04064374 (Loss_D_real: 0.02954357 Loss_D_fake: 0.01110017) Loss_G: 0.25974718 Loss_Enh_Dec: -1.02168882\n",
      "| epoch  49 |   500/ 1249 batches | lr 0.000000 | ms/batch 357.26 | loss  3.26 | ppl    26.11 | acc     0.61 | train_ae_norm     1.00\n",
      "[49/200][599/1249] Loss_D: 0.11278725 (Loss_D_real: 0.08713409 Loss_D_fake: 0.02565316) Loss_G: 0.24109645 Loss_Enh_Dec: -0.83959675\n",
      "| epoch  49 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.05 | loss  3.25 | ppl    25.74 | acc     0.65 | train_ae_norm     1.00\n",
      "[49/200][699/1249] Loss_D: 0.03946776 (Loss_D_real: 0.01385240 Loss_D_fake: 0.02561535) Loss_G: 0.18889274 Loss_Enh_Dec: -0.83341563\n",
      "| epoch  49 |   700/ 1249 batches | lr 0.000000 | ms/batch 357.48 | loss  3.23 | ppl    25.33 | acc     0.61 | train_ae_norm     1.00\n",
      "[49/200][799/1249] Loss_D: 0.02707519 (Loss_D_real: 0.01561073 Loss_D_fake: 0.01146446) Loss_G: 0.24765407 Loss_Enh_Dec: -0.89608878\n",
      "| epoch  49 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.92 | loss  3.26 | ppl    26.17 | acc     0.61 | train_ae_norm     1.00\n",
      "[49/200][899/1249] Loss_D: 0.02620121 (Loss_D_real: 0.01820646 Loss_D_fake: 0.00799476) Loss_G: 0.24594736 Loss_Enh_Dec: -0.66973203\n",
      "| epoch  49 |   900/ 1249 batches | lr 0.000000 | ms/batch 357.05 | loss  3.19 | ppl    24.35 | acc     0.60 | train_ae_norm     1.00\n",
      "[49/200][999/1249] Loss_D: 0.02918917 (Loss_D_real: 0.01060991 Loss_D_fake: 0.01857926) Loss_G: 0.21308227 Loss_Enh_Dec: -0.71283674\n",
      "| epoch  49 |  1000/ 1249 batches | lr 0.000000 | ms/batch 357.10 | loss  3.24 | ppl    25.46 | acc     0.63 | train_ae_norm     1.00\n",
      "[49/200][1099/1249] Loss_D: 0.03828801 (Loss_D_real: 0.02494520 Loss_D_fake: 0.01334281) Loss_G: 0.21373896 Loss_Enh_Dec: -0.57498950\n",
      "| epoch  49 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.70 | loss  3.21 | ppl    24.83 | acc     0.61 | train_ae_norm     1.00\n",
      "[49/200][1199/1249] Loss_D: 0.06270661 (Loss_D_real: 0.05297533 Loss_D_fake: 0.00973128) Loss_G: 0.23959807 Loss_Enh_Dec: -0.76689070\n",
      "| epoch  49 |  1200/ 1249 batches | lr 0.000000 | ms/batch 357.15 | loss  3.16 | ppl    23.61 | acc     0.61 | train_ae_norm     1.00\n",
      "| end of epoch  49 | time: 501.28s | test loss  2.68 | test ppl 14.56 | acc 0.710\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 50 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.739\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.385\n",
      "  Test Loss: 4.004\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  50 |     0/ 1249 batches | lr 0.000000 | ms/batch 490.86 | loss  0.03 | ppl     1.03 | acc     0.64 | train_ae_norm     1.00\n",
      "[50/200][99/1249] Loss_D: 0.03822530 (Loss_D_real: 0.02036383 Loss_D_fake: 0.01786146) Loss_G: 0.20558842 Loss_Enh_Dec: -0.70561713\n",
      "| epoch  50 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.47 | loss  3.23 | ppl    25.30 | acc     0.63 | train_ae_norm     1.00\n",
      "[50/200][199/1249] Loss_D: 0.03383374 (Loss_D_real: 0.01627070 Loss_D_fake: 0.01756304) Loss_G: 0.22376256 Loss_Enh_Dec: -0.85150987\n",
      "| epoch  50 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.68 | loss  3.21 | ppl    24.77 | acc     0.64 | train_ae_norm     1.00\n",
      "[50/200][299/1249] Loss_D: 0.02349801 (Loss_D_real: 0.01377644 Loss_D_fake: 0.00972157) Loss_G: 0.24570628 Loss_Enh_Dec: -0.89538592\n",
      "| epoch  50 |   300/ 1249 batches | lr 0.000000 | ms/batch 357.03 | loss  3.23 | ppl    25.28 | acc     0.62 | train_ae_norm     1.00\n",
      "[50/200][399/1249] Loss_D: 0.02089318 (Loss_D_real: 0.01056771 Loss_D_fake: 0.01032547) Loss_G: 0.23839326 Loss_Enh_Dec: -0.90585339\n",
      "| epoch  50 |   400/ 1249 batches | lr 0.000000 | ms/batch 358.05 | loss  3.17 | ppl    23.87 | acc     0.68 | train_ae_norm     1.00\n",
      "[50/200][499/1249] Loss_D: 0.04849508 (Loss_D_real: 0.02380200 Loss_D_fake: 0.02469308) Loss_G: 0.21719456 Loss_Enh_Dec: -1.02741337\n",
      "| epoch  50 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.58 | loss  3.22 | ppl    24.98 | acc     0.58 | train_ae_norm     1.00\n",
      "[50/200][599/1249] Loss_D: 0.02080033 (Loss_D_real: 0.00909101 Loss_D_fake: 0.01170932) Loss_G: 0.23706321 Loss_Enh_Dec: -0.89608955\n",
      "| epoch  50 |   600/ 1249 batches | lr 0.000000 | ms/batch 357.69 | loss  3.19 | ppl    24.36 | acc     0.65 | train_ae_norm     1.00\n",
      "[50/200][699/1249] Loss_D: 0.03841588 (Loss_D_real: 0.02861883 Loss_D_fake: 0.00979704) Loss_G: 0.24913143 Loss_Enh_Dec: -0.86663485\n",
      "| epoch  50 |   700/ 1249 batches | lr 0.000000 | ms/batch 357.58 | loss  3.17 | ppl    23.79 | acc     0.61 | train_ae_norm     1.00\n",
      "[50/200][799/1249] Loss_D: 0.03446330 (Loss_D_real: 0.01704432 Loss_D_fake: 0.01741898) Loss_G: 0.20944567 Loss_Enh_Dec: -0.95823139\n",
      "| epoch  50 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.67 | loss  3.25 | ppl    25.87 | acc     0.60 | train_ae_norm     1.00\n",
      "[50/200][899/1249] Loss_D: 0.02747827 (Loss_D_real: 0.01104188 Loss_D_fake: 0.01643639) Loss_G: 0.22675610 Loss_Enh_Dec: -0.65229052\n",
      "| epoch  50 |   900/ 1249 batches | lr 0.000000 | ms/batch 357.52 | loss  3.15 | ppl    23.40 | acc     0.64 | train_ae_norm     1.00\n",
      "[50/200][999/1249] Loss_D: 0.02004384 (Loss_D_real: 0.00811415 Loss_D_fake: 0.01192969) Loss_G: 0.23211800 Loss_Enh_Dec: -0.88627046\n",
      "| epoch  50 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.28 | loss  3.13 | ppl    22.84 | acc     0.65 | train_ae_norm     1.00\n",
      "[50/200][1099/1249] Loss_D: 0.02164835 (Loss_D_real: 0.01174307 Loss_D_fake: 0.00990529) Loss_G: 0.24178259 Loss_Enh_Dec: -1.12436664\n",
      "| epoch  50 |  1100/ 1249 batches | lr 0.000000 | ms/batch 357.74 | loss  3.19 | ppl    24.41 | acc     0.64 | train_ae_norm     1.00\n",
      "[50/200][1199/1249] Loss_D: 0.02723453 (Loss_D_real: 0.01652806 Loss_D_fake: 0.01070648) Loss_G: 0.24956417 Loss_Enh_Dec: -1.05512702\n",
      "| epoch  50 |  1200/ 1249 batches | lr 0.000000 | ms/batch 356.45 | loss  3.11 | ppl    22.37 | acc     0.64 | train_ae_norm     1.00\n",
      "| end of epoch  50 | time: 501.50s | test loss  2.65 | test ppl 14.15 | acc 0.710\n",
      "bleu_self:  [5.78869047e-01 4.09593940e-01 1.02551373e-01 1.67147766e-05\n",
      " 9.59010252e-08]\n",
      "bleu_test:  [7.01636905e-01 4.61088273e-01 3.49268922e-06 1.02437797e-08\n",
      " 3.30208719e-10]\n",
      "bleu_self: [0.57886905,0.40959394,0.10255137,0.00001671,0.00000010]\n",
      "bleu_test: [0.70163690,0.46108827,0.00000349,0.00000001,0.00000000]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 51 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.730\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.390\n",
      "  Test Loss: 3.906\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  51 |     0/ 1249 batches | lr 0.000000 | ms/batch 491.42 | loss  0.03 | ppl     1.03 | acc     0.62 | train_ae_norm     1.00\n",
      "[51/200][99/1249] Loss_D: 0.01571525 (Loss_D_real: 0.00616235 Loss_D_fake: 0.00955289) Loss_G: 0.26142102 Loss_Enh_Dec: -0.90870887\n",
      "| epoch  51 |   100/ 1249 batches | lr 0.000000 | ms/batch 357.61 | loss  3.12 | ppl    22.73 | acc     0.62 | train_ae_norm     1.00\n",
      "[51/200][199/1249] Loss_D: 0.03090544 (Loss_D_real: 0.01471159 Loss_D_fake: 0.01619385) Loss_G: 0.22127299 Loss_Enh_Dec: -0.91403073\n",
      "| epoch  51 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.23 | loss  3.13 | ppl    22.95 | acc     0.67 | train_ae_norm     1.00\n",
      "[51/200][299/1249] Loss_D: 0.02146993 (Loss_D_real: 0.00946871 Loss_D_fake: 0.01200122) Loss_G: 0.23789202 Loss_Enh_Dec: -0.88261890\n",
      "| epoch  51 |   300/ 1249 batches | lr 0.000000 | ms/batch 357.09 | loss  3.15 | ppl    23.22 | acc     0.64 | train_ae_norm     1.00\n",
      "[51/200][399/1249] Loss_D: 0.01877567 (Loss_D_real: 0.00672424 Loss_D_fake: 0.01205143) Loss_G: 0.24913028 Loss_Enh_Dec: -0.99392194\n",
      "| epoch  51 |   400/ 1249 batches | lr 0.000000 | ms/batch 356.96 | loss  3.05 | ppl    21.09 | acc     0.68 | train_ae_norm     1.00\n",
      "[51/200][499/1249] Loss_D: 0.02096273 (Loss_D_real: 0.01256468 Loss_D_fake: 0.00839805) Loss_G: 0.25492308 Loss_Enh_Dec: -1.09287953\n",
      "| epoch  51 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.97 | loss  3.05 | ppl    21.18 | acc     0.65 | train_ae_norm     1.00\n",
      "[51/200][599/1249] Loss_D: 0.01507130 (Loss_D_real: 0.00815253 Loss_D_fake: 0.00691878) Loss_G: 0.26543787 Loss_Enh_Dec: -1.13124692\n",
      "| epoch  51 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.71 | loss  3.05 | ppl    21.16 | acc     0.63 | train_ae_norm     1.00\n",
      "[51/200][699/1249] Loss_D: 0.02673264 (Loss_D_real: 0.01935376 Loss_D_fake: 0.00737888) Loss_G: 0.26394254 Loss_Enh_Dec: -0.92858678\n",
      "| epoch  51 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.67 | loss  3.01 | ppl    20.25 | acc     0.65 | train_ae_norm     1.00\n",
      "[51/200][799/1249] Loss_D: 0.01564818 (Loss_D_real: 0.00954743 Loss_D_fake: 0.00610075) Loss_G: 0.27096260 Loss_Enh_Dec: -0.96186346\n",
      "| epoch  51 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.05 | loss  3.08 | ppl    21.75 | acc     0.61 | train_ae_norm     1.00\n",
      "[51/200][899/1249] Loss_D: 0.01745752 (Loss_D_real: 0.00796383 Loss_D_fake: 0.00949369) Loss_G: 0.26574677 Loss_Enh_Dec: -0.94576168\n",
      "| epoch  51 |   900/ 1249 batches | lr 0.000000 | ms/batch 355.90 | loss  3.05 | ppl    21.10 | acc     0.64 | train_ae_norm     1.00\n",
      "[51/200][999/1249] Loss_D: 0.01391093 (Loss_D_real: 0.00889260 Loss_D_fake: 0.00501833) Loss_G: 0.28121564 Loss_Enh_Dec: -1.19704509\n",
      "| epoch  51 |  1000/ 1249 batches | lr 0.000000 | ms/batch 357.13 | loss  3.07 | ppl    21.49 | acc     0.64 | train_ae_norm     1.00\n",
      "[51/200][1099/1249] Loss_D: 0.03047188 (Loss_D_real: 0.01664445 Loss_D_fake: 0.01382743) Loss_G: 0.23025222 Loss_Enh_Dec: -0.93343490\n",
      "| epoch  51 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.49 | loss  3.12 | ppl    22.58 | acc     0.60 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51/200][1199/1249] Loss_D: 0.01709270 (Loss_D_real: 0.00759241 Loss_D_fake: 0.00950029) Loss_G: 0.24591303 Loss_Enh_Dec: -0.94086343\n",
      "| epoch  51 |  1200/ 1249 batches | lr 0.000000 | ms/batch 356.30 | loss  3.01 | ppl    20.33 | acc     0.62 | train_ae_norm     1.00\n",
      "| end of epoch  51 | time: 501.18s | test loss  2.55 | test ppl 12.82 | acc 0.725\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 52 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.721\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.400\n",
      "  Test Loss: 3.991\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  52 |     0/ 1249 batches | lr 0.000000 | ms/batch 489.24 | loss  0.03 | ppl     1.03 | acc     0.65 | train_ae_norm     1.00\n",
      "[52/200][99/1249] Loss_D: 0.01576179 (Loss_D_real: 0.00936414 Loss_D_fake: 0.00639765) Loss_G: 0.26849315 Loss_Enh_Dec: -1.03460443\n",
      "| epoch  52 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.63 | loss  3.04 | ppl    20.98 | acc     0.66 | train_ae_norm     1.00\n",
      "[52/200][199/1249] Loss_D: 0.01596586 (Loss_D_real: 0.00867314 Loss_D_fake: 0.00729271) Loss_G: 0.26866889 Loss_Enh_Dec: -1.04423225\n",
      "| epoch  52 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.36 | loss  3.05 | ppl    21.14 | acc     0.65 | train_ae_norm     1.00\n",
      "[52/200][299/1249] Loss_D: 0.01578402 (Loss_D_real: 0.00741145 Loss_D_fake: 0.00837257) Loss_G: 0.27327278 Loss_Enh_Dec: -1.04441726\n",
      "| epoch  52 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.58 | loss  3.08 | ppl    21.68 | acc     0.66 | train_ae_norm     1.00\n",
      "[52/200][399/1249] Loss_D: 0.01168901 (Loss_D_real: 0.00512813 Loss_D_fake: 0.00656088) Loss_G: 0.27239457 Loss_Enh_Dec: -1.09407866\n",
      "| epoch  52 |   400/ 1249 batches | lr 0.000000 | ms/batch 356.52 | loss  3.04 | ppl    21.00 | acc     0.65 | train_ae_norm     1.00\n",
      "[52/200][499/1249] Loss_D: 0.01938448 (Loss_D_real: 0.01335028 Loss_D_fake: 0.00603420) Loss_G: 0.27255201 Loss_Enh_Dec: -1.06485438\n",
      "| epoch  52 |   500/ 1249 batches | lr 0.000000 | ms/batch 357.08 | loss  3.10 | ppl    22.25 | acc     0.61 | train_ae_norm     1.00\n",
      "[52/200][599/1249] Loss_D: 0.02546221 (Loss_D_real: 0.01409892 Loss_D_fake: 0.01136329) Loss_G: 0.25985485 Loss_Enh_Dec: -0.91370040\n",
      "| epoch  52 |   600/ 1249 batches | lr 0.000000 | ms/batch 355.43 | loss  3.12 | ppl    22.57 | acc     0.63 | train_ae_norm     1.00\n",
      "[52/200][699/1249] Loss_D: 0.02806862 (Loss_D_real: 0.01775724 Loss_D_fake: 0.01031139) Loss_G: 0.26071766 Loss_Enh_Dec: -0.77100694\n",
      "| epoch  52 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.15 | loss  3.19 | ppl    24.24 | acc     0.59 | train_ae_norm     1.00\n",
      "[52/200][799/1249] Loss_D: 0.02722481 (Loss_D_real: 0.01563944 Loss_D_fake: 0.01158537) Loss_G: 0.24488817 Loss_Enh_Dec: -1.01392806\n",
      "| epoch  52 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.55 | loss  3.30 | ppl    27.09 | acc     0.58 | train_ae_norm     1.00\n",
      "[52/200][899/1249] Loss_D: 0.01644631 (Loss_D_real: 0.00659146 Loss_D_fake: 0.00985486) Loss_G: 0.24105965 Loss_Enh_Dec: -1.05023837\n",
      "| epoch  52 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.78 | loss  3.26 | ppl    26.13 | acc     0.63 | train_ae_norm     1.00\n",
      "[52/200][999/1249] Loss_D: 0.03059798 (Loss_D_real: 0.01551378 Loss_D_fake: 0.01508419) Loss_G: 0.25237510 Loss_Enh_Dec: -0.84434813\n",
      "| epoch  52 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.54 | loss  3.33 | ppl    27.81 | acc     0.63 | train_ae_norm     1.00\n",
      "[52/200][1099/1249] Loss_D: 0.01900747 (Loss_D_real: 0.01167790 Loss_D_fake: 0.00732957) Loss_G: 0.25813070 Loss_Enh_Dec: -1.00399208\n",
      "| epoch  52 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.92 | loss  3.34 | ppl    28.10 | acc     0.61 | train_ae_norm     1.00\n",
      "[52/200][1199/1249] Loss_D: 0.01196493 (Loss_D_real: 0.00455551 Loss_D_fake: 0.00740942) Loss_G: 0.26846525 Loss_Enh_Dec: -1.09240520\n",
      "| epoch  52 |  1200/ 1249 batches | lr 0.000000 | ms/batch 356.32 | loss  3.29 | ppl    26.94 | acc     0.60 | train_ae_norm     1.00\n",
      "| end of epoch  52 | time: 500.58s | test loss  2.73 | test ppl 15.36 | acc 0.703\n",
      "bleu_self:  [7.29166666e-02 2.75483198e-09 1.12128669e-11 8.02423671e-11\n",
      " 3.53332391e-10]\n",
      "bleu_test:  [9.99999999e-01 3.18520914e-01 3.00727238e-06 5.98458386e-07\n",
      " 4.46839193e-07]\n",
      "bleu_self: [0.07291667,0.00000000,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [1.00000000,0.31852091,0.00000301,0.00000060,0.00000045]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 53 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.724\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.405\n",
      "  Test Loss: 3.975\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  53 |     0/ 1249 batches | lr 0.000000 | ms/batch 488.58 | loss  0.03 | ppl     1.03 | acc     0.59 | train_ae_norm     1.00\n",
      "[53/200][99/1249] Loss_D: 0.01284694 (Loss_D_real: 0.00399591 Loss_D_fake: 0.00885103) Loss_G: 0.28064805 Loss_Enh_Dec: -1.08517969\n",
      "| epoch  53 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.55 | loss  3.35 | ppl    28.62 | acc     0.59 | train_ae_norm     1.00\n",
      "[53/200][199/1249] Loss_D: 0.01381534 (Loss_D_real: 0.00704971 Loss_D_fake: 0.00676563) Loss_G: 0.27043092 Loss_Enh_Dec: -0.91643488\n",
      "| epoch  53 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.11 | loss  3.40 | ppl    29.90 | acc     0.62 | train_ae_norm     1.00\n",
      "[53/200][299/1249] Loss_D: 0.02585036 (Loss_D_real: 0.01509018 Loss_D_fake: 0.01076018) Loss_G: 0.22896777 Loss_Enh_Dec: -1.05145597\n",
      "| epoch  53 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.58 | loss  3.46 | ppl    31.74 | acc     0.61 | train_ae_norm     1.00\n",
      "[53/200][399/1249] Loss_D: 0.02327804 (Loss_D_real: 0.01400738 Loss_D_fake: 0.00927066) Loss_G: 0.24136634 Loss_Enh_Dec: -0.91984046\n",
      "| epoch  53 |   400/ 1249 batches | lr 0.000000 | ms/batch 356.27 | loss  3.44 | ppl    31.09 | acc     0.59 | train_ae_norm     1.00\n",
      "[53/200][499/1249] Loss_D: 0.02146850 (Loss_D_real: 0.01409848 Loss_D_fake: 0.00737002) Loss_G: 0.25665483 Loss_Enh_Dec: -1.09378004\n",
      "| epoch  53 |   500/ 1249 batches | lr 0.000000 | ms/batch 355.15 | loss  3.48 | ppl    32.44 | acc     0.58 | train_ae_norm     1.00\n",
      "[53/200][599/1249] Loss_D: 0.02674956 (Loss_D_real: 0.01721701 Loss_D_fake: 0.00953255) Loss_G: 0.26250741 Loss_Enh_Dec: -1.00422895\n",
      "| epoch  53 |   600/ 1249 batches | lr 0.000000 | ms/batch 355.24 | loss  3.42 | ppl    30.54 | acc     0.59 | train_ae_norm     1.00\n",
      "[53/200][699/1249] Loss_D: 0.02665119 (Loss_D_real: 0.01543551 Loss_D_fake: 0.01121568) Loss_G: 0.25745091 Loss_Enh_Dec: -1.04086626\n",
      "| epoch  53 |   700/ 1249 batches | lr 0.000000 | ms/batch 355.78 | loss  3.35 | ppl    28.37 | acc     0.61 | train_ae_norm     1.00\n",
      "[53/200][799/1249] Loss_D: 0.03364422 (Loss_D_real: 0.02021728 Loss_D_fake: 0.01342694) Loss_G: 0.26161644 Loss_Enh_Dec: -0.94167560\n",
      "| epoch  53 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.38 | loss  3.30 | ppl    27.06 | acc     0.61 | train_ae_norm     1.00\n",
      "[53/200][899/1249] Loss_D: 0.02179696 (Loss_D_real: 0.00791377 Loss_D_fake: 0.01388319) Loss_G: 0.26009464 Loss_Enh_Dec: -0.76534492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  53 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.45 | loss  3.26 | ppl    25.93 | acc     0.59 | train_ae_norm     1.00\n",
      "[53/200][999/1249] Loss_D: 0.01296582 (Loss_D_real: 0.00523848 Loss_D_fake: 0.00772734) Loss_G: 0.27523389 Loss_Enh_Dec: -1.01116502\n",
      "| epoch  53 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.95 | loss  3.28 | ppl    26.54 | acc     0.61 | train_ae_norm     1.00\n",
      "[53/200][1099/1249] Loss_D: 0.01836319 (Loss_D_real: 0.01120115 Loss_D_fake: 0.00716204) Loss_G: 0.28084636 Loss_Enh_Dec: -0.99136060\n",
      "| epoch  53 |  1100/ 1249 batches | lr 0.000000 | ms/batch 357.97 | loss  3.26 | ppl    26.15 | acc     0.61 | train_ae_norm     1.00\n",
      "[53/200][1199/1249] Loss_D: 0.01441925 (Loss_D_real: 0.00759413 Loss_D_fake: 0.00682512) Loss_G: 0.27186725 Loss_Enh_Dec: -0.81671447\n",
      "| epoch  53 |  1200/ 1249 batches | lr 0.000000 | ms/batch 357.32 | loss  3.25 | ppl    25.76 | acc     0.60 | train_ae_norm     1.00\n",
      "| end of epoch  53 | time: 500.51s | test loss  2.66 | test ppl 14.27 | acc 0.710\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 54 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.703\n",
      "  Average training loss discriminator: 0.735\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.385\n",
      "  Test Loss: 4.035\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  54 |     0/ 1249 batches | lr 0.000000 | ms/batch 488.71 | loss  0.03 | ppl     1.03 | acc     0.61 | train_ae_norm     1.00\n",
      "[54/200][99/1249] Loss_D: 0.01811730 (Loss_D_real: 0.01333180 Loss_D_fake: 0.00478549) Loss_G: 0.27393132 Loss_Enh_Dec: -1.11751068\n",
      "| epoch  54 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.79 | loss  3.23 | ppl    25.35 | acc     0.63 | train_ae_norm     1.00\n",
      "[54/200][199/1249] Loss_D: 0.02599346 (Loss_D_real: 0.02044682 Loss_D_fake: 0.00554663) Loss_G: 0.27934700 Loss_Enh_Dec: -1.06987560\n",
      "| epoch  54 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.87 | loss  3.24 | ppl    25.60 | acc     0.63 | train_ae_norm     1.00\n",
      "[54/200][299/1249] Loss_D: 0.01879773 (Loss_D_real: 0.01181819 Loss_D_fake: 0.00697954) Loss_G: 0.26745978 Loss_Enh_Dec: -0.70572871\n",
      "| epoch  54 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.85 | loss  3.20 | ppl    24.47 | acc     0.63 | train_ae_norm     1.00\n",
      "[54/200][399/1249] Loss_D: 0.01266636 (Loss_D_real: 0.00530654 Loss_D_fake: 0.00735982) Loss_G: 0.27744681 Loss_Enh_Dec: -0.75081247\n",
      "| epoch  54 |   400/ 1249 batches | lr 0.000000 | ms/batch 357.57 | loss  3.18 | ppl    23.98 | acc     0.61 | train_ae_norm     1.00\n",
      "[54/200][499/1249] Loss_D: 0.01954980 (Loss_D_real: 0.01278686 Loss_D_fake: 0.00676294) Loss_G: 0.26599595 Loss_Enh_Dec: -0.94502735\n",
      "| epoch  54 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.86 | loss  3.20 | ppl    24.52 | acc     0.61 | train_ae_norm     1.00\n",
      "[54/200][599/1249] Loss_D: 0.02171333 (Loss_D_real: 0.00921992 Loss_D_fake: 0.01249342) Loss_G: 0.26215360 Loss_Enh_Dec: -1.00970757\n",
      "| epoch  54 |   600/ 1249 batches | lr 0.000000 | ms/batch 357.77 | loss  3.23 | ppl    25.30 | acc     0.64 | train_ae_norm     1.00\n",
      "[54/200][699/1249] Loss_D: 0.02084849 (Loss_D_real: 0.00628629 Loss_D_fake: 0.01456220) Loss_G: 0.25359127 Loss_Enh_Dec: -1.24145401\n",
      "| epoch  54 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.70 | loss  3.28 | ppl    26.48 | acc     0.60 | train_ae_norm     1.00\n",
      "[54/200][799/1249] Loss_D: 0.03630357 (Loss_D_real: 0.02439841 Loss_D_fake: 0.01190515) Loss_G: 0.24796419 Loss_Enh_Dec: -1.10668671\n",
      "| epoch  54 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.99 | loss  3.34 | ppl    28.36 | acc     0.57 | train_ae_norm     1.00\n",
      "[54/200][899/1249] Loss_D: 0.02822003 (Loss_D_real: 0.01597693 Loss_D_fake: 0.01224310) Loss_G: 0.26253501 Loss_Enh_Dec: -0.97486305\n",
      "| epoch  54 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.53 | loss  3.35 | ppl    28.51 | acc     0.57 | train_ae_norm     1.00\n",
      "[54/200][999/1249] Loss_D: 0.03392019 (Loss_D_real: 0.02838833 Loss_D_fake: 0.00553187) Loss_G: 0.26906046 Loss_Enh_Dec: -0.87277359\n",
      "| epoch  54 |  1000/ 1249 batches | lr 0.000000 | ms/batch 357.23 | loss  3.40 | ppl    29.98 | acc     0.58 | train_ae_norm     1.00\n",
      "[54/200][1099/1249] Loss_D: 0.01720789 (Loss_D_real: 0.00851719 Loss_D_fake: 0.00869070) Loss_G: 0.27052051 Loss_Enh_Dec: -0.90764582\n",
      "| epoch  54 |  1100/ 1249 batches | lr 0.000000 | ms/batch 357.31 | loss  3.42 | ppl    30.52 | acc     0.58 | train_ae_norm     1.00\n",
      "[54/200][1199/1249] Loss_D: 0.01517027 (Loss_D_real: 0.00408888 Loss_D_fake: 0.01108139) Loss_G: 0.26645136 Loss_Enh_Dec: -0.92245102\n",
      "| epoch  54 |  1200/ 1249 batches | lr 0.000000 | ms/batch 357.47 | loss  3.38 | ppl    29.51 | acc     0.60 | train_ae_norm     1.00\n",
      "| end of epoch  54 | time: 501.23s | test loss  2.85 | test ppl 17.31 | acc 0.683\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 55 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.716\n",
      "  Training epcoh took: 0:00:48\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.390\n",
      "  Test Loss: 4.072\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  55 |     0/ 1249 batches | lr 0.000000 | ms/batch 487.59 | loss  0.03 | ppl     1.04 | acc     0.58 | train_ae_norm     1.00\n",
      "[55/200][99/1249] Loss_D: 0.01711031 (Loss_D_real: 0.00861002 Loss_D_fake: 0.00850029) Loss_G: 0.26034865 Loss_Enh_Dec: -1.02105319\n",
      "| epoch  55 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.16 | loss  3.40 | ppl    29.87 | acc     0.55 | train_ae_norm     1.00\n",
      "[55/200][199/1249] Loss_D: 0.01116546 (Loss_D_real: 0.00374525 Loss_D_fake: 0.00742021) Loss_G: 0.26096770 Loss_Enh_Dec: -0.98014766\n",
      "| epoch  55 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.67 | loss  3.31 | ppl    27.27 | acc     0.65 | train_ae_norm     1.00\n",
      "[55/200][299/1249] Loss_D: 0.02116988 (Loss_D_real: 0.00550184 Loss_D_fake: 0.01566804) Loss_G: 0.25614330 Loss_Enh_Dec: -1.08695972\n",
      "| epoch  55 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.73 | loss  3.27 | ppl    26.38 | acc     0.63 | train_ae_norm     1.00\n",
      "[55/200][399/1249] Loss_D: 0.01894000 (Loss_D_real: 0.00592559 Loss_D_fake: 0.01301441) Loss_G: 0.26697689 Loss_Enh_Dec: -1.14610279\n",
      "| epoch  55 |   400/ 1249 batches | lr 0.000000 | ms/batch 356.58 | loss  3.19 | ppl    24.36 | acc     0.63 | train_ae_norm     1.00\n",
      "[55/200][499/1249] Loss_D: 0.01280681 (Loss_D_real: 0.00739698 Loss_D_fake: 0.00540983) Loss_G: 0.30466840 Loss_Enh_Dec: -0.95568258\n",
      "| epoch  55 |   500/ 1249 batches | lr 0.000000 | ms/batch 357.10 | loss  3.28 | ppl    26.68 | acc     0.58 | train_ae_norm     1.00\n",
      "[55/200][599/1249] Loss_D: 0.01729756 (Loss_D_real: 0.00929204 Loss_D_fake: 0.00800552) Loss_G: 0.26261115 Loss_Enh_Dec: -0.69337797\n",
      "| epoch  55 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.11 | loss  3.32 | ppl    27.56 | acc     0.63 | train_ae_norm     1.00\n",
      "[55/200][699/1249] Loss_D: 0.02561786 (Loss_D_real: 0.01312048 Loss_D_fake: 0.01249737) Loss_G: 0.25065824 Loss_Enh_Dec: -0.91516763\n",
      "| epoch  55 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.12 | loss  3.33 | ppl    28.05 | acc     0.58 | train_ae_norm     1.00\n",
      "[55/200][799/1249] Loss_D: 0.02955008 (Loss_D_real: 0.01841561 Loss_D_fake: 0.01113446) Loss_G: 0.27224663 Loss_Enh_Dec: -0.80719948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  55 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.50 | loss  3.44 | ppl    31.19 | acc     0.59 | train_ae_norm     1.00\n",
      "[55/200][899/1249] Loss_D: 0.04771990 (Loss_D_real: 0.03177933 Loss_D_fake: 0.01594057) Loss_G: 0.24857847 Loss_Enh_Dec: -0.77367836\n",
      "| epoch  55 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.49 | loss  3.45 | ppl    31.41 | acc     0.59 | train_ae_norm     1.00\n",
      "[55/200][999/1249] Loss_D: 0.03355242 (Loss_D_real: 0.01918854 Loss_D_fake: 0.01436388) Loss_G: 0.26339397 Loss_Enh_Dec: -0.83239710\n",
      "| epoch  55 |  1000/ 1249 batches | lr 0.000000 | ms/batch 355.66 | loss  3.50 | ppl    33.06 | acc     0.59 | train_ae_norm     1.00\n",
      "[55/200][1099/1249] Loss_D: 0.02367746 (Loss_D_real: 0.01481211 Loss_D_fake: 0.00886535) Loss_G: 0.26224741 Loss_Enh_Dec: -0.89936012\n",
      "| epoch  55 |  1100/ 1249 batches | lr 0.000000 | ms/batch 355.65 | loss  3.49 | ppl    32.88 | acc     0.56 | train_ae_norm     1.00\n",
      "[55/200][1199/1249] Loss_D: 0.02454851 (Loss_D_real: 0.00959644 Loss_D_fake: 0.01495207) Loss_G: 0.26487407 Loss_Enh_Dec: -1.05904424\n",
      "| epoch  55 |  1200/ 1249 batches | lr 0.000000 | ms/batch 356.39 | loss  3.50 | ppl    32.96 | acc     0.58 | train_ae_norm     1.00\n",
      "| end of epoch  55 | time: 500.28s | test loss  2.90 | test ppl 18.20 | acc 0.681\n",
      "bleu_self:  [0.8087415  0.67984718 0.42105325 0.01186826 0.00149423]\n",
      "bleu_test:  [9.58333333e-01 5.43103119e-01 1.75688998e-01 3.96381612e-03\n",
      " 4.99449480e-04]\n",
      "bleu_self: [0.80874150,0.67984718,0.42105325,0.01186826,0.00149423]\n",
      "bleu_test: [0.95833333,0.54310312,0.17568900,0.00396382,0.00049945]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 56 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.717\n",
      "  Training epcoh took: 0:00:48\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.395\n",
      "  Test Loss: 4.099\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  56 |     0/ 1249 batches | lr 0.000000 | ms/batch 488.27 | loss  0.04 | ppl     1.04 | acc     0.56 | train_ae_norm     1.00\n",
      "[56/200][99/1249] Loss_D: 0.03818846 (Loss_D_real: 0.01433508 Loss_D_fake: 0.02385338) Loss_G: 0.27725247 Loss_Enh_Dec: -0.64490050\n",
      "| epoch  56 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.44 | loss  3.50 | ppl    33.24 | acc     0.57 | train_ae_norm     1.00\n",
      "[56/200][199/1249] Loss_D: 0.05099809 (Loss_D_real: 0.01443955 Loss_D_fake: 0.03655854) Loss_G: 0.24941449 Loss_Enh_Dec: -0.86416513\n",
      "| epoch  56 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.00 | loss  3.48 | ppl    32.55 | acc     0.63 | train_ae_norm     1.00\n",
      "[56/200][299/1249] Loss_D: 0.05757554 (Loss_D_real: 0.03823372 Loss_D_fake: 0.01934182) Loss_G: 0.25410601 Loss_Enh_Dec: -0.60362834\n",
      "| epoch  56 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.21 | loss  3.43 | ppl    30.86 | acc     0.58 | train_ae_norm     1.00\n",
      "[56/200][399/1249] Loss_D: 0.02886708 (Loss_D_real: 0.01326704 Loss_D_fake: 0.01560004) Loss_G: 0.25222936 Loss_Enh_Dec: -0.80022204\n",
      "| epoch  56 |   400/ 1249 batches | lr 0.000000 | ms/batch 357.00 | loss  3.42 | ppl    30.66 | acc     0.62 | train_ae_norm     1.00\n",
      "[56/200][499/1249] Loss_D: 0.02275746 (Loss_D_real: 0.01366786 Loss_D_fake: 0.00908960) Loss_G: 0.26409838 Loss_Enh_Dec: -0.75653118\n",
      "| epoch  56 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.90 | loss  3.48 | ppl    32.43 | acc     0.57 | train_ae_norm     1.00\n",
      "[56/200][599/1249] Loss_D: 0.04666288 (Loss_D_real: 0.03390346 Loss_D_fake: 0.01275942) Loss_G: 0.26256308 Loss_Enh_Dec: -1.05583274\n",
      "| epoch  56 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.83 | loss  3.48 | ppl    32.59 | acc     0.58 | train_ae_norm     1.00\n",
      "[56/200][699/1249] Loss_D: 0.03191616 (Loss_D_real: 0.01326958 Loss_D_fake: 0.01864658) Loss_G: 0.26536262 Loss_Enh_Dec: -1.08706439\n",
      "| epoch  56 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.47 | loss  3.41 | ppl    30.41 | acc     0.52 | train_ae_norm     1.00\n",
      "[56/200][799/1249] Loss_D: 0.04958827 (Loss_D_real: 0.01834227 Loss_D_fake: 0.03124600) Loss_G: 0.26200151 Loss_Enh_Dec: -1.02735794\n",
      "| epoch  56 |   800/ 1249 batches | lr 0.000000 | ms/batch 357.30 | loss  3.47 | ppl    32.14 | acc     0.58 | train_ae_norm     1.00\n",
      "[56/200][899/1249] Loss_D: 0.14005712 (Loss_D_real: 0.07311273 Loss_D_fake: 0.06694438) Loss_G: 0.35769811 Loss_Enh_Dec: -0.97422737\n",
      "| epoch  56 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.78 | loss  3.39 | ppl    29.54 | acc     0.57 | train_ae_norm     1.00\n",
      "[56/200][999/1249] Loss_D: 0.02797194 (Loss_D_real: 0.00897367 Loss_D_fake: 0.01899827) Loss_G: 0.22201625 Loss_Enh_Dec: -0.92642844\n",
      "| epoch  56 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.22 | loss  3.35 | ppl    28.50 | acc     0.60 | train_ae_norm     1.00\n",
      "[56/200][1099/1249] Loss_D: 0.02875458 (Loss_D_real: 0.01748187 Loss_D_fake: 0.01127271) Loss_G: 0.24780388 Loss_Enh_Dec: -1.04540539\n",
      "| epoch  56 |  1100/ 1249 batches | lr 0.000000 | ms/batch 355.76 | loss  3.38 | ppl    29.38 | acc     0.58 | train_ae_norm     1.00\n",
      "[56/200][1199/1249] Loss_D: 0.02971073 (Loss_D_real: 0.01472767 Loss_D_fake: 0.01498306) Loss_G: 0.25370011 Loss_Enh_Dec: -1.08684671\n",
      "| epoch  56 |  1200/ 1249 batches | lr 0.000000 | ms/batch 355.89 | loss  3.38 | ppl    29.42 | acc     0.62 | train_ae_norm     1.00\n",
      "| end of epoch  56 | time: 500.61s | test loss  2.85 | test ppl 17.34 | acc 0.694\n",
      "bleu_self:  [3.92857142e-01 2.50005174e-04 2.50000194e-05 7.90569554e-06\n",
      " 3.96224971e-06]\n",
      "bleu_test:  [9.99999999e-01 2.88925139e-01 5.07120803e-02 1.59000608e-05\n",
      " 4.02302299e-06]\n",
      "bleu_self: [0.39285714,0.00025001,0.00002500,0.00000791,0.00000396]\n",
      "bleu_test: [1.00000000,0.28892514,0.05071208,0.00001590,0.00000402]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 57 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.729\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.388\n",
      "  Test Loss: 4.221\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  57 |     0/ 1249 batches | lr 0.000000 | ms/batch 489.51 | loss  0.04 | ppl     1.04 | acc     0.58 | train_ae_norm     1.00\n",
      "[57/200][99/1249] Loss_D: 0.02720695 (Loss_D_real: 0.01008101 Loss_D_fake: 0.01712595) Loss_G: 0.26695189 Loss_Enh_Dec: -1.07433665\n",
      "| epoch  57 |   100/ 1249 batches | lr 0.000000 | ms/batch 357.01 | loss  3.46 | ppl    31.79 | acc     0.61 | train_ae_norm     1.00\n",
      "[57/200][199/1249] Loss_D: 0.03891462 (Loss_D_real: 0.02880614 Loss_D_fake: 0.01010848) Loss_G: 0.24561505 Loss_Enh_Dec: -1.11920071\n",
      "| epoch  57 |   200/ 1249 batches | lr 0.000000 | ms/batch 355.97 | loss  3.46 | ppl    31.73 | acc     0.60 | train_ae_norm     1.00\n",
      "[57/200][299/1249] Loss_D: 0.02159512 (Loss_D_real: 0.00465405 Loss_D_fake: 0.01694107) Loss_G: 0.26370478 Loss_Enh_Dec: -0.96818489\n",
      "| epoch  57 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.25 | loss  3.42 | ppl    30.64 | acc     0.60 | train_ae_norm     1.00\n",
      "[57/200][399/1249] Loss_D: 0.03595325 (Loss_D_real: 0.02160943 Loss_D_fake: 0.01434382) Loss_G: 0.26855761 Loss_Enh_Dec: -0.99901879\n",
      "| epoch  57 |   400/ 1249 batches | lr 0.000000 | ms/batch 355.65 | loss  3.34 | ppl    28.24 | acc     0.65 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57/200][499/1249] Loss_D: 0.01842082 (Loss_D_real: 0.00965186 Loss_D_fake: 0.00876896) Loss_G: 0.28425714 Loss_Enh_Dec: -1.00327933\n",
      "| epoch  57 |   500/ 1249 batches | lr 0.000000 | ms/batch 355.88 | loss  3.37 | ppl    29.07 | acc     0.58 | train_ae_norm     1.00\n",
      "[57/200][599/1249] Loss_D: 0.02124988 (Loss_D_real: 0.01103097 Loss_D_fake: 0.01021891) Loss_G: 0.25561267 Loss_Enh_Dec: -0.98029131\n",
      "| epoch  57 |   600/ 1249 batches | lr 0.000000 | ms/batch 357.34 | loss  3.35 | ppl    28.52 | acc     0.62 | train_ae_norm     1.00\n",
      "[57/200][699/1249] Loss_D: 0.01954842 (Loss_D_real: 0.00965133 Loss_D_fake: 0.00989709) Loss_G: 0.29798770 Loss_Enh_Dec: -0.83549577\n",
      "| epoch  57 |   700/ 1249 batches | lr 0.000000 | ms/batch 355.97 | loss  3.29 | ppl    26.85 | acc     0.60 | train_ae_norm     1.00\n",
      "[57/200][799/1249] Loss_D: 0.05251194 (Loss_D_real: 0.02295109 Loss_D_fake: 0.02956085) Loss_G: 0.17648084 Loss_Enh_Dec: -0.69311029\n",
      "| epoch  57 |   800/ 1249 batches | lr 0.000000 | ms/batch 357.63 | loss  3.29 | ppl    26.71 | acc     0.57 | train_ae_norm     1.00\n",
      "[57/200][899/1249] Loss_D: 0.03649261 (Loss_D_real: 0.01376551 Loss_D_fake: 0.02272710) Loss_G: 0.18861163 Loss_Enh_Dec: -0.77881449\n",
      "| epoch  57 |   900/ 1249 batches | lr 0.000000 | ms/batch 357.02 | loss  3.22 | ppl    25.04 | acc     0.61 | train_ae_norm     1.00\n",
      "[57/200][999/1249] Loss_D: 0.02995636 (Loss_D_real: 0.01232247 Loss_D_fake: 0.01763389) Loss_G: 0.20163682 Loss_Enh_Dec: -0.77462018\n",
      "| epoch  57 |  1000/ 1249 batches | lr 0.000000 | ms/batch 357.45 | loss  3.20 | ppl    24.45 | acc     0.62 | train_ae_norm     1.00\n",
      "[57/200][1099/1249] Loss_D: 0.03407532 (Loss_D_real: 0.01809735 Loss_D_fake: 0.01597797) Loss_G: 0.20579755 Loss_Enh_Dec: -0.58231306\n",
      "| epoch  57 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.77 | loss  3.19 | ppl    24.25 | acc     0.60 | train_ae_norm     1.00\n",
      "[57/200][1199/1249] Loss_D: 0.02280954 (Loss_D_real: 0.00878466 Loss_D_fake: 0.01402487) Loss_G: 0.21366191 Loss_Enh_Dec: -0.54815668\n",
      "| epoch  57 |  1200/ 1249 batches | lr 0.000000 | ms/batch 357.22 | loss  3.14 | ppl    23.08 | acc     0.63 | train_ae_norm     1.00\n",
      "| end of epoch  57 | time: 500.82s | test loss  2.59 | test ppl 13.36 | acc 0.719\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 58 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.724\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.393\n",
      "  Test Loss: 4.235\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  58 |     0/ 1249 batches | lr 0.000000 | ms/batch 493.50 | loss  0.03 | ppl     1.03 | acc     0.64 | train_ae_norm     1.00\n",
      "[58/200][99/1249] Loss_D: 0.04228522 (Loss_D_real: 0.02907574 Loss_D_fake: 0.01320948) Loss_G: 0.21984620 Loss_Enh_Dec: -0.41259405\n",
      "| epoch  58 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.28 | loss  3.11 | ppl    22.42 | acc     0.61 | train_ae_norm     1.00\n",
      "[58/200][199/1249] Loss_D: 0.02795588 (Loss_D_real: 0.01526167 Loss_D_fake: 0.01269421) Loss_G: 0.22190309 Loss_Enh_Dec: -0.78416055\n",
      "| epoch  58 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.13 | loss  3.09 | ppl    21.93 | acc     0.69 | train_ae_norm     1.00\n",
      "[58/200][299/1249] Loss_D: 0.04376748 (Loss_D_real: 0.02751555 Loss_D_fake: 0.01625193) Loss_G: 0.20763512 Loss_Enh_Dec: -1.13834465\n",
      "| epoch  58 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.78 | loss  3.14 | ppl    23.10 | acc     0.63 | train_ae_norm     1.00\n",
      "[58/200][399/1249] Loss_D: 0.01860568 (Loss_D_real: 0.00325925 Loss_D_fake: 0.01534642) Loss_G: 0.20622258 Loss_Enh_Dec: -1.12828863\n",
      "| epoch  58 |   400/ 1249 batches | lr 0.000000 | ms/batch 356.34 | loss  3.10 | ppl    22.30 | acc     0.63 | train_ae_norm     1.00\n",
      "[58/200][499/1249] Loss_D: 0.02916083 (Loss_D_real: 0.01534936 Loss_D_fake: 0.01381148) Loss_G: 0.21825059 Loss_Enh_Dec: -1.03479409\n",
      "| epoch  58 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.77 | loss  3.13 | ppl    22.93 | acc     0.62 | train_ae_norm     1.00\n",
      "[58/200][599/1249] Loss_D: 0.02037351 (Loss_D_real: 0.00865634 Loss_D_fake: 0.01171717) Loss_G: 0.22659598 Loss_Enh_Dec: -0.91092443\n",
      "| epoch  58 |   600/ 1249 batches | lr 0.000000 | ms/batch 355.86 | loss  3.08 | ppl    21.73 | acc     0.66 | train_ae_norm     1.00\n",
      "[58/200][699/1249] Loss_D: 0.01879071 (Loss_D_real: 0.00671623 Loss_D_fake: 0.01207448) Loss_G: 0.22360671 Loss_Enh_Dec: -1.03076220\n",
      "| epoch  58 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.58 | loss  3.08 | ppl    21.75 | acc     0.62 | train_ae_norm     1.00\n",
      "[58/200][799/1249] Loss_D: 0.01944472 (Loss_D_real: 0.00825657 Loss_D_fake: 0.01118814) Loss_G: 0.23904514 Loss_Enh_Dec: -1.06031990\n",
      "| epoch  58 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.27 | loss  3.19 | ppl    24.19 | acc     0.58 | train_ae_norm     1.00\n",
      "[58/200][899/1249] Loss_D: 0.01764989 (Loss_D_real: 0.00797207 Loss_D_fake: 0.00967782) Loss_G: 0.23647884 Loss_Enh_Dec: -1.02656364\n",
      "| epoch  58 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.26 | loss  3.17 | ppl    23.88 | acc     0.64 | train_ae_norm     1.00\n",
      "[58/200][999/1249] Loss_D: 0.01347498 (Loss_D_real: 0.00458733 Loss_D_fake: 0.00888764) Loss_G: 0.24344349 Loss_Enh_Dec: -1.16266763\n",
      "| epoch  58 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.32 | loss  3.17 | ppl    23.82 | acc     0.67 | train_ae_norm     1.00\n",
      "[58/200][1099/1249] Loss_D: 0.02292130 (Loss_D_real: 0.01057953 Loss_D_fake: 0.01234177) Loss_G: 0.25320917 Loss_Enh_Dec: -0.64948958\n",
      "| epoch  58 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.73 | loss  3.21 | ppl    24.74 | acc     0.62 | train_ae_norm     1.00\n",
      "[58/200][1199/1249] Loss_D: 0.01707536 (Loss_D_real: 0.01363287 Loss_D_fake: 0.00344248) Loss_G: 0.30707121 Loss_Enh_Dec: -0.95064086\n",
      "| epoch  58 |  1200/ 1249 batches | lr 0.000000 | ms/batch 356.82 | loss  3.15 | ppl    23.40 | acc     0.63 | train_ae_norm     1.00\n",
      "| end of epoch  58 | time: 501.04s | test loss  2.60 | test ppl 13.53 | acc 0.719\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 59 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.716\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.388\n",
      "  Test Loss: 4.298\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  59 |     0/ 1249 batches | lr 0.000000 | ms/batch 493.09 | loss  0.03 | ppl     1.03 | acc     0.64 | train_ae_norm     1.00\n",
      "[59/200][99/1249] Loss_D: 0.01603501 (Loss_D_real: 0.00414996 Loss_D_fake: 0.01188505) Loss_G: 0.24753022 Loss_Enh_Dec: -0.71432638\n",
      "| epoch  59 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.81 | loss  3.10 | ppl    22.25 | acc     0.62 | train_ae_norm     1.00\n",
      "[59/200][199/1249] Loss_D: 0.08934399 (Loss_D_real: 0.08587223 Loss_D_fake: 0.00347176) Loss_G: 0.34710616 Loss_Enh_Dec: -0.81709272\n",
      "| epoch  59 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.43 | loss  3.09 | ppl    22.01 | acc     0.67 | train_ae_norm     1.00\n",
      "[59/200][299/1249] Loss_D: 0.01875209 (Loss_D_real: 0.01004533 Loss_D_fake: 0.00870676) Loss_G: 0.24542475 Loss_Enh_Dec: -0.80101031\n",
      "| epoch  59 |   300/ 1249 batches | lr 0.000000 | ms/batch 357.64 | loss  3.05 | ppl    21.14 | acc     0.66 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59/200][399/1249] Loss_D: 0.01594627 (Loss_D_real: 0.00809066 Loss_D_fake: 0.00785562) Loss_G: 0.25187704 Loss_Enh_Dec: -0.88321495\n",
      "| epoch  59 |   400/ 1249 batches | lr 0.000000 | ms/batch 357.09 | loss  3.02 | ppl    20.43 | acc     0.66 | train_ae_norm     1.00\n",
      "[59/200][499/1249] Loss_D: 0.02236137 (Loss_D_real: 0.01304640 Loss_D_fake: 0.00931497) Loss_G: 0.25892806 Loss_Enh_Dec: -0.73571372\n",
      "| epoch  59 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.54 | loss  3.02 | ppl    20.43 | acc     0.64 | train_ae_norm     1.00\n",
      "[59/200][599/1249] Loss_D: 0.01172464 (Loss_D_real: 0.00471779 Loss_D_fake: 0.00700685) Loss_G: 0.26437417 Loss_Enh_Dec: -0.63992643\n",
      "| epoch  59 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.79 | loss  2.99 | ppl    19.91 | acc     0.66 | train_ae_norm     1.00\n",
      "[59/200][699/1249] Loss_D: 0.01333489 (Loss_D_real: 0.00657039 Loss_D_fake: 0.00676449) Loss_G: 0.27094552 Loss_Enh_Dec: -0.89397508\n",
      "| epoch  59 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.57 | loss  2.97 | ppl    19.43 | acc     0.65 | train_ae_norm     1.00\n",
      "[59/200][799/1249] Loss_D: 0.01656934 (Loss_D_real: 0.00985847 Loss_D_fake: 0.00671086) Loss_G: 0.27322313 Loss_Enh_Dec: -0.87860107\n",
      "| epoch  59 |   800/ 1249 batches | lr 0.000000 | ms/batch 357.18 | loss  2.99 | ppl    19.97 | acc     0.63 | train_ae_norm     1.00\n",
      "[59/200][899/1249] Loss_D: 0.00793691 (Loss_D_real: 0.00282911 Loss_D_fake: 0.00510780) Loss_G: 0.27848613 Loss_Enh_Dec: -1.11227596\n",
      "| epoch  59 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.64 | loss  2.94 | ppl    19.01 | acc     0.66 | train_ae_norm     1.00\n",
      "[59/200][999/1249] Loss_D: 0.02715916 (Loss_D_real: 0.01765713 Loss_D_fake: 0.00950203) Loss_G: 0.24579166 Loss_Enh_Dec: -1.07178140\n",
      "| epoch  59 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.57 | loss  2.93 | ppl    18.65 | acc     0.64 | train_ae_norm     1.00\n",
      "[59/200][1099/1249] Loss_D: 0.01163440 (Loss_D_real: 0.00334379 Loss_D_fake: 0.00829061) Loss_G: 0.26314715 Loss_Enh_Dec: -1.04620016\n",
      "| epoch  59 |  1100/ 1249 batches | lr 0.000000 | ms/batch 355.69 | loss  2.94 | ppl    18.93 | acc     0.66 | train_ae_norm     1.00\n",
      "[59/200][1199/1249] Loss_D: 0.00964506 (Loss_D_real: 0.00355454 Loss_D_fake: 0.00609052) Loss_G: 0.28132826 Loss_Enh_Dec: -1.03410614\n",
      "| epoch  59 |  1200/ 1249 batches | lr 0.000000 | ms/batch 355.99 | loss  2.88 | ppl    17.89 | acc     0.67 | train_ae_norm     1.00\n",
      "| end of epoch  59 | time: 501.24s | test loss  2.45 | test ppl 11.60 | acc 0.734\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 60 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.712\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.388\n",
      "  Test Loss: 4.338\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  60 |     0/ 1249 batches | lr 0.000000 | ms/batch 488.54 | loss  0.03 | ppl     1.03 | acc     0.65 | train_ae_norm     1.00\n",
      "[60/200][99/1249] Loss_D: 0.02438847 (Loss_D_real: 0.01651269 Loss_D_fake: 0.00787578) Loss_G: 0.27217031 Loss_Enh_Dec: -1.08580351\n",
      "| epoch  60 |   100/ 1249 batches | lr 0.000000 | ms/batch 355.94 | loss  2.94 | ppl    18.88 | acc     0.64 | train_ae_norm     1.00\n",
      "[60/200][199/1249] Loss_D: 0.01065230 (Loss_D_real: 0.00560651 Loss_D_fake: 0.00504579) Loss_G: 0.27988338 Loss_Enh_Dec: -1.12533629\n",
      "| epoch  60 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.33 | loss  2.91 | ppl    18.39 | acc     0.68 | train_ae_norm     1.00\n",
      "[60/200][299/1249] Loss_D: 0.00858696 (Loss_D_real: 0.00387857 Loss_D_fake: 0.00470839) Loss_G: 0.29154566 Loss_Enh_Dec: -0.95864677\n",
      "| epoch  60 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.13 | loss  2.93 | ppl    18.71 | acc     0.67 | train_ae_norm     1.00\n",
      "[60/200][399/1249] Loss_D: 0.01011173 (Loss_D_real: 0.00388790 Loss_D_fake: 0.00622382) Loss_G: 0.28440389 Loss_Enh_Dec: -1.19174802\n",
      "| epoch  60 |   400/ 1249 batches | lr 0.000000 | ms/batch 356.46 | loss  2.93 | ppl    18.78 | acc     0.69 | train_ae_norm     1.00\n",
      "[60/200][499/1249] Loss_D: 0.01013478 (Loss_D_real: 0.00322916 Loss_D_fake: 0.00690561) Loss_G: 0.27764603 Loss_Enh_Dec: -1.12161005\n",
      "| epoch  60 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.95 | loss  2.95 | ppl    19.03 | acc     0.63 | train_ae_norm     1.00\n",
      "[60/200][599/1249] Loss_D: 0.01747597 (Loss_D_real: 0.01382951 Loss_D_fake: 0.00364646) Loss_G: 0.28179184 Loss_Enh_Dec: -1.07992232\n",
      "| epoch  60 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.87 | loss  2.93 | ppl    18.64 | acc     0.68 | train_ae_norm     1.00\n",
      "[60/200][699/1249] Loss_D: 0.02073474 (Loss_D_real: 0.01219030 Loss_D_fake: 0.00854444) Loss_G: 0.27647147 Loss_Enh_Dec: -0.94913912\n",
      "| epoch  60 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.39 | loss  2.97 | ppl    19.51 | acc     0.64 | train_ae_norm     1.00\n",
      "[60/200][799/1249] Loss_D: 0.01266815 (Loss_D_real: 0.00659613 Loss_D_fake: 0.00607201) Loss_G: 0.26774141 Loss_Enh_Dec: -1.24344945\n",
      "| epoch  60 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.52 | loss  3.05 | ppl    21.13 | acc     0.60 | train_ae_norm     1.00\n",
      "[60/200][899/1249] Loss_D: 0.01001668 (Loss_D_real: 0.00440421 Loss_D_fake: 0.00561247) Loss_G: 0.29594666 Loss_Enh_Dec: -1.09852564\n",
      "| epoch  60 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.45 | loss  2.99 | ppl    19.94 | acc     0.64 | train_ae_norm     1.00\n",
      "[60/200][999/1249] Loss_D: 0.01373192 (Loss_D_real: 0.00567105 Loss_D_fake: 0.00806087) Loss_G: 0.26595676 Loss_Enh_Dec: -0.97130364\n",
      "| epoch  60 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.26 | loss  3.00 | ppl    20.05 | acc     0.66 | train_ae_norm     1.00\n",
      "[60/200][1099/1249] Loss_D: 0.03354049 (Loss_D_real: 0.02726142 Loss_D_fake: 0.00627907) Loss_G: 0.27680299 Loss_Enh_Dec: -1.10795653\n",
      "| epoch  60 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.05 | loss  3.02 | ppl    20.42 | acc     0.60 | train_ae_norm     1.00\n",
      "[60/200][1199/1249] Loss_D: 0.34264141 (Loss_D_real: 0.00613864 Loss_D_fake: 0.33650276) Loss_G: 0.21520841 Loss_Enh_Dec: -1.00038218\n",
      "| epoch  60 |  1200/ 1249 batches | lr 0.000000 | ms/batch 355.56 | loss  2.95 | ppl    19.12 | acc     0.65 | train_ae_norm     1.00\n",
      "| end of epoch  60 | time: 500.26s | test loss  2.45 | test ppl 11.53 | acc 0.733\n",
      "bleu_self:  [3.61105471e-01 1.25000006e-01 1.25002431e-06 4.75935922e-09\n",
      " 1.06687231e-08]\n",
      "bleu_test:  [8.14583333e-01 2.23394519e-01 2.01442654e-06 7.15638438e-09\n",
      " 1.15110269e-08]\n",
      "bleu_self: [0.36110547,0.12500001,0.00000125,0.00000000,0.00000001]\n",
      "bleu_test: [0.81458333,0.22339452,0.00000201,0.00000001,0.00000001]\n",
      "New saving model: epoch 060.\n",
      "Saving models to ./results/yahoo_merge_results\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 61 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.720\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.398\n",
      "  Test Loss: 4.299\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  61 |     0/ 1249 batches | lr 0.000000 | ms/batch 488.47 | loss  0.03 | ppl     1.03 | acc     0.64 | train_ae_norm     1.00\n",
      "[61/200][99/1249] Loss_D: 0.01766352 (Loss_D_real: 0.00702514 Loss_D_fake: 0.01063838) Loss_G: 0.26217780 Loss_Enh_Dec: -0.93040556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  61 |   100/ 1249 batches | lr 0.000000 | ms/batch 355.70 | loss  2.93 | ppl    18.79 | acc     0.66 | train_ae_norm     1.00\n",
      "[61/200][199/1249] Loss_D: 0.01562789 (Loss_D_real: 0.00461541 Loss_D_fake: 0.01101248) Loss_G: 0.24686269 Loss_Enh_Dec: -1.02788544\n",
      "| epoch  61 |   200/ 1249 batches | lr 0.000000 | ms/batch 355.45 | loss  2.88 | ppl    17.89 | acc     0.65 | train_ae_norm     1.00\n",
      "[61/200][299/1249] Loss_D: 0.02016710 (Loss_D_real: 0.00736282 Loss_D_fake: 0.01280428) Loss_G: 0.27484980 Loss_Enh_Dec: -0.99212515\n",
      "| epoch  61 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.94 | loss  2.91 | ppl    18.41 | acc     0.64 | train_ae_norm     1.00\n",
      "[61/200][399/1249] Loss_D: 0.07685243 (Loss_D_real: 0.06953932 Loss_D_fake: 0.00731310) Loss_G: 0.25473070 Loss_Enh_Dec: -1.05587101\n",
      "| epoch  61 |   400/ 1249 batches | lr 0.000000 | ms/batch 355.46 | loss  2.89 | ppl    18.04 | acc     0.68 | train_ae_norm     1.00\n",
      "[61/200][499/1249] Loss_D: 0.02394084 (Loss_D_real: 0.01637172 Loss_D_fake: 0.00756912) Loss_G: 0.27462706 Loss_Enh_Dec: -1.08366966\n",
      "| epoch  61 |   500/ 1249 batches | lr 0.000000 | ms/batch 355.41 | loss  2.93 | ppl    18.80 | acc     0.63 | train_ae_norm     1.00\n",
      "[61/200][599/1249] Loss_D: 0.01231919 (Loss_D_real: 0.00792344 Loss_D_fake: 0.00439575) Loss_G: 0.28808436 Loss_Enh_Dec: -1.15960145\n",
      "| epoch  61 |   600/ 1249 batches | lr 0.000000 | ms/batch 355.25 | loss  2.98 | ppl    19.78 | acc     0.64 | train_ae_norm     1.00\n",
      "[61/200][699/1249] Loss_D: 0.01129195 (Loss_D_real: 0.00447383 Loss_D_fake: 0.00681813) Loss_G: 0.27626809 Loss_Enh_Dec: -0.91452897\n",
      "| epoch  61 |   700/ 1249 batches | lr 0.000000 | ms/batch 355.82 | loss  2.95 | ppl    19.07 | acc     0.61 | train_ae_norm     1.00\n",
      "[61/200][799/1249] Loss_D: 0.00919082 (Loss_D_real: 0.00403351 Loss_D_fake: 0.00515731) Loss_G: 0.29763231 Loss_Enh_Dec: -1.13101685\n",
      "| epoch  61 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.31 | loss  3.00 | ppl    20.03 | acc     0.64 | train_ae_norm     1.00\n",
      "[61/200][899/1249] Loss_D: 0.01712601 (Loss_D_real: 0.00890261 Loss_D_fake: 0.00822340) Loss_G: 0.28093773 Loss_Enh_Dec: -1.04102397\n",
      "| epoch  61 |   900/ 1249 batches | lr 0.000000 | ms/batch 355.68 | loss  3.04 | ppl    20.92 | acc     0.65 | train_ae_norm     1.00\n",
      "[61/200][999/1249] Loss_D: 0.01705544 (Loss_D_real: 0.00505342 Loss_D_fake: 0.01200202) Loss_G: 0.25338951 Loss_Enh_Dec: -0.93797225\n",
      "| epoch  61 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.51 | loss  2.97 | ppl    19.58 | acc     0.64 | train_ae_norm     1.00\n",
      "[61/200][1099/1249] Loss_D: 0.00865822 (Loss_D_real: 0.00477360 Loss_D_fake: 0.00388462) Loss_G: 0.28112867 Loss_Enh_Dec: -1.08133721\n",
      "| epoch  61 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.49 | loss  2.99 | ppl    19.89 | acc     0.64 | train_ae_norm     1.00\n",
      "[61/200][1199/1249] Loss_D: 0.01267305 (Loss_D_real: 0.00681176 Loss_D_fake: 0.00586129) Loss_G: 0.27458975 Loss_Enh_Dec: -1.07746220\n",
      "| epoch  61 |  1200/ 1249 batches | lr 0.000000 | ms/batch 356.88 | loss  2.97 | ppl    19.53 | acc     0.63 | train_ae_norm     1.00\n",
      "| end of epoch  61 | time: 499.91s | test loss  2.52 | test ppl 12.37 | acc 0.728\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 62 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.716\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.385\n",
      "  Test Loss: 4.366\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  62 |     0/ 1249 batches | lr 0.000000 | ms/batch 488.90 | loss  0.03 | ppl     1.03 | acc     0.63 | train_ae_norm     1.00\n",
      "[62/200][99/1249] Loss_D: 0.00945870 (Loss_D_real: 0.00566897 Loss_D_fake: 0.00378973) Loss_G: 0.30801716 Loss_Enh_Dec: -1.09887838\n",
      "| epoch  62 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.59 | loss  3.00 | ppl    20.17 | acc     0.64 | train_ae_norm     1.00\n",
      "[62/200][199/1249] Loss_D: 0.00966299 (Loss_D_real: 0.00312009 Loss_D_fake: 0.00654290) Loss_G: 0.29183835 Loss_Enh_Dec: -1.14764714\n",
      "| epoch  62 |   200/ 1249 batches | lr 0.000000 | ms/batch 355.88 | loss  2.95 | ppl    19.04 | acc     0.66 | train_ae_norm     1.00\n",
      "[62/200][299/1249] Loss_D: 0.00862182 (Loss_D_real: 0.00334680 Loss_D_fake: 0.00527502) Loss_G: 0.28880700 Loss_Enh_Dec: -1.01876616\n",
      "| epoch  62 |   300/ 1249 batches | lr 0.000000 | ms/batch 355.30 | loss  2.94 | ppl    18.83 | acc     0.67 | train_ae_norm     1.00\n",
      "[62/200][399/1249] Loss_D: 0.01810098 (Loss_D_real: 0.01451353 Loss_D_fake: 0.00358745) Loss_G: 0.30668545 Loss_Enh_Dec: -1.18636048\n",
      "| epoch  62 |   400/ 1249 batches | lr 0.000000 | ms/batch 355.51 | loss  2.91 | ppl    18.29 | acc     0.67 | train_ae_norm     1.00\n",
      "[62/200][499/1249] Loss_D: 0.01292073 (Loss_D_real: 0.00408855 Loss_D_fake: 0.00883218) Loss_G: 0.25270757 Loss_Enh_Dec: -0.91412085\n",
      "| epoch  62 |   500/ 1249 batches | lr 0.000000 | ms/batch 355.89 | loss  2.93 | ppl    18.82 | acc     0.63 | train_ae_norm     1.00\n",
      "[62/200][599/1249] Loss_D: 0.00889267 (Loss_D_real: 0.00430524 Loss_D_fake: 0.00458744) Loss_G: 0.29528528 Loss_Enh_Dec: -1.01371777\n",
      "| epoch  62 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.31 | loss  2.87 | ppl    17.68 | acc     0.71 | train_ae_norm     1.00\n",
      "[62/200][699/1249] Loss_D: 0.00911538 (Loss_D_real: 0.00458215 Loss_D_fake: 0.00453323) Loss_G: 0.28916588 Loss_Enh_Dec: -0.90590763\n",
      "| epoch  62 |   700/ 1249 batches | lr 0.000000 | ms/batch 355.91 | loss  2.88 | ppl    17.76 | acc     0.63 | train_ae_norm     1.00\n",
      "[62/200][799/1249] Loss_D: 0.02255379 (Loss_D_real: 0.01616520 Loss_D_fake: 0.00638859) Loss_G: 0.28946236 Loss_Enh_Dec: -1.08059871\n",
      "| epoch  62 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.37 | loss  2.88 | ppl    17.84 | acc     0.66 | train_ae_norm     1.00\n",
      "[62/200][899/1249] Loss_D: 0.00914947 (Loss_D_real: 0.00628977 Loss_D_fake: 0.00285969) Loss_G: 0.31675908 Loss_Enh_Dec: -0.85073549\n",
      "| epoch  62 |   900/ 1249 batches | lr 0.000000 | ms/batch 357.26 | loss  2.87 | ppl    17.57 | acc     0.62 | train_ae_norm     1.00\n",
      "[62/200][999/1249] Loss_D: 0.01533426 (Loss_D_real: 0.01039591 Loss_D_fake: 0.00493835) Loss_G: 0.31217918 Loss_Enh_Dec: -1.28590512\n",
      "| epoch  62 |  1000/ 1249 batches | lr 0.000000 | ms/batch 357.63 | loss  2.90 | ppl    18.16 | acc     0.66 | train_ae_norm     1.00\n",
      "[62/200][1099/1249] Loss_D: 0.01243164 (Loss_D_real: 0.00941804 Loss_D_fake: 0.00301360) Loss_G: 0.30225819 Loss_Enh_Dec: -0.85792679\n",
      "| epoch  62 |  1100/ 1249 batches | lr 0.000000 | ms/batch 357.06 | loss  2.90 | ppl    18.15 | acc     0.65 | train_ae_norm     1.00\n",
      "[62/200][1199/1249] Loss_D: 0.01118977 (Loss_D_real: 0.00714109 Loss_D_fake: 0.00404867) Loss_G: 0.31302193 Loss_Enh_Dec: -1.18728864\n",
      "| epoch  62 |  1200/ 1249 batches | lr 0.000000 | ms/batch 357.40 | loss  2.84 | ppl    17.18 | acc     0.66 | train_ae_norm     1.00\n",
      "| end of epoch  62 | time: 500.58s | test loss  2.43 | test ppl 11.32 | acc 0.738\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 63 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.724\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.390\n",
      "  Test Loss: 4.404\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  63 |     0/ 1249 batches | lr 0.000000 | ms/batch 491.65 | loss  0.03 | ppl     1.03 | acc     0.67 | train_ae_norm     1.00\n",
      "[63/200][99/1249] Loss_D: 0.01450302 (Loss_D_real: 0.00982724 Loss_D_fake: 0.00467578) Loss_G: 0.31333664 Loss_Enh_Dec: -1.29599822\n",
      "| epoch  63 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.72 | loss  2.88 | ppl    17.73 | acc     0.66 | train_ae_norm     1.00\n",
      "[63/200][199/1249] Loss_D: 0.00850676 (Loss_D_real: 0.00276048 Loss_D_fake: 0.00574628) Loss_G: 0.31146508 Loss_Enh_Dec: -1.35811925\n",
      "| epoch  63 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.99 | loss  2.87 | ppl    17.63 | acc     0.65 | train_ae_norm     1.00\n",
      "[63/200][299/1249] Loss_D: 0.01393913 (Loss_D_real: 0.00919366 Loss_D_fake: 0.00474547) Loss_G: 0.29558522 Loss_Enh_Dec: -0.91552204\n",
      "| epoch  63 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.99 | loss  2.89 | ppl    17.91 | acc     0.66 | train_ae_norm     1.00\n",
      "[63/200][399/1249] Loss_D: 0.01621675 (Loss_D_real: 0.01192028 Loss_D_fake: 0.00429648) Loss_G: 0.31489360 Loss_Enh_Dec: -0.98147768\n",
      "| epoch  63 |   400/ 1249 batches | lr 0.000000 | ms/batch 357.68 | loss  2.83 | ppl    16.99 | acc     0.68 | train_ae_norm     1.00\n",
      "[63/200][499/1249] Loss_D: 0.01691715 (Loss_D_real: 0.01273775 Loss_D_fake: 0.00417940) Loss_G: 0.33171555 Loss_Enh_Dec: -1.07202029\n",
      "| epoch  63 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.90 | loss  2.89 | ppl    17.91 | acc     0.65 | train_ae_norm     1.00\n",
      "[63/200][599/1249] Loss_D: 0.01030639 (Loss_D_real: 0.00638748 Loss_D_fake: 0.00391891) Loss_G: 0.31306484 Loss_Enh_Dec: -1.17290330\n",
      "| epoch  63 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.71 | loss  2.86 | ppl    17.47 | acc     0.67 | train_ae_norm     1.00\n",
      "[63/200][699/1249] Loss_D: 0.00637284 (Loss_D_real: 0.00313116 Loss_D_fake: 0.00324168) Loss_G: 0.32273853 Loss_Enh_Dec: -1.19490743\n",
      "| epoch  63 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.92 | loss  2.87 | ppl    17.64 | acc     0.63 | train_ae_norm     1.00\n",
      "[63/200][799/1249] Loss_D: 0.00834608 (Loss_D_real: 0.00319218 Loss_D_fake: 0.00515391) Loss_G: 0.30674940 Loss_Enh_Dec: -1.21077883\n",
      "| epoch  63 |   800/ 1249 batches | lr 0.000000 | ms/batch 357.33 | loss  2.89 | ppl    18.01 | acc     0.64 | train_ae_norm     1.00\n",
      "[63/200][899/1249] Loss_D: 0.01144503 (Loss_D_real: 0.00453938 Loss_D_fake: 0.00690565) Loss_G: 0.29600430 Loss_Enh_Dec: -0.99516910\n",
      "| epoch  63 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.93 | loss  2.83 | ppl    16.92 | acc     0.65 | train_ae_norm     1.00\n",
      "[63/200][999/1249] Loss_D: 0.00968751 (Loss_D_real: 0.00383660 Loss_D_fake: 0.00585091) Loss_G: 0.31522149 Loss_Enh_Dec: -1.02953243\n",
      "| epoch  63 |  1000/ 1249 batches | lr 0.000000 | ms/batch 357.66 | loss  2.78 | ppl    16.09 | acc     0.67 | train_ae_norm     1.00\n",
      "[63/200][1099/1249] Loss_D: 0.00797296 (Loss_D_real: 0.00573608 Loss_D_fake: 0.00223687) Loss_G: 0.33419576 Loss_Enh_Dec: -0.96885031\n",
      "| epoch  63 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.62 | loss  2.79 | ppl    16.30 | acc     0.64 | train_ae_norm     1.00\n",
      "[63/200][1199/1249] Loss_D: 0.01541120 (Loss_D_real: 0.01016839 Loss_D_fake: 0.00524281) Loss_G: 0.29178092 Loss_Enh_Dec: -1.01713741\n",
      "| epoch  63 |  1200/ 1249 batches | lr 0.000000 | ms/batch 358.07 | loss  2.74 | ppl    15.56 | acc     0.65 | train_ae_norm     1.00\n",
      "| end of epoch  63 | time: 501.73s | test loss  2.33 | test ppl 10.24 | acc 0.751\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 64 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.715\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.400\n",
      "  Test Loss: 4.438\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  64 |     0/ 1249 batches | lr 0.000000 | ms/batch 489.82 | loss  0.03 | ppl     1.03 | acc     0.67 | train_ae_norm     1.00\n",
      "[64/200][99/1249] Loss_D: 0.00799952 (Loss_D_real: 0.00300512 Loss_D_fake: 0.00499441) Loss_G: 0.32630500 Loss_Enh_Dec: -1.00969434\n",
      "| epoch  64 |   100/ 1249 batches | lr 0.000000 | ms/batch 357.36 | loss  2.79 | ppl    16.27 | acc     0.68 | train_ae_norm     1.00\n",
      "[64/200][199/1249] Loss_D: 0.00819099 (Loss_D_real: 0.00458120 Loss_D_fake: 0.00360979) Loss_G: 0.30071267 Loss_Enh_Dec: -1.12045622\n",
      "| epoch  64 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.23 | loss  2.81 | ppl    16.67 | acc     0.69 | train_ae_norm     1.00\n",
      "[64/200][299/1249] Loss_D: 0.00603290 (Loss_D_real: 0.00258772 Loss_D_fake: 0.00344518) Loss_G: 0.33073536 Loss_Enh_Dec: -1.17293167\n",
      "| epoch  64 |   300/ 1249 batches | lr 0.000000 | ms/batch 357.04 | loss  2.82 | ppl    16.82 | acc     0.67 | train_ae_norm     1.00\n",
      "[64/200][399/1249] Loss_D: 0.00559461 (Loss_D_real: 0.00277459 Loss_D_fake: 0.00282002) Loss_G: 0.33144855 Loss_Enh_Dec: -0.93916482\n",
      "| epoch  64 |   400/ 1249 batches | lr 0.000000 | ms/batch 357.58 | loss  2.82 | ppl    16.86 | acc     0.70 | train_ae_norm     1.00\n",
      "[64/200][499/1249] Loss_D: 0.01145027 (Loss_D_real: 0.00740768 Loss_D_fake: 0.00404259) Loss_G: 0.30631196 Loss_Enh_Dec: -1.30287480\n",
      "| epoch  64 |   500/ 1249 batches | lr 0.000000 | ms/batch 357.60 | loss  2.86 | ppl    17.52 | acc     0.63 | train_ae_norm     1.00\n",
      "[64/200][599/1249] Loss_D: 0.00586476 (Loss_D_real: 0.00161240 Loss_D_fake: 0.00425237) Loss_G: 0.31576234 Loss_Enh_Dec: -1.09871328\n",
      "| epoch  64 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.59 | loss  2.89 | ppl    18.03 | acc     0.67 | train_ae_norm     1.00\n",
      "[64/200][699/1249] Loss_D: 0.00663501 (Loss_D_real: 0.00182424 Loss_D_fake: 0.00481076) Loss_G: 0.30479047 Loss_Enh_Dec: -1.34063613\n",
      "| epoch  64 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.82 | loss  2.84 | ppl    17.08 | acc     0.63 | train_ae_norm     1.00\n",
      "[64/200][799/1249] Loss_D: 0.00609736 (Loss_D_real: 0.00263548 Loss_D_fake: 0.00346189) Loss_G: 0.32638711 Loss_Enh_Dec: -1.07980454\n",
      "| epoch  64 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.13 | loss  2.91 | ppl    18.32 | acc     0.65 | train_ae_norm     1.00\n",
      "[64/200][899/1249] Loss_D: 0.03163979 (Loss_D_real: 0.02269062 Loss_D_fake: 0.00894917) Loss_G: 0.24590912 Loss_Enh_Dec: -1.10176086\n",
      "| epoch  64 |   900/ 1249 batches | lr 0.000000 | ms/batch 357.85 | loss  2.91 | ppl    18.37 | acc     0.61 | train_ae_norm     1.00\n",
      "[64/200][999/1249] Loss_D: 0.01168007 (Loss_D_real: 0.00389463 Loss_D_fake: 0.00778544) Loss_G: 0.28848702 Loss_Enh_Dec: -1.14087284\n",
      "| epoch  64 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.94 | loss  3.04 | ppl    20.82 | acc     0.66 | train_ae_norm     1.00\n",
      "[64/200][1099/1249] Loss_D: 0.00819164 (Loss_D_real: 0.00424981 Loss_D_fake: 0.00394182) Loss_G: 0.31038347 Loss_Enh_Dec: -1.22300112\n",
      "| epoch  64 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.01 | loss  3.05 | ppl    21.13 | acc     0.61 | train_ae_norm     1.00\n",
      "[64/200][1199/1249] Loss_D: 0.00882163 (Loss_D_real: 0.00364830 Loss_D_fake: 0.00517334) Loss_G: 0.32145613 Loss_Enh_Dec: -0.85892767\n",
      "| epoch  64 |  1200/ 1249 batches | lr 0.000000 | ms/batch 355.79 | loss  3.08 | ppl    21.75 | acc     0.62 | train_ae_norm     1.00\n",
      "| end of epoch  64 | time: 501.19s | test loss  2.45 | test ppl 11.56 | acc 0.735\n",
      "bleu_self:  [4.87500000e-01 2.11588175e-01 2.20017082e-06 5.39282933e-07\n",
      " 4.11346041e-07]\n",
      "bleu_test:  [8.04166666e-01 4.36445485e-01 7.87486206e-02 1.70169993e-05\n",
      " 6.91956670e-06]\n",
      "bleu_self: [0.48750000,0.21158818,0.00000220,0.00000054,0.00000041]\n",
      "bleu_test: [0.80416667,0.43644548,0.07874862,0.00001702,0.00000692]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 65 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.724\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.388\n",
      "  Test Loss: 4.476\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  65 |     0/ 1249 batches | lr 0.000000 | ms/batch 489.66 | loss  0.03 | ppl     1.03 | acc     0.63 | train_ae_norm     1.00\n",
      "[65/200][99/1249] Loss_D: 0.01134262 (Loss_D_real: 0.00835328 Loss_D_fake: 0.00298935) Loss_G: 0.32456732 Loss_Enh_Dec: -1.09460747\n",
      "| epoch  65 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.72 | loss  3.03 | ppl    20.69 | acc     0.65 | train_ae_norm     1.00\n",
      "[65/200][199/1249] Loss_D: 0.00696039 (Loss_D_real: 0.00333154 Loss_D_fake: 0.00362884) Loss_G: 0.31357023 Loss_Enh_Dec: -1.03724754\n",
      "| epoch  65 |   200/ 1249 batches | lr 0.000000 | ms/batch 355.98 | loss  2.97 | ppl    19.41 | acc     0.65 | train_ae_norm     1.00\n",
      "[65/200][299/1249] Loss_D: 0.00440677 (Loss_D_real: 0.00243694 Loss_D_fake: 0.00196983) Loss_G: 0.33296987 Loss_Enh_Dec: -1.07646084\n",
      "| epoch  65 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.38 | loss  2.95 | ppl    19.01 | acc     0.65 | train_ae_norm     1.00\n",
      "[65/200][399/1249] Loss_D: 0.00684273 (Loss_D_real: 0.00336714 Loss_D_fake: 0.00347560) Loss_G: 0.31843239 Loss_Enh_Dec: -1.23748910\n",
      "| epoch  65 |   400/ 1249 batches | lr 0.000000 | ms/batch 357.69 | loss  2.94 | ppl    18.98 | acc     0.66 | train_ae_norm     1.00\n",
      "[65/200][499/1249] Loss_D: 0.01211038 (Loss_D_real: 0.00171780 Loss_D_fake: 0.01039258) Loss_G: 0.30842897 Loss_Enh_Dec: -1.16592157\n",
      "| epoch  65 |   500/ 1249 batches | lr 0.000000 | ms/batch 357.03 | loss  2.93 | ppl    18.79 | acc     0.64 | train_ae_norm     1.00\n",
      "[65/200][599/1249] Loss_D: 0.01418210 (Loss_D_real: 0.00544732 Loss_D_fake: 0.00873478) Loss_G: 0.26923537 Loss_Enh_Dec: -0.56995243\n",
      "| epoch  65 |   600/ 1249 batches | lr 0.000000 | ms/batch 357.00 | loss  2.89 | ppl    18.02 | acc     0.68 | train_ae_norm     1.00\n",
      "[65/200][699/1249] Loss_D: 0.01707551 (Loss_D_real: 0.00475187 Loss_D_fake: 0.01232364) Loss_G: 0.22109847 Loss_Enh_Dec: -0.68132734\n",
      "| epoch  65 |   700/ 1249 batches | lr 0.000000 | ms/batch 358.24 | loss  2.85 | ppl    17.35 | acc     0.64 | train_ae_norm     1.00\n",
      "[65/200][799/1249] Loss_D: 0.02246711 (Loss_D_real: 0.00930910 Loss_D_fake: 0.01315801) Loss_G: 0.23071496 Loss_Enh_Dec: -0.75930893\n",
      "| epoch  65 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.94 | loss  2.85 | ppl    17.34 | acc     0.63 | train_ae_norm     1.00\n",
      "[65/200][899/1249] Loss_D: 0.01689336 (Loss_D_real: 0.00587442 Loss_D_fake: 0.01101893) Loss_G: 0.23606840 Loss_Enh_Dec: -0.91552448\n",
      "| epoch  65 |   900/ 1249 batches | lr 0.000000 | ms/batch 357.27 | loss  2.84 | ppl    17.04 | acc     0.65 | train_ae_norm     1.00\n",
      "[65/200][999/1249] Loss_D: 0.02042616 (Loss_D_real: 0.01043272 Loss_D_fake: 0.00999344) Loss_G: 0.23715873 Loss_Enh_Dec: -1.01783979\n",
      "| epoch  65 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.89 | loss  2.90 | ppl    18.23 | acc     0.71 | train_ae_norm     1.00\n",
      "[65/200][1099/1249] Loss_D: 0.01506704 (Loss_D_real: 0.00671353 Loss_D_fake: 0.00835351) Loss_G: 0.24017039 Loss_Enh_Dec: -0.97036904\n",
      "| epoch  65 |  1100/ 1249 batches | lr 0.000000 | ms/batch 357.80 | loss  2.88 | ppl    17.82 | acc     0.61 | train_ae_norm     1.00\n",
      "[65/200][1199/1249] Loss_D: 0.01183789 (Loss_D_real: 0.00263522 Loss_D_fake: 0.00920266) Loss_G: 0.24679594 Loss_Enh_Dec: -0.98289919\n",
      "| epoch  65 |  1200/ 1249 batches | lr 0.000000 | ms/batch 356.67 | loss  2.88 | ppl    17.85 | acc     0.62 | train_ae_norm     1.00\n",
      "| end of epoch  65 | time: 501.38s | test loss  2.39 | test ppl 10.91 | acc 0.747\n",
      "bleu_self:  [2.00000000e-01 7.21600511e-09 2.69129431e-11 1.90752994e-12\n",
      " 1.68562481e-11]\n",
      "bleu_test:  [6.62500000e-01 1.99300648e-01 1.87298176e-06 6.71902630e-09\n",
      " 1.12557573e-08]\n",
      "bleu_self: [0.20000000,0.00000001,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.66250000,0.19930065,0.00000187,0.00000001,0.00000001]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 66 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.727\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.395\n",
      "  Test Loss: 4.278\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  66 |     0/ 1249 batches | lr 0.000000 | ms/batch 490.43 | loss  0.03 | ppl     1.03 | acc     0.66 | train_ae_norm     1.00\n",
      "[66/200][99/1249] Loss_D: 0.01001921 (Loss_D_real: 0.00275133 Loss_D_fake: 0.00726787) Loss_G: 0.24704085 Loss_Enh_Dec: -1.09444034\n",
      "| epoch  66 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.94 | loss  2.88 | ppl    17.87 | acc     0.67 | train_ae_norm     1.00\n",
      "[66/200][199/1249] Loss_D: 0.01182080 (Loss_D_real: 0.00563842 Loss_D_fake: 0.00618239) Loss_G: 0.26536903 Loss_Enh_Dec: -1.11500585\n",
      "| epoch  66 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.23 | loss  2.83 | ppl    16.94 | acc     0.68 | train_ae_norm     1.00\n",
      "[66/200][299/1249] Loss_D: 0.00964212 (Loss_D_real: 0.00405094 Loss_D_fake: 0.00559118) Loss_G: 0.26224947 Loss_Enh_Dec: -0.84318352\n",
      "| epoch  66 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.03 | loss  2.85 | ppl    17.32 | acc     0.67 | train_ae_norm     1.00\n",
      "[66/200][399/1249] Loss_D: 0.06892368 (Loss_D_real: 0.05951272 Loss_D_fake: 0.00941097) Loss_G: 0.24009390 Loss_Enh_Dec: -1.18958187\n",
      "| epoch  66 |   400/ 1249 batches | lr 0.000000 | ms/batch 356.19 | loss  2.87 | ppl    17.69 | acc     0.66 | train_ae_norm     1.00\n",
      "[66/200][499/1249] Loss_D: 0.04118608 (Loss_D_real: 0.03288349 Loss_D_fake: 0.00830259) Loss_G: 0.23925360 Loss_Enh_Dec: -1.34456408\n",
      "| epoch  66 |   500/ 1249 batches | lr 0.000000 | ms/batch 355.84 | loss  2.93 | ppl    18.69 | acc     0.66 | train_ae_norm     1.00\n",
      "[66/200][599/1249] Loss_D: 0.01641468 (Loss_D_real: 0.00882192 Loss_D_fake: 0.00759276) Loss_G: 0.24408031 Loss_Enh_Dec: -1.33646190\n",
      "| epoch  66 |   600/ 1249 batches | lr 0.000000 | ms/batch 355.97 | loss  2.97 | ppl    19.51 | acc     0.67 | train_ae_norm     1.00\n",
      "[66/200][699/1249] Loss_D: 0.00917706 (Loss_D_real: 0.00322897 Loss_D_fake: 0.00594808) Loss_G: 0.25568500 Loss_Enh_Dec: -1.23922002\n",
      "| epoch  66 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.03 | loss  2.96 | ppl    19.34 | acc     0.61 | train_ae_norm     1.00\n",
      "[66/200][799/1249] Loss_D: 0.01178034 (Loss_D_real: 0.00518108 Loss_D_fake: 0.00659925) Loss_G: 0.25896373 Loss_Enh_Dec: -1.06190419\n",
      "| epoch  66 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.16 | loss  2.94 | ppl    18.97 | acc     0.64 | train_ae_norm     1.00\n",
      "[66/200][899/1249] Loss_D: 0.00655169 (Loss_D_real: 0.00217928 Loss_D_fake: 0.00437241) Loss_G: 0.28524673 Loss_Enh_Dec: -1.19199836\n",
      "| epoch  66 |   900/ 1249 batches | lr 0.000000 | ms/batch 355.23 | loss  2.87 | ppl    17.61 | acc     0.66 | train_ae_norm     1.00\n",
      "[66/200][999/1249] Loss_D: 0.00911384 (Loss_D_real: 0.00420303 Loss_D_fake: 0.00491082) Loss_G: 0.27476063 Loss_Enh_Dec: -1.27911747\n",
      "| epoch  66 |  1000/ 1249 batches | lr 0.000000 | ms/batch 355.89 | loss  2.84 | ppl    17.18 | acc     0.65 | train_ae_norm     1.00\n",
      "[66/200][1099/1249] Loss_D: 0.00750250 (Loss_D_real: 0.00324677 Loss_D_fake: 0.00425573) Loss_G: 0.27581128 Loss_Enh_Dec: -1.11554873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  66 |  1100/ 1249 batches | lr 0.000000 | ms/batch 355.78 | loss  2.83 | ppl    16.98 | acc     0.65 | train_ae_norm     1.00\n",
      "[66/200][1199/1249] Loss_D: 0.00770418 (Loss_D_real: 0.00341187 Loss_D_fake: 0.00429231) Loss_G: 0.28620034 Loss_Enh_Dec: -1.11481082\n",
      "| epoch  66 |  1200/ 1249 batches | lr 0.000000 | ms/batch 356.16 | loss  2.84 | ppl    17.05 | acc     0.65 | train_ae_norm     1.00\n",
      "| end of epoch  66 | time: 500.15s | test loss  2.35 | test ppl 10.48 | acc 0.751\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 67 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.717\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.395\n",
      "  Test Loss: 4.369\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  67 |     0/ 1249 batches | lr 0.000000 | ms/batch 489.41 | loss  0.03 | ppl     1.03 | acc     0.64 | train_ae_norm     1.00\n",
      "[67/200][99/1249] Loss_D: 0.02582759 (Loss_D_real: 0.01792040 Loss_D_fake: 0.00790719) Loss_G: 0.27097487 Loss_Enh_Dec: -0.96554816\n",
      "| epoch  67 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.27 | loss  2.82 | ppl    16.82 | acc     0.66 | train_ae_norm     1.00\n",
      "[67/200][199/1249] Loss_D: 0.01432431 (Loss_D_real: 0.01101294 Loss_D_fake: 0.00331138) Loss_G: 0.29765376 Loss_Enh_Dec: -1.06747746\n",
      "| epoch  67 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.64 | loss  2.82 | ppl    16.81 | acc     0.66 | train_ae_norm     1.00\n",
      "[67/200][299/1249] Loss_D: 0.01161962 (Loss_D_real: 0.00761750 Loss_D_fake: 0.00400212) Loss_G: 0.30208546 Loss_Enh_Dec: -1.12209499\n",
      "| epoch  67 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.25 | loss  2.86 | ppl    17.38 | acc     0.66 | train_ae_norm     1.00\n",
      "[67/200][399/1249] Loss_D: 0.00796747 (Loss_D_real: 0.00306745 Loss_D_fake: 0.00490002) Loss_G: 0.29625118 Loss_Enh_Dec: -1.15726113\n",
      "| epoch  67 |   400/ 1249 batches | lr 0.000000 | ms/batch 356.95 | loss  2.84 | ppl    17.10 | acc     0.68 | train_ae_norm     1.00\n",
      "[67/200][499/1249] Loss_D: 0.00960078 (Loss_D_real: 0.00612913 Loss_D_fake: 0.00347164) Loss_G: 0.30506253 Loss_Enh_Dec: -1.34791362\n",
      "| epoch  67 |   500/ 1249 batches | lr 0.000000 | ms/batch 357.61 | loss  2.91 | ppl    18.34 | acc     0.61 | train_ae_norm     1.00\n",
      "[67/200][599/1249] Loss_D: 0.01400210 (Loss_D_real: 0.00899370 Loss_D_fake: 0.00500840) Loss_G: 0.30246943 Loss_Enh_Dec: -1.02582061\n",
      "| epoch  67 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.35 | loss  2.83 | ppl    16.98 | acc     0.69 | train_ae_norm     1.00\n",
      "[67/200][699/1249] Loss_D: 0.01505737 (Loss_D_real: 0.00655082 Loss_D_fake: 0.00850654) Loss_G: 0.27084291 Loss_Enh_Dec: -1.09181821\n",
      "| epoch  67 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.89 | loss  2.80 | ppl    16.38 | acc     0.63 | train_ae_norm     1.00\n",
      "[67/200][799/1249] Loss_D: 0.01006812 (Loss_D_real: 0.00542810 Loss_D_fake: 0.00464002) Loss_G: 0.27692860 Loss_Enh_Dec: -1.10292971\n",
      "| epoch  67 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.82 | loss  2.77 | ppl    15.99 | acc     0.65 | train_ae_norm     1.00\n",
      "[67/200][899/1249] Loss_D: 0.01412292 (Loss_D_real: 0.00704295 Loss_D_fake: 0.00707997) Loss_G: 0.27412680 Loss_Enh_Dec: -0.91053641\n",
      "| epoch  67 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.74 | loss  2.70 | ppl    14.82 | acc     0.68 | train_ae_norm     1.00\n",
      "[67/200][999/1249] Loss_D: 0.00695687 (Loss_D_real: 0.00385213 Loss_D_fake: 0.00310474) Loss_G: 0.29823634 Loss_Enh_Dec: -1.04931140\n",
      "| epoch  67 |  1000/ 1249 batches | lr 0.000000 | ms/batch 357.57 | loss  2.72 | ppl    15.18 | acc     0.68 | train_ae_norm     1.00\n",
      "[67/200][1099/1249] Loss_D: 0.01315321 (Loss_D_real: 0.00832422 Loss_D_fake: 0.00482899) Loss_G: 0.28074336 Loss_Enh_Dec: -0.96805364\n",
      "| epoch  67 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.94 | loss  2.74 | ppl    15.47 | acc     0.68 | train_ae_norm     1.00\n",
      "[67/200][1199/1249] Loss_D: 0.01157379 (Loss_D_real: 0.00711359 Loss_D_fake: 0.00446020) Loss_G: 0.28714791 Loss_Enh_Dec: -0.87705576\n",
      "| epoch  67 |  1200/ 1249 batches | lr 0.000000 | ms/batch 357.24 | loss  2.70 | ppl    14.93 | acc     0.67 | train_ae_norm     1.00\n",
      "| end of epoch  67 | time: 501.13s | test loss  2.27 | test ppl  9.67 | acc 0.758\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 68 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.713\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.400\n",
      "  Test Loss: 4.431\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  68 |     0/ 1249 batches | lr 0.000000 | ms/batch 490.36 | loss  0.03 | ppl     1.03 | acc     0.68 | train_ae_norm     1.00\n",
      "[68/200][99/1249] Loss_D: 0.01161519 (Loss_D_real: 0.00370539 Loss_D_fake: 0.00790980) Loss_G: 0.26565567 Loss_Enh_Dec: -0.68763125\n",
      "| epoch  68 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.78 | loss  2.65 | ppl    14.20 | acc     0.67 | train_ae_norm     1.00\n",
      "[68/200][199/1249] Loss_D: 0.01188250 (Loss_D_real: 0.00376194 Loss_D_fake: 0.00812056) Loss_G: 0.28748044 Loss_Enh_Dec: -0.88902950\n",
      "| epoch  68 |   200/ 1249 batches | lr 0.000000 | ms/batch 355.50 | loss  2.68 | ppl    14.51 | acc     0.68 | train_ae_norm     1.00\n",
      "[68/200][299/1249] Loss_D: 0.01021833 (Loss_D_real: 0.00443883 Loss_D_fake: 0.00577950) Loss_G: 0.27578846 Loss_Enh_Dec: -0.85776913\n",
      "| epoch  68 |   300/ 1249 batches | lr 0.000000 | ms/batch 354.99 | loss  2.71 | ppl    15.01 | acc     0.69 | train_ae_norm     1.00\n",
      "[68/200][399/1249] Loss_D: 0.00902419 (Loss_D_real: 0.00269955 Loss_D_fake: 0.00632465) Loss_G: 0.28921860 Loss_Enh_Dec: -0.73965615\n",
      "| epoch  68 |   400/ 1249 batches | lr 0.000000 | ms/batch 355.25 | loss  2.67 | ppl    14.42 | acc     0.70 | train_ae_norm     1.00\n",
      "[68/200][499/1249] Loss_D: 0.00946390 (Loss_D_real: 0.00467086 Loss_D_fake: 0.00479304) Loss_G: 0.29077300 Loss_Enh_Dec: -0.82610190\n",
      "| epoch  68 |   500/ 1249 batches | lr 0.000000 | ms/batch 355.50 | loss  2.73 | ppl    15.26 | acc     0.66 | train_ae_norm     1.00\n",
      "[68/200][599/1249] Loss_D: 0.00658962 (Loss_D_real: 0.00253448 Loss_D_fake: 0.00405514) Loss_G: 0.29771480 Loss_Enh_Dec: -0.98040754\n",
      "| epoch  68 |   600/ 1249 batches | lr 0.000000 | ms/batch 355.33 | loss  2.71 | ppl    15.04 | acc     0.68 | train_ae_norm     1.00\n",
      "[68/200][699/1249] Loss_D: 0.01011423 (Loss_D_real: 0.00736511 Loss_D_fake: 0.00274912) Loss_G: 0.31430221 Loss_Enh_Dec: -0.77219689\n",
      "| epoch  68 |   700/ 1249 batches | lr 0.000000 | ms/batch 355.96 | loss  2.69 | ppl    14.75 | acc     0.64 | train_ae_norm     1.00\n",
      "[68/200][799/1249] Loss_D: 0.02019318 (Loss_D_real: 0.01281956 Loss_D_fake: 0.00737362) Loss_G: 0.27106473 Loss_Enh_Dec: -0.69783527\n",
      "| epoch  68 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.61 | loss  2.74 | ppl    15.56 | acc     0.64 | train_ae_norm     1.00\n",
      "[68/200][899/1249] Loss_D: 0.01262123 (Loss_D_real: 0.00519852 Loss_D_fake: 0.00742271) Loss_G: 0.30296618 Loss_Enh_Dec: -0.93558264\n",
      "| epoch  68 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.17 | loss  2.72 | ppl    15.13 | acc     0.65 | train_ae_norm     1.00\n",
      "[68/200][999/1249] Loss_D: 0.01028459 (Loss_D_real: 0.00229385 Loss_D_fake: 0.00799073) Loss_G: 0.30398676 Loss_Enh_Dec: -1.08136547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  68 |  1000/ 1249 batches | lr 0.000000 | ms/batch 357.00 | loss  2.67 | ppl    14.39 | acc     0.68 | train_ae_norm     1.00\n",
      "[68/200][1099/1249] Loss_D: 0.00755339 (Loss_D_real: 0.00358139 Loss_D_fake: 0.00397201) Loss_G: 0.30932817 Loss_Enh_Dec: -0.80790007\n",
      "| epoch  68 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.19 | loss  2.72 | ppl    15.22 | acc     0.66 | train_ae_norm     1.00\n",
      "[68/200][1199/1249] Loss_D: 0.03318804 (Loss_D_real: 0.02713674 Loss_D_fake: 0.00605130) Loss_G: 0.29934835 Loss_Enh_Dec: -1.01085949\n",
      "| epoch  68 |  1200/ 1249 batches | lr 0.000000 | ms/batch 356.85 | loss  2.66 | ppl    14.31 | acc     0.69 | train_ae_norm     1.00\n",
      "| end of epoch  68 | time: 500.09s | test loss  2.25 | test ppl  9.52 | acc 0.758\n",
      "bleu_self:  [2.34229962e-01 6.76883639e-09 9.97990925e-09 1.89578797e-08\n",
      " 2.81224062e-08]\n",
      "bleu_test:  [8.54910714e-01 2.35786648e-01 4.11786255e-02 1.04829782e-05\n",
      " 2.11209090e-06]\n",
      "bleu_self: [0.23422996,0.00000001,0.00000001,0.00000002,0.00000003]\n",
      "bleu_test: [0.85491071,0.23578665,0.04117863,0.00001048,0.00000211]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 69 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.718\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.393\n",
      "  Test Loss: 4.529\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  69 |     0/ 1249 batches | lr 0.000000 | ms/batch 489.31 | loss  0.03 | ppl     1.03 | acc     0.67 | train_ae_norm     1.00\n",
      "[69/200][99/1249] Loss_D: 0.01305570 (Loss_D_real: 0.00335590 Loss_D_fake: 0.00969980) Loss_G: 0.28809759 Loss_Enh_Dec: -0.91909820\n",
      "| epoch  69 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.28 | loss  2.71 | ppl    15.02 | acc     0.65 | train_ae_norm     1.00\n",
      "[69/200][199/1249] Loss_D: 0.02231980 (Loss_D_real: 0.01398933 Loss_D_fake: 0.00833047) Loss_G: 0.27222735 Loss_Enh_Dec: -0.73899889\n",
      "| epoch  69 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.85 | loss  2.70 | ppl    14.94 | acc     0.67 | train_ae_norm     1.00\n",
      "[69/200][299/1249] Loss_D: 0.01957032 (Loss_D_real: 0.01301463 Loss_D_fake: 0.00655570) Loss_G: 0.28434816 Loss_Enh_Dec: -0.96279162\n",
      "| epoch  69 |   300/ 1249 batches | lr 0.000000 | ms/batch 357.09 | loss  2.68 | ppl    14.55 | acc     0.67 | train_ae_norm     1.00\n",
      "[69/200][399/1249] Loss_D: 0.01242847 (Loss_D_real: 0.00828579 Loss_D_fake: 0.00414267) Loss_G: 0.29756287 Loss_Enh_Dec: -1.14232433\n",
      "| epoch  69 |   400/ 1249 batches | lr 0.000000 | ms/batch 357.20 | loss  2.63 | ppl    13.88 | acc     0.72 | train_ae_norm     1.00\n",
      "[69/200][499/1249] Loss_D: 0.01488443 (Loss_D_real: 0.00738394 Loss_D_fake: 0.00750049) Loss_G: 0.29244438 Loss_Enh_Dec: -0.92041844\n",
      "| epoch  69 |   500/ 1249 batches | lr 0.000000 | ms/batch 357.09 | loss  2.70 | ppl    14.89 | acc     0.67 | train_ae_norm     1.00\n",
      "[69/200][599/1249] Loss_D: 0.01255991 (Loss_D_real: 0.00783676 Loss_D_fake: 0.00472315) Loss_G: 0.30358735 Loss_Enh_Dec: -0.84059638\n",
      "| epoch  69 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.82 | loss  2.67 | ppl    14.43 | acc     0.70 | train_ae_norm     1.00\n",
      "[69/200][699/1249] Loss_D: 0.01580444 (Loss_D_real: 0.01172426 Loss_D_fake: 0.00408018) Loss_G: 0.29903850 Loss_Enh_Dec: -0.75298637\n",
      "| epoch  69 |   700/ 1249 batches | lr 0.000000 | ms/batch 357.01 | loss  2.67 | ppl    14.41 | acc     0.64 | train_ae_norm     1.00\n",
      "[69/200][799/1249] Loss_D: 0.00928372 (Loss_D_real: 0.00275717 Loss_D_fake: 0.00652655) Loss_G: 0.29118592 Loss_Enh_Dec: -0.87746996\n",
      "| epoch  69 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.28 | loss  2.73 | ppl    15.30 | acc     0.68 | train_ae_norm     1.00\n",
      "[69/200][899/1249] Loss_D: 0.01460975 (Loss_D_real: 0.01093839 Loss_D_fake: 0.00367136) Loss_G: 0.32962406 Loss_Enh_Dec: -0.78519124\n",
      "| epoch  69 |   900/ 1249 batches | lr 0.000000 | ms/batch 357.91 | loss  2.65 | ppl    14.20 | acc     0.67 | train_ae_norm     1.00\n",
      "[69/200][999/1249] Loss_D: 0.01405697 (Loss_D_real: 0.00525632 Loss_D_fake: 0.00880065) Loss_G: 0.31685835 Loss_Enh_Dec: -0.83845824\n",
      "| epoch  69 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.97 | loss  2.69 | ppl    14.66 | acc     0.72 | train_ae_norm     1.00\n",
      "[69/200][1099/1249] Loss_D: 0.01072598 (Loss_D_real: 0.00723463 Loss_D_fake: 0.00349135) Loss_G: 0.29339284 Loss_Enh_Dec: -0.72072119\n",
      "| epoch  69 |  1100/ 1249 batches | lr 0.000000 | ms/batch 357.97 | loss  2.67 | ppl    14.47 | acc     0.67 | train_ae_norm     1.00\n",
      "[69/200][1199/1249] Loss_D: 0.00649445 (Loss_D_real: 0.00254728 Loss_D_fake: 0.00394716) Loss_G: 0.30946136 Loss_Enh_Dec: -0.98944610\n",
      "| epoch  69 |  1200/ 1249 batches | lr 0.000000 | ms/batch 355.81 | loss  2.65 | ppl    14.12 | acc     0.68 | train_ae_norm     1.00\n",
      "| end of epoch  69 | time: 501.19s | test loss  2.23 | test ppl  9.28 | acc 0.763\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 70 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.712\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.410\n",
      "  Test Loss: 4.488\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  70 |     0/ 1249 batches | lr 0.000000 | ms/batch 491.37 | loss  0.02 | ppl     1.02 | acc     0.72 | train_ae_norm     1.00\n",
      "[70/200][99/1249] Loss_D: 0.01552592 (Loss_D_real: 0.01017370 Loss_D_fake: 0.00535222) Loss_G: 0.30633909 Loss_Enh_Dec: -1.05907989\n",
      "| epoch  70 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.56 | loss  2.67 | ppl    14.46 | acc     0.69 | train_ae_norm     1.00\n",
      "[70/200][199/1249] Loss_D: 0.01368721 (Loss_D_real: 0.00778014 Loss_D_fake: 0.00590707) Loss_G: 0.29683369 Loss_Enh_Dec: -1.05147588\n",
      "| epoch  70 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.10 | loss  2.69 | ppl    14.72 | acc     0.69 | train_ae_norm     1.00\n",
      "[70/200][299/1249] Loss_D: 0.00555073 (Loss_D_real: 0.00291624 Loss_D_fake: 0.00263449) Loss_G: 0.32250115 Loss_Enh_Dec: -1.18689156\n",
      "| epoch  70 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.89 | loss  2.70 | ppl    14.88 | acc     0.67 | train_ae_norm     1.00\n",
      "[70/200][399/1249] Loss_D: 0.00619836 (Loss_D_real: 0.00227036 Loss_D_fake: 0.00392800) Loss_G: 0.32096025 Loss_Enh_Dec: -1.19300079\n",
      "| epoch  70 |   400/ 1249 batches | lr 0.000000 | ms/batch 358.01 | loss  2.69 | ppl    14.77 | acc     0.70 | train_ae_norm     1.00\n",
      "[70/200][499/1249] Loss_D: 0.00840393 (Loss_D_real: 0.00317726 Loss_D_fake: 0.00522667) Loss_G: 0.31596032 Loss_Enh_Dec: -1.16288221\n",
      "| epoch  70 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.83 | loss  2.82 | ppl    16.74 | acc     0.66 | train_ae_norm     1.00\n",
      "[70/200][599/1249] Loss_D: 0.01311066 (Loss_D_real: 0.00788709 Loss_D_fake: 0.00522357) Loss_G: 0.30273995 Loss_Enh_Dec: -1.18920219\n",
      "| epoch  70 |   600/ 1249 batches | lr 0.000000 | ms/batch 355.96 | loss  2.77 | ppl    16.01 | acc     0.71 | train_ae_norm     1.00\n",
      "[70/200][699/1249] Loss_D: 0.01103711 (Loss_D_real: 0.00805699 Loss_D_fake: 0.00298012) Loss_G: 0.30791864 Loss_Enh_Dec: -1.25536156\n",
      "| epoch  70 |   700/ 1249 batches | lr 0.000000 | ms/batch 355.59 | loss  2.75 | ppl    15.58 | acc     0.67 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70/200][799/1249] Loss_D: 0.00413002 (Loss_D_real: 0.00155027 Loss_D_fake: 0.00257975) Loss_G: 0.29672459 Loss_Enh_Dec: -1.09627366\n",
      "| epoch  70 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.99 | loss  2.73 | ppl    15.33 | acc     0.68 | train_ae_norm     1.00\n",
      "[70/200][899/1249] Loss_D: 0.00547447 (Loss_D_real: 0.00266880 Loss_D_fake: 0.00280567) Loss_G: 0.30729845 Loss_Enh_Dec: -1.45532382\n",
      "| epoch  70 |   900/ 1249 batches | lr 0.000000 | ms/batch 357.97 | loss  2.71 | ppl    15.08 | acc     0.65 | train_ae_norm     1.00\n",
      "[70/200][999/1249] Loss_D: 0.00681868 (Loss_D_real: 0.00441500 Loss_D_fake: 0.00240368) Loss_G: 0.32249838 Loss_Enh_Dec: -1.24469507\n",
      "| epoch  70 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.15 | loss  2.72 | ppl    15.24 | acc     0.67 | train_ae_norm     1.00\n",
      "[70/200][1099/1249] Loss_D: 0.00790290 (Loss_D_real: 0.00533933 Loss_D_fake: 0.00256357) Loss_G: 0.32285294 Loss_Enh_Dec: -1.21881032\n",
      "| epoch  70 |  1100/ 1249 batches | lr 0.000000 | ms/batch 355.58 | loss  2.77 | ppl    15.97 | acc     0.65 | train_ae_norm     1.00\n",
      "[70/200][1199/1249] Loss_D: 0.00561611 (Loss_D_real: 0.00185682 Loss_D_fake: 0.00375929) Loss_G: 0.30785081 Loss_Enh_Dec: -1.15875018\n",
      "| epoch  70 |  1200/ 1249 batches | lr 0.000000 | ms/batch 357.28 | loss  2.75 | ppl    15.69 | acc     0.67 | train_ae_norm     1.00\n",
      "| end of epoch  70 | time: 501.00s | test loss  2.38 | test ppl 10.77 | acc 0.747\n",
      "bleu_self:  [4.03830689e-01 2.08806824e-01 6.95926603e-02 1.07346691e-05\n",
      " 5.76777800e-08]\n",
      "bleu_test:  [9.36557539e-01 5.13262088e-01 1.50257470e-01 2.04585422e-05\n",
      " 1.01844245e-07]\n",
      "bleu_self: [0.40383069,0.20880682,0.06959266,0.00001073,0.00000006]\n",
      "bleu_test: [0.93655754,0.51326209,0.15025747,0.00002046,0.00000010]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 71 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.711\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.410\n",
      "  Test Loss: 4.491\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  71 |     0/ 1249 batches | lr 0.000000 | ms/batch 492.10 | loss  0.03 | ppl     1.03 | acc     0.67 | train_ae_norm     1.00\n",
      "[71/200][99/1249] Loss_D: 0.00939749 (Loss_D_real: 0.00484049 Loss_D_fake: 0.00455701) Loss_G: 0.30937394 Loss_Enh_Dec: -1.02927542\n",
      "| epoch  71 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.65 | loss  2.86 | ppl    17.52 | acc     0.67 | train_ae_norm     1.00\n",
      "[71/200][199/1249] Loss_D: 0.01334755 (Loss_D_real: 0.00277524 Loss_D_fake: 0.01057231) Loss_G: 0.31842735 Loss_Enh_Dec: -0.75337160\n",
      "| epoch  71 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.82 | loss  2.76 | ppl    15.77 | acc     0.67 | train_ae_norm     1.00\n",
      "[71/200][299/1249] Loss_D: 0.00845024 (Loss_D_real: 0.00515093 Loss_D_fake: 0.00329931) Loss_G: 0.32916161 Loss_Enh_Dec: -1.04889715\n",
      "| epoch  71 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.87 | loss  2.77 | ppl    15.97 | acc     0.68 | train_ae_norm     1.00\n",
      "[71/200][399/1249] Loss_D: 0.02206014 (Loss_D_real: 0.01767905 Loss_D_fake: 0.00438109) Loss_G: 0.30535981 Loss_Enh_Dec: -1.24434173\n",
      "| epoch  71 |   400/ 1249 batches | lr 0.000000 | ms/batch 357.02 | loss  2.75 | ppl    15.59 | acc     0.69 | train_ae_norm     1.00\n",
      "[71/200][499/1249] Loss_D: 0.00836705 (Loss_D_real: 0.00470700 Loss_D_fake: 0.00366004) Loss_G: 0.30279014 Loss_Enh_Dec: -1.02048910\n",
      "| epoch  71 |   500/ 1249 batches | lr 0.000000 | ms/batch 357.28 | loss  2.79 | ppl    16.28 | acc     0.64 | train_ae_norm     1.00\n",
      "[71/200][599/1249] Loss_D: 0.00524227 (Loss_D_real: 0.00194750 Loss_D_fake: 0.00329477) Loss_G: 0.32315999 Loss_Enh_Dec: -1.29768682\n",
      "| epoch  71 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.20 | loss  2.79 | ppl    16.33 | acc     0.67 | train_ae_norm     1.00\n",
      "[71/200][699/1249] Loss_D: 0.01206063 (Loss_D_real: 0.00527458 Loss_D_fake: 0.00678605) Loss_G: 0.26737157 Loss_Enh_Dec: -0.96820778\n",
      "| epoch  71 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.66 | loss  2.82 | ppl    16.71 | acc     0.61 | train_ae_norm     1.00\n",
      "[71/200][799/1249] Loss_D: 0.01442622 (Loss_D_real: 0.00860479 Loss_D_fake: 0.00582143) Loss_G: 0.28241768 Loss_Enh_Dec: -1.05340886\n",
      "| epoch  71 |   800/ 1249 batches | lr 0.000000 | ms/batch 357.25 | loss  2.82 | ppl    16.85 | acc     0.65 | train_ae_norm     1.00\n",
      "[71/200][899/1249] Loss_D: 0.01733080 (Loss_D_real: 0.00552330 Loss_D_fake: 0.01180750) Loss_G: 0.30751234 Loss_Enh_Dec: -1.05601227\n",
      "| epoch  71 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.77 | loss  2.76 | ppl    15.78 | acc     0.67 | train_ae_norm     1.00\n",
      "[71/200][999/1249] Loss_D: 0.00718129 (Loss_D_real: 0.00271587 Loss_D_fake: 0.00446542) Loss_G: 0.30158558 Loss_Enh_Dec: -1.09741771\n",
      "| epoch  71 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.34 | loss  2.76 | ppl    15.80 | acc     0.68 | train_ae_norm     1.00\n",
      "[71/200][1099/1249] Loss_D: 0.01095353 (Loss_D_real: 0.00700354 Loss_D_fake: 0.00395000) Loss_G: 0.31107903 Loss_Enh_Dec: -1.09833705\n",
      "| epoch  71 |  1100/ 1249 batches | lr 0.000000 | ms/batch 357.17 | loss  2.82 | ppl    16.82 | acc     0.65 | train_ae_norm     1.00\n",
      "[71/200][1199/1249] Loss_D: 0.00848516 (Loss_D_real: 0.00395477 Loss_D_fake: 0.00453039) Loss_G: 0.30775335 Loss_Enh_Dec: -1.09901500\n",
      "| epoch  71 |  1200/ 1249 batches | lr 0.000000 | ms/batch 357.55 | loss  2.75 | ppl    15.66 | acc     0.66 | train_ae_norm     1.00\n",
      "| end of epoch  71 | time: 501.42s | test loss  2.28 | test ppl  9.77 | acc 0.759\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 72 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.713\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.425\n",
      "  Test Loss: 4.458\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  72 |     0/ 1249 batches | lr 0.000000 | ms/batch 490.01 | loss  0.03 | ppl     1.03 | acc     0.67 | train_ae_norm     1.00\n",
      "[72/200][99/1249] Loss_D: 0.01637409 (Loss_D_real: 0.01193433 Loss_D_fake: 0.00443976) Loss_G: 0.30834663 Loss_Enh_Dec: -1.00429749\n",
      "| epoch  72 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.66 | loss  2.75 | ppl    15.62 | acc     0.67 | train_ae_norm     1.00\n",
      "[72/200][199/1249] Loss_D: 0.00491947 (Loss_D_real: 0.00179016 Loss_D_fake: 0.00312930) Loss_G: 0.30116928 Loss_Enh_Dec: -0.90617579\n",
      "| epoch  72 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.79 | loss  2.72 | ppl    15.22 | acc     0.66 | train_ae_norm     1.00\n",
      "[72/200][299/1249] Loss_D: 0.02237831 (Loss_D_real: 0.01870662 Loss_D_fake: 0.00367169) Loss_G: 0.40475002 Loss_Enh_Dec: -0.53872126\n",
      "| epoch  72 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.01 | loss  2.73 | ppl    15.32 | acc     0.67 | train_ae_norm     1.00\n",
      "[72/200][399/1249] Loss_D: 0.01218891 (Loss_D_real: 0.00596030 Loss_D_fake: 0.00622861) Loss_G: 0.29736209 Loss_Enh_Dec: -0.98939699\n",
      "| epoch  72 |   400/ 1249 batches | lr 0.000000 | ms/batch 356.74 | loss  2.68 | ppl    14.63 | acc     0.70 | train_ae_norm     1.00\n",
      "[72/200][499/1249] Loss_D: 0.00932517 (Loss_D_real: 0.00397831 Loss_D_fake: 0.00534686) Loss_G: 0.31983906 Loss_Enh_Dec: -1.01859093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  72 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.54 | loss  2.76 | ppl    15.74 | acc     0.66 | train_ae_norm     1.00\n",
      "[72/200][599/1249] Loss_D: 0.00667381 (Loss_D_real: 0.00347823 Loss_D_fake: 0.00319558) Loss_G: 0.30549479 Loss_Enh_Dec: -0.76119500\n",
      "| epoch  72 |   600/ 1249 batches | lr 0.000000 | ms/batch 355.99 | loss  2.78 | ppl    16.09 | acc     0.67 | train_ae_norm     1.00\n",
      "[72/200][699/1249] Loss_D: 0.00945673 (Loss_D_real: 0.00395725 Loss_D_fake: 0.00549947) Loss_G: 0.31706485 Loss_Enh_Dec: -0.37527171\n",
      "| epoch  72 |   700/ 1249 batches | lr 0.000000 | ms/batch 357.81 | loss  2.84 | ppl    17.10 | acc     0.63 | train_ae_norm     1.00\n",
      "[72/200][799/1249] Loss_D: 0.01007658 (Loss_D_real: 0.00508020 Loss_D_fake: 0.00499638) Loss_G: 0.28878188 Loss_Enh_Dec: -0.86510992\n",
      "| epoch  72 |   800/ 1249 batches | lr 0.000000 | ms/batch 357.53 | loss  3.03 | ppl    20.75 | acc     0.64 | train_ae_norm     1.00\n",
      "[72/200][899/1249] Loss_D: 0.00750534 (Loss_D_real: 0.00381199 Loss_D_fake: 0.00369336) Loss_G: 0.30793381 Loss_Enh_Dec: -1.15459049\n",
      "| epoch  72 |   900/ 1249 batches | lr 0.000000 | ms/batch 357.70 | loss  3.01 | ppl    20.33 | acc     0.65 | train_ae_norm     1.00\n",
      "[72/200][999/1249] Loss_D: 0.01166052 (Loss_D_real: 0.00762049 Loss_D_fake: 0.00404003) Loss_G: 0.29773647 Loss_Enh_Dec: -0.91050291\n",
      "| epoch  72 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.73 | loss  2.95 | ppl    19.01 | acc     0.64 | train_ae_norm     1.00\n",
      "[72/200][1099/1249] Loss_D: 0.00811994 (Loss_D_real: 0.00409297 Loss_D_fake: 0.00402697) Loss_G: 0.30133381 Loss_Enh_Dec: -1.13245904\n",
      "| epoch  72 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.06 | loss  2.93 | ppl    18.72 | acc     0.61 | train_ae_norm     1.00\n",
      "[72/200][1199/1249] Loss_D: 0.03081135 (Loss_D_real: 0.02851618 Loss_D_fake: 0.00229517) Loss_G: 0.39621693 Loss_Enh_Dec: -0.67340046\n",
      "| epoch  72 |  1200/ 1249 batches | lr 0.000000 | ms/batch 357.39 | loss  2.92 | ppl    18.61 | acc     0.64 | train_ae_norm     1.00\n",
      "| end of epoch  72 | time: 501.22s | test loss  2.39 | test ppl 10.91 | acc 0.744\n",
      "bleu_self:  [1.39880952e-01 4.32910826e-09 6.17990990e-11 3.45630299e-10\n",
      " 1.13134172e-09]\n",
      "bleu_test:  [7.65625000e-01 3.34783852e-01 2.78396910e-06 6.37705772e-07\n",
      " 4.94084679e-07]\n",
      "bleu_self: [0.13988095,0.00000000,0.00000000,0.00000000,0.00000000]\n",
      "bleu_test: [0.76562500,0.33478385,0.00000278,0.00000064,0.00000049]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 73 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.722\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.400\n",
      "  Test Loss: 4.492\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  73 |     0/ 1249 batches | lr 0.000000 | ms/batch 489.88 | loss  0.03 | ppl     1.03 | acc     0.66 | train_ae_norm     1.00\n",
      "[73/200][99/1249] Loss_D: 0.02374428 (Loss_D_real: 0.01370343 Loss_D_fake: 0.01004085) Loss_G: 0.28838140 Loss_Enh_Dec: -1.10924017\n",
      "| epoch  73 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.96 | loss  2.94 | ppl    18.88 | acc     0.62 | train_ae_norm     1.00\n",
      "[73/200][199/1249] Loss_D: 0.46218982 (Loss_D_real: 0.01219458 Loss_D_fake: 0.44999525) Loss_G: 0.47412881 Loss_Enh_Dec: -0.95746607\n",
      "| epoch  73 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.98 | loss  3.00 | ppl    20.00 | acc     0.64 | train_ae_norm     1.00\n",
      "[73/200][299/1249] Loss_D: 0.02897507 (Loss_D_real: 0.02161404 Loss_D_fake: 0.00736103) Loss_G: 0.25887105 Loss_Enh_Dec: -0.83429873\n",
      "| epoch  73 |   300/ 1249 batches | lr 0.000000 | ms/batch 357.18 | loss  3.04 | ppl    20.95 | acc     0.61 | train_ae_norm     1.00\n",
      "[73/200][399/1249] Loss_D: 0.01405307 (Loss_D_real: 0.00351975 Loss_D_fake: 0.01053332) Loss_G: 0.28591520 Loss_Enh_Dec: -0.89356720\n",
      "| epoch  73 |   400/ 1249 batches | lr 0.000000 | ms/batch 356.68 | loss  3.08 | ppl    21.85 | acc     0.65 | train_ae_norm     1.00\n",
      "[73/200][499/1249] Loss_D: 0.01888476 (Loss_D_real: 0.01340559 Loss_D_fake: 0.00547917) Loss_G: 0.28968713 Loss_Enh_Dec: -0.98680985\n",
      "| epoch  73 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.63 | loss  3.14 | ppl    22.99 | acc     0.62 | train_ae_norm     1.00\n",
      "[73/200][599/1249] Loss_D: 0.01868463 (Loss_D_real: 0.01146265 Loss_D_fake: 0.00722198) Loss_G: 0.27174792 Loss_Enh_Dec: -0.75951594\n",
      "| epoch  73 |   600/ 1249 batches | lr 0.000000 | ms/batch 355.31 | loss  3.21 | ppl    24.76 | acc     0.63 | train_ae_norm     1.00\n",
      "[73/200][699/1249] Loss_D: 0.07791347 (Loss_D_real: 0.07015203 Loss_D_fake: 0.00776144) Loss_G: 0.34928587 Loss_Enh_Dec: -0.83743441\n",
      "| epoch  73 |   700/ 1249 batches | lr 0.000000 | ms/batch 354.83 | loss  3.12 | ppl    22.67 | acc     0.63 | train_ae_norm     1.00\n",
      "[73/200][799/1249] Loss_D: 0.01446490 (Loss_D_real: 0.00835155 Loss_D_fake: 0.00611335) Loss_G: 0.28225011 Loss_Enh_Dec: -0.73683161\n",
      "| epoch  73 |   800/ 1249 batches | lr 0.000000 | ms/batch 355.23 | loss  3.09 | ppl    22.00 | acc     0.64 | train_ae_norm     1.00\n",
      "[73/200][899/1249] Loss_D: 0.01430714 (Loss_D_real: 0.00259113 Loss_D_fake: 0.01171601) Loss_G: 0.28638417 Loss_Enh_Dec: -0.89863473\n",
      "| epoch  73 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.19 | loss  3.01 | ppl    20.37 | acc     0.65 | train_ae_norm     1.00\n",
      "[73/200][999/1249] Loss_D: 0.01076648 (Loss_D_real: 0.00406101 Loss_D_fake: 0.00670547) Loss_G: 0.29451585 Loss_Enh_Dec: -1.09470630\n",
      "| epoch  73 |  1000/ 1249 batches | lr 0.000000 | ms/batch 355.35 | loss  2.93 | ppl    18.68 | acc     0.66 | train_ae_norm     1.00\n",
      "[73/200][1099/1249] Loss_D: 0.00968944 (Loss_D_real: 0.00389435 Loss_D_fake: 0.00579509) Loss_G: 0.30973968 Loss_Enh_Dec: -0.97533619\n",
      "| epoch  73 |  1100/ 1249 batches | lr 0.000000 | ms/batch 355.29 | loss  2.91 | ppl    18.32 | acc     0.64 | train_ae_norm     1.00\n",
      "[73/200][1199/1249] Loss_D: 0.01240549 (Loss_D_real: 0.00665129 Loss_D_fake: 0.00575420) Loss_G: 0.27365157 Loss_Enh_Dec: -1.21809614\n",
      "| epoch  73 |  1200/ 1249 batches | lr 0.000000 | ms/batch 355.68 | loss  2.82 | ppl    16.80 | acc     0.66 | train_ae_norm     1.00\n",
      "| end of epoch  73 | time: 500.12s | test loss  2.29 | test ppl  9.86 | acc 0.754\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 74 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.698\n",
      "  Average training loss discriminator: 0.721\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.403\n",
      "  Test Loss: 4.520\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  74 |     0/ 1249 batches | lr 0.000000 | ms/batch 489.32 | loss  0.03 | ppl     1.03 | acc     0.68 | train_ae_norm     1.00\n",
      "[74/200][99/1249] Loss_D: 0.02416637 (Loss_D_real: 0.01162442 Loss_D_fake: 0.01254196) Loss_G: 0.25874731 Loss_Enh_Dec: -0.91703522\n",
      "| epoch  74 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.28 | loss  2.84 | ppl    17.12 | acc     0.64 | train_ae_norm     1.00\n",
      "[74/200][199/1249] Loss_D: 0.03318874 (Loss_D_real: 0.02709454 Loss_D_fake: 0.00609420) Loss_G: 0.30079317 Loss_Enh_Dec: -1.15385771\n",
      "| epoch  74 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.21 | loss  2.84 | ppl    17.04 | acc     0.64 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74/200][299/1249] Loss_D: 0.00880685 (Loss_D_real: 0.00170547 Loss_D_fake: 0.00710138) Loss_G: 0.27900529 Loss_Enh_Dec: -0.75399840\n",
      "| epoch  74 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.17 | loss  2.90 | ppl    18.26 | acc     0.65 | train_ae_norm     1.00\n",
      "[74/200][399/1249] Loss_D: 0.01375808 (Loss_D_real: 0.00757418 Loss_D_fake: 0.00618390) Loss_G: 0.28280708 Loss_Enh_Dec: -1.02066040\n",
      "| epoch  74 |   400/ 1249 batches | lr 0.000000 | ms/batch 356.00 | loss  2.93 | ppl    18.69 | acc     0.63 | train_ae_norm     1.00\n",
      "[74/200][499/1249] Loss_D: 0.01187762 (Loss_D_real: 0.00330128 Loss_D_fake: 0.00857634) Loss_G: 0.28356859 Loss_Enh_Dec: -1.20929420\n",
      "| epoch  74 |   500/ 1249 batches | lr 0.000000 | ms/batch 354.62 | loss  3.09 | ppl    21.91 | acc     0.58 | train_ae_norm     1.00\n",
      "[74/200][599/1249] Loss_D: 0.02416619 (Loss_D_real: 0.01607152 Loss_D_fake: 0.00809468) Loss_G: 0.27194855 Loss_Enh_Dec: -0.77822536\n",
      "| epoch  74 |   600/ 1249 batches | lr 0.000000 | ms/batch 354.58 | loss  3.07 | ppl    21.59 | acc     0.66 | train_ae_norm     1.00\n",
      "[74/200][699/1249] Loss_D: 0.05171894 (Loss_D_real: 0.04440163 Loss_D_fake: 0.00731732) Loss_G: 0.28989792 Loss_Enh_Dec: -0.77611154\n",
      "| epoch  74 |   700/ 1249 batches | lr 0.000000 | ms/batch 354.76 | loss  3.02 | ppl    20.46 | acc     0.62 | train_ae_norm     1.00\n",
      "[74/200][799/1249] Loss_D: 0.00834365 (Loss_D_real: 0.00424330 Loss_D_fake: 0.00410035) Loss_G: 0.29196283 Loss_Enh_Dec: -0.79117256\n",
      "| epoch  74 |   800/ 1249 batches | lr 0.000000 | ms/batch 354.84 | loss  3.06 | ppl    21.24 | acc     0.60 | train_ae_norm     1.00\n",
      "[74/200][899/1249] Loss_D: 0.00844634 (Loss_D_real: 0.00284369 Loss_D_fake: 0.00560265) Loss_G: 0.28188294 Loss_Enh_Dec: -1.25553417\n",
      "| epoch  74 |   900/ 1249 batches | lr 0.000000 | ms/batch 354.50 | loss  2.98 | ppl    19.62 | acc     0.64 | train_ae_norm     1.00\n",
      "[74/200][999/1249] Loss_D: 0.01415337 (Loss_D_real: 0.00876297 Loss_D_fake: 0.00539040) Loss_G: 0.29039791 Loss_Enh_Dec: -1.14596069\n",
      "| epoch  74 |  1000/ 1249 batches | lr 0.000000 | ms/batch 355.78 | loss  2.95 | ppl    19.12 | acc     0.65 | train_ae_norm     1.00\n",
      "[74/200][1099/1249] Loss_D: 0.02029655 (Loss_D_real: 0.01394905 Loss_D_fake: 0.00634750) Loss_G: 0.32043901 Loss_Enh_Dec: -1.19142175\n",
      "| epoch  74 |  1100/ 1249 batches | lr 0.000000 | ms/batch 355.65 | loss  2.97 | ppl    19.43 | acc     0.60 | train_ae_norm     1.00\n",
      "[74/200][1199/1249] Loss_D: 0.02007158 (Loss_D_real: 0.01247580 Loss_D_fake: 0.00759578) Loss_G: 0.27592006 Loss_Enh_Dec: -0.98452324\n",
      "| epoch  74 |  1200/ 1249 batches | lr 0.000000 | ms/batch 355.49 | loss  3.01 | ppl    20.39 | acc     0.62 | train_ae_norm     1.00\n",
      "| end of epoch  74 | time: 499.25s | test loss  2.42 | test ppl 11.27 | acc 0.741\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 75 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.717\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.395\n",
      "  Test Loss: 4.618\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  75 |     0/ 1249 batches | lr 0.000000 | ms/batch 488.44 | loss  0.03 | ppl     1.03 | acc     0.65 | train_ae_norm     1.00\n",
      "[75/200][99/1249] Loss_D: 0.05925839 (Loss_D_real: 0.00742286 Loss_D_fake: 0.05183553) Loss_G: 0.26295102 Loss_Enh_Dec: -0.79490143\n",
      "| epoch  75 |   100/ 1249 batches | lr 0.000000 | ms/batch 355.33 | loss  3.03 | ppl    20.66 | acc     0.63 | train_ae_norm     1.00\n",
      "[75/200][199/1249] Loss_D: 0.01431008 (Loss_D_real: 0.00631365 Loss_D_fake: 0.00799642) Loss_G: 0.28085849 Loss_Enh_Dec: -1.00295246\n",
      "| epoch  75 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.42 | loss  2.99 | ppl    19.84 | acc     0.62 | train_ae_norm     1.00\n",
      "[75/200][299/1249] Loss_D: 0.01140792 (Loss_D_real: 0.00466684 Loss_D_fake: 0.00674108) Loss_G: 0.28556108 Loss_Enh_Dec: -0.92534131\n",
      "| epoch  75 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.19 | loss  3.06 | ppl    21.33 | acc     0.64 | train_ae_norm     1.00\n",
      "[75/200][399/1249] Loss_D: 0.00984539 (Loss_D_real: 0.00368341 Loss_D_fake: 0.00616199) Loss_G: 0.29532287 Loss_Enh_Dec: -1.20647645\n",
      "| epoch  75 |   400/ 1249 batches | lr 0.000000 | ms/batch 357.31 | loss  3.04 | ppl    20.86 | acc     0.67 | train_ae_norm     1.00\n",
      "[75/200][499/1249] Loss_D: 0.01105463 (Loss_D_real: 0.00406600 Loss_D_fake: 0.00698863) Loss_G: 0.28516698 Loss_Enh_Dec: -1.21514690\n",
      "| epoch  75 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.40 | loss  3.11 | ppl    22.41 | acc     0.62 | train_ae_norm     1.00\n",
      "[75/200][599/1249] Loss_D: 0.01745227 (Loss_D_real: 0.00819340 Loss_D_fake: 0.00925887) Loss_G: 0.31397948 Loss_Enh_Dec: -1.41894758\n",
      "| epoch  75 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.17 | loss  3.10 | ppl    22.31 | acc     0.65 | train_ae_norm     1.00\n",
      "[75/200][699/1249] Loss_D: 0.01293979 (Loss_D_real: 0.00892315 Loss_D_fake: 0.00401664) Loss_G: 0.30020839 Loss_Enh_Dec: -1.39375150\n",
      "| epoch  75 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.78 | loss  3.10 | ppl    22.27 | acc     0.60 | train_ae_norm     1.00\n",
      "[75/200][799/1249] Loss_D: 0.01343078 (Loss_D_real: 0.00806731 Loss_D_fake: 0.00536347) Loss_G: 0.30642828 Loss_Enh_Dec: -1.35254741\n",
      "| epoch  75 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.67 | loss  3.26 | ppl    26.14 | acc     0.59 | train_ae_norm     1.00\n",
      "[75/200][899/1249] Loss_D: 0.01721686 (Loss_D_real: 0.01290193 Loss_D_fake: 0.00431493) Loss_G: 0.31071347 Loss_Enh_Dec: -1.08829343\n",
      "| epoch  75 |   900/ 1249 batches | lr 0.000000 | ms/batch 357.19 | loss  3.29 | ppl    26.88 | acc     0.59 | train_ae_norm     1.00\n",
      "[75/200][999/1249] Loss_D: 0.01661984 (Loss_D_real: 0.01067709 Loss_D_fake: 0.00594276) Loss_G: 0.29381320 Loss_Enh_Dec: -1.38612545\n",
      "| epoch  75 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.93 | loss  3.31 | ppl    27.50 | acc     0.61 | train_ae_norm     1.00\n",
      "[75/200][1099/1249] Loss_D: 0.00815033 (Loss_D_real: 0.00326473 Loss_D_fake: 0.00488560) Loss_G: 0.31194416 Loss_Enh_Dec: -1.40972805\n",
      "| epoch  75 |  1100/ 1249 batches | lr 0.000000 | ms/batch 357.10 | loss  3.30 | ppl    27.20 | acc     0.59 | train_ae_norm     1.00\n",
      "[75/200][1199/1249] Loss_D: 0.01939977 (Loss_D_real: 0.00217772 Loss_D_fake: 0.01722205) Loss_G: 0.34751901 Loss_Enh_Dec: -1.34767234\n",
      "| epoch  75 |  1200/ 1249 batches | lr 0.000000 | ms/batch 357.46 | loss  3.33 | ppl    27.94 | acc     0.57 | train_ae_norm     1.00\n",
      "| end of epoch  75 | time: 500.74s | test loss  2.77 | test ppl 16.02 | acc 0.692\n",
      "bleu_self:  [3.14783999e-01 1.51371080e-01 6.03290115e-02 9.46312336e-06\n",
      " 5.12667922e-08]\n",
      "bleu_test:  [9.11111111e-01 2.27053016e-01 1.46858172e-06 3.96841482e-09\n",
      " 5.46178667e-10]\n",
      "bleu_self: [0.31478400,0.15137108,0.06032901,0.00000946,0.00000005]\n",
      "bleu_test: [0.91111111,0.22705302,0.00000147,0.00000000,0.00000000]\n",
      "New saving model: epoch 075.\n",
      "Saving models to ./results/yahoo_merge_results\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 76 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.710\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.398\n",
      "  Test Loss: 4.673\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  76 |     0/ 1249 batches | lr 0.000000 | ms/batch 493.06 | loss  0.03 | ppl     1.03 | acc     0.58 | train_ae_norm     1.00\n",
      "[76/200][99/1249] Loss_D: 0.01298507 (Loss_D_real: 0.00383780 Loss_D_fake: 0.00914727) Loss_G: 0.29553291 Loss_Enh_Dec: -1.19549906\n",
      "| epoch  76 |   100/ 1249 batches | lr 0.000000 | ms/batch 357.22 | loss  3.49 | ppl    32.90 | acc     0.59 | train_ae_norm     1.00\n",
      "[76/200][199/1249] Loss_D: 0.03398678 (Loss_D_real: 0.00563376 Loss_D_fake: 0.02835302) Loss_G: 0.45872459 Loss_Enh_Dec: -1.20742404\n",
      "| epoch  76 |   200/ 1249 batches | lr 0.000000 | ms/batch 355.64 | loss  3.56 | ppl    35.16 | acc     0.54 | train_ae_norm     1.00\n",
      "[76/200][299/1249] Loss_D: 0.04543857 (Loss_D_real: 0.03839685 Loss_D_fake: 0.00704171) Loss_G: 0.31955954 Loss_Enh_Dec: -0.88388872\n",
      "| epoch  76 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.52 | loss  3.52 | ppl    33.74 | acc     0.59 | train_ae_norm     1.00\n",
      "[76/200][399/1249] Loss_D: 0.01561332 (Loss_D_real: 0.00897896 Loss_D_fake: 0.00663435) Loss_G: 0.28031701 Loss_Enh_Dec: -1.11515081\n",
      "| epoch  76 |   400/ 1249 batches | lr 0.000000 | ms/batch 356.74 | loss  3.50 | ppl    33.04 | acc     0.62 | train_ae_norm     1.00\n",
      "[76/200][499/1249] Loss_D: 0.01292181 (Loss_D_real: 0.00400943 Loss_D_fake: 0.00891239) Loss_G: 0.30359438 Loss_Enh_Dec: -0.99980509\n",
      "| epoch  76 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.08 | loss  3.54 | ppl    34.36 | acc     0.55 | train_ae_norm     1.00\n",
      "[76/200][599/1249] Loss_D: 0.03162131 (Loss_D_real: 0.01054296 Loss_D_fake: 0.02107835) Loss_G: 0.19697206 Loss_Enh_Dec: -1.06815660\n",
      "| epoch  76 |   600/ 1249 batches | lr 0.000000 | ms/batch 355.64 | loss  3.43 | ppl    30.76 | acc     0.60 | train_ae_norm     1.00\n",
      "[76/200][699/1249] Loss_D: 0.02688947 (Loss_D_real: 0.01076034 Loss_D_fake: 0.01612912) Loss_G: 0.21132927 Loss_Enh_Dec: -1.03946769\n",
      "| epoch  76 |   700/ 1249 batches | lr 0.000000 | ms/batch 355.66 | loss  3.38 | ppl    29.38 | acc     0.56 | train_ae_norm     1.00\n",
      "[76/200][799/1249] Loss_D: 0.01848232 (Loss_D_real: 0.00656450 Loss_D_fake: 0.01191782) Loss_G: 0.22199564 Loss_Enh_Dec: -1.10404289\n",
      "| epoch  76 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.42 | loss  3.42 | ppl    30.69 | acc     0.57 | train_ae_norm     1.00\n",
      "[76/200][899/1249] Loss_D: 0.01285729 (Loss_D_real: 0.00401187 Loss_D_fake: 0.00884541) Loss_G: 0.23640029 Loss_Enh_Dec: -0.79877090\n",
      "| epoch  76 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.30 | loss  3.28 | ppl    26.53 | acc     0.60 | train_ae_norm     1.00\n",
      "[76/200][999/1249] Loss_D: 0.04372373 (Loss_D_real: 0.03483752 Loss_D_fake: 0.00888621) Loss_G: 0.24833579 Loss_Enh_Dec: -1.16964829\n",
      "| epoch  76 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.50 | loss  3.25 | ppl    25.89 | acc     0.61 | train_ae_norm     1.00\n",
      "[76/200][1099/1249] Loss_D: 0.01073157 (Loss_D_real: 0.00319223 Loss_D_fake: 0.00753933) Loss_G: 0.25669304 Loss_Enh_Dec: -0.97079831\n",
      "| epoch  76 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.40 | loss  3.23 | ppl    25.24 | acc     0.63 | train_ae_norm     1.00\n",
      "[76/200][1199/1249] Loss_D: 0.01204925 (Loss_D_real: 0.00509214 Loss_D_fake: 0.00695711) Loss_G: 0.24280262 Loss_Enh_Dec: -1.01983869\n",
      "| epoch  76 |  1200/ 1249 batches | lr 0.000000 | ms/batch 357.61 | loss  3.14 | ppl    23.17 | acc     0.62 | train_ae_norm     1.00\n",
      "| end of epoch  76 | time: 500.88s | test loss  2.70 | test ppl 14.95 | acc 0.707\n",
      "bleu_self:  [5.23286319e-01 3.17323766e-01 9.31031257e-02 7.07835625e-02\n",
      " 6.16988968e-05]\n",
      "bleu_test:  [9.64583333e-01 3.72549451e-01 3.05957239e-06 5.97151173e-07\n",
      " 4.33418902e-07]\n",
      "bleu_self: [0.52328632,0.31732377,0.09310313,0.07078356,0.00006170]\n",
      "bleu_test: [0.96458333,0.37254945,0.00000306,0.00000060,0.00000043]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 77 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.710\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.405\n",
      "  Test Loss: 4.697\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  77 |     0/ 1249 batches | lr 0.000000 | ms/batch 491.40 | loss  0.03 | ppl     1.03 | acc     0.60 | train_ae_norm     1.00\n",
      "[77/200][99/1249] Loss_D: 0.01451086 (Loss_D_real: 0.00667951 Loss_D_fake: 0.00783135) Loss_G: 0.24827416 Loss_Enh_Dec: -1.13927519\n",
      "| epoch  77 |   100/ 1249 batches | lr 0.000000 | ms/batch 355.90 | loss  3.17 | ppl    23.84 | acc     0.62 | train_ae_norm     1.00\n",
      "[77/200][199/1249] Loss_D: 0.01973552 (Loss_D_real: 0.01263262 Loss_D_fake: 0.00710290) Loss_G: 0.25244898 Loss_Enh_Dec: -1.21061993\n",
      "| epoch  77 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.81 | loss  3.12 | ppl    22.59 | acc     0.62 | train_ae_norm     1.00\n",
      "[77/200][299/1249] Loss_D: 0.04343960 (Loss_D_real: 0.03641829 Loss_D_fake: 0.00702131) Loss_G: 0.25355899 Loss_Enh_Dec: -1.25823772\n",
      "| epoch  77 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.43 | loss  3.14 | ppl    23.10 | acc     0.62 | train_ae_norm     1.00\n",
      "[77/200][399/1249] Loss_D: 0.00986179 (Loss_D_real: 0.00323634 Loss_D_fake: 0.00662545) Loss_G: 0.26312962 Loss_Enh_Dec: -1.23156571\n",
      "| epoch  77 |   400/ 1249 batches | lr 0.000000 | ms/batch 356.88 | loss  3.07 | ppl    21.53 | acc     0.65 | train_ae_norm     1.00\n",
      "[77/200][499/1249] Loss_D: 0.00961649 (Loss_D_real: 0.00377942 Loss_D_fake: 0.00583707) Loss_G: 0.26103103 Loss_Enh_Dec: -1.16039157\n",
      "| epoch  77 |   500/ 1249 batches | lr 0.000000 | ms/batch 357.24 | loss  3.14 | ppl    23.05 | acc     0.65 | train_ae_norm     1.00\n",
      "[77/200][599/1249] Loss_D: 0.00976082 (Loss_D_real: 0.00293418 Loss_D_fake: 0.00682664) Loss_G: 0.26082054 Loss_Enh_Dec: -1.03834987\n",
      "| epoch  77 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.78 | loss  3.13 | ppl    22.76 | acc     0.63 | train_ae_norm     1.00\n",
      "[77/200][699/1249] Loss_D: 0.01449069 (Loss_D_real: 0.00960884 Loss_D_fake: 0.00488185) Loss_G: 0.27345443 Loss_Enh_Dec: -1.21207905\n",
      "| epoch  77 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.68 | loss  3.16 | ppl    23.52 | acc     0.59 | train_ae_norm     1.00\n",
      "[77/200][799/1249] Loss_D: 0.00968867 (Loss_D_real: 0.00359792 Loss_D_fake: 0.00609075) Loss_G: 0.26664996 Loss_Enh_Dec: -1.08459175\n",
      "| epoch  77 |   800/ 1249 batches | lr 0.000000 | ms/batch 355.50 | loss  3.18 | ppl    24.11 | acc     0.59 | train_ae_norm     1.00\n",
      "[77/200][899/1249] Loss_D: 0.00954830 (Loss_D_real: 0.00423124 Loss_D_fake: 0.00531706) Loss_G: 0.25909281 Loss_Enh_Dec: -1.00623214\n",
      "| epoch  77 |   900/ 1249 batches | lr 0.000000 | ms/batch 355.61 | loss  3.12 | ppl    22.67 | acc     0.65 | train_ae_norm     1.00\n",
      "[77/200][999/1249] Loss_D: 0.00680396 (Loss_D_real: 0.00229598 Loss_D_fake: 0.00450797) Loss_G: 0.27305469 Loss_Enh_Dec: -1.27441597\n",
      "| epoch  77 |  1000/ 1249 batches | lr 0.000000 | ms/batch 355.71 | loss  3.11 | ppl    22.52 | acc     0.66 | train_ae_norm     1.00\n",
      "[77/200][1099/1249] Loss_D: 0.01062310 (Loss_D_real: 0.00560815 Loss_D_fake: 0.00501495) Loss_G: 0.26250386 Loss_Enh_Dec: -1.25415373\n",
      "| epoch  77 |  1100/ 1249 batches | lr 0.000000 | ms/batch 355.84 | loss  3.14 | ppl    23.12 | acc     0.61 | train_ae_norm     1.00\n",
      "[77/200][1199/1249] Loss_D: 0.02585131 (Loss_D_real: 0.02041258 Loss_D_fake: 0.00543873) Loss_G: 0.27300712 Loss_Enh_Dec: -1.41943896\n",
      "| epoch  77 |  1200/ 1249 batches | lr 0.000000 | ms/batch 356.03 | loss  3.09 | ppl    22.01 | acc     0.61 | train_ae_norm     1.00\n",
      "| end of epoch  77 | time: 500.54s | test loss  2.59 | test ppl 13.34 | acc 0.727\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 78 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    20  of    130.    Elapsed: 0:00:07.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:22.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:37.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.709\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.407\n",
      "  Test Loss: 4.688\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  78 |     0/ 1249 batches | lr 0.000000 | ms/batch 489.02 | loss  0.03 | ppl     1.03 | acc     0.62 | train_ae_norm     1.00\n",
      "[78/200][99/1249] Loss_D: 0.00857758 (Loss_D_real: 0.00433805 Loss_D_fake: 0.00423953) Loss_G: 0.27268729 Loss_Enh_Dec: -1.45203483\n",
      "| epoch  78 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.30 | loss  3.14 | ppl    23.02 | acc     0.62 | train_ae_norm     1.00\n",
      "[78/200][199/1249] Loss_D: 0.00736589 (Loss_D_real: 0.00308400 Loss_D_fake: 0.00428189) Loss_G: 0.27934295 Loss_Enh_Dec: -1.33400810\n",
      "| epoch  78 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.99 | loss  3.08 | ppl    21.83 | acc     0.63 | train_ae_norm     1.00\n",
      "[78/200][299/1249] Loss_D: 0.00575519 (Loss_D_real: 0.00158992 Loss_D_fake: 0.00416527) Loss_G: 0.29109573 Loss_Enh_Dec: -1.29095304\n",
      "| epoch  78 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.00 | loss  3.10 | ppl    22.14 | acc     0.64 | train_ae_norm     1.00\n",
      "[78/200][399/1249] Loss_D: 0.00740543 (Loss_D_real: 0.00452108 Loss_D_fake: 0.00288435) Loss_G: 0.29105520 Loss_Enh_Dec: -1.45616376\n",
      "| epoch  78 |   400/ 1249 batches | lr 0.000000 | ms/batch 356.43 | loss  3.02 | ppl    20.42 | acc     0.67 | train_ae_norm     1.00\n",
      "[78/200][499/1249] Loss_D: 0.00983439 (Loss_D_real: 0.00578066 Loss_D_fake: 0.00405373) Loss_G: 0.28954950 Loss_Enh_Dec: -1.35294521\n",
      "| epoch  78 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.47 | loss  3.08 | ppl    21.68 | acc     0.61 | train_ae_norm     1.00\n",
      "[78/200][599/1249] Loss_D: 0.00635043 (Loss_D_real: 0.00288172 Loss_D_fake: 0.00346871) Loss_G: 0.30249420 Loss_Enh_Dec: -1.72114599\n",
      "| epoch  78 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.17 | loss  3.09 | ppl    21.98 | acc     0.63 | train_ae_norm     1.00\n",
      "[78/200][699/1249] Loss_D: 0.01246782 (Loss_D_real: 0.00567419 Loss_D_fake: 0.00679363) Loss_G: 0.26961908 Loss_Enh_Dec: -1.19461882\n",
      "| epoch  78 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.00 | loss  3.13 | ppl    22.77 | acc     0.61 | train_ae_norm     1.00\n",
      "[78/200][799/1249] Loss_D: 0.01284239 (Loss_D_real: 0.00649329 Loss_D_fake: 0.00634910) Loss_G: 0.26495200 Loss_Enh_Dec: -1.30009389\n",
      "| epoch  78 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.67 | loss  3.08 | ppl    21.69 | acc     0.60 | train_ae_norm     1.00\n",
      "[78/200][899/1249] Loss_D: 0.01320961 (Loss_D_real: 0.00818454 Loss_D_fake: 0.00502506) Loss_G: 0.28198370 Loss_Enh_Dec: -1.15555143\n",
      "| epoch  78 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.65 | loss  3.04 | ppl    20.89 | acc     0.60 | train_ae_norm     1.00\n",
      "[78/200][999/1249] Loss_D: 0.00604034 (Loss_D_real: 0.00186439 Loss_D_fake: 0.00417596) Loss_G: 0.28687391 Loss_Enh_Dec: -1.40513217\n",
      "| epoch  78 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.22 | loss  2.99 | ppl    19.96 | acc     0.67 | train_ae_norm     1.00\n",
      "[78/200][1099/1249] Loss_D: 0.03259013 (Loss_D_real: 0.01861306 Loss_D_fake: 0.01397707) Loss_G: 0.35741049 Loss_Enh_Dec: -1.31795561\n",
      "| epoch  78 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.59 | loss  3.07 | ppl    21.44 | acc     0.59 | train_ae_norm     1.00\n",
      "[78/200][1199/1249] Loss_D: 0.02123193 (Loss_D_real: 0.01564930 Loss_D_fake: 0.00558263) Loss_G: 0.27817371 Loss_Enh_Dec: -1.24806440\n",
      "| epoch  78 |  1200/ 1249 batches | lr 0.000000 | ms/batch 357.29 | loss  3.00 | ppl    20.05 | acc     0.64 | train_ae_norm     1.00\n",
      "| end of epoch  78 | time: 500.57s | test loss  2.53 | test ppl 12.51 | acc 0.732\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 79 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.713\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.388\n",
      "  Test Loss: 4.892\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  79 |     0/ 1249 batches | lr 0.000000 | ms/batch 489.92 | loss  0.03 | ppl     1.03 | acc     0.62 | train_ae_norm     1.00\n",
      "[79/200][99/1249] Loss_D: 0.01024663 (Loss_D_real: 0.00430147 Loss_D_fake: 0.00594516) Loss_G: 0.29018378 Loss_Enh_Dec: -1.25918090\n",
      "| epoch  79 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.22 | loss  3.01 | ppl    20.36 | acc     0.64 | train_ae_norm     1.00\n",
      "[79/200][199/1249] Loss_D: 0.00871004 (Loss_D_real: 0.00567326 Loss_D_fake: 0.00303678) Loss_G: 0.30064601 Loss_Enh_Dec: -1.05690730\n",
      "| epoch  79 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.25 | loss  2.98 | ppl    19.65 | acc     0.62 | train_ae_norm     1.00\n",
      "[79/200][299/1249] Loss_D: 0.00760225 (Loss_D_real: 0.00336751 Loss_D_fake: 0.00423475) Loss_G: 0.30384588 Loss_Enh_Dec: -1.23020160\n",
      "| epoch  79 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.75 | loss  3.02 | ppl    20.55 | acc     0.66 | train_ae_norm     1.00\n",
      "[79/200][399/1249] Loss_D: 0.01191613 (Loss_D_real: 0.00556868 Loss_D_fake: 0.00634746) Loss_G: 0.28996786 Loss_Enh_Dec: -1.28683054\n",
      "| epoch  79 |   400/ 1249 batches | lr 0.000000 | ms/batch 356.62 | loss  2.98 | ppl    19.74 | acc     0.68 | train_ae_norm     1.00\n",
      "[79/200][499/1249] Loss_D: 0.00535089 (Loss_D_real: 0.00139095 Loss_D_fake: 0.00395994) Loss_G: 0.29945886 Loss_Enh_Dec: -1.28410900\n",
      "| epoch  79 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.61 | loss  2.98 | ppl    19.67 | acc     0.60 | train_ae_norm     1.00\n",
      "[79/200][599/1249] Loss_D: 0.00822716 (Loss_D_real: 0.00466119 Loss_D_fake: 0.00356597) Loss_G: 0.31092963 Loss_Enh_Dec: -1.38607180\n",
      "| epoch  79 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.94 | loss  2.98 | ppl    19.62 | acc     0.65 | train_ae_norm     1.00\n",
      "[79/200][699/1249] Loss_D: 0.00804353 (Loss_D_real: 0.00460498 Loss_D_fake: 0.00343855) Loss_G: 0.30307958 Loss_Enh_Dec: -1.11465716\n",
      "| epoch  79 |   700/ 1249 batches | lr 0.000000 | ms/batch 355.94 | loss  2.97 | ppl    19.41 | acc     0.62 | train_ae_norm     1.00\n",
      "[79/200][799/1249] Loss_D: 0.00894417 (Loss_D_real: 0.00569361 Loss_D_fake: 0.00325055) Loss_G: 0.29957461 Loss_Enh_Dec: -1.17145848\n",
      "| epoch  79 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.17 | loss  2.94 | ppl    18.93 | acc     0.62 | train_ae_norm     1.00\n",
      "[79/200][899/1249] Loss_D: 0.00762295 (Loss_D_real: 0.00517219 Loss_D_fake: 0.00245076) Loss_G: 0.32086983 Loss_Enh_Dec: -1.34845054\n",
      "| epoch  79 |   900/ 1249 batches | lr 0.000000 | ms/batch 355.93 | loss  2.88 | ppl    17.87 | acc     0.66 | train_ae_norm     1.00\n",
      "[79/200][999/1249] Loss_D: 0.01401682 (Loss_D_real: 0.00974920 Loss_D_fake: 0.00426762) Loss_G: 0.29812390 Loss_Enh_Dec: -1.10870540\n",
      "| epoch  79 |  1000/ 1249 batches | lr 0.000000 | ms/batch 357.69 | loss  2.92 | ppl    18.55 | acc     0.66 | train_ae_norm     1.00\n",
      "[79/200][1099/1249] Loss_D: 0.00532011 (Loss_D_real: 0.00183783 Loss_D_fake: 0.00348228) Loss_G: 0.32302245 Loss_Enh_Dec: -1.21962261\n",
      "| epoch  79 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.81 | loss  2.91 | ppl    18.29 | acc     0.67 | train_ae_norm     1.00\n",
      "[79/200][1199/1249] Loss_D: 0.01753776 (Loss_D_real: 0.00449000 Loss_D_fake: 0.01304776) Loss_G: 0.25222191 Loss_Enh_Dec: -0.83764726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  79 |  1200/ 1249 batches | lr 0.000000 | ms/batch 358.19 | loss  2.90 | ppl    18.12 | acc     0.67 | train_ae_norm     1.00\n",
      "| end of epoch  79 | time: 501.02s | test loss  2.43 | test ppl 11.42 | acc 0.747\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 80 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:27.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:46.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.707\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.407\n",
      "  Test Loss: 4.823\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  80 |     0/ 1249 batches | lr 0.000000 | ms/batch 495.01 | loss  0.03 | ppl     1.03 | acc     0.62 | train_ae_norm     1.00\n",
      "[80/200][99/1249] Loss_D: 0.01055918 (Loss_D_real: 0.00507236 Loss_D_fake: 0.00548682) Loss_G: 0.29403588 Loss_Enh_Dec: -1.07005298\n",
      "| epoch  80 |   100/ 1249 batches | lr 0.000000 | ms/batch 357.85 | loss  2.83 | ppl    16.99 | acc     0.65 | train_ae_norm     1.00\n",
      "[80/200][199/1249] Loss_D: 0.01712772 (Loss_D_real: 0.01178668 Loss_D_fake: 0.00534104) Loss_G: 0.28765717 Loss_Enh_Dec: -0.50575215\n",
      "| epoch  80 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.77 | loss  2.83 | ppl    16.89 | acc     0.67 | train_ae_norm     1.00\n",
      "[80/200][299/1249] Loss_D: 0.04404218 (Loss_D_real: 0.01359259 Loss_D_fake: 0.03044960) Loss_G: 0.35237452 Loss_Enh_Dec: -0.86960977\n",
      "| epoch  80 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.18 | loss  2.84 | ppl    17.08 | acc     0.64 | train_ae_norm     1.00\n",
      "[80/200][399/1249] Loss_D: 0.00873816 (Loss_D_real: 0.00384157 Loss_D_fake: 0.00489659) Loss_G: 0.29468831 Loss_Enh_Dec: -1.26167548\n",
      "| epoch  80 |   400/ 1249 batches | lr 0.000000 | ms/batch 356.22 | loss  2.82 | ppl    16.80 | acc     0.67 | train_ae_norm     1.00\n",
      "[80/200][499/1249] Loss_D: 0.00593448 (Loss_D_real: 0.00185936 Loss_D_fake: 0.00407512) Loss_G: 0.29668745 Loss_Enh_Dec: -1.21726036\n",
      "| epoch  80 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.43 | loss  2.85 | ppl    17.29 | acc     0.67 | train_ae_norm     1.00\n",
      "[80/200][599/1249] Loss_D: 0.01062631 (Loss_D_real: 0.00498540 Loss_D_fake: 0.00564091) Loss_G: 0.29982620 Loss_Enh_Dec: -1.14265823\n",
      "| epoch  80 |   600/ 1249 batches | lr 0.000000 | ms/batch 357.09 | loss  2.83 | ppl    16.89 | acc     0.68 | train_ae_norm     1.00\n",
      "[80/200][699/1249] Loss_D: 0.00779676 (Loss_D_real: 0.00453801 Loss_D_fake: 0.00325875) Loss_G: 0.30566239 Loss_Enh_Dec: -1.33624208\n",
      "| epoch  80 |   700/ 1249 batches | lr 0.000000 | ms/batch 357.85 | loss  2.83 | ppl    16.98 | acc     0.65 | train_ae_norm     1.00\n",
      "[80/200][799/1249] Loss_D: 0.01063301 (Loss_D_real: 0.00760409 Loss_D_fake: 0.00302892) Loss_G: 0.32465225 Loss_Enh_Dec: -1.22957528\n",
      "| epoch  80 |   800/ 1249 batches | lr 0.000000 | ms/batch 357.64 | loss  2.84 | ppl    17.19 | acc     0.65 | train_ae_norm     1.00\n",
      "[80/200][899/1249] Loss_D: 0.00667012 (Loss_D_real: 0.00335167 Loss_D_fake: 0.00331845) Loss_G: 0.31044865 Loss_Enh_Dec: -1.29878223\n",
      "| epoch  80 |   900/ 1249 batches | lr 0.000000 | ms/batch 357.64 | loss  2.81 | ppl    16.68 | acc     0.66 | train_ae_norm     1.00\n",
      "[80/200][999/1249] Loss_D: 0.00761742 (Loss_D_real: 0.00554330 Loss_D_fake: 0.00207412) Loss_G: 0.32505521 Loss_Enh_Dec: -1.13211703\n",
      "| epoch  80 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.99 | loss  2.80 | ppl    16.43 | acc     0.67 | train_ae_norm     1.00\n",
      "[80/200][1099/1249] Loss_D: 0.07723068 (Loss_D_real: 0.06868576 Loss_D_fake: 0.00854492) Loss_G: 0.32134494 Loss_Enh_Dec: -1.45448685\n",
      "| epoch  80 |  1100/ 1249 batches | lr 0.000000 | ms/batch 357.08 | loss  2.83 | ppl    16.94 | acc     0.67 | train_ae_norm     1.00\n",
      "[80/200][1199/1249] Loss_D: 0.01668347 (Loss_D_real: 0.00504863 Loss_D_fake: 0.01163484) Loss_G: 0.29029453 Loss_Enh_Dec: -1.11109257\n",
      "[81/200][99/1249] Loss_D: 0.01350722 (Loss_D_real: 0.00428361 Loss_D_fake: 0.00922361) Loss_G: 0.30053750 Loss_Enh_Dec: -1.21738994\n",
      "| epoch  81 |   100/ 1249 batches | lr 0.000000 | ms/batch 357.28 | loss  2.76 | ppl    15.79 | acc     0.67 | train_ae_norm     1.00\n",
      "[81/200][199/1249] Loss_D: 0.00962048 (Loss_D_real: 0.00539930 Loss_D_fake: 0.00422118) Loss_G: 0.30276802 Loss_Enh_Dec: -1.13984096\n",
      "| epoch  81 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.09 | loss  2.76 | ppl    15.76 | acc     0.68 | train_ae_norm     1.00\n",
      "[81/200][299/1249] Loss_D: 0.00741491 (Loss_D_real: 0.00419943 Loss_D_fake: 0.00321548) Loss_G: 0.31073165 Loss_Enh_Dec: -0.95305675\n",
      "| epoch  81 |   300/ 1249 batches | lr 0.000000 | ms/batch 357.00 | loss  2.77 | ppl    15.94 | acc     0.67 | train_ae_norm     1.00\n",
      "[81/200][399/1249] Loss_D: 0.00961236 (Loss_D_real: 0.00726472 Loss_D_fake: 0.00234763) Loss_G: 0.33230123 Loss_Enh_Dec: -1.19018698\n",
      "| epoch  81 |   400/ 1249 batches | lr 0.000000 | ms/batch 358.55 | loss  2.78 | ppl    16.15 | acc     0.69 | train_ae_norm     1.00\n",
      "[81/200][499/1249] Loss_D: 0.01200667 (Loss_D_real: 0.00961829 Loss_D_fake: 0.00238837) Loss_G: 0.32135105 Loss_Enh_Dec: -0.94442904\n",
      "| epoch  81 |   500/ 1249 batches | lr 0.000000 | ms/batch 357.20 | loss  2.91 | ppl    18.45 | acc     0.63 | train_ae_norm     1.00\n",
      "[81/200][599/1249] Loss_D: 0.00975339 (Loss_D_real: 0.00676523 Loss_D_fake: 0.00298816) Loss_G: 0.33864686 Loss_Enh_Dec: -0.90356869\n",
      "| epoch  81 |   600/ 1249 batches | lr 0.000000 | ms/batch 355.92 | loss  2.90 | ppl    18.19 | acc     0.69 | train_ae_norm     1.00\n",
      "[81/200][699/1249] Loss_D: 0.01609082 (Loss_D_real: 0.01017438 Loss_D_fake: 0.00591643) Loss_G: 0.29624295 Loss_Enh_Dec: -0.97826970\n",
      "| epoch  81 |   700/ 1249 batches | lr 0.000000 | ms/batch 357.69 | loss  2.88 | ppl    17.74 | acc     0.63 | train_ae_norm     1.00\n",
      "[81/200][799/1249] Loss_D: 0.00616426 (Loss_D_real: 0.00335898 Loss_D_fake: 0.00280527) Loss_G: 0.31921348 Loss_Enh_Dec: -1.27893674\n",
      "| epoch  81 |   800/ 1249 batches | lr 0.000000 | ms/batch 357.67 | loss  2.90 | ppl    18.09 | acc     0.63 | train_ae_norm     1.00\n",
      "[81/200][899/1249] Loss_D: 0.00945270 (Loss_D_real: 0.00653379 Loss_D_fake: 0.00291892) Loss_G: 0.32798019 Loss_Enh_Dec: -1.33284223\n",
      "| epoch  81 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.21 | loss  2.83 | ppl    16.93 | acc     0.65 | train_ae_norm     1.00\n",
      "[81/200][999/1249] Loss_D: 0.00874854 (Loss_D_real: 0.00273273 Loss_D_fake: 0.00601581) Loss_G: 0.31067234 Loss_Enh_Dec: -1.03746307\n",
      "| epoch  81 |  1000/ 1249 batches | lr 0.000000 | ms/batch 357.11 | loss  2.81 | ppl    16.53 | acc     0.67 | train_ae_norm     1.00\n",
      "[81/200][1099/1249] Loss_D: 0.01063710 (Loss_D_real: 0.00759359 Loss_D_fake: 0.00304351) Loss_G: 0.31984934 Loss_Enh_Dec: -1.18862951\n",
      "| epoch  81 |  1100/ 1249 batches | lr 0.000000 | ms/batch 357.48 | loss  2.83 | ppl    17.00 | acc     0.67 | train_ae_norm     1.00\n",
      "[81/200][1199/1249] Loss_D: 0.00702639 (Loss_D_real: 0.00221915 Loss_D_fake: 0.00480724) Loss_G: 0.29523247 Loss_Enh_Dec: -1.31126201\n",
      "| epoch  81 |  1200/ 1249 batches | lr 0.000000 | ms/batch 356.83 | loss  2.77 | ppl    15.89 | acc     0.69 | train_ae_norm     1.00\n",
      "| end of epoch  81 | time: 501.68s | test loss  2.37 | test ppl 10.70 | acc 0.750\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 82 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.707\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.403\n",
      "  Test Loss: 4.940\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  82 |     0/ 1249 batches | lr 0.000000 | ms/batch 492.06 | loss  0.03 | ppl     1.03 | acc     0.64 | train_ae_norm     1.00\n",
      "[82/200][99/1249] Loss_D: 0.00795735 (Loss_D_real: 0.00546526 Loss_D_fake: 0.00249209) Loss_G: 0.32301632 Loss_Enh_Dec: -1.28492641\n",
      "| epoch  82 |   100/ 1249 batches | lr 0.000000 | ms/batch 357.13 | loss  2.78 | ppl    16.08 | acc     0.68 | train_ae_norm     1.00\n",
      "[82/200][199/1249] Loss_D: 0.00722449 (Loss_D_real: 0.00309042 Loss_D_fake: 0.00413407) Loss_G: 0.31760338 Loss_Enh_Dec: -1.43866122\n",
      "| epoch  82 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.68 | loss  2.79 | ppl    16.25 | acc     0.67 | train_ae_norm     1.00\n",
      "[82/200][299/1249] Loss_D: 0.00672049 (Loss_D_real: 0.00232694 Loss_D_fake: 0.00439356) Loss_G: 0.30340505 Loss_Enh_Dec: -1.40926445\n",
      "| epoch  82 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.84 | loss  2.79 | ppl    16.30 | acc     0.67 | train_ae_norm     1.00\n",
      "[82/200][399/1249] Loss_D: 0.00795923 (Loss_D_real: 0.00326773 Loss_D_fake: 0.00469150) Loss_G: 0.30330893 Loss_Enh_Dec: -1.38581431\n",
      "| epoch  82 |   400/ 1249 batches | lr 0.000000 | ms/batch 357.34 | loss  2.74 | ppl    15.41 | acc     0.72 | train_ae_norm     1.00\n",
      "[82/200][499/1249] Loss_D: 0.01301113 (Loss_D_real: 0.00886398 Loss_D_fake: 0.00414715) Loss_G: 0.30624685 Loss_Enh_Dec: -1.39883196\n",
      "| epoch  82 |   500/ 1249 batches | lr 0.000000 | ms/batch 357.44 | loss  2.80 | ppl    16.52 | acc     0.64 | train_ae_norm     1.00\n",
      "[82/200][599/1249] Loss_D: 0.01596382 (Loss_D_real: 0.00955684 Loss_D_fake: 0.00640699) Loss_G: 0.33420524 Loss_Enh_Dec: -1.42265761\n",
      "| epoch  82 |   600/ 1249 batches | lr 0.000000 | ms/batch 357.19 | loss  2.84 | ppl    17.04 | acc     0.66 | train_ae_norm     1.00\n",
      "[82/200][699/1249] Loss_D: 0.00886814 (Loss_D_real: 0.00459022 Loss_D_fake: 0.00427791) Loss_G: 0.30170032 Loss_Enh_Dec: -1.34573853\n",
      "| epoch  82 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.93 | loss  2.88 | ppl    17.74 | acc     0.64 | train_ae_norm     1.00\n",
      "[82/200][799/1249] Loss_D: 0.01293032 (Loss_D_real: 0.00786749 Loss_D_fake: 0.00506283) Loss_G: 0.31040475 Loss_Enh_Dec: -1.41131496\n",
      "| epoch  82 |   800/ 1249 batches | lr 0.000000 | ms/batch 357.58 | loss  2.92 | ppl    18.57 | acc     0.62 | train_ae_norm     1.00\n",
      "[82/200][899/1249] Loss_D: 0.00912080 (Loss_D_real: 0.00644916 Loss_D_fake: 0.00267164) Loss_G: 0.30353764 Loss_Enh_Dec: -1.39581585\n",
      "| epoch  82 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.98 | loss  2.86 | ppl    17.43 | acc     0.67 | train_ae_norm     1.00\n",
      "[82/200][999/1249] Loss_D: 0.00622978 (Loss_D_real: 0.00240741 Loss_D_fake: 0.00382238) Loss_G: 0.30608121 Loss_Enh_Dec: -1.20470762\n",
      "| epoch  82 |  1000/ 1249 batches | lr 0.000000 | ms/batch 357.26 | loss  2.81 | ppl    16.65 | acc     0.67 | train_ae_norm     1.00\n",
      "[82/200][1099/1249] Loss_D: 0.00907604 (Loss_D_real: 0.00357085 Loss_D_fake: 0.00550520) Loss_G: 0.28175813 Loss_Enh_Dec: -1.32088017\n",
      "| epoch  82 |  1100/ 1249 batches | lr 0.000000 | ms/batch 357.35 | loss  2.83 | ppl    16.93 | acc     0.66 | train_ae_norm     1.00\n",
      "[82/200][1199/1249] Loss_D: 0.00664483 (Loss_D_real: 0.00220942 Loss_D_fake: 0.00443540) Loss_G: 0.30480546 Loss_Enh_Dec: -1.41526449\n",
      "| epoch  82 |  1200/ 1249 batches | lr 0.000000 | ms/batch 357.37 | loss  2.76 | ppl    15.74 | acc     0.67 | train_ae_norm     1.00\n",
      "| end of epoch  82 | time: 501.79s | test loss  2.37 | test ppl 10.69 | acc 0.751\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 83 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.704\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.407\n",
      "  Test Loss: 4.969\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  83 |     0/ 1249 batches | lr 0.000000 | ms/batch 493.61 | loss  0.03 | ppl     1.03 | acc     0.66 | train_ae_norm     1.00\n",
      "[83/200][99/1249] Loss_D: 0.00866770 (Loss_D_real: 0.00289485 Loss_D_fake: 0.00577285) Loss_G: 0.29920897 Loss_Enh_Dec: -1.21902835\n",
      "| epoch  83 |   100/ 1249 batches | lr 0.000000 | ms/batch 358.15 | loss  2.80 | ppl    16.50 | acc     0.64 | train_ae_norm     1.00\n",
      "[83/200][199/1249] Loss_D: 0.01125076 (Loss_D_real: 0.00509370 Loss_D_fake: 0.00615706) Loss_G: 0.29773033 Loss_Enh_Dec: -1.39323270\n",
      "| epoch  83 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.46 | loss  2.86 | ppl    17.38 | acc     0.65 | train_ae_norm     1.00\n",
      "[83/200][299/1249] Loss_D: 0.01049784 (Loss_D_real: 0.00597078 Loss_D_fake: 0.00452706) Loss_G: 0.28147820 Loss_Enh_Dec: -1.13271618\n",
      "| epoch  83 |   300/ 1249 batches | lr 0.000000 | ms/batch 358.00 | loss  2.88 | ppl    17.89 | acc     0.67 | train_ae_norm     1.00\n",
      "[83/200][399/1249] Loss_D: 0.00855996 (Loss_D_real: 0.00460486 Loss_D_fake: 0.00395511) Loss_G: 0.31679162 Loss_Enh_Dec: -1.02716660\n",
      "| epoch  83 |   400/ 1249 batches | lr 0.000000 | ms/batch 356.75 | loss  2.78 | ppl    16.05 | acc     0.69 | train_ae_norm     1.00\n",
      "[83/200][499/1249] Loss_D: 0.00459732 (Loss_D_real: 0.00191556 Loss_D_fake: 0.00268176) Loss_G: 0.32353631 Loss_Enh_Dec: -0.70272934\n",
      "| epoch  83 |   500/ 1249 batches | lr 0.000000 | ms/batch 357.42 | loss  2.78 | ppl    16.19 | acc     0.67 | train_ae_norm     1.00\n",
      "[83/200][599/1249] Loss_D: 0.01928888 (Loss_D_real: 0.01402703 Loss_D_fake: 0.00526185) Loss_G: 0.32331550 Loss_Enh_Dec: -0.80372566\n",
      "| epoch  83 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.97 | loss  2.80 | ppl    16.52 | acc     0.67 | train_ae_norm     1.00\n",
      "[83/200][699/1249] Loss_D: 0.00944524 (Loss_D_real: 0.00488173 Loss_D_fake: 0.00456350) Loss_G: 0.29921338 Loss_Enh_Dec: -0.87320203\n",
      "| epoch  83 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.17 | loss  2.76 | ppl    15.83 | acc     0.65 | train_ae_norm     1.00\n",
      "[83/200][799/1249] Loss_D: 0.00594714 (Loss_D_real: 0.00103013 Loss_D_fake: 0.00491701) Loss_G: 0.31230578 Loss_Enh_Dec: -0.70898223\n",
      "| epoch  83 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.29 | loss  2.76 | ppl    15.85 | acc     0.65 | train_ae_norm     1.00\n",
      "[83/200][899/1249] Loss_D: 0.01161159 (Loss_D_real: 0.00790823 Loss_D_fake: 0.00370336) Loss_G: 0.31798294 Loss_Enh_Dec: -0.69301045\n",
      "| epoch  83 |   900/ 1249 batches | lr 0.000000 | ms/batch 355.91 | loss  2.69 | ppl    14.69 | acc     0.65 | train_ae_norm     1.00\n",
      "[83/200][999/1249] Loss_D: 0.00914388 (Loss_D_real: 0.00517555 Loss_D_fake: 0.00396832) Loss_G: 0.32401645 Loss_Enh_Dec: -0.89993763\n",
      "| epoch  83 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.75 | loss  2.72 | ppl    15.13 | acc     0.70 | train_ae_norm     1.00\n",
      "[83/200][1099/1249] Loss_D: 0.00673259 (Loss_D_real: 0.00143499 Loss_D_fake: 0.00529761) Loss_G: 0.29711705 Loss_Enh_Dec: -0.83062327\n",
      "| epoch  83 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.66 | loss  2.73 | ppl    15.33 | acc     0.66 | train_ae_norm     1.00\n",
      "[83/200][1199/1249] Loss_D: 0.01083481 (Loss_D_real: 0.00713236 Loss_D_fake: 0.00370246) Loss_G: 0.32058227 Loss_Enh_Dec: -0.66827011\n",
      "| epoch  83 |  1200/ 1249 batches | lr 0.000000 | ms/batch 357.37 | loss  2.68 | ppl    14.65 | acc     0.68 | train_ae_norm     1.00\n",
      "| end of epoch  83 | time: 501.71s | test loss  2.36 | test ppl 10.60 | acc 0.750\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 84 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:27.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:31.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:46.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.708\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.390\n",
      "  Test Loss: 4.881\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  84 |     0/ 1249 batches | lr 0.000000 | ms/batch 498.03 | loss  0.03 | ppl     1.03 | acc     0.66 | train_ae_norm     1.00\n",
      "[84/200][99/1249] Loss_D: 0.01216458 (Loss_D_real: 0.00773982 Loss_D_fake: 0.00442476) Loss_G: 0.31082782 Loss_Enh_Dec: -0.89346659\n",
      "| epoch  84 |   100/ 1249 batches | lr 0.000000 | ms/batch 357.49 | loss  2.71 | ppl    15.10 | acc     0.65 | train_ae_norm     1.00\n",
      "[84/200][199/1249] Loss_D: 0.00909382 (Loss_D_real: 0.00545943 Loss_D_fake: 0.00363440) Loss_G: 0.29767120 Loss_Enh_Dec: -1.06389034\n",
      "| epoch  84 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.11 | loss  2.68 | ppl    14.64 | acc     0.71 | train_ae_norm     1.00\n",
      "[84/200][299/1249] Loss_D: 0.00989120 (Loss_D_real: 0.00769918 Loss_D_fake: 0.00219202) Loss_G: 0.32850188 Loss_Enh_Dec: -1.15609443\n",
      "| epoch  84 |   300/ 1249 batches | lr 0.000000 | ms/batch 357.17 | loss  2.73 | ppl    15.28 | acc     0.67 | train_ae_norm     1.00\n",
      "[84/200][399/1249] Loss_D: 0.00878037 (Loss_D_real: 0.00498350 Loss_D_fake: 0.00379687) Loss_G: 0.30067277 Loss_Enh_Dec: -1.18103254\n",
      "| epoch  84 |   400/ 1249 batches | lr 0.000000 | ms/batch 357.11 | loss  2.74 | ppl    15.48 | acc     0.70 | train_ae_norm     1.00\n",
      "[84/200][499/1249] Loss_D: 0.00888856 (Loss_D_real: 0.00555962 Loss_D_fake: 0.00332894) Loss_G: 0.29777789 Loss_Enh_Dec: -1.07028663\n",
      "| epoch  84 |   500/ 1249 batches | lr 0.000000 | ms/batch 357.81 | loss  2.81 | ppl    16.69 | acc     0.63 | train_ae_norm     1.00\n",
      "[84/200][599/1249] Loss_D: 0.01566058 (Loss_D_real: 0.00613765 Loss_D_fake: 0.00952293) Loss_G: 0.30544385 Loss_Enh_Dec: -0.99353898\n",
      "| epoch  84 |   600/ 1249 batches | lr 0.000000 | ms/batch 357.58 | loss  2.80 | ppl    16.41 | acc     0.69 | train_ae_norm     1.00\n",
      "[84/200][699/1249] Loss_D: 0.00680239 (Loss_D_real: 0.00323249 Loss_D_fake: 0.00356990) Loss_G: 0.31585175 Loss_Enh_Dec: -0.88886422\n",
      "| epoch  84 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.27 | loss  2.76 | ppl    15.78 | acc     0.66 | train_ae_norm     1.00\n",
      "[84/200][799/1249] Loss_D: 0.00630728 (Loss_D_real: 0.00182162 Loss_D_fake: 0.00448566) Loss_G: 0.31746137 Loss_Enh_Dec: -1.24196649\n",
      "| epoch  84 |   800/ 1249 batches | lr 0.000000 | ms/batch 357.17 | loss  2.78 | ppl    16.10 | acc     0.64 | train_ae_norm     1.00\n",
      "[84/200][899/1249] Loss_D: 0.00402608 (Loss_D_real: 0.00114270 Loss_D_fake: 0.00288339) Loss_G: 0.32307011 Loss_Enh_Dec: -1.08090007\n",
      "| epoch  84 |   900/ 1249 batches | lr 0.000000 | ms/batch 357.96 | loss  2.71 | ppl    15.09 | acc     0.67 | train_ae_norm     1.00\n",
      "[84/200][999/1249] Loss_D: 0.00529414 (Loss_D_real: 0.00241151 Loss_D_fake: 0.00288263) Loss_G: 0.31400037 Loss_Enh_Dec: -1.12404513\n",
      "| epoch  84 |  1000/ 1249 batches | lr 0.000000 | ms/batch 357.69 | loss  2.69 | ppl    14.80 | acc     0.70 | train_ae_norm     1.00\n",
      "[84/200][1099/1249] Loss_D: 0.00548423 (Loss_D_real: 0.00138613 Loss_D_fake: 0.00409810) Loss_G: 0.31886697 Loss_Enh_Dec: -0.94923180\n",
      "| epoch  84 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.72 | loss  2.72 | ppl    15.14 | acc     0.66 | train_ae_norm     1.00\n",
      "[84/200][1199/1249] Loss_D: 0.00625147 (Loss_D_real: 0.00354985 Loss_D_fake: 0.00270163) Loss_G: 0.35284573 Loss_Enh_Dec: -1.00643086\n",
      "| epoch  84 |  1200/ 1249 batches | lr 0.000000 | ms/batch 358.10 | loss  2.68 | ppl    14.56 | acc     0.69 | train_ae_norm     1.00\n",
      "| end of epoch  84 | time: 502.59s | test loss  2.29 | test ppl  9.92 | acc 0.762\n",
      "bleu_self:  [6.80882069e-01 5.20192907e-01 3.66261121e-01 1.78861128e-01\n",
      " 1.72828765e-04]\n",
      "bleu_test:  [8.36518164e-01 4.18832025e-01 2.82372602e-06 8.17882759e-09\n",
      " 5.68428163e-09]\n",
      "bleu_self: [0.68088207,0.52019291,0.36626112,0.17886113,0.00017283]\n",
      "bleu_test: [0.83651816,0.41883203,0.00000282,0.00000001,0.00000001]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 85 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:27.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:46.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.704\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.405\n",
      "  Test Loss: 4.887\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  85 |     0/ 1249 batches | lr 0.000000 | ms/batch 496.11 | loss  0.03 | ppl     1.03 | acc     0.67 | train_ae_norm     1.00\n",
      "[85/200][99/1249] Loss_D: 0.02017775 (Loss_D_real: 0.01693000 Loss_D_fake: 0.00324775) Loss_G: 0.32401311 Loss_Enh_Dec: -1.17528796\n",
      "| epoch  85 |   100/ 1249 batches | lr 0.000000 | ms/batch 357.55 | loss  2.69 | ppl    14.80 | acc     0.66 | train_ae_norm     1.00\n",
      "[85/200][199/1249] Loss_D: 0.00788652 (Loss_D_real: 0.00190396 Loss_D_fake: 0.00598255) Loss_G: 0.30380264 Loss_Enh_Dec: -1.20289159\n",
      "| epoch  85 |   200/ 1249 batches | lr 0.000000 | ms/batch 358.05 | loss  2.68 | ppl    14.57 | acc     0.70 | train_ae_norm     1.00\n",
      "[85/200][299/1249] Loss_D: 0.00362882 (Loss_D_real: 0.00152245 Loss_D_fake: 0.00210637) Loss_G: 0.32566127 Loss_Enh_Dec: -1.11145365\n",
      "| epoch  85 |   300/ 1249 batches | lr 0.000000 | ms/batch 357.71 | loss  2.71 | ppl    14.98 | acc     0.69 | train_ae_norm     1.00\n",
      "[85/200][399/1249] Loss_D: 0.01266528 (Loss_D_real: 0.00879333 Loss_D_fake: 0.00387195) Loss_G: 0.33817250 Loss_Enh_Dec: -0.90969294\n",
      "| epoch  85 |   400/ 1249 batches | lr 0.000000 | ms/batch 358.01 | loss  2.67 | ppl    14.38 | acc     0.70 | train_ae_norm     1.00\n",
      "[85/200][499/1249] Loss_D: 0.00694368 (Loss_D_real: 0.00266778 Loss_D_fake: 0.00427590) Loss_G: 0.35224673 Loss_Enh_Dec: -1.09308040\n",
      "| epoch  85 |   500/ 1249 batches | lr 0.000000 | ms/batch 356.81 | loss  2.70 | ppl    14.94 | acc     0.67 | train_ae_norm     1.00\n",
      "[85/200][599/1249] Loss_D: 0.00594845 (Loss_D_real: 0.00333464 Loss_D_fake: 0.00261381) Loss_G: 0.32626873 Loss_Enh_Dec: -0.81931943\n",
      "| epoch  85 |   600/ 1249 batches | lr 0.000000 | ms/batch 357.26 | loss  2.67 | ppl    14.51 | acc     0.68 | train_ae_norm     1.00\n",
      "[85/200][699/1249] Loss_D: 0.00897800 (Loss_D_real: 0.00534699 Loss_D_fake: 0.00363101) Loss_G: 0.32710168 Loss_Enh_Dec: -0.89396876\n",
      "| epoch  85 |   700/ 1249 batches | lr 0.000000 | ms/batch 357.22 | loss  2.74 | ppl    15.54 | acc     0.67 | train_ae_norm     1.00\n",
      "[85/200][799/1249] Loss_D: 0.00949534 (Loss_D_real: 0.00584183 Loss_D_fake: 0.00365352) Loss_G: 0.29821098 Loss_Enh_Dec: -1.42257595\n",
      "| epoch  85 |   800/ 1249 batches | lr 0.000000 | ms/batch 357.42 | loss  2.80 | ppl    16.44 | acc     0.66 | train_ae_norm     1.00\n",
      "[85/200][899/1249] Loss_D: 0.00880619 (Loss_D_real: 0.00473763 Loss_D_fake: 0.00406856) Loss_G: 0.30039921 Loss_Enh_Dec: -1.27024174\n",
      "| epoch  85 |   900/ 1249 batches | lr 0.000000 | ms/batch 357.84 | loss  2.74 | ppl    15.52 | acc     0.64 | train_ae_norm     1.00\n",
      "[85/200][999/1249] Loss_D: 0.00676539 (Loss_D_real: 0.00226576 Loss_D_fake: 0.00449963) Loss_G: 0.31786934 Loss_Enh_Dec: -1.27531433\n",
      "| epoch  85 |  1000/ 1249 batches | lr 0.000000 | ms/batch 357.45 | loss  2.78 | ppl    16.06 | acc     0.67 | train_ae_norm     1.00\n",
      "[85/200][1099/1249] Loss_D: 0.00843044 (Loss_D_real: 0.00541417 Loss_D_fake: 0.00301626) Loss_G: 0.31872767 Loss_Enh_Dec: -1.35856366\n",
      "| epoch  85 |  1100/ 1249 batches | lr 0.000000 | ms/batch 357.22 | loss  2.78 | ppl    16.13 | acc     0.66 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85/200][1199/1249] Loss_D: 0.00869769 (Loss_D_real: 0.00490318 Loss_D_fake: 0.00379451) Loss_G: 0.31699657 Loss_Enh_Dec: -1.29576671\n",
      "| epoch  85 |  1200/ 1249 batches | lr 0.000000 | ms/batch 357.68 | loss  2.77 | ppl    15.96 | acc     0.66 | train_ae_norm     1.00\n",
      "| end of epoch  85 | time: 502.59s | test loss  2.39 | test ppl 10.87 | acc 0.753\n",
      "bleu_self:  [4.70012815e-01 2.69775049e-01 1.10597702e-01 6.48472076e-02\n",
      " 5.49105403e-05]\n",
      "bleu_test:  [9.12500000e-01 5.37107511e-01 2.48930531e-02 3.85947220e-06\n",
      " 4.48829340e-07]\n",
      "bleu_self: [0.47001282,0.26977505,0.11059770,0.06484721,0.00005491]\n",
      "bleu_test: [0.91250000,0.53710751,0.02489305,0.00000386,0.00000045]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 86 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:27.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:46.\n",
      "\n",
      "  Average training loss generetor: 0.699\n",
      "  Average training loss discriminator: 0.707\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.405\n",
      "  Test Loss: 4.912\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  86 |     0/ 1249 batches | lr 0.000000 | ms/batch 495.75 | loss  0.03 | ppl     1.03 | acc     0.65 | train_ae_norm     1.00\n",
      "[86/200][99/1249] Loss_D: 0.00991761 (Loss_D_real: 0.00490692 Loss_D_fake: 0.00501069) Loss_G: 0.30426580 Loss_Enh_Dec: -1.26180398\n",
      "| epoch  86 |   100/ 1249 batches | lr 0.000000 | ms/batch 357.19 | loss  2.83 | ppl    16.89 | acc     0.67 | train_ae_norm     1.00\n",
      "[86/200][199/1249] Loss_D: 0.00836368 (Loss_D_real: 0.00239072 Loss_D_fake: 0.00597295) Loss_G: 0.32009560 Loss_Enh_Dec: -1.07289684\n",
      "| epoch  86 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.56 | loss  2.79 | ppl    16.25 | acc     0.68 | train_ae_norm     1.00\n",
      "[86/200][299/1249] Loss_D: 0.00865175 (Loss_D_real: 0.00245989 Loss_D_fake: 0.00619186) Loss_G: 0.37282902 Loss_Enh_Dec: -1.00489485\n",
      "| epoch  86 |   300/ 1249 batches | lr 0.000000 | ms/batch 358.43 | loss  2.77 | ppl    15.98 | acc     0.70 | train_ae_norm     1.00\n",
      "[86/200][399/1249] Loss_D: 0.00942899 (Loss_D_real: 0.00479727 Loss_D_fake: 0.00463173) Loss_G: 0.31977555 Loss_Enh_Dec: -1.08007777\n",
      "| epoch  86 |   400/ 1249 batches | lr 0.000000 | ms/batch 358.26 | loss  2.74 | ppl    15.49 | acc     0.72 | train_ae_norm     1.00\n",
      "[86/200][499/1249] Loss_D: 0.00865787 (Loss_D_real: 0.00491350 Loss_D_fake: 0.00374437) Loss_G: 0.32386142 Loss_Enh_Dec: -0.94066238\n",
      "| epoch  86 |   500/ 1249 batches | lr 0.000000 | ms/batch 358.44 | loss  2.78 | ppl    16.04 | acc     0.68 | train_ae_norm     1.00\n",
      "[86/200][599/1249] Loss_D: 0.01653762 (Loss_D_real: 0.01207830 Loss_D_fake: 0.00445932) Loss_G: 0.36914757 Loss_Enh_Dec: -0.80046332\n",
      "| epoch  86 |   600/ 1249 batches | lr 0.000000 | ms/batch 357.31 | loss  2.81 | ppl    16.54 | acc     0.69 | train_ae_norm     1.00\n",
      "[86/200][699/1249] Loss_D: 0.00515215 (Loss_D_real: 0.00218489 Loss_D_fake: 0.00296726) Loss_G: 0.32612869 Loss_Enh_Dec: -0.98577160\n",
      "| epoch  86 |   700/ 1249 batches | lr 0.000000 | ms/batch 357.21 | loss  2.83 | ppl    16.88 | acc     0.66 | train_ae_norm     1.00\n",
      "[86/200][799/1249] Loss_D: 0.02502766 (Loss_D_real: 0.00518328 Loss_D_fake: 0.01984438) Loss_G: 0.35616195 Loss_Enh_Dec: -0.89815694\n",
      "| epoch  86 |   800/ 1249 batches | lr 0.000000 | ms/batch 358.50 | loss  2.82 | ppl    16.73 | acc     0.65 | train_ae_norm     1.00\n",
      "[86/200][899/1249] Loss_D: 0.00590810 (Loss_D_real: 0.00250501 Loss_D_fake: 0.00340309) Loss_G: 0.30614838 Loss_Enh_Dec: -1.17196143\n",
      "| epoch  86 |   900/ 1249 batches | lr 0.000000 | ms/batch 357.88 | loss  2.82 | ppl    16.71 | acc     0.66 | train_ae_norm     1.00\n",
      "[86/200][999/1249] Loss_D: 0.00509394 (Loss_D_real: 0.00144012 Loss_D_fake: 0.00365382) Loss_G: 0.31586972 Loss_Enh_Dec: -0.80212015\n",
      "| epoch  86 |  1000/ 1249 batches | lr 0.000000 | ms/batch 357.76 | loss  2.78 | ppl    16.05 | acc     0.67 | train_ae_norm     1.00\n",
      "[86/200][1099/1249] Loss_D: 0.00491443 (Loss_D_real: 0.00109256 Loss_D_fake: 0.00382186) Loss_G: 0.32722577 Loss_Enh_Dec: -0.96582073\n",
      "| epoch  86 |  1100/ 1249 batches | lr 0.000000 | ms/batch 357.68 | loss  2.77 | ppl    16.00 | acc     0.67 | train_ae_norm     1.00\n",
      "[86/200][1199/1249] Loss_D: 0.01607395 (Loss_D_real: 0.01242618 Loss_D_fake: 0.00364777) Loss_G: 0.31415290 Loss_Enh_Dec: -1.00341809\n",
      "| epoch  86 |  1200/ 1249 batches | lr 0.000000 | ms/batch 357.80 | loss  2.76 | ppl    15.83 | acc     0.65 | train_ae_norm     1.00\n",
      "| end of epoch  86 | time: 502.91s | test loss  2.41 | test ppl 11.16 | acc 0.750\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 87 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.707\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.395\n",
      "  Test Loss: 4.950\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  87 |     0/ 1249 batches | lr 0.000000 | ms/batch 492.23 | loss  0.03 | ppl     1.03 | acc     0.62 | train_ae_norm     1.00\n",
      "[87/200][99/1249] Loss_D: 0.01489192 (Loss_D_real: 0.01205711 Loss_D_fake: 0.00283481) Loss_G: 0.34218207 Loss_Enh_Dec: -1.19637287\n",
      "| epoch  87 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.93 | loss  2.80 | ppl    16.53 | acc     0.69 | train_ae_norm     1.00\n",
      "[87/200][199/1249] Loss_D: 0.01018971 (Loss_D_real: 0.00672047 Loss_D_fake: 0.00346924) Loss_G: 0.31803107 Loss_Enh_Dec: -1.04482019\n",
      "| epoch  87 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.67 | loss  2.81 | ppl    16.65 | acc     0.67 | train_ae_norm     1.00\n",
      "[87/200][299/1249] Loss_D: 0.01023463 (Loss_D_real: 0.00731007 Loss_D_fake: 0.00292456) Loss_G: 0.30388123 Loss_Enh_Dec: -1.16358531\n",
      "| epoch  87 |   300/ 1249 batches | lr 0.000000 | ms/batch 357.81 | loss  2.89 | ppl    17.97 | acc     0.66 | train_ae_norm     1.00\n",
      "[87/200][399/1249] Loss_D: 0.01259887 (Loss_D_real: 0.01036907 Loss_D_fake: 0.00222980) Loss_G: 0.35572281 Loss_Enh_Dec: -0.99038118\n",
      "| epoch  87 |   400/ 1249 batches | lr 0.000000 | ms/batch 357.62 | loss  2.78 | ppl    16.07 | acc     0.70 | train_ae_norm     1.00\n",
      "[87/200][499/1249] Loss_D: 0.01048700 (Loss_D_real: 0.00580034 Loss_D_fake: 0.00468666) Loss_G: 0.31860578 Loss_Enh_Dec: -1.18576467\n",
      "| epoch  87 |   500/ 1249 batches | lr 0.000000 | ms/batch 357.09 | loss  2.81 | ppl    16.54 | acc     0.64 | train_ae_norm     1.00\n",
      "[87/200][599/1249] Loss_D: 0.01465310 (Loss_D_real: 0.00880403 Loss_D_fake: 0.00584907) Loss_G: 0.32178670 Loss_Enh_Dec: -1.15282595\n",
      "| epoch  87 |   600/ 1249 batches | lr 0.000000 | ms/batch 357.40 | loss  2.72 | ppl    15.23 | acc     0.69 | train_ae_norm     1.00\n",
      "[87/200][699/1249] Loss_D: 0.00783638 (Loss_D_real: 0.00288630 Loss_D_fake: 0.00495009) Loss_G: 0.32042423 Loss_Enh_Dec: -0.88575059\n",
      "| epoch  87 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.53 | loss  2.73 | ppl    15.33 | acc     0.65 | train_ae_norm     1.00\n",
      "[87/200][799/1249] Loss_D: 0.00947507 (Loss_D_real: 0.00633473 Loss_D_fake: 0.00314034) Loss_G: 0.34250316 Loss_Enh_Dec: -0.94459021\n",
      "| epoch  87 |   800/ 1249 batches | lr 0.000000 | ms/batch 357.49 | loss  2.84 | ppl    17.13 | acc     0.63 | train_ae_norm     1.00\n",
      "[87/200][899/1249] Loss_D: 0.00862582 (Loss_D_real: 0.00460247 Loss_D_fake: 0.00402335) Loss_G: 0.31915110 Loss_Enh_Dec: -1.21007872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  87 |   900/ 1249 batches | lr 0.000000 | ms/batch 357.37 | loss  2.80 | ppl    16.47 | acc     0.68 | train_ae_norm     1.00\n",
      "[87/200][999/1249] Loss_D: 0.07463679 (Loss_D_real: 0.00641620 Loss_D_fake: 0.06822059) Loss_G: 0.57425839 Loss_Enh_Dec: -0.99937481\n",
      "| epoch  87 |  1000/ 1249 batches | lr 0.000000 | ms/batch 358.00 | loss  2.78 | ppl    16.15 | acc     0.67 | train_ae_norm     1.00\n",
      "[87/200][1099/1249] Loss_D: 0.00793078 (Loss_D_real: 0.00237800 Loss_D_fake: 0.00555277) Loss_G: 0.32037425 Loss_Enh_Dec: -1.23580158\n",
      "| epoch  87 |  1100/ 1249 batches | lr 0.000000 | ms/batch 357.74 | loss  2.75 | ppl    15.70 | acc     0.65 | train_ae_norm     1.00\n",
      "[87/200][1199/1249] Loss_D: 0.00759755 (Loss_D_real: 0.00334238 Loss_D_fake: 0.00425517) Loss_G: 0.34146425 Loss_Enh_Dec: -1.29100323\n",
      "| epoch  87 |  1200/ 1249 batches | lr 0.000000 | ms/batch 357.73 | loss  2.68 | ppl    14.57 | acc     0.69 | train_ae_norm     1.00\n",
      "| end of epoch  87 | time: 502.11s | test loss  2.29 | test ppl  9.89 | acc 0.766\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 88 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:27.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:46.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.703\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.407\n",
      "  Test Loss: 4.987\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  88 |     0/ 1249 batches | lr 0.000000 | ms/batch 496.79 | loss  0.03 | ppl     1.03 | acc     0.68 | train_ae_norm     1.00\n",
      "[88/200][99/1249] Loss_D: 0.03873313 (Loss_D_real: 0.03050442 Loss_D_fake: 0.00822871) Loss_G: 0.32851139 Loss_Enh_Dec: -1.19807518\n",
      "| epoch  88 |   100/ 1249 batches | lr 0.000000 | ms/batch 357.60 | loss  2.72 | ppl    15.22 | acc     0.70 | train_ae_norm     1.00\n",
      "[88/200][199/1249] Loss_D: 0.01799032 (Loss_D_real: 0.01231452 Loss_D_fake: 0.00567581) Loss_G: 0.29901198 Loss_Enh_Dec: -1.04172325\n",
      "| epoch  88 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.83 | loss  2.69 | ppl    14.77 | acc     0.66 | train_ae_norm     1.00\n",
      "[88/200][299/1249] Loss_D: 0.00479697 (Loss_D_real: 0.00168385 Loss_D_fake: 0.00311312) Loss_G: 0.31299567 Loss_Enh_Dec: -0.88665336\n",
      "| epoch  88 |   300/ 1249 batches | lr 0.000000 | ms/batch 357.07 | loss  2.78 | ppl    16.11 | acc     0.68 | train_ae_norm     1.00\n",
      "[88/200][399/1249] Loss_D: 0.00807596 (Loss_D_real: 0.00459323 Loss_D_fake: 0.00348273) Loss_G: 0.33744860 Loss_Enh_Dec: -1.24364269\n",
      "| epoch  88 |   400/ 1249 batches | lr 0.000000 | ms/batch 358.16 | loss  2.74 | ppl    15.56 | acc     0.68 | train_ae_norm     1.00\n",
      "[88/200][499/1249] Loss_D: 0.00548487 (Loss_D_real: 0.00241597 Loss_D_fake: 0.00306890) Loss_G: 0.33230504 Loss_Enh_Dec: -1.24511445\n",
      "| epoch  88 |   500/ 1249 batches | lr 0.000000 | ms/batch 358.11 | loss  2.77 | ppl    15.89 | acc     0.66 | train_ae_norm     1.00\n",
      "[88/200][599/1249] Loss_D: 0.01365598 (Loss_D_real: 0.00228122 Loss_D_fake: 0.01137476) Loss_G: 0.39079642 Loss_Enh_Dec: -0.80807936\n",
      "| epoch  88 |   600/ 1249 batches | lr 0.000000 | ms/batch 357.91 | loss  2.73 | ppl    15.36 | acc     0.68 | train_ae_norm     1.00\n",
      "[88/200][699/1249] Loss_D: 0.01544813 (Loss_D_real: 0.00549629 Loss_D_fake: 0.00995184) Loss_G: 0.32752249 Loss_Enh_Dec: -1.25113583\n",
      "| epoch  88 |   700/ 1249 batches | lr 0.000000 | ms/batch 357.52 | loss  2.71 | ppl    15.01 | acc     0.67 | train_ae_norm     1.00\n",
      "[88/200][799/1249] Loss_D: 0.00879391 (Loss_D_real: 0.00499921 Loss_D_fake: 0.00379470) Loss_G: 0.30306911 Loss_Enh_Dec: -1.52964461\n",
      "| epoch  88 |   800/ 1249 batches | lr 0.000000 | ms/batch 357.53 | loss  2.75 | ppl    15.71 | acc     0.65 | train_ae_norm     1.00\n",
      "[88/200][899/1249] Loss_D: 0.02481573 (Loss_D_real: 0.01242247 Loss_D_fake: 0.01239326) Loss_G: 0.36584234 Loss_Enh_Dec: -1.20748210\n",
      "| epoch  88 |   900/ 1249 batches | lr 0.000000 | ms/batch 357.57 | loss  2.70 | ppl    14.88 | acc     0.67 | train_ae_norm     1.00\n",
      "[88/200][999/1249] Loss_D: 0.02244681 (Loss_D_real: 0.00146521 Loss_D_fake: 0.02098161) Loss_G: 0.22396772 Loss_Enh_Dec: -1.30106199\n",
      "| epoch  88 |  1000/ 1249 batches | lr 0.000000 | ms/batch 357.22 | loss  2.72 | ppl    15.20 | acc     0.69 | train_ae_norm     1.00\n",
      "[88/200][1099/1249] Loss_D: 0.01909355 (Loss_D_real: 0.00801371 Loss_D_fake: 0.01107984) Loss_G: 0.23813808 Loss_Enh_Dec: -1.20840681\n",
      "| epoch  88 |  1100/ 1249 batches | lr 0.000000 | ms/batch 358.07 | loss  2.71 | ppl    15.04 | acc     0.68 | train_ae_norm     1.00\n",
      "[88/200][1199/1249] Loss_D: 0.01684378 (Loss_D_real: 0.00709683 Loss_D_fake: 0.00974695) Loss_G: 0.23224942 Loss_Enh_Dec: -1.19236553\n",
      "| epoch  88 |  1200/ 1249 batches | lr 0.000000 | ms/batch 357.88 | loss  2.69 | ppl    14.75 | acc     0.68 | train_ae_norm     1.00\n",
      "| end of epoch  88 | time: 502.87s | test loss  2.32 | test ppl 10.14 | acc 0.762\n",
      "bleu_self:  [0.79182692 0.69792916 0.55373399 0.4220016  0.25019169]\n",
      "bleu_test:  [9.71153846e-01 3.96819256e-01 3.33407449e-06 1.08782441e-08\n",
      " 6.62244222e-09]\n",
      "bleu_self: [0.79182692,0.69792916,0.55373399,0.42200160,0.25019169]\n",
      "bleu_test: [0.97115385,0.39681926,0.00000333,0.00000001,0.00000001]\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 89 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:26.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:45.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.707\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.420\n",
      "  Test Loss: 4.862\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  89 |     0/ 1249 batches | lr 0.000000 | ms/batch 490.65 | loss  0.03 | ppl     1.03 | acc     0.69 | train_ae_norm     1.00\n",
      "[89/200][99/1249] Loss_D: 0.00690539 (Loss_D_real: 0.00095277 Loss_D_fake: 0.00595262) Loss_G: 0.25620893 Loss_Enh_Dec: -1.02676666\n",
      "| epoch  89 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.85 | loss  2.70 | ppl    14.91 | acc     0.67 | train_ae_norm     1.00\n",
      "[89/200][199/1249] Loss_D: 0.02247761 (Loss_D_real: 0.01741695 Loss_D_fake: 0.00506065) Loss_G: 0.26781327 Loss_Enh_Dec: -1.20237994\n",
      "| epoch  89 |   200/ 1249 batches | lr 0.000000 | ms/batch 357.39 | loss  2.64 | ppl    13.95 | acc     0.70 | train_ae_norm     1.00\n",
      "[89/200][299/1249] Loss_D: 0.00942509 (Loss_D_real: 0.00274808 Loss_D_fake: 0.00667700) Loss_G: 0.25319305 Loss_Enh_Dec: -1.07348740\n",
      "| epoch  89 |   300/ 1249 batches | lr 0.000000 | ms/batch 357.45 | loss  2.66 | ppl    14.24 | acc     0.70 | train_ae_norm     1.00\n",
      "[89/200][399/1249] Loss_D: 0.00857720 (Loss_D_real: 0.00306201 Loss_D_fake: 0.00551520) Loss_G: 0.26310822 Loss_Enh_Dec: -1.17404282\n",
      "| epoch  89 |   400/ 1249 batches | lr 0.000000 | ms/batch 357.30 | loss  2.62 | ppl    13.75 | acc     0.71 | train_ae_norm     1.00\n",
      "[89/200][499/1249] Loss_D: 0.01184888 (Loss_D_real: 0.00613882 Loss_D_fake: 0.00571006) Loss_G: 0.25914869 Loss_Enh_Dec: -1.25841320\n",
      "| epoch  89 |   500/ 1249 batches | lr 0.000000 | ms/batch 357.41 | loss  2.68 | ppl    14.62 | acc     0.69 | train_ae_norm     1.00\n",
      "[89/200][599/1249] Loss_D: 0.00682638 (Loss_D_real: 0.00198078 Loss_D_fake: 0.00484560) Loss_G: 0.27339211 Loss_Enh_Dec: -1.09320331\n",
      "| epoch  89 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.87 | loss  2.66 | ppl    14.26 | acc     0.74 | train_ae_norm     1.00\n",
      "[89/200][699/1249] Loss_D: 0.01020146 (Loss_D_real: 0.00584984 Loss_D_fake: 0.00435162) Loss_G: 0.27164516 Loss_Enh_Dec: -1.05804658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  89 |   700/ 1249 batches | lr 0.000000 | ms/batch 357.37 | loss  2.65 | ppl    14.11 | acc     0.67 | train_ae_norm     1.00\n",
      "[89/200][799/1249] Loss_D: 0.00471401 (Loss_D_real: 0.00235411 Loss_D_fake: 0.00235990) Loss_G: 0.36806032 Loss_Enh_Dec: -0.89100707\n",
      "| epoch  89 |   800/ 1249 batches | lr 0.000000 | ms/batch 357.37 | loss  2.67 | ppl    14.45 | acc     0.66 | train_ae_norm     1.00\n",
      "[89/200][899/1249] Loss_D: 0.00723094 (Loss_D_real: 0.00473551 Loss_D_fake: 0.00249544) Loss_G: 0.35142550 Loss_Enh_Dec: -1.09386146\n",
      "| epoch  89 |   900/ 1249 batches | lr 0.000000 | ms/batch 357.34 | loss  2.61 | ppl    13.64 | acc     0.67 | train_ae_norm     1.00\n",
      "[89/200][999/1249] Loss_D: 0.00751902 (Loss_D_real: 0.00337191 Loss_D_fake: 0.00414711) Loss_G: 0.29854593 Loss_Enh_Dec: -0.82314360\n",
      "| epoch  89 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.16 | loss  2.60 | ppl    13.40 | acc     0.70 | train_ae_norm     1.00\n",
      "[89/200][1099/1249] Loss_D: 0.00436825 (Loss_D_real: 0.00244151 Loss_D_fake: 0.00192674) Loss_G: 0.35645983 Loss_Enh_Dec: -1.18871725\n",
      "| epoch  89 |  1100/ 1249 batches | lr 0.000000 | ms/batch 357.22 | loss  2.64 | ppl    14.04 | acc     0.67 | train_ae_norm     1.00\n",
      "[89/200][1199/1249] Loss_D: 0.00466303 (Loss_D_real: 0.00216470 Loss_D_fake: 0.00249833) Loss_G: 0.33264786 Loss_Enh_Dec: -1.06495440\n",
      "| epoch  89 |  1200/ 1249 batches | lr 0.000000 | ms/batch 358.30 | loss  2.64 | ppl    13.98 | acc     0.69 | train_ae_norm     1.00\n",
      "| end of epoch  89 | time: 501.72s | test loss  2.28 | test ppl  9.77 | acc 0.768\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 90 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:27.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:31.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:46.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.707\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.417\n",
      "  Test Loss: 4.966\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  90 |     0/ 1249 batches | lr 0.000000 | ms/batch 497.75 | loss  0.03 | ppl     1.03 | acc     0.70 | train_ae_norm     1.00\n",
      "[90/200][99/1249] Loss_D: 0.00954744 (Loss_D_real: 0.00681978 Loss_D_fake: 0.00272766) Loss_G: 0.34170625 Loss_Enh_Dec: -0.72958654\n",
      "| epoch  90 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.48 | loss  2.69 | ppl    14.71 | acc     0.70 | train_ae_norm     1.00\n",
      "[90/200][199/1249] Loss_D: 0.00394726 (Loss_D_real: 0.00222086 Loss_D_fake: 0.00172640) Loss_G: 0.34582958 Loss_Enh_Dec: -0.91159880\n",
      "| epoch  90 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.54 | loss  2.68 | ppl    14.54 | acc     0.67 | train_ae_norm     1.00\n",
      "[90/200][299/1249] Loss_D: 0.02968081 (Loss_D_real: 0.02426207 Loss_D_fake: 0.00541874) Loss_G: 0.35694295 Loss_Enh_Dec: -1.08331966\n",
      "| epoch  90 |   300/ 1249 batches | lr 0.000000 | ms/batch 356.66 | loss  2.68 | ppl    14.54 | acc     0.67 | train_ae_norm     1.00\n",
      "[90/200][399/1249] Loss_D: 0.00457288 (Loss_D_real: 0.00246542 Loss_D_fake: 0.00210746) Loss_G: 0.33711424 Loss_Enh_Dec: -1.05250192\n",
      "| epoch  90 |   400/ 1249 batches | lr 0.000000 | ms/batch 356.69 | loss  2.65 | ppl    14.11 | acc     0.71 | train_ae_norm     1.00\n",
      "[90/200][499/1249] Loss_D: 0.00690018 (Loss_D_real: 0.00354070 Loss_D_fake: 0.00335948) Loss_G: 0.29408336 Loss_Enh_Dec: -1.20714784\n",
      "| epoch  90 |   500/ 1249 batches | lr 0.000000 | ms/batch 358.41 | loss  2.68 | ppl    14.56 | acc     0.71 | train_ae_norm     1.00\n",
      "[90/200][599/1249] Loss_D: 0.00673374 (Loss_D_real: 0.00289413 Loss_D_fake: 0.00383962) Loss_G: 0.32787284 Loss_Enh_Dec: -1.10015595\n",
      "| epoch  90 |   600/ 1249 batches | lr 0.000000 | ms/batch 356.93 | loss  2.68 | ppl    14.65 | acc     0.70 | train_ae_norm     1.00\n",
      "[90/200][699/1249] Loss_D: 0.00769807 (Loss_D_real: 0.00247227 Loss_D_fake: 0.00522579) Loss_G: 0.33374497 Loss_Enh_Dec: -1.09101427\n",
      "| epoch  90 |   700/ 1249 batches | lr 0.000000 | ms/batch 356.17 | loss  2.63 | ppl    13.83 | acc     0.69 | train_ae_norm     1.00\n",
      "[90/200][799/1249] Loss_D: 0.00725633 (Loss_D_real: 0.00366936 Loss_D_fake: 0.00358697) Loss_G: 0.33720121 Loss_Enh_Dec: -0.86905253\n",
      "| epoch  90 |   800/ 1249 batches | lr 0.000000 | ms/batch 356.75 | loss  2.65 | ppl    14.12 | acc     0.67 | train_ae_norm     1.00\n",
      "[90/200][899/1249] Loss_D: 0.00612903 (Loss_D_real: 0.00296318 Loss_D_fake: 0.00316585) Loss_G: 0.31852600 Loss_Enh_Dec: -0.69732112\n",
      "| epoch  90 |   900/ 1249 batches | lr 0.000000 | ms/batch 356.16 | loss  2.63 | ppl    13.92 | acc     0.66 | train_ae_norm     1.00\n",
      "[90/200][999/1249] Loss_D: 0.00854475 (Loss_D_real: 0.00381901 Loss_D_fake: 0.00472574) Loss_G: 0.33023259 Loss_Enh_Dec: -0.88103026\n",
      "| epoch  90 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.99 | loss  2.62 | ppl    13.70 | acc     0.73 | train_ae_norm     1.00\n",
      "[90/200][1099/1249] Loss_D: 0.00956886 (Loss_D_real: 0.00606625 Loss_D_fake: 0.00350261) Loss_G: 0.31740853 Loss_Enh_Dec: -1.08049190\n",
      "| epoch  90 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.51 | loss  2.74 | ppl    15.46 | acc     0.66 | train_ae_norm     1.00\n",
      "[90/200][1199/1249] Loss_D: 0.01353821 (Loss_D_real: 0.01020032 Loss_D_fake: 0.00333789) Loss_G: 0.31109378 Loss_Enh_Dec: -0.85479873\n",
      "| epoch  90 |  1200/ 1249 batches | lr 0.000000 | ms/batch 357.32 | loss  2.69 | ppl    14.72 | acc     0.68 | train_ae_norm     1.00\n",
      "| end of epoch  90 | time: 501.92s | test loss  2.28 | test ppl  9.78 | acc 0.767\n",
      "bleu_self:  [0.8875     0.83638474 0.77341796 0.71281681 0.64560929]\n",
      "bleu_test:  [2.81250000e-01 6.25208964e-02 4.76568202e-07 1.33543978e-09\n",
      " 3.97878863e-11]\n",
      "bleu_self: [0.88750000,0.83638474,0.77341796,0.71281681,0.64560929]\n",
      "bleu_test: [0.28125000,0.06252090,0.00000048,0.00000000,0.00000000]\n",
      "New saving model: epoch 090.\n",
      "Saving models to ./results/yahoo_merge_results\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 91 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:27.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:31.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:46.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.703\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.407\n",
      "  Test Loss: 5.062\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  91 |     0/ 1249 batches | lr 0.000000 | ms/batch 497.70 | loss  0.03 | ppl     1.03 | acc     0.70 | train_ae_norm     1.00\n",
      "[91/200][99/1249] Loss_D: 0.01344963 (Loss_D_real: 0.01042626 Loss_D_fake: 0.00302337) Loss_G: 0.32281527 Loss_Enh_Dec: -0.82043713\n",
      "| epoch  91 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.98 | loss  2.65 | ppl    14.10 | acc     0.71 | train_ae_norm     1.00\n",
      "[91/200][199/1249] Loss_D: 0.03703947 (Loss_D_real: 0.01192950 Loss_D_fake: 0.02510996) Loss_G: 0.49717638 Loss_Enh_Dec: -0.88604593\n",
      "| epoch  91 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.77 | loss  2.63 | ppl    13.84 | acc     0.67 | train_ae_norm     1.00\n",
      "[91/200][299/1249] Loss_D: 0.00936514 (Loss_D_real: 0.00224359 Loss_D_fake: 0.00712155) Loss_G: 0.27996513 Loss_Enh_Dec: -0.79828846\n",
      "| epoch  91 |   300/ 1249 batches | lr 0.000000 | ms/batch 357.56 | loss  2.65 | ppl    14.20 | acc     0.67 | train_ae_norm     1.00\n",
      "[91/200][399/1249] Loss_D: 0.00447419 (Loss_D_real: 0.00112022 Loss_D_fake: 0.00335397) Loss_G: 0.29850128 Loss_Enh_Dec: -1.13633311\n",
      "| epoch  91 |   400/ 1249 batches | lr 0.000000 | ms/batch 357.14 | loss  2.63 | ppl    13.88 | acc     0.71 | train_ae_norm     1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[91/200][499/1249] Loss_D: 0.00830164 (Loss_D_real: 0.00573450 Loss_D_fake: 0.00256714) Loss_G: 0.33332428 Loss_Enh_Dec: -0.99773258\n",
      "| epoch  91 |   500/ 1249 batches | lr 0.000000 | ms/batch 357.06 | loss  2.66 | ppl    14.36 | acc     0.64 | train_ae_norm     1.00\n",
      "[91/200][599/1249] Loss_D: 0.02293660 (Loss_D_real: 0.01905632 Loss_D_fake: 0.00388028) Loss_G: 0.34528181 Loss_Enh_Dec: -1.15436435\n",
      "| epoch  91 |   600/ 1249 batches | lr 0.000000 | ms/batch 357.77 | loss  2.65 | ppl    14.11 | acc     0.72 | train_ae_norm     1.00\n",
      "[91/200][699/1249] Loss_D: 0.00877777 (Loss_D_real: 0.00348749 Loss_D_fake: 0.00529028) Loss_G: 0.33806935 Loss_Enh_Dec: -1.03753281\n",
      "| epoch  91 |   700/ 1249 batches | lr 0.000000 | ms/batch 357.48 | loss  2.62 | ppl    13.75 | acc     0.68 | train_ae_norm     1.00\n",
      "[91/200][799/1249] Loss_D: 0.00658731 (Loss_D_real: 0.00369690 Loss_D_fake: 0.00289041) Loss_G: 0.32661682 Loss_Enh_Dec: -1.23418105\n",
      "| epoch  91 |   800/ 1249 batches | lr 0.000000 | ms/batch 358.13 | loss  2.66 | ppl    14.34 | acc     0.67 | train_ae_norm     1.00\n",
      "[91/200][899/1249] Loss_D: 0.00475836 (Loss_D_real: 0.00133157 Loss_D_fake: 0.00342679) Loss_G: 0.35548249 Loss_Enh_Dec: -0.92969763\n",
      "| epoch  91 |   900/ 1249 batches | lr 0.000000 | ms/batch 357.60 | loss  2.65 | ppl    14.22 | acc     0.65 | train_ae_norm     1.00\n",
      "[91/200][999/1249] Loss_D: 0.00450290 (Loss_D_real: 0.00235683 Loss_D_fake: 0.00214607) Loss_G: 0.36280316 Loss_Enh_Dec: -0.99751073\n",
      "| epoch  91 |  1000/ 1249 batches | lr 0.000000 | ms/batch 357.03 | loss  2.66 | ppl    14.26 | acc     0.68 | train_ae_norm     1.00\n",
      "[91/200][1099/1249] Loss_D: 0.00323229 (Loss_D_real: 0.00112095 Loss_D_fake: 0.00211134) Loss_G: 0.32970721 Loss_Enh_Dec: -0.87399626\n",
      "| epoch  91 |  1100/ 1249 batches | lr 0.000000 | ms/batch 357.42 | loss  2.73 | ppl    15.41 | acc     0.68 | train_ae_norm     1.00\n",
      "[91/200][1199/1249] Loss_D: 0.00420108 (Loss_D_real: 0.00170062 Loss_D_fake: 0.00250046) Loss_G: 0.31776044 Loss_Enh_Dec: -0.94495863\n",
      "| epoch  91 |  1200/ 1249 batches | lr 0.000000 | ms/batch 356.79 | loss  2.68 | ppl    14.63 | acc     0.70 | train_ae_norm     1.00\n",
      "| end of epoch  91 | time: 502.45s | test loss  2.31 | test ppl 10.12 | acc 0.759\n",
      "Train classification discriminator\n",
      "\n",
      "======== Epoch 92 / 200 ========\n",
      "Training...\n",
      "  Batch    10  of    130.    Elapsed: 0:00:04.\n",
      "  Batch    20  of    130.    Elapsed: 0:00:08.\n",
      "  Batch    30  of    130.    Elapsed: 0:00:11.\n",
      "  Batch    40  of    130.    Elapsed: 0:00:15.\n",
      "  Batch    50  of    130.    Elapsed: 0:00:19.\n",
      "  Batch    60  of    130.    Elapsed: 0:00:23.\n",
      "  Batch    70  of    130.    Elapsed: 0:00:27.\n",
      "  Batch    80  of    130.    Elapsed: 0:00:30.\n",
      "  Batch    90  of    130.    Elapsed: 0:00:34.\n",
      "  Batch   100  of    130.    Elapsed: 0:00:38.\n",
      "  Batch   110  of    130.    Elapsed: 0:00:42.\n",
      "  Batch   120  of    130.    Elapsed: 0:00:46.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.705\n",
      "  Training epcoh took: 0:00:49\n",
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.398\n",
      "  Test Loss: 5.116\n",
      "  Test took: 0:00:00\n",
      "Train other shit\n",
      "| epoch  92 |     0/ 1249 batches | lr 0.000000 | ms/batch 495.52 | loss  0.03 | ppl     1.03 | acc     0.68 | train_ae_norm     1.00\n",
      "[92/200][99/1249] Loss_D: 0.01051188 (Loss_D_real: 0.00464592 Loss_D_fake: 0.00586596) Loss_G: 0.31069943 Loss_Enh_Dec: -1.21649683\n",
      "| epoch  92 |   100/ 1249 batches | lr 0.000000 | ms/batch 356.52 | loss  2.72 | ppl    15.14 | acc     0.66 | train_ae_norm     1.00\n",
      "[92/200][199/1249] Loss_D: 0.01245600 (Loss_D_real: 0.00835485 Loss_D_fake: 0.00410115) Loss_G: 0.30851647 Loss_Enh_Dec: -0.91347355\n",
      "| epoch  92 |   200/ 1249 batches | lr 0.000000 | ms/batch 356.88 | loss  2.75 | ppl    15.64 | acc     0.70 | train_ae_norm     1.00\n",
      "[92/200][299/1249] Loss_D: 0.00693834 (Loss_D_real: 0.00214603 Loss_D_fake: 0.00479231) Loss_G: 0.30214769 Loss_Enh_Dec: -0.89671052\n",
      "| epoch  92 |   300/ 1249 batches | lr 0.000000 | ms/batch 357.21 | loss  2.67 | ppl    14.43 | acc     0.67 | train_ae_norm     1.00\n",
      "[92/200][399/1249] Loss_D: 0.01304985 (Loss_D_real: 0.00648295 Loss_D_fake: 0.00656690) Loss_G: 0.30207720 Loss_Enh_Dec: -1.05540264\n",
      "| epoch  92 |   400/ 1249 batches | lr 0.000000 | ms/batch 357.04 | loss  2.64 | ppl    14.01 | acc     0.71 | train_ae_norm     1.00\n",
      "[92/200][499/1249] Loss_D: 0.01214920 (Loss_D_real: 0.00715682 Loss_D_fake: 0.00499239) Loss_G: 0.28716394 Loss_Enh_Dec: -0.84820855\n",
      "| epoch  92 |   500/ 1249 batches | lr 0.000000 | ms/batch 357.28 | loss  2.64 | ppl    14.05 | acc     0.69 | train_ae_norm     1.00\n",
      "[92/200][599/1249] Loss_D: 0.00645379 (Loss_D_real: 0.00312371 Loss_D_fake: 0.00333008) Loss_G: 0.33517393 Loss_Enh_Dec: -0.98066902\n",
      "| epoch  92 |   600/ 1249 batches | lr 0.000000 | ms/batch 357.37 | loss  2.63 | ppl    13.91 | acc     0.71 | train_ae_norm     1.00\n",
      "[92/200][699/1249] Loss_D: 0.00957012 (Loss_D_real: 0.00636113 Loss_D_fake: 0.00320899) Loss_G: 0.33052722 Loss_Enh_Dec: -0.98650497\n",
      "| epoch  92 |   700/ 1249 batches | lr 0.000000 | ms/batch 357.54 | loss  2.60 | ppl    13.51 | acc     0.69 | train_ae_norm     1.00\n",
      "[92/200][799/1249] Loss_D: 0.00919832 (Loss_D_real: 0.00648634 Loss_D_fake: 0.00271198) Loss_G: 0.31279731 Loss_Enh_Dec: -0.83968890\n",
      "| epoch  92 |   800/ 1249 batches | lr 0.000000 | ms/batch 357.26 | loss  2.64 | ppl    13.97 | acc     0.68 | train_ae_norm     1.00\n",
      "[92/200][899/1249] Loss_D: 0.00571768 (Loss_D_real: 0.00328353 Loss_D_fake: 0.00243415) Loss_G: 0.34578884 Loss_Enh_Dec: -1.09416735\n",
      "| epoch  92 |   900/ 1249 batches | lr 0.000000 | ms/batch 357.91 | loss  2.58 | ppl    13.16 | acc     0.69 | train_ae_norm     1.00\n",
      "[92/200][999/1249] Loss_D: 0.00668325 (Loss_D_real: 0.00330670 Loss_D_fake: 0.00337655) Loss_G: 0.32503125 Loss_Enh_Dec: -1.06535184\n",
      "| epoch  92 |  1000/ 1249 batches | lr 0.000000 | ms/batch 356.92 | loss  2.60 | ppl    13.42 | acc     0.69 | train_ae_norm     1.00\n",
      "[92/200][1099/1249] Loss_D: 0.00505202 (Loss_D_real: 0.00114434 Loss_D_fake: 0.00390768) Loss_G: 0.33963910 Loss_Enh_Dec: -1.29536366\n",
      "| epoch  92 |  1100/ 1249 batches | lr 0.000000 | ms/batch 356.41 | loss  2.63 | ppl    13.93 | acc     0.67 | train_ae_norm     1.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-9111bc907503>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    280\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0menhance_dec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniters_gan_dec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m                         \u001b[0merrG_enh_dec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gan_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                     \u001b[0merrG_enh_dec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-836e5a3c78a8>\u001b[0m in \u001b[0;36mtrain_gan_dec\u001b[0;34m(gan_type)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# 1. decoder  - soft distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0menhance_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_indices\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_enh_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# 2. soft distribution - > encoder  -> fake_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0menhance_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menhance_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/practice/gpu-files/TILGAN/unconditional_generation/models.py\u001b[0m in \u001b[0;36mgenerate_enh_dec\u001b[0;34m(self, hidden, maxlen, sample, temp)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;31m#     indices = torch.multinomial(probs, 1) #[64,1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [64,3455]    [batch_size, num_tokens]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [64,1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [1, 64,3455]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d879385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd84efb2da0>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accuracy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "91448c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.455\n"
     ]
    }
   ],
   "source": [
    "print(max(accuracy_array))\n",
    "#0.4025 previous best accuracy (bert-base-cased, epoch 50, 5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d6680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_array[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdef8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_save = pd.DataFrame(accuracy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e624b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_save.to_csv('accuracy_array_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa94da89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
