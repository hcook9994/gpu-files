{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "673b1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils import to_gpu, Corpus, batchify, train_ngram_lm, get_ppl, create_exp_dir\n",
    "from models import Seq2Seq, MLP_D, MLP_D_local, MLP_G\n",
    "from bleu_self import *\n",
    "from bleu_test import *\n",
    "import datetime\n",
    "now_time = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f208b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='TILGAN for unconditional generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "520324a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "228c8e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path=\"data/MS_COCO\"\n",
    "data_path=\"../../yelp/unlabelled\"\n",
    "save=\"./results/yelp_results\"\n",
    "maxlen=64\n",
    "batch_size=24\n",
    "eval_batch_size = 32\n",
    "noise_seq_length = 15\n",
    "add_noise=True #what does this do? - question applies to most parameters\n",
    "emsize=512\n",
    "nhidden=512\n",
    "nlayers=2\n",
    "nheads=4\n",
    "nff=1024\n",
    "aehidden=56\n",
    "noise_r=0.05\n",
    "hidden_init=True\n",
    "dropout=0.3\n",
    "gpu=True\n",
    "z_size=100\n",
    "arch_g='300-300'\n",
    "gan_g_activation=True\n",
    "arch_d='300-300'\n",
    "gan_d_local=True\n",
    "gan_d_local_windowsize=3\n",
    "arch_d_local='300-300'\n",
    "lr_ae=0.12\n",
    "lr_gan_e=1e-04\n",
    "beta1=0.5\n",
    "lr_gan_g=4e-04\n",
    "lr_gan_d=1e-04\n",
    "epochs=1\n",
    "sample=True\n",
    "clip=1\n",
    "log_interval=100\n",
    "gan_lambda=0.1\n",
    "niters_gan_d=1\n",
    "niters_gan_g=1\n",
    "niters_gan_ae=1\n",
    "niters_gan_dec=1\n",
    "niters_gan_schedule=''\n",
    "niters_ae=1\n",
    "gan_type='kl'\n",
    "enhance_dec=True\n",
    "gan_gp_lambda=1\n",
    "vocab_size=0\n",
    "lowercase=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24e5f2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original vocab 111333; pruned to 111337\n",
      "Number of sentences dropped from ../../yelp/unlabelled/train.txt: 8 out of 114051 total\n",
      "Number of sentences dropped from ../../yelp/unlabelled/test.txt: 0 out of 9073 total\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(data_path,\n",
    "                maxlen=maxlen,\n",
    "                vocab_size=vocab_size,\n",
    "                lowercase=lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daa4ee98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 111337\n"
     ]
    }
   ],
   "source": [
    "# save arguments\n",
    "ntokens = len(corpus.dictionary.word2idx)\n",
    "print(\"Vocabulary Size: {}\".format(ntokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f66f2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : ./results/yelp_results\n"
     ]
    }
   ],
   "source": [
    "# exp dir\n",
    "create_exp_dir(os.path.join(save), ['train.py', 'models.py', 'utils.py'],\n",
    "        dict=corpus.dictionary.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a05112cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging(str, to_stdout=True):\n",
    "    with open(os.path.join(save, 'log.txt'), 'a') as f:\n",
    "        f.write(str + '\\n')\n",
    "    if to_stdout:\n",
    "        print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4042220b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data!\n"
     ]
    }
   ],
   "source": [
    "test_data = batchify(corpus.test, eval_batch_size, maxlen, shuffle=False)\n",
    "train_data = batchify(corpus.train, batch_size, maxlen,  shuffle=True)\n",
    "\n",
    "print(\"Loaded data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51cc7067",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Build the models\n",
    "###############################################################################\n",
    "autoencoder = Seq2Seq(add_noise=add_noise,\n",
    "                      emsize=emsize,\n",
    "                      nhidden=nhidden,\n",
    "                      ntokens=ntokens,\n",
    "                      nlayers=nlayers,\n",
    "                      nheads=nheads,\n",
    "                      nff=nff,\n",
    "                      aehidden=aehidden,\n",
    "                      noise_r=noise_r,\n",
    "                      hidden_init=hidden_init,\n",
    "                      dropout=dropout,\n",
    "                      gpu=True)\n",
    "nlatent = aehidden * (maxlen+1)\n",
    "gan_gen = MLP_G(ninput=z_size, noutput=nlatent, layers=arch_g, gan_g_activation=gan_g_activation)\n",
    "gan_disc = MLP_D(ninput=nlatent, noutput=1, layers=arch_d)\n",
    "gan_disc_local = MLP_D_local(ninput=gan_d_local_windowsize * aehidden, noutput=1, layers=arch_d_local)\n",
    "\n",
    "optimizer_ae = optim.SGD(autoencoder.parameters(), lr=lr_ae)\n",
    "\n",
    "\n",
    "optimizer_gan_e = optim.Adam(autoencoder.encoder.parameters(),\n",
    "                             lr=lr_gan_e,\n",
    "                             betas=(beta1, 0.999))\n",
    "optimizer_gan_g = optim.Adam(gan_gen.parameters(),\n",
    "                             lr=lr_gan_g,\n",
    "                             betas=(beta1, 0.999))\n",
    "optimizer_gan_d = optim.Adam(gan_disc.parameters(),\n",
    "                             lr=lr_gan_d,\n",
    "                             betas=(beta1, 0.999))\n",
    "optimizer_gan_d_local = optim.Adam(gan_disc_local.parameters(),\n",
    "                             lr=lr_gan_d,\n",
    "                             betas=(beta1, 0.999))\n",
    "optimizer_gan_dec = optim.Adam(autoencoder.decoder.parameters(),\n",
    "                             lr=lr_gan_e,\n",
    "                             betas=(beta1, 0.999))\n",
    "\n",
    "autoencoder = autoencoder.to(device)\n",
    "gan_gen = gan_gen.to(device)\n",
    "gan_disc = gan_disc.to(device)\n",
    "gan_disc_local = gan_disc_local.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42a2dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    print(\"Saving models to {}\".format(save))\n",
    "    torch.save({\n",
    "        \"ae\": autoencoder.state_dict(),\n",
    "        \"gan_g\": gan_gen.state_dict(),\n",
    "        \"gan_d\": gan_disc.state_dict(),\n",
    "        \"gan_d_local\": gan_disc_local.state_dict()\n",
    "\n",
    "        },\n",
    "        os.path.join(save, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b528fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        param_norm = p.grad.data.norm(2)\n",
    "        total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** (1. / 2)\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7a9fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    model_args = json.load(open(os.path.join(save, 'options.json'), 'r'))\n",
    "    word2idx = json.load(open(os.path.join(save, 'vocab.json'), 'r'))\n",
    "    idx2word = {v: k for k, v in word2idx.items()}\n",
    "\n",
    "    print('Loading models from {}'.format(save))\n",
    "    loaded = torch.load(os.path.join(save, \"model.pt\"))\n",
    "    autoencoder.load_state_dict(loaded.get('ae'))\n",
    "    gan_gen.load_state_dict(loaded.get('gan_g'))\n",
    "    gan_disc.load_state_dict(loaded.get('gan_d'))\n",
    "    gan_disc_local.load_state_dict(loaded.get('gan_d_local'))\n",
    "    return model_args, idx2word, autoencoder, gan_gen, gan_disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03edc3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_autoencoder(data_source, epoch):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    autoencoder.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary.word2idx)\n",
    "    all_accuracies = 0\n",
    "    bcnt = 0\n",
    "    for i, batch in enumerate(data_source):\n",
    "        source, target, lengths = batch\n",
    "        with torch.no_grad():\n",
    "            source = Variable(source.to(device))\n",
    "            target = Variable(target.to(device))\n",
    "            mask = target.gt(0)\n",
    "            masked_target = target.masked_select(mask)\n",
    "            # examples x ntokens\n",
    "            output_mask = mask.unsqueeze(1).expand(mask.size(0), ntokens)\n",
    "\n",
    "            # output: batch x seq_len x ntokens\n",
    "            output = autoencoder(source, lengths, source, add_noise=add_noise, soft=False)\n",
    "            flattened_output = output.view(-1, ntokens)\n",
    "\n",
    "            masked_output = \\\n",
    "                flattened_output.masked_select(output_mask).view(-1, ntokens)\n",
    "            total_loss += F.cross_entropy(masked_output, masked_target)\n",
    "\n",
    "            # accuracy\n",
    "            max_vals, max_indices = torch.max(masked_output, 1)\n",
    "            accuracy = torch.mean(max_indices.eq(masked_target).float()).data.item()\n",
    "            all_accuracies += accuracy\n",
    "            bcnt += 1\n",
    "\n",
    "        aeoutf = os.path.join(save, \"autoencoder.txt\")\n",
    "        with open(aeoutf, \"w\") as f:\n",
    "            max_values, max_indices = torch.max(output, 2)\n",
    "            max_indices = \\\n",
    "                max_indices.view(output.size(0), -1).data.cpu().numpy()\n",
    "            target = target.view(output.size(0), -1).data.cpu().numpy()\n",
    "            for t, idx in zip(target, max_indices):\n",
    "                # real sentence\n",
    "                chars = \" \".join([corpus.dictionary.idx2word[x] for x in t])\n",
    "                f.write(chars + '\\n')\n",
    "                # autoencoder output sentence\n",
    "                chars = \" \".join([corpus.dictionary.idx2word[x] for x in idx])\n",
    "                f.write(chars + '\\n'*2)\n",
    "\n",
    "    return total_loss.item() / len(data_source), all_accuracies/bcnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b838544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fixed_noise(noise, to_save):\n",
    "    gan_gen.eval()\n",
    "    autoencoder.eval()\n",
    "\n",
    "    fake_hidden = gan_gen(noise)\n",
    "    max_indices = autoencoder.generate(fake_hidden, maxlen, sample=sample)\n",
    "\n",
    "    with open(to_save, \"w\") as f:\n",
    "        max_indices = max_indices.data.cpu().numpy()\n",
    "        for idx in max_indices:\n",
    "            # generated sentence\n",
    "            words = [corpus.dictionary.idx2word[x] for x in idx]\n",
    "            # truncate sentences to first occurrence of <eos>\n",
    "            truncated_sent = []\n",
    "            for w in words:\n",
    "                if w != '<eos>':\n",
    "                    truncated_sent.append(w)\n",
    "                else:\n",
    "                    break\n",
    "            chars = \" \".join(truncated_sent)\n",
    "            f.write(chars + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58dced27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(gen_text_savepath):\n",
    "    selfbleu = bleu_self(gen_text_savepath)\n",
    "    real_text = os.path.join(data_path, \"test.txt\")\n",
    "    testbleu = bleu_test(real_text, gen_text_savepath)\n",
    "    return selfbleu, testbleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "124dcc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae(epoch, batch, total_loss_ae, start_time, i):\n",
    "    '''Train AE with the negative log-likelihood loss'''\n",
    "    autoencoder.train()\n",
    "    optimizer_ae.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = Variable(source.to(device))\n",
    "    target = Variable(target.to(device))\n",
    "    output = autoencoder(source, lengths, source, add_noise=add_noise, soft=False)\n",
    "\n",
    "    mask = target.gt(0)\n",
    "    masked_target = target.masked_select(mask)\n",
    "    output_mask = mask.unsqueeze(1).expand(mask.size(0), ntokens)\n",
    "    flat_output = output.view(-1, ntokens)\n",
    "    masked_output = flat_output.masked_select(output_mask).view(-1, ntokens)\n",
    "    loss = F.cross_entropy(masked_output, masked_target)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(autoencoder.parameters(), clip)\n",
    "    train_ae_norm = cal_norm(autoencoder)\n",
    "    optimizer_ae.step()\n",
    "\n",
    "    total_loss_ae += loss.data.item()\n",
    "    if i % log_interval == 0:\n",
    "        probs = F.softmax(masked_output, dim=-1)\n",
    "        max_vals, max_indices = torch.max(probs, 1)\n",
    "        accuracy = torch.mean(max_indices.eq(masked_target).float()).data.item()\n",
    "        cur_loss = total_loss_ae / log_interval\n",
    "        elapsed = time.time() - start_time\n",
    "        logging('| epoch {:3d} | {:5d}/{:5d} batches | lr {:08.6f} | ms/batch {:5.2f} | '\n",
    "                'loss {:5.2f} | ppl {:8.2f} | acc {:8.2f} | train_ae_norm {:8.2f}'.format(\n",
    "                epoch, i, len(train_data), 0,\n",
    "                elapsed * 1000 / log_interval,\n",
    "                cur_loss, math.exp(cur_loss), accuracy, train_ae_norm))\n",
    "\n",
    "        total_loss_ae = 0\n",
    "        start_time = time.time()\n",
    "    return total_loss_ae, start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64870e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_g(gan_type='kl'):\n",
    "    gan_gen.train()\n",
    "    optimizer_gan_g.zero_grad()\n",
    "\n",
    "    z = Variable(torch.Tensor(batch_size, z_size).normal_(0, 1).to(device))\n",
    "    fake_hidden = gan_gen(z)\n",
    "    fake_score = gan_disc(fake_hidden)\n",
    "\n",
    "    if gan_d_local:\n",
    "        idx = random.randint(0, maxlen - gan_d_local_windowsize)\n",
    "        fake_hidden_local = fake_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        fake_score_local = gan_disc_local(fake_hidden_local)\n",
    "\n",
    "        if gan_type == 'kl':\n",
    "            errG = -(torch.exp(fake_score.detach()).clamp(0.5, 2) * fake_score).mean() -(torch.exp(fake_score_local.detach()).clamp(0.5, 2) * fake_score_local).mean()\n",
    "        else:\n",
    "            errG = -fake_score.mean() -fake_score_local.mean()\n",
    "    else:\n",
    "        if gan_type == 'kl':\n",
    "            errG = -(torch.exp(fake_score.detach()).clamp(0.5, 2) * fake_score).mean()\n",
    "        else:\n",
    "            errG = -fake_score.mean()\n",
    "\n",
    "\n",
    "    errG *= gan_lambda\n",
    "    errG.backward()\n",
    "    optimizer_gan_g.step()\n",
    "\n",
    "    return errG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33244527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_dec(gan_type='kl'):\n",
    "    autoencoder.decoder.train()\n",
    "    optimizer_gan_dec.zero_grad()\n",
    "\n",
    "    z = Variable(torch.Tensor(batch_size, z_size).normal_(0, 1).to(device))\n",
    "    fake_hidden = gan_gen(z)\n",
    "\n",
    "    # 1. decoder  - soft distribution\n",
    "    enhance_source, max_indices= autoencoder.generate_enh_dec(fake_hidden, maxlen, sample=sample)\n",
    "    # 2. soft distribution - > encoder  -> fake_hidden\n",
    "    enhance_hidden = autoencoder(enhance_source, None, max_indices, add_noise=add_noise, soft=True, encode_only=True)\n",
    "    fake_score = gan_disc(enhance_hidden)\n",
    "\n",
    "    if gan_d_local:\n",
    "        idx = random.randint(0, maxlen - gan_d_local_windowsize)\n",
    "        fake_hidden_local = fake_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        fake_score_local = gan_disc_local(fake_hidden_local)\n",
    "\n",
    "        if gan_type == 'kl':\n",
    "            errG = -(torch.exp(fake_score.detach()).clamp(0.5, 2) * fake_score).mean() -(torch.exp(fake_score_local.detach()).clamp(0.5, 2) * fake_score_local).mean()\n",
    "        else:\n",
    "            errG = -fake_score.mean() -fake_score_local.mean()\n",
    "    else:\n",
    "        if gan_type == 'kl':\n",
    "            errG = -(torch.exp(fake_score.detach()).clamp(0.5, 2) * fake_score).mean()\n",
    "        else:\n",
    "            errG = -fake_score.mean()\n",
    "\n",
    "\n",
    "    errG *= gan_lambda\n",
    "    errG.backward()\n",
    "    optimizer_gan_dec.step()\n",
    "\n",
    "    return errG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce1e7869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_hook(grad):\n",
    "    return grad * gan_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5cf668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Steal from https://github.com/caogang/wgan-gp/blob/master/gan_cifar10.py '''\n",
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    bsz = real_data.size(0)\n",
    "    alpha = torch.rand(bsz, 1)\n",
    "    alpha = alpha.expand(bsz, real_data.size(1))  # only works for 2D XXX\n",
    "    alpha = alpha.to(device)\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                                    grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
    "                                    create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * gan_gp_lambda\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e65e6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_d(batch, gan_type='kl'):\n",
    "    gan_disc.train()\n",
    "    gan_disc_local.train()\n",
    "    optimizer_gan_d.zero_grad()\n",
    "    optimizer_gan_d_local.zero_grad()\n",
    "\n",
    "    # + samples\n",
    "    source, target, lengths = batch\n",
    "    source = Variable(source.to(device))\n",
    "    target = Variable(target.to(device))\n",
    "    real_hidden = autoencoder(source, lengths, source, add_noise=add_noise, soft=False, encode_only=True)\n",
    "    real_score = gan_disc(real_hidden.detach())\n",
    "\n",
    "    idx = random.randint(0, maxlen - gan_d_local_windowsize)\n",
    "    if gan_d_local:\n",
    "        real_hidden_local = real_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        real_score_local = gan_disc_local(real_hidden_local)\n",
    "        real_score += real_score_local\n",
    "\n",
    "\n",
    "    if gan_type == 'wgan':\n",
    "        errD_real = -real_score.mean()\n",
    "    else: # kl or all\n",
    "        errD_real = F.softplus(-real_score).mean()\n",
    "    errD_real.backward()\n",
    "\n",
    "    # - samples\n",
    "    z = Variable(torch.Tensor(batch_size, z_size).normal_(0, 1).to(device))\n",
    "    fake_hidden = gan_gen(z)\n",
    "    fake_score = gan_disc(fake_hidden.detach())\n",
    "\n",
    "    if gan_d_local:\n",
    "        fake_hidden_local = fake_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        fake_score_local = gan_disc_local(fake_hidden_local)\n",
    "        fake_score += fake_score_local\n",
    "\n",
    "    if gan_type == 'wgan':\n",
    "        errD_fake = fake_score.mean()\n",
    "    else:  # kl or all\n",
    "        errD_fake = F.softplus(fake_score).mean()\n",
    "    errD_fake.backward()\n",
    "\n",
    "    # gradient penalty\n",
    "    if gan_type == 'wgan':\n",
    "        gradient_penalty = calc_gradient_penalty(gan_disc, real_hidden.data, fake_hidden.data)\n",
    "        gradient_penalty.backward()\n",
    "\n",
    "    optimizer_gan_d.step()\n",
    "    optimizer_gan_d_local.step()\n",
    "    return errD_real + errD_fake, errD_real, errD_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f12ead25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_d_into_ae(batch):\n",
    "    autoencoder.train()\n",
    "    optimizer_gan_e.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = Variable(source.to(device))\n",
    "    target = Variable(target.to(device))\n",
    "    real_hidden = autoencoder(source, lengths, source, add_noise=add_noise, soft=False, encode_only=True)\n",
    "\n",
    "    if gan_d_local:\n",
    "        idx = random.randint(0, maxlen - gan_d_local_windowsize)\n",
    "        real_hidden_local = real_hidden[:, idx * aehidden : (idx + gan_d_local_windowsize) * aehidden]\n",
    "        real_score_local = gan_disc_local(real_hidden_local)\n",
    "        errD_real = gan_disc(real_hidden).mean() + real_score_local.mean()\n",
    "    else:\n",
    "        errD_real = gan_disc(real_hidden).mean()\n",
    "\n",
    "    errD_real *= gan_lambda\n",
    "    errD_real.backward()\n",
    "    torch.nn.utils.clip_grad_norm(autoencoder.parameters(), clip)\n",
    "\n",
    "    optimizer_gan_e.step()\n",
    "    return errD_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44e7895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    logging(\"Training\")\n",
    "    train_data = batchify(corpus.train, batch_size, maxlen, shuffle=True)\n",
    "\n",
    "    # gan: preparation\n",
    "    if niters_gan_schedule != \"\":\n",
    "        gan_schedule = [int(x) for x in niters_gan_schedule.split(\"-\")]\n",
    "    else:\n",
    "        gan_schedule = []\n",
    "    niter_gan = 1\n",
    "    fixed_noise = Variable(torch.ones(eval_batch_size, z_size).normal_(0, 1).to(device))\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # update gan training schedule\n",
    "        if epoch in gan_schedule:\n",
    "            niter_gan += 1\n",
    "            logging(\"GAN training loop schedule: {}\".format(niter_gan))\n",
    "\n",
    "        total_loss_ae = 0\n",
    "        epoch_start_time = time.time()\n",
    "        start_time = time.time()\n",
    "        niter = 0\n",
    "        niter_g = 1\n",
    "\n",
    "        while niter < len(train_data):\n",
    "            # train ae\n",
    "            for i in range(niters_ae):\n",
    "                if niter >= len(train_data):\n",
    "                    break  # end of epoch\n",
    "                total_loss_ae, start_time = train_ae(epoch, train_data[niter],\n",
    "                                total_loss_ae, start_time, niter)\n",
    "                niter += 1\n",
    "            # train gan\n",
    "            for k in range(niter_gan):\n",
    "                for i in range(niters_gan_d):\n",
    "                    errD, errD_real, errD_fake = train_gan_d(\n",
    "                            train_data[random.randint(0, len(train_data)-1)], gan_type)\n",
    "                for i in range(niters_gan_ae):\n",
    "                    train_gan_d_into_ae(train_data[random.randint(0, len(train_data)-1)])\n",
    "                for i in range(niters_gan_g):\n",
    "                    errG = train_gan_g(gan_type)\n",
    "                if enhance_dec:\n",
    "                    for i in range(niters_gan_dec):\n",
    "                        errG_enh_dec = train_gan_dec()\n",
    "                else:\n",
    "                    errG_enh_dec = torch.Tensor([0])\n",
    "\n",
    "            niter_g += 1\n",
    "            if niter_g % log_interval == 0:\n",
    "                logging('[{}/{}][{}/{}] Loss_D: {:.8f} (Loss_D_real: {:.8f} '\n",
    "                        'Loss_D_fake: {:.8f}) Loss_G: {:.8f} Loss_Enh_Dec: {:.8f}'.format(\n",
    "                         epoch, epochs, niter, len(train_data),\n",
    "                         errD.data.item(), errD_real.data.item(),\n",
    "                         errD_fake.data.item(), errG.data.item(), errG_enh_dec.data.item()))\n",
    "        # eval\n",
    "        test_loss, accuracy = evaluate_autoencoder(test_data, epoch)\n",
    "        logging('| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} | '\n",
    "                'test ppl {:5.2f} | acc {:3.3f}'.format(epoch,\n",
    "                (time.time() - epoch_start_time), test_loss,\n",
    "                math.exp(test_loss), accuracy))\n",
    "\n",
    "        gen_text_savepath = os.path.join(save, \"{:03d}_examplar_gen\".format(epoch))\n",
    "        gen_fixed_noise(fixed_noise, gen_text_savepath)\n",
    "        if epoch % 5 == 0 or epoch % 4 == 0 or (epochs - epoch) <=2:\n",
    "            selfbleu, testbleu = eval_bleu(gen_text_savepath)\n",
    "            logging('bleu_self: [{:.8f},{:.8f},{:.8f},{:.8f},{:.8f}]'.format(selfbleu[0], selfbleu[1], selfbleu[2], selfbleu[3], selfbleu[4]))\n",
    "            logging('bleu_test: [{:.8f},{:.8f},{:.8f},{:.8f},{:.8f}]'.format(testbleu[0], testbleu[1], testbleu[2], testbleu[3], testbleu[4]))\n",
    "\n",
    "        if epoch % 15 == 0 or epoch == epochs-1:\n",
    "            logging(\"New saving model: epoch {:03d}.\".format(epoch))\n",
    "            save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a24898f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harry/venv/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/harry/venv/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |     0/ 4751 batches | lr 0.000000 | ms/batch  1.74 | loss  0.13 | ppl     1.13 | acc     0.00 | train_ae_norm     1.00\n",
      "[1/1][99/4751] Loss_D: 1.37549269 (Loss_D_real: 0.68372476 Loss_D_fake: 0.69176793) Loss_G: 0.00010793 Loss_Enh_Dec: 0.00049265\n",
      "| epoch   1 |   100/ 4751 batches | lr 0.000000 | ms/batch 1290.56 | loss  9.23 | ppl 10221.87 | acc     0.05 | train_ae_norm     1.00\n",
      "[1/1][199/4751] Loss_D: 1.38061059 (Loss_D_real: 0.68789238 Loss_D_fake: 0.69271821) Loss_G: 0.00169142 Loss_Enh_Dec: -0.00014457\n",
      "| epoch   1 |   200/ 4751 batches | lr 0.000000 | ms/batch 1288.82 | loss  8.09 | ppl  3257.50 | acc     0.04 | train_ae_norm     1.00\n",
      "[1/1][299/4751] Loss_D: 1.37406278 (Loss_D_real: 0.68637270 Loss_D_fake: 0.68769002) Loss_G: -0.00069796 Loss_Enh_Dec: -0.00060447\n",
      "| epoch   1 |   300/ 4751 batches | lr 0.000000 | ms/batch 1287.06 | loss  7.69 | ppl  2182.39 | acc     0.07 | train_ae_norm     1.00\n",
      "[1/1][399/4751] Loss_D: 1.37255669 (Loss_D_real: 0.68537891 Loss_D_fake: 0.68717784) Loss_G: 0.00133899 Loss_Enh_Dec: -0.00153466\n",
      "| epoch   1 |   400/ 4751 batches | lr 0.000000 | ms/batch 1287.28 | loss  7.52 | ppl  1841.00 | acc     0.08 | train_ae_norm     1.00\n",
      "[1/1][599/4751] Loss_D: 1.35935354 (Loss_D_real: 0.68006194 Loss_D_fake: 0.67929161) Loss_G: 0.00363994 Loss_Enh_Dec: -0.00220606\n",
      "| epoch   1 |   600/ 4751 batches | lr 0.000000 | ms/batch 1288.40 | loss  7.30 | ppl  1481.77 | acc     0.11 | train_ae_norm     1.00\n",
      "[1/1][699/4751] Loss_D: 1.36658359 (Loss_D_real: 0.67653340 Loss_D_fake: 0.69005024) Loss_G: 0.00126179 Loss_Enh_Dec: -0.00294310\n",
      "| epoch   1 |   700/ 4751 batches | lr 0.000000 | ms/batch 1283.86 | loss  7.22 | ppl  1372.01 | acc     0.08 | train_ae_norm     1.00\n",
      "[1/1][799/4751] Loss_D: 1.34555507 (Loss_D_real: 0.66442925 Loss_D_fake: 0.68112582) Loss_G: 0.00185784 Loss_Enh_Dec: -0.00403259\n",
      "| epoch   1 |   800/ 4751 batches | lr 0.000000 | ms/batch 1285.48 | loss  7.17 | ppl  1294.73 | acc     0.09 | train_ae_norm     1.00\n",
      "[1/1][899/4751] Loss_D: 1.37218785 (Loss_D_real: 0.68814158 Loss_D_fake: 0.68404627) Loss_G: 0.00053446 Loss_Enh_Dec: -0.00157480\n",
      "| epoch   1 |   900/ 4751 batches | lr 0.000000 | ms/batch 1284.13 | loss  7.11 | ppl  1227.88 | acc     0.10 | train_ae_norm     1.00\n",
      "[1/1][999/4751] Loss_D: 1.37072289 (Loss_D_real: 0.68265486 Loss_D_fake: 0.68806803) Loss_G: -0.00062578 Loss_Enh_Dec: -0.00129138\n",
      "| epoch   1 |  1000/ 4751 batches | lr 0.000000 | ms/batch 1284.29 | loss  7.05 | ppl  1147.98 | acc     0.10 | train_ae_norm     1.00\n",
      "[1/1][1099/4751] Loss_D: 1.36944437 (Loss_D_real: 0.68468398 Loss_D_fake: 0.68476033) Loss_G: -0.00078523 Loss_Enh_Dec: -0.00254207\n",
      "| epoch   1 |  1100/ 4751 batches | lr 0.000000 | ms/batch 1285.71 | loss  6.95 | ppl  1045.07 | acc     0.12 | train_ae_norm     1.00\n",
      "[1/1][1199/4751] Loss_D: 1.36786604 (Loss_D_real: 0.68139160 Loss_D_fake: 0.68647450) Loss_G: -0.00213797 Loss_Enh_Dec: -0.00395227\n",
      "| epoch   1 |  1200/ 4751 batches | lr 0.000000 | ms/batch 1283.02 | loss  6.84 | ppl   935.56 | acc     0.11 | train_ae_norm     1.00\n",
      "[1/1][1299/4751] Loss_D: 1.36330736 (Loss_D_real: 0.67811346 Loss_D_fake: 0.68519390) Loss_G: -0.00295899 Loss_Enh_Dec: -0.00652255\n",
      "| epoch   1 |  1300/ 4751 batches | lr 0.000000 | ms/batch 1280.01 | loss  6.80 | ppl   897.20 | acc     0.13 | train_ae_norm     1.00\n",
      "[1/1][1399/4751] Loss_D: 1.36030257 (Loss_D_real: 0.67756051 Loss_D_fake: 0.68274206) Loss_G: -0.00249555 Loss_Enh_Dec: -0.00743759\n",
      "| epoch   1 |  1400/ 4751 batches | lr 0.000000 | ms/batch 1281.66 | loss  6.72 | ppl   831.27 | acc     0.13 | train_ae_norm     1.00\n",
      "[1/1][1499/4751] Loss_D: 1.35180259 (Loss_D_real: 0.67708826 Loss_D_fake: 0.67471427) Loss_G: -0.00444293 Loss_Enh_Dec: -0.00877601\n",
      "| epoch   1 |  1500/ 4751 batches | lr 0.000000 | ms/batch 1282.20 | loss  6.76 | ppl   860.92 | acc     0.11 | train_ae_norm     1.00\n",
      "[1/1][1599/4751] Loss_D: 1.34837556 (Loss_D_real: 0.67652458 Loss_D_fake: 0.67185104) Loss_G: -0.00662392 Loss_Enh_Dec: -0.00879852\n",
      "| epoch   1 |  1600/ 4751 batches | lr 0.000000 | ms/batch 1282.66 | loss  6.80 | ppl   896.97 | acc     0.10 | train_ae_norm     1.00\n",
      "[1/1][1699/4751] Loss_D: 1.33798993 (Loss_D_real: 0.66922444 Loss_D_fake: 0.66876549) Loss_G: -0.00724987 Loss_Enh_Dec: -0.01032258\n",
      "| epoch   1 |  1700/ 4751 batches | lr 0.000000 | ms/batch 1279.81 | loss  6.69 | ppl   802.42 | acc     0.14 | train_ae_norm     1.00\n",
      "[1/1][1799/4751] Loss_D: 1.33906353 (Loss_D_real: 0.66839302 Loss_D_fake: 0.67067051) Loss_G: -0.00403953 Loss_Enh_Dec: -0.01040850\n",
      "| epoch   1 |  1800/ 4751 batches | lr 0.000000 | ms/batch 1277.97 | loss  6.67 | ppl   786.09 | acc     0.12 | train_ae_norm     1.00\n",
      "[1/1][1899/4751] Loss_D: 1.33515120 (Loss_D_real: 0.65983307 Loss_D_fake: 0.67531818) Loss_G: -0.00751385 Loss_Enh_Dec: -0.01258624\n",
      "| epoch   1 |  1900/ 4751 batches | lr 0.000000 | ms/batch 1282.85 | loss  6.62 | ppl   746.56 | acc     0.12 | train_ae_norm     1.00\n",
      "[1/1][1999/4751] Loss_D: 1.31025600 (Loss_D_real: 0.65321422 Loss_D_fake: 0.65704173) Loss_G: -0.00603435 Loss_Enh_Dec: -0.01060125\n",
      "| epoch   1 |  2000/ 4751 batches | lr 0.000000 | ms/batch 1280.01 | loss  6.66 | ppl   776.71 | acc     0.10 | train_ae_norm     1.00\n",
      "[1/1][2099/4751] Loss_D: 1.30749381 (Loss_D_real: 0.64782071 Loss_D_fake: 0.65967309) Loss_G: -0.00660729 Loss_Enh_Dec: -0.01609138\n",
      "| epoch   1 |  2100/ 4751 batches | lr 0.000000 | ms/batch 1279.57 | loss  6.59 | ppl   724.42 | acc     0.14 | train_ae_norm     1.00\n",
      "[1/1][2199/4751] Loss_D: 1.28642988 (Loss_D_real: 0.63897246 Loss_D_fake: 0.64745736) Loss_G: -0.00319447 Loss_Enh_Dec: -0.01232423\n",
      "| epoch   1 |  2200/ 4751 batches | lr 0.000000 | ms/batch 1288.64 | loss  6.58 | ppl   722.10 | acc     0.11 | train_ae_norm     1.00\n",
      "[1/1][2299/4751] Loss_D: 1.26451683 (Loss_D_real: 0.63527745 Loss_D_fake: 0.62923932) Loss_G: -0.00865203 Loss_Enh_Dec: -0.01133416\n",
      "| epoch   1 |  2300/ 4751 batches | lr 0.000000 | ms/batch 1285.51 | loss  6.66 | ppl   783.77 | acc     0.12 | train_ae_norm     1.00\n",
      "[1/1][2399/4751] Loss_D: 1.25096393 (Loss_D_real: 0.62288159 Loss_D_fake: 0.62808239) Loss_G: 0.00308199 Loss_Enh_Dec: -0.01845867\n",
      "| epoch   1 |  2400/ 4751 batches | lr 0.000000 | ms/batch 1287.41 | loss  6.55 | ppl   698.57 | acc     0.12 | train_ae_norm     1.00\n",
      "[1/1][2499/4751] Loss_D: 1.19615483 (Loss_D_real: 0.59439957 Loss_D_fake: 0.60175532) Loss_G: -0.00324608 Loss_Enh_Dec: -0.03162422\n",
      "| epoch   1 |  2500/ 4751 batches | lr 0.000000 | ms/batch 1286.79 | loss  6.65 | ppl   771.91 | acc     0.10 | train_ae_norm     1.00\n",
      "[1/1][2599/4751] Loss_D: 1.19505501 (Loss_D_real: 0.60044461 Loss_D_fake: 0.59461045) Loss_G: 0.00971537 Loss_Enh_Dec: -0.01885273\n",
      "| epoch   1 |  2600/ 4751 batches | lr 0.000000 | ms/batch 1282.90 | loss  6.59 | ppl   725.64 | acc     0.14 | train_ae_norm     1.00\n",
      "[1/1][2699/4751] Loss_D: 1.18082213 (Loss_D_real: 0.58678931 Loss_D_fake: 0.59403276) Loss_G: 0.00918706 Loss_Enh_Dec: -0.01636349\n",
      "| epoch   1 |  2700/ 4751 batches | lr 0.000000 | ms/batch 1284.14 | loss  6.51 | ppl   672.52 | acc     0.13 | train_ae_norm     1.00\n",
      "[1/1][2799/4751] Loss_D: 1.15850806 (Loss_D_real: 0.56714928 Loss_D_fake: 0.59135878) Loss_G: 0.01364725 Loss_Enh_Dec: -0.02737803\n",
      "| epoch   1 |  2800/ 4751 batches | lr 0.000000 | ms/batch 1282.40 | loss  6.53 | ppl   688.17 | acc     0.12 | train_ae_norm     1.00\n",
      "[1/1][2899/4751] Loss_D: 1.12418222 (Loss_D_real: 0.56115758 Loss_D_fake: 0.56302470) Loss_G: 0.01209320 Loss_Enh_Dec: -0.02200714\n",
      "| epoch   1 |  2900/ 4751 batches | lr 0.000000 | ms/batch 1284.11 | loss  6.60 | ppl   734.99 | acc     0.13 | train_ae_norm     1.00\n",
      "[1/1][2999/4751] Loss_D: 1.04340148 (Loss_D_real: 0.54183948 Loss_D_fake: 0.50156194) Loss_G: 0.01626789 Loss_Enh_Dec: -0.02040795\n",
      "| epoch   1 |  3000/ 4751 batches | lr 0.000000 | ms/batch 1284.28 | loss  6.53 | ppl   682.65 | acc     0.15 | train_ae_norm     1.00\n",
      "[1/1][3099/4751] Loss_D: 1.06100702 (Loss_D_real: 0.51882869 Loss_D_fake: 0.54217839) Loss_G: 0.02366652 Loss_Enh_Dec: -0.02303438\n",
      "| epoch   1 |  3100/ 4751 batches | lr 0.000000 | ms/batch 1283.71 | loss  6.46 | ppl   642.21 | acc     0.12 | train_ae_norm     1.00\n",
      "[1/1][3199/4751] Loss_D: 0.90993148 (Loss_D_real: 0.50232565 Loss_D_fake: 0.40760583) Loss_G: 0.00730386 Loss_Enh_Dec: -0.04469339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  3200/ 4751 batches | lr 0.000000 | ms/batch 1282.82 | loss  6.43 | ppl   620.19 | acc     0.15 | train_ae_norm     1.00\n",
      "[1/1][3299/4751] Loss_D: 0.98846215 (Loss_D_real: 0.47712666 Loss_D_fake: 0.51133549) Loss_G: 0.02205069 Loss_Enh_Dec: -0.06222705\n",
      "| epoch   1 |  3300/ 4751 batches | lr 0.000000 | ms/batch 1282.91 | loss  6.40 | ppl   600.18 | acc     0.12 | train_ae_norm     1.00\n",
      "[1/1][3399/4751] Loss_D: 0.92951918 (Loss_D_real: 0.43925422 Loss_D_fake: 0.49026498) Loss_G: -0.00152913 Loss_Enh_Dec: -0.06839810\n",
      "| epoch   1 |  3400/ 4751 batches | lr 0.000000 | ms/batch 1284.40 | loss  6.38 | ppl   592.78 | acc     0.14 | train_ae_norm     1.00\n",
      "[1/1][3499/4751] Loss_D: 0.87691104 (Loss_D_real: 0.39867100 Loss_D_fake: 0.47824001) Loss_G: 0.04229193 Loss_Enh_Dec: -0.05985690\n",
      "| epoch   1 |  3500/ 4751 batches | lr 0.000000 | ms/batch 1283.83 | loss  6.41 | ppl   607.95 | acc     0.11 | train_ae_norm     1.00\n",
      "[1/1][3599/4751] Loss_D: 0.88113302 (Loss_D_real: 0.40208185 Loss_D_fake: 0.47905117) Loss_G: 0.03893932 Loss_Enh_Dec: -0.09634892\n",
      "| epoch   1 |  3600/ 4751 batches | lr 0.000000 | ms/batch 1287.55 | loss  6.36 | ppl   577.08 | acc     0.14 | train_ae_norm     1.00\n",
      "[1/1][3699/4751] Loss_D: 0.74684370 (Loss_D_real: 0.40969595 Loss_D_fake: 0.33714771) Loss_G: -0.02294903 Loss_Enh_Dec: -0.06619946\n",
      "| epoch   1 |  3700/ 4751 batches | lr 0.000000 | ms/batch 1301.70 | loss  6.36 | ppl   578.20 | acc     0.14 | train_ae_norm     1.00\n",
      "[1/1][3799/4751] Loss_D: 1.02281213 (Loss_D_real: 0.41524088 Loss_D_fake: 0.60757130) Loss_G: 0.01273934 Loss_Enh_Dec: -0.18327393\n",
      "| epoch   1 |  3800/ 4751 batches | lr 0.000000 | ms/batch 1290.09 | loss  6.33 | ppl   561.92 | acc     0.13 | train_ae_norm     1.00\n",
      "[1/1][3899/4751] Loss_D: 0.91354865 (Loss_D_real: 0.41183883 Loss_D_fake: 0.50170982) Loss_G: 0.03938962 Loss_Enh_Dec: -0.09184090\n",
      "| epoch   1 |  3900/ 4751 batches | lr 0.000000 | ms/batch 1289.95 | loss  6.30 | ppl   543.80 | acc     0.11 | train_ae_norm     1.00\n",
      "[1/1][3999/4751] Loss_D: 0.76796663 (Loss_D_real: 0.42303878 Loss_D_fake: 0.34492788) Loss_G: -0.04420717 Loss_Enh_Dec: -0.03466356\n",
      "| epoch   1 |  4000/ 4751 batches | lr 0.000000 | ms/batch 1289.92 | loss  6.29 | ppl   540.73 | acc     0.17 | train_ae_norm     1.00\n",
      "[1/1][4099/4751] Loss_D: 0.97236860 (Loss_D_real: 0.40890154 Loss_D_fake: 0.56346709) Loss_G: 0.04820989 Loss_Enh_Dec: 0.00117982\n",
      "| epoch   1 |  4100/ 4751 batches | lr 0.000000 | ms/batch 1290.25 | loss  6.29 | ppl   540.58 | acc     0.14 | train_ae_norm     1.00\n",
      "[1/1][4199/4751] Loss_D: 0.77644229 (Loss_D_real: 0.35027319 Loss_D_fake: 0.42616910) Loss_G: 0.02058600 Loss_Enh_Dec: 0.00075011\n",
      "| epoch   1 |  4200/ 4751 batches | lr 0.000000 | ms/batch 1290.34 | loss  6.27 | ppl   530.33 | acc     0.14 | train_ae_norm     1.00\n",
      "[1/1][4299/4751] Loss_D: 0.96512091 (Loss_D_real: 0.52254802 Loss_D_fake: 0.44257289) Loss_G: 0.06809954 Loss_Enh_Dec: -0.03065404\n",
      "| epoch   1 |  4300/ 4751 batches | lr 0.000000 | ms/batch 1289.44 | loss  6.24 | ppl   511.40 | acc     0.14 | train_ae_norm     1.00\n",
      "[1/1][4399/4751] Loss_D: 0.85202265 (Loss_D_real: 0.33893561 Loss_D_fake: 0.51308703) Loss_G: 0.07086179 Loss_Enh_Dec: -0.17207293\n",
      "| epoch   1 |  4400/ 4751 batches | lr 0.000000 | ms/batch 1292.44 | loss  6.25 | ppl   516.39 | acc     0.15 | train_ae_norm     1.00\n",
      "[1/1][4499/4751] Loss_D: 0.68887126 (Loss_D_real: 0.35379085 Loss_D_fake: 0.33508039) Loss_G: -0.02895386 Loss_Enh_Dec: -0.02971959\n",
      "| epoch   1 |  4500/ 4751 batches | lr 0.000000 | ms/batch 1293.45 | loss  6.20 | ppl   494.61 | acc     0.14 | train_ae_norm     1.00\n",
      "[1/1][4599/4751] Loss_D: 0.53948587 (Loss_D_real: 0.31649289 Loss_D_fake: 0.22299300) Loss_G: 0.02577935 Loss_Enh_Dec: -0.14535534\n",
      "| epoch   1 |  4600/ 4751 batches | lr 0.000000 | ms/batch 1294.13 | loss  6.23 | ppl   508.88 | acc     0.15 | train_ae_norm     1.00\n",
      "[1/1][4699/4751] Loss_D: 0.76372695 (Loss_D_real: 0.32339120 Loss_D_fake: 0.44033575) Loss_G: 0.04036368 Loss_Enh_Dec: -0.20508993\n",
      "| epoch   1 |  4700/ 4751 batches | lr 0.000000 | ms/batch 1293.59 | loss  6.20 | ppl   492.20 | acc     0.15 | train_ae_norm     1.00\n",
      "| end of epoch   1 | time: 6133.84s | test loss  6.14 | test ppl 464.92 | acc 0.154\n",
      "bleu_self:  [5.93777906e-01 2.59267402e-01 4.07993558e-02 5.08063444e-06\n",
      " 2.34960905e-08]\n",
      "bleu_test:  [8.72335654e-01 6.69606943e-01 3.30275072e-01 7.91199508e-02\n",
      " 5.47004192e-05]\n",
      "bleu_self: [0.59377791,0.25926740,0.04079936,0.00000508,0.00000002]\n",
      "bleu_test: [0.87233565,0.66960694,0.33027507,0.07911995,0.00005470]\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4af6d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fixed_noise_new(noise, length, maxlength):\n",
    "    gan_gen.eval()\n",
    "    autoencoder.eval()\n",
    "\n",
    "    fake_hidden = gan_gen(noise)\n",
    "    max_indices = autoencoder.generate(fake_hidden, maxlength, sample=sample)\n",
    "    \n",
    "    sent_list = []\n",
    "    \n",
    "    #with open(to_save, \"w\") as f:\n",
    "    max_indices = max_indices.data.cpu().numpy()\n",
    "    print(len(max_indices))\n",
    "    print(len(max_indices[0]))\n",
    "    print(max_indices)\n",
    "    for idx in max_indices:\n",
    "        # generated sentence\n",
    "        words = [corpus.dictionary.idx2word[x] for x in idx]\n",
    "        # truncate sentences to first occurrence of <eos>\n",
    "        truncated_sent = []\n",
    "        for w in words:\n",
    "            if w != '<eos>':\n",
    "                truncated_sent.append(w)\n",
    "            else:\n",
    "                break\n",
    "        chars = \" \".join(truncated_sent)\n",
    "        #f.write(chars + '\\n')\n",
    "        sent_list.append(chars)\n",
    "    print(sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6be4b71c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './results/yelp_results/options.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-3cb1167e0b9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#model_args, idx2word, autoencoder, gan_gen, gan_disc,gan_disc_local = load_models()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan_disc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mautoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgan_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgan_disc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan_disc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-18705c3ed092>\u001b[0m in \u001b[0;36mload_models\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'options.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mword2idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocab.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0midx2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './results/yelp_results/options.json'"
     ]
    }
   ],
   "source": [
    "#model_args, idx2word, autoencoder, gan_gen, gan_disc,gan_disc_local = load_models()\n",
    "model_args, idx2word, autoencoder, gan_gen, gan_disc = load_models()\n",
    "autoencoder = autoencoder.to(device)\n",
    "gan_gen = gan_gen.to(device)\n",
    "gan_disc = gan_disc.to(device)\n",
    "#gan_disc_local = gan_disc_local.to(device)\n",
    "print(\"Load Ckpt Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ccc129",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = Variable(torch.ones(eval_batch_size, z_size).normal_(0, 1).to(device))\n",
    "gen_fixed_noise_new(fixed_noise, 64, 20)\n",
    "print(\"\")\n",
    "print(\"Infer Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d879385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
