{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be767ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: word2vec in /Users/harrison.cook/miniconda3/lib/python3.9/site-packages (0.11.1)\r\n",
      "Requirement already satisfied: joblib in /Users/harrison.cook/miniconda3/lib/python3.9/site-packages (from word2vec) (1.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.9.2 in /Users/harrison.cook/miniconda3/lib/python3.9/site-packages (from word2vec) (1.22.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab433c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/harrison.cook/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'word2vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mreplay_memory\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mEvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Evaluator\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgenerator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Generator\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Accenture/Thesis/Python Code/gpu-files/DPAC-DialogueGAN-master/evaluation/Evaluator.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# from torchnlp.metrics import *\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mword2vec\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEvaluator\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_loader_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8000\u001b[39m, min_seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, max_seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'word2vec'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import discriminator\n",
    "import discriminator_LM2\n",
    "import critic\n",
    "\n",
    "from helpers import *\n",
    "from dataloader.dp_corpus import DPCorpus\n",
    "from dataloader.dp_data_loader import DPDataLoader\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import replay_memory\n",
    "import numpy as np\n",
    "from evaluation.Evaluator import Evaluator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from generator import Generator\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda:0')\n",
    "    print(\"RUNNIG ON CUDA\") #'\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')  #'cuda:0'\n",
    "    print(\"RUNNING ON CPU\")\n",
    "\n",
    "\n",
    "VOCAB_SIZE = 8000\n",
    "MIN_SEQ_LEN = 5\n",
    "MAX_SEQ_LEN = 20\n",
    "BATCH_SIZE = 64\n",
    "MLE_TRAIN_EPOCHS = 100\n",
    "ADV_TRAIN_EPOCHS = 50\n",
    "DIS_TRAIN_EPOCHS = 2\n",
    "\n",
    "GEN_EMBEDDING_DIM = 256\n",
    "GEN_HIDDEN_DIM = 256\n",
    "DIS_EMBEDDING_DIM = 128\n",
    "DIS_HIDDEN_DIM = 128\n",
    "\n",
    "CAPACITY_RM = 100000\n",
    "PRETRAIN_GENERATOR = False\n",
    "PRETRAIN_DISCRIMINATOR = False\n",
    "POLICY_GRADIENT = True\n",
    "ACTOR_CHECKPOINT = \"generator_checkpoint19.pth.tar\"\n",
    "DISCRIMINATOR_MLE_LR = 5e-2\n",
    "ACTOR_LR = 1e-2\n",
    "CRITIC_LR = 1e-2\n",
    "DISCRIMINATOR_LR = 1e-2\n",
    "AC = True\n",
    "SEQGAN = False\n",
    "if SEQGAN:\n",
    "    DISCRIMINATOR_CHECKPOINT = \"discriminator_final.pth.tar\"\n",
    "else:\n",
    "    DISCRIMINATOR_CHECKPOINT = None#\"discriminator_final_LM2.pth.tar\"\n",
    "\n",
    "AC_WARMUP = 1000\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "BATCH_SIZE_TESTING = 256\n",
    "NUM_SAMPLES = 3\n",
    "# Number of gen\n",
    "\n",
    "def train_generator_PG(context, reply, gen, gen_opt, dis, num_samples=0, TF=0):\n",
    "    \"\"\"\n",
    "    The generator is trained using policy gradients, using the reward from the discriminator.\n",
    "    Training is done for one batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Forward pass\n",
    "    fake_reply, word_probabilities, hiddens = gen.sample(context, reply, TF=TF)\n",
    "\n",
    "    if TF==1:\n",
    "        if SEQGAN:\n",
    "            rewards = torch.ones(BATCH_SIZE, MAX_SEQ_LEN-1).to(DEVICE)\n",
    "        else:\n",
    "            rewards = torch.ones(BATCH_SIZE, MAX_SEQ_LEN-1).to(DEVICE)\n",
    "\n",
    "    # Compute word-level rewards\n",
    "    elif SEQGAN:\n",
    "        rewards = gen.monte_carlo(dis, context, fake_reply, hiddens, num_samples, corpus).detach()\n",
    "    else:\n",
    "        # Compute word-level rewards\n",
    "        rewards, sentence_level_rewards = dis.get_rewards(fake_reply.long().to(DEVICE), PAD)\n",
    "\n",
    "    # Compute perplexity\n",
    "    entropy = torch.mean(word_probabilities.log(), dim=1)\n",
    "    perplexity = torch.mean(2**(-entropy)).item()\n",
    "\n",
    "    # Compute REINFORCE loss with the assumption that G = R_t\n",
    "    pg_loss = gen.compute_reinforce_loss(rewards.detach(), word_probabilities)\n",
    "\n",
    "    # Backward pass\n",
    "    gen_opt.zero_grad()\n",
    "    pg_loss.backward()\n",
    "    gen_opt.step()\n",
    "\n",
    "    # Print the generator and real reply for testing purposes\n",
    "    # print(\"Generated reply\")\n",
    "    # print(corpus.ids_to_tokens([int(i) for i in fake_reply[0]]))\n",
    "    # print(\"Real  reply\")\n",
    "    # print(corpus.ids_to_tokens([int(i) for i in reply[0]]))\n",
    "\n",
    "    return perplexity\n",
    "\n",
    "def train_generator_PGAC(context, reply, gen, dis, memory, critic, AC_optimizer, EOU,PAD):\n",
    "    \"\"\"\n",
    "    Actor Critic Pseudocode:\n",
    "    for word, t in enumerate(setence):\n",
    "        state = [word_0, ..., word_t]\n",
    "        action = gen.forward(word)\n",
    "        next_state = [word_0, ..., word_{t+1}]\n",
    "        reward = dis(word{t+1} | state)\n",
    "        store (s, a, r, s', done ) in replay memory\n",
    "        # Training\n",
    "        sample batch from replay memory\n",
    "        Update critic --> r + discount_facot * V(s') - V(s)   NOTE: target with no grad!\n",
    "        update actor --> torch.mean(V(s)) NOTE: not like policy gradient, but according to Deepmind DDPG\n",
    "        Question: Could also update discriminator in this loop?\n",
    "    \"\"\"\n",
    "    # Run input through encoder\n",
    "    encoder_output, hidden = gen.encoder(context)\n",
    "    hidden = gen.decoder._init_state(hidden)\n",
    "    input = torch.autograd.Variable(context.data[:, 0])  # sos\n",
    "    samples = torch.autograd.Variable(PAD*torch.ones(BATCH_SIZE,MAX_SEQ_LEN)).to(DEVICE)\n",
    "    samples[:,0] = input\n",
    "    active_ep_idx = torch.ones(BATCH_SIZE).to(DEVICE)\n",
    "    EOU = torch.tensor(EOU).repeat(BATCH_SIZE).to(DEVICE)\n",
    "    function = torch.nn.functional.log_softmax\n",
    "\n",
    "    # Pass through decoder and sample action (word) from resulting vocab distribution\n",
    "    for t in range(1, MAX_SEQ_LEN):\n",
    "        output, hidden, attn_weights = gen.decoder.forward_step(\n",
    "                input.unsqueeze(1), hidden, encoder_output, function)\n",
    "\n",
    "        # Sample action (token) for entire batch from predicted vocab distribution\n",
    "        # and set input for next forward pass\n",
    "        output = output.squeeze(1)\n",
    "        action = torch.multinomial(torch.exp(output), 1).view(-1).data\n",
    "        log_p = output.gather(1, action.unsqueeze(1)).view(-1).data\n",
    "        input = torch.autograd.Variable(action).to(DEVICE)\n",
    "\n",
    "        # Check which episodes (sampled sentences) have not encountered a EOU token\n",
    "        done = (action == EOU).float()\n",
    "        if active_ep_idx.nonzero().numel() > 1:\n",
    "            active_index = active_ep_idx.nonzero().squeeze(1)\n",
    "\n",
    "            # Only put states of active episodes in replay memory\n",
    "            old_state = samples.clone()\n",
    "            reward = dis.get_reward(samples[active_index,:t], action[active_index])\n",
    "            samples[:, t] = action\n",
    "            done_index = done.nonzero()\n",
    "            active_ep_idx[done_index] = 0\n",
    "\n",
    "        for j,i in enumerate(active_index):\n",
    "            memory.push((old_state[i,:], action[i], log_p[i], reward[j], samples[i,:], done[i]))\n",
    "\n",
    "        if memory.__len__() > AC_WARMUP:\n",
    "            # Retrieve batch from replay memory\n",
    "            info = tuple(zip(*memory.sample(BATCH_SIZE)))\n",
    "            state, action, log_p, reward, next_state, done = [torch.stack(i).to(DEVICE) for i in info]\n",
    "\n",
    "            # Estimate state-action values for each state in batch using critic\n",
    "            q_values = critic.forward(state.long())[torch.arange(BATCH_SIZE).to(DEVICE), action]\n",
    "            with torch.no_grad():\n",
    "                mask = (done==False).float()\n",
    "                q_values_target = mask.float()*(DISCOUNT_FACTOR * \\\n",
    "                    torch.max(critic.forward(next_state.long()), dim=1)[0].float()) \\\n",
    "                    + reward\n",
    "\n",
    "            # Compute combined actor critic loss and backprop\n",
    "            actor_loss = -torch.mean(q_values)\n",
    "            critic_loss = torch.nn.functional.smooth_l1_loss(q_values, q_values_target)\n",
    "            loss = actor_loss + critic_loss\n",
    "            AC_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            AC_optimizer.step()\n",
    "            return loss\n",
    "    return None\n",
    "\n",
    "\n",
    "def fill_with_padding(sentences, u_token, pad_token):\n",
    "    \"\"\"\n",
    "    Takes a batch of sentences with equal lengths as input.\n",
    "    Returns same size of batch but with padding filling after the first\n",
    "    end of utterence token.\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(sentences.size(0)):\n",
    "        sent = sentences[i]\n",
    "        idx = (sent == u_token).nonzero()\n",
    "        if len(idx) > 0:\n",
    "            idx = idx[0].item()\n",
    "            split = torch.split(sent, idx+1)[0].to(DEVICE)\n",
    "            padding = pad_token * torch.ones(sentences.size(1) - len(split))\n",
    "            padding = padding.to(DEVICE)\n",
    "            pad_sent = torch.cat((split, padding))\n",
    "            sentences[i][:] = pad_sent\n",
    "    return sentences\n",
    "\n",
    "def calc_mean(rewards):\n",
    "    batch_size, length = rewards.shape\n",
    "    total = 0\n",
    "    for i in range(batch_size):\n",
    "        reward = rewards[i]\n",
    "        idx = (reward == 0).nonzero()\n",
    "        if len(idx) > 0:\n",
    "            idx = idx[0].item()\n",
    "        else:\n",
    "            idx = length\n",
    "        total += torch.mean(reward[0:idx])\n",
    "    return total/batch_size\n",
    "\n",
    "def train_discriminator(context,real_reply,gen, dis, dis_opt):\n",
    "    \"\"\"\n",
    "    Training the discriminator on real_data_samples (positive) and generated samples from generator (negative).\n",
    "    Samples are drawn d_steps times, and the discriminator is trained for epochs epochs.\n",
    "    \"\"\"\n",
    "    if SEQGAN:\n",
    "        fake_labels = torch.from_numpy(np.random.uniform(0, 0.3, size=(BATCH_SIZE))).float().to(DEVICE)\n",
    "        real_labels = torch.from_numpy(np.random.uniform(0.7, 1.2, size=(BATCH_SIZE))).float().to(DEVICE)\n",
    "        loss = nn.BCELoss()\n",
    "\n",
    "        dis_opt.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fake_reply, _ , _= gen.sample(context, real_reply)\n",
    "        fake_reply = fill_with_padding(fake_reply, EOU, PAD).detach()\n",
    "\n",
    "        # Get probabilities/rewards for real/fake\n",
    "        real_r = dis.batchClassify(real_reply, context)\n",
    "        fake_r = dis.batchClassify(fake_reply.to(DEVICE), context)\n",
    "\n",
    "        # Learn with fake_r\n",
    "        dis_opt.zero_grad()\n",
    "        loss_fake = loss(fake_r, fake_labels)\n",
    "\n",
    "        loss_real = loss(real_r, real_labels)\n",
    "        loss_total = loss_real + loss_fake\n",
    "        loss_total.backward()\n",
    "\n",
    "        dis_opt.step()\n",
    "    else:\n",
    "        dis_opt.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fake_reply, _,_= gen.sample(context, real_reply)\n",
    "        fake_reply = fill_with_padding(fake_reply, EOU, PAD).detach()\n",
    "\n",
    "        _, sentence_level_rewards_real = dis.get_rewards(real_reply.to(DEVICE), PAD)\n",
    "        _, sentence_level_rewards_fake = dis.get_rewards(fake_reply.long().to(DEVICE).detach(), PAD)\n",
    "\n",
    "        loss_fake = torch.mean(sentence_level_rewards_fake)\n",
    "        loss_real = torch.mean(sentence_level_rewards_real)\n",
    "        total_loss =  -1 * (loss_real - loss_fake)\n",
    "        total_loss.backward()\n",
    "        dis_opt.step()\n",
    "\n",
    "def pre_train_discriminator(dis, dis_opt, gen, corpus, epochs):\n",
    "    \"\"\"\n",
    "    Training the discriminator on real_data_samples (positive) and generated samples from generator (negative).\n",
    "    Samples are drawn d_steps times, and the discriminator is trained for epochs epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    start_epoch = 0\n",
    "    loss_per_epoch = []\n",
    "    losses = []\n",
    "    real_list = []\n",
    "    fake_list = []\n",
    "    count = 0\n",
    "    print(\"Number of epochs\", epochs)\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print('epoch %d : ' % (epoch + 1))\n",
    "        total_loss = 0\n",
    "        loss = nn.BCELoss()\n",
    "        for (iter, (context, real_reply)) in enumerate(train_data_loader):\n",
    "\n",
    "            context = context.to(DEVICE)\n",
    "            real_reply = real_reply.to(DEVICE)\n",
    "\n",
    "            dis_opt.zero_grad()\n",
    "\n",
    "            # Sample setences\n",
    "            with torch.no_grad():\n",
    "                fake_reply, _, _ = gen.sample(context, real_reply)\n",
    "\n",
    "            # Add padding\n",
    "            fake_reply = fill_with_padding(fake_reply, EOU, PAD).detach()\n",
    "\n",
    "            if SEQGAN:\n",
    "\n",
    "                fake_labels = torch.from_numpy(np.random.uniform(0, 0.3, size=(BATCH_SIZE))).float().to(DEVICE)\n",
    "                real_labels = torch.from_numpy(np.random.uniform(0.7, 1.2, size=(BATCH_SIZE))).float().to(DEVICE)\n",
    "\n",
    "                # Get probabilities/rewards for real/fake\n",
    "                real_r = dis.batchClassify(real_reply, context)\n",
    "                fake_r = dis.batchClassify(fake_reply.to(DEVICE), context)\n",
    "\n",
    "                # Learn with fake_r\n",
    "\n",
    "                loss_fake = loss(fake_r, fake_labels)\n",
    "\n",
    "                loss_real = loss(real_r, real_labels)\n",
    "                loss_total = loss_real + loss_fake\n",
    "                loss_total.backward()\n",
    "                losses.append(loss_total.item())\n",
    "            else:\n",
    "                rewards_real, sentence_level_rewards_real = dis.get_rewards(real_reply.to(DEVICE), PAD)\n",
    "                rewards, sentence_level_rewards_fake = dis.get_rewards(fake_reply.long().to(DEVICE), PAD)\n",
    "\n",
    "                real_list.append(torch.mean(sentence_level_rewards_real).item())\n",
    "                fake_list.append(torch.mean(sentence_level_rewards_fake).item())\n",
    "\n",
    "                loss_fake = torch.mean(sentence_level_rewards_fake)\n",
    "                loss_real = torch.mean(sentence_level_rewards_real)\n",
    "\n",
    "                total_loss =  -1 * (loss_real - loss_fake)\n",
    "                total_loss.backward()\n",
    "\n",
    "            dis_opt.step()\n",
    "\n",
    "\n",
    "    # smooth results\n",
    "    real = []\n",
    "    fake = []\n",
    "    interval = 20\n",
    "    for i in range(len(real_list)):\n",
    "        if i % interval == 0:\n",
    "            real_mean = np.mean(real_list[i:i+interval])\n",
    "            fake_mean = np.mean(fake_list[i:i+interval])\n",
    "            print(\"real mean \", real_mean)\n",
    "            print(\"fake mean \", fake_mean)\n",
    "            real.append(real_mean)\n",
    "            fake.append(fake_mean)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot(real, label='real')\n",
    "    plt.plot(fake, label='fake')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.xlabel('Iterations x'+ str(interval))\n",
    "    plt.legend()\n",
    "    plt.savefig('rewards.png')\n",
    "\n",
    "    torch.save(dis.state_dict(), \"discriminator_final.pth.tar\")\n",
    "    plt.figure(2)\n",
    "    plt.plot(losses)\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"iterations x \"+ str(interval))\n",
    "    plt.savefig(\"loss_disc_pretrain.png\")\n",
    "\n",
    "def load_data(path='dataset.pickle'):\n",
    "    \"\"\"\n",
    "    Load data set\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(path):\n",
    "        # print(\"Saving the data set\")\n",
    "        corpus = DPCorpus(vocabulary_limit=VOCAB_SIZE)\n",
    "        train_dataset = corpus.get_train_dataset(min_reply_length=MIN_SEQ_LEN,\\\n",
    "            max_reply_length=MAX_SEQ_LEN)\n",
    "\n",
    "        with open(path, 'wb') as handle:\n",
    "            pickle.dump(train_dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        train_data_loader = DPDataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    else:\n",
    "        # print(\"Loading the data set\")\n",
    "        with open(path, 'rb') as handle:\n",
    "            train_dataset = pickle.load(handle)\n",
    "        train_data_loader = DPDataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "    return train_data_loader\n",
    "\n",
    "def save_models(actor, discriminator, epoch, PG_optimizer, dis_optimizer):\n",
    "    torch.save({\n",
    "                        'epoch': epoch+1,\n",
    "                        'actor': actor.state_dict(),\n",
    "                        'act_optimizer' : PG_optimizer.state_dict(),\n",
    "                        'dis_optimizer' : dis_optimizer.state_dict(),\n",
    "                        'discriminator': discriminator.state_dict()\n",
    "                    },'adversial_checkpoint{}.pth.tar'.format(epoch))\n",
    "    print(\"Models and Optimizers saved\")\n",
    "\n",
    "def perform_evaluation(evaluator, actor):\n",
    "    actor = actor.eval()\n",
    "    result = evaluator.evaluate_embeddings(actor)\n",
    "    print(\"Evaluation\")\n",
    "    print(\"Greedy Match: \", result['greedy_match'][0])\n",
    "    print(\"Extrema Score: \", result['extrema_score'][0])\n",
    "    print(\"Average (Cosine similarity): \", result['average'][0])\n",
    "    actor = actor.train()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    Main training loop. Pre-trains the generator and discriminator using MLE\n",
    "    and then uses PG to alternately train them.\n",
    "    '''\n",
    "    # Load data set\n",
    "    train_data_loader = load_data()\n",
    "    corpus = train_data_loader.dataset.corpus\n",
    "    SOS = train_data_loader.dataset.corpus.token_to_id(DPCorpus.SOS)\n",
    "    EOU = train_data_loader.dataset.corpus.token_to_id(DPCorpus.EOU)\n",
    "    PAD = train_data_loader.dataset.corpus.token_to_id(DPCorpus.PAD)\n",
    "\n",
    "    # Pretrain generator and discriminator\n",
    "    if PRETRAIN_GENERATOR:\n",
    "        print('Starting Generator MLE Training...')\n",
    "        gen = Generator(SOS,EOU,VOCAB_SIZE, GEN_HIDDEN_DIM, GEN_EMBEDDING_DIM, MAX_SEQ_LEN).to(DEVICE)\n",
    "        genMLE_optimizer = optim.Adam(gen.parameters(), lr = GEN_MLE_LR)\n",
    "        gen.train_generator_MLE(genMLE_optimizer, train_data_loader, MLE_TRAIN_EPOCHS)\n",
    "\n",
    "    if PRETRAIN_DISCRIMINATOR:\n",
    "        print('\\nStarting Discriminator MLE Training...')\n",
    "        # Initialize discriminator\n",
    "        if SEQGAN:\n",
    "            dis = discriminator.Discriminator(DIS_EMBEDDING_DIM,\\\n",
    "                DIS_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, device=DEVICE).to(DEVICE)\n",
    "        else:\n",
    "            # dis = discriminator_LM.Discriminator(DIS_EMBEDDING_DIM, DIS_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, device=DEVICE).to(DEVICE)\n",
    "            dis = discriminator_LM2.LM(DIS_EMBEDDING_DIM, VOCAB_SIZE, device=DEVICE).to(DEVICE)\n",
    "        dis_optimizer = optim.Adam(dis.parameters(),lr = DISCRIMINATOR_MLE_LR)\n",
    "\n",
    "        # Load pretrained generator\n",
    "        gen = Generator(SOS,EOU,VOCAB_SIZE, GEN_HIDDEN_DIM, GEN_EMBEDDING_DIM, MAX_SEQ_LEN).to(DEVICE)\n",
    "        saved_gen = torch.load(ACTOR_CHECKPOINT, map_location=DEVICE)\n",
    "        gen.load_state_dict(saved_gen['state_dict'])\n",
    "        pre_train_discriminator(dis, dis_optimizer, gen, corpus, DIS_TRAIN_EPOCHS)\n",
    "    if POLICY_GRADIENT:\n",
    "        ## ADVERSARIAL TRAINING\n",
    "        # Initialize actor and discriminator using pre-trained state-dict\n",
    "        actor = Generator(SOS,EOU, VOCAB_SIZE, GEN_HIDDEN_DIM, GEN_EMBEDDING_DIM,\\\n",
    "            MAX_SEQ_LEN).to(DEVICE)\n",
    "        actor.load_state_dict(torch.load(ACTOR_CHECKPOINT,map_location=DEVICE)['state_dict'])\n",
    "        if SEQGAN:\n",
    "            discriminator = discriminator.Discriminator(DIS_EMBEDDING_DIM,\\\n",
    "                DIS_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, device=DEVICE).to(DEVICE)\n",
    "        else:\n",
    "            discriminator = discriminator_LM2.LM(DIS_EMBEDDING_DIM, VOCAB_SIZE, device=DEVICE).to(DEVICE)\n",
    "\n",
    "        if DISCRIMINATOR_CHECKPOINT:\n",
    "            discriminator.load_state_dict(torch.load(DISCRIMINATOR_CHECKPOINT,map_location=DEVICE))\n",
    "\n",
    "        dis_optimizer = optim.Adagrad(discriminator.parameters(),lr=DISCRIMINATOR_LR)\n",
    "        evaluator = Evaluator(vocab_size=VOCAB_SIZE, min_seq_len=MIN_SEQ_LEN, max_seq_len=MAX_SEQ_LEN, batch_size=BATCH_SIZE_TESTING, device=DEVICE)\n",
    "\n",
    "        # Define critic and dual optimizer\n",
    "        if AC:\n",
    "            critic = critic.Critic(DIS_EMBEDDING_DIM, DIS_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, device=DEVICE).to(DEVICE)\n",
    "            AC_optimizer = optim.Adagrad([\n",
    "                {'params': actor.parameters(), 'lr': ACTOR_LR},\n",
    "                {'params': critic.parameters(), 'lr': CRITIC_LR}\n",
    "            ])\n",
    "            memory = replay_memory.ReplayMemory(CAPACITY_RM)\n",
    "        # Use optimizer for baseline DP-GAN\n",
    "        else:\n",
    "            PG_optimizer = optim.Adagrad(actor.parameters(),ACTOR_LR)\n",
    "\n",
    "        # Adversarial training loop\n",
    "        gen_data_loader = iter(load_data())\n",
    "        gen_data_loader_tf = iter(load_data())\n",
    "        dis_data_loader = iter(load_data())\n",
    "        num_batches = int(len(gen_data_loader)/2)\n",
    "        N = ADV_TRAIN_EPOCHS * num_batches\n",
    "        M = 1\n",
    "        K = 5\n",
    "        for n in range(N):\n",
    "            if n % num_batches == 0:\n",
    "                print('Iteration {}'.format(n))\n",
    "                perform_evaluation(evaluator, actor)\n",
    "\n",
    "            if n % num_batches == 0 and n > 0:\n",
    "                if AC:\n",
    "                    save_models(actor, discriminator, n, AC_optimizer, dis_optimizer)\n",
    "                else:\n",
    "                    save_models(actor, discriminator, n, PG_optimizer, dis_optimizer)\n",
    "\n",
    "            # TRAIN GENERATOR (ACTOR)\n",
    "            for m in range(M):\n",
    "                try:\n",
    "                    context,reply = gen_data_loader.next()\n",
    "                except StopIteration:\n",
    "                    gen_data_loader = iter(load_data())\n",
    "                # AC step\n",
    "                if AC:\n",
    "                    perplexity = train_generator_PGAC(context.to(DEVICE), reply.to(DEVICE),\\\n",
    "                        actor, discriminator, memory, critic, AC_optimizer,EOU,PAD)\n",
    "\n",
    "                    # Teacher forcing\n",
    "                    try:\n",
    "                        context, reply = gen_data_loader_tf.next()\n",
    "                    except:\n",
    "                        gen_data_loader_tf = iter(load_data())\n",
    "                    perplexity = train_generator_PG(context.to(DEVICE), reply.to(DEVICE), \\\n",
    "                        actor, AC_optimizer, discriminator, num_samples=NUM_SAMPLES,TF=1)\n",
    "\n",
    "                # PG step\n",
    "                else:\n",
    "                    perplexity = train_generator_PG(context.to(DEVICE), reply.to(DEVICE),\\\n",
    "                        actor, PG_optimizer,discriminator,num_samples=NUM_SAMPLES)\n",
    "\n",
    "                    # Teacher forcing\n",
    "                    try:\n",
    "                        context, reply = gen_data_loader_tf.next()\n",
    "                    except:\n",
    "                        gen_data_loader_tf = iter(load_data())\n",
    "                    perplexity = train_generator_PG(context.to(DEVICE), reply.to(DEVICE), \\\n",
    "                        actor, PG_optimizer, discriminator, num_samples=NUM_SAMPLES,TF=1)\n",
    "            # TRAIN DISCRIMINATOR\n",
    "            for k in range(K):\n",
    "                try:\n",
    "                    context, reply = dis_data_loader.next()\n",
    "                except StopIteration:\n",
    "                    dis_data_loader = iter(load_data())\n",
    "                train_discriminator(context.to(DEVICE),reply.to(DEVICE), actor, discriminator, dis_optimizer)\n",
    "\n",
    "    print(\"DO NOT FORGET TO SAVE YOUR DATA IF YOU ARE RUNNING IN COLLAB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938e1e74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
